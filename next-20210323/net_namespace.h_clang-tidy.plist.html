<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/net/core/dev.c", "content": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *      NET3    Protocol independent device support routines.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/skbuff.h>\n#include <linux/kthread.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dsa.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/gro.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_ingress.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n#include <linux/net_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/devlink.h>\n#include <linux/pm_runtime.h>\n#include <linux/prandom.h>\n\n#include \"net-sysfs.h\"\n\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic DEFINE_SPINLOCK(offload_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\nstatic struct list_head offload_base __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct netdev_notifier_info *info);\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic DEFINE_MUTEX(ifalias_mutex);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic DECLARE_RWSEM(devnet_rename_sem);\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,\n\t\t\t\t\t\t       const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = kmalloc(sizeof(*name_node), GFP_KERNEL);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_HLIST_NODE(&name_node->hlist);\n\tname_node->dev = dev;\n\tname_node->name = name;\n\treturn name_node;\n}\n\nstatic struct netdev_name_node *\nnetdev_name_node_head_alloc(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = netdev_name_node_alloc(dev, dev->name);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&name_node->list);\n\treturn name_node;\n}\n\nstatic void netdev_name_node_free(struct netdev_name_node *name_node)\n{\n\tkfree(name_node);\n}\n\nstatic void netdev_name_node_add(struct net *net,\n\t\t\t\t struct netdev_name_node *name_node)\n{\n\thlist_add_head_rcu(&name_node->hlist,\n\t\t\t   dev_name_hash(net, name_node->name));\n}\n\nstatic void netdev_name_node_del(struct netdev_name_node *name_node)\n{\n\thlist_del_rcu(&name_node->hlist);\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup(struct net *net,\n\t\t\t\t\t\t\tconst char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,\n\t\t\t\t\t\t\t    const char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry_rcu(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (name_node)\n\t\treturn -EEXIST;\n\tname_node = netdev_name_node_alloc(dev, name);\n\tif (!name_node)\n\t\treturn -ENOMEM;\n\tnetdev_name_node_add(net, name_node);\n\t/* The node that holds dev->name acts as a head of per-device list. */\n\tlist_add_tail(&name_node->list, &dev->name_node->list);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_name_node_alt_create);\n\nstatic void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)\n{\n\tlist_del(&name_node->list);\n\tnetdev_name_node_del(name_node);\n\tkfree(name_node->name);\n\tnetdev_name_node_free(name_node);\n}\n\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (!name_node)\n\t\treturn -ENOENT;\n\t/* lookup might have found our primary name or a name belonging\n\t * to another device.\n\t */\n\tif (name_node == dev->name_node || name_node->dev != dev)\n\t\treturn -EINVAL;\n\n\t__netdev_name_node_alt_destroy(name_node);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_name_node_alt_destroy);\n\nstatic void netdev_name_node_alt_flush(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\n\tlist_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list)\n\t\t__netdev_name_node_alt_destroy(name_node);\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\tnetdev_name_node_add(net, dev->name_node);\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\tnetdev_name_node_del(dev->name_node);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/**\n *\tdev_add_offload - register offload handlers\n *\t@po: protocol offload declaration\n *\n *\tAdd protocol offload handlers to the networking stack. The passed\n *\t&proto_offload is linked into kernel lists and may not be freed until\n *\tit has been removed from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new offload handlers (until the next received packet).\n */\nvoid dev_add_offload(struct packet_offload *po)\n{\n\tstruct packet_offload *elem;\n\n\tspin_lock(&offload_lock);\n\tlist_for_each_entry(elem, &offload_base, list) {\n\t\tif (po->priority < elem->priority)\n\t\t\tbreak;\n\t}\n\tlist_add_rcu(&po->list, elem->list.prev);\n\tspin_unlock(&offload_lock);\n}\nEXPORT_SYMBOL(dev_add_offload);\n\n/**\n *\t__dev_remove_offload\t - remove offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a protocol offload handler that was previously added to the\n *\tkernel offload handlers by dev_add_offload(). The passed &offload_type\n *\tis removed from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nstatic void __dev_remove_offload(struct packet_offload *po)\n{\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *po1;\n\n\tspin_lock(&offload_lock);\n\n\tlist_for_each_entry(po1, head, list) {\n\t\tif (po == po1) {\n\t\t\tlist_del_rcu(&po->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_offload: %p not found\\n\", po);\nout:\n\tspin_unlock(&offload_lock);\n}\n\n/**\n *\tdev_remove_offload\t - remove packet offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a packet offload handler that was previously added to the kernel\n *\toffload handlers by dev_add_offload(). The passed &offload_type is\n *\tremoved from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_offload(struct packet_offload *po)\n{\n\t__dev_remove_offload(po);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_offload);\n\n/******************************************************************************\n *\n *\t\t      Device Boot-time Settings Routines\n *\n ******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n * netdev_boot_setup_check\t- check boot time settings\n * @dev: the netdevice\n *\n * Check boot time settings for the device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq = s[i].map.irq;\n\t\t\tdev->base_addr = s[i].map.base_addr;\n\t\t\tdev->mem_start = s[i].map.mem_start;\n\t\t\tdev->mem_end = s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n * netdev_boot_base\t- get address from boot time settings\n * @prefix: prefix for network device\n * @unit: id for network device\n *\n * Check boot time settings for the base address of device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup_rcu(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tint ret;\n\n\tdown_read(&devnet_rename_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tstrcpy(name, dev->name);\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\tup_read(&devnet_rename_sem);\n\treturn ret;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tallow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strnlen(name, IFNAMSIZ) == IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tp = strchr(name, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tstruct netdev_name_node *name_node;\n\t\t\tlist_for_each_entry(name_node, &d->name_node->list, list) {\n\t\t\t\tif (!sscanf(name_node->name, name, &i))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\t\tif (!strncmp(buf, name_node->name, IFNAMSIZ))\n\t\t\t\t\tset_bit(i, inuse);\n\t\t\t}\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tBUG_ON(!net);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\treturn dev_alloc_name_ns(dev_net(dev), dev, name);\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\n\t/* Some auto-enslaved devices e.g. failover slaves are\n\t * special, as userspace might rename the device after\n\t * the interface had been brought up and running since\n\t * the point kernel initiated auto-enslavement. Allow\n\t * live name change even when these slave devices are\n\t * up and running.\n\t *\n\t * Typically, users of these auto-enslaving devices\n\t * don't actually care about slave name change, as\n\t * they are supposed to operate on master interface\n\t * directly.\n\t */\n\tif (dev->flags & IFF_UP &&\n\t    likely(!(dev->priv_flags & IFF_LIVE_RENAME_OK)))\n\t\treturn -EBUSY;\n\n\tdown_write(&devnet_rename_sem);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\tup_write(&devnet_rename_sem);\n\t\treturn ret;\n\t}\n\n\tup_write(&devnet_rename_sem);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock_bh(&dev_base_lock);\n\tnetdev_name_node_del(dev->name_node);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\tnetdev_name_node_add(net, dev->name_node);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tdown_write(&devnet_rename_sem);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tpr_err(\"%s: name change rollback failed: %d\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tstruct dev_ifalias *new_alias = NULL;\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (len) {\n\t\tnew_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);\n\t\tif (!new_alias)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(new_alias->ifalias, alias, len);\n\t\tnew_alias->ifalias[len] = 0;\n\t}\n\n\tmutex_lock(&ifalias_mutex);\n\tnew_alias = rcu_replace_pointer(dev->ifalias, new_alias,\n\t\t\t\t\tmutex_is_locked(&ifalias_mutex));\n\tmutex_unlock(&ifalias_mutex);\n\n\tif (new_alias)\n\t\tkfree_rcu(new_alias, rcuhead);\n\n\treturn len;\n}\nEXPORT_SYMBOL(dev_set_alias);\n\n/**\n *\tdev_get_alias - get ifalias of a device\n *\t@dev: device\n *\t@name: buffer to store name of ifalias\n *\t@len: size of buffer\n *\n *\tget ifalias for a device.  Caller must make sure dev cannot go\n *\taway,  e.g. rcu read lock or own a reference count to device.\n */\nint dev_get_alias(const struct net_device *dev, char *name, size_t len)\n{\n\tconst struct dev_ifalias *alias;\n\tint ret = 0;\n\n\trcu_read_lock();\n\talias = rcu_dereference(dev->ifalias);\n\tif (alias)\n\t\tret = snprintf(name, len, \"%s\", alias->ifalias);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info.dev = dev,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * __netdev_notify_peers - notify network peers about existence of @dev,\n * to be called when rtnl lock is already held.\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid __netdev_notify_peers(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n}\nEXPORT_SYMBOL(__netdev_notify_peers);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\t__netdev_notify_peers(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int napi_threaded_poll(void *data);\n\nstatic int napi_kthread_create(struct napi_struct *n)\n{\n\tint err = 0;\n\n\t/* Create and wake up the kthread once to put it in\n\t * TASK_INTERRUPTIBLE mode to avoid the blocked task\n\t * warning and work with loadavg.\n\t */\n\tn->thread = kthread_run(napi_threaded_poll, n, \"napi/%s-%d\",\n\t\t\t\tn->dev->name, n->napi_id);\n\tif (IS_ERR(n->thread)) {\n\t\terr = PTR_ERR(n->thread);\n\t\tpr_err(\"kthread_run failed with err %d\\n\", err);\n\t\tn->thread = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\tif (!netif_device_present(dev)) {\n\t\t/* may be detached because parent is runtime-suspended */\n\t\tif (dev->dev.parent)\n\t\t\tpm_runtime_resume(dev->dev.parent);\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t}\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev: device to open\n *\t@extack: netlink extended ack\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n/**\n *\tdev_disable_gro_hw - disable HW Generic Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable HW Generic Receive Offload (GRO_HW) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if Generic XDP is installed on\n *\tthe device.\n */\nstatic void dev_disable_gro_hw(struct net_device *dev)\n{\n\tdev->wanted_features &= ~NETIF_F_GRO_HW;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_GRO_HW))\n\t\tnetdev_WARN(dev, \"failed to disable GRO_HW!\\n\");\n}\n\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd)\n{\n#define N(val) \t\t\t\t\t\t\\\n\tcase NETDEV_##val:\t\t\t\t\\\n\t\treturn \"NETDEV_\" __stringify(val);\n\tswitch (cmd) {\n\tN(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)\n\tN(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)\n\tN(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)\n\tN(POST_INIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN) N(CHANGEUPPER)\n\tN(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA) N(BONDING_INFO)\n\tN(PRECHANGEUPPER) N(CHANGELOWERSTATE) N(UDP_TUNNEL_PUSH_INFO)\n\tN(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)\n\tN(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)\n\tN(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)\n\tN(PRE_CHANGEADDR)\n\t}\n#undef N\n\treturn \"UNKNOWN_NETDEV_EVENT\";\n}\nEXPORT_SYMBOL_GPL(netdev_cmd_to_name);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t};\n\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int call_netdevice_register_notifiers(struct notifier_block *nb,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tint err;\n\n\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\treturn 0;\n}\n\nstatic void call_netdevice_unregister_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\tstruct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\tdev);\n\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t}\n\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n}\n\nstatic int call_netdevice_register_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t struct net *net)\n{\n\tstruct net_device *dev;\n\tint err;\n\n\tfor_each_netdev(net, dev) {\n\t\terr = call_netdevice_register_notifiers(nb, dev);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\treturn 0;\n\nrollback:\n\tfor_each_netdev_continue_reverse(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n\treturn err;\n}\n\nstatic void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t    struct net *net)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\terr = call_netdevice_register_net_notifiers(nb, net);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n\nrollback:\n\tfor_each_net_continue_reverse(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\nstatic int __register_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t     struct notifier_block *nb,\n\t\t\t\t\t     bool ignore_call_fail)\n{\n\tint err;\n\n\terr = raw_notifier_chain_register(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\tif (dev_boot_phase)\n\t\treturn 0;\n\n\terr = call_netdevice_register_net_notifiers(nb, net);\n\tif (err && !ignore_call_fail)\n\t\tgoto chain_unregister;\n\n\treturn 0;\n\nchain_unregister:\n\traw_notifier_chain_unregister(&net->netdev_chain, nb);\n\treturn err;\n}\n\nstatic int __unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t       struct notifier_block *nb)\n{\n\tint err;\n\n\terr = raw_notifier_chain_unregister(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\n\tcall_netdevice_unregister_net_notifiers(nb, net);\n\treturn 0;\n}\n\n/**\n * register_netdevice_notifier_net - register a per-netns network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(net, nb, false);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_net);\n\n/**\n * unregister_netdevice_notifier_net - unregister a per-netns\n *                                     network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __unregister_netdevice_notifier_net(net, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_net);\n\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(dev_net(dev), nb, false);\n\tif (!err) {\n\t\tnn->nb = nb;\n\t\tlist_add(&nn->list, &dev->net_notifier_list);\n\t}\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_dev_net);\n\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\tlist_del(&nn->list);\n\terr = __unregister_netdevice_notifier_net(dev_net(dev), nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);\n\nstatic void move_netdevice_notifiers_dev_net(struct net_device *dev,\n\t\t\t\t\t     struct net *net)\n{\n\tstruct netdev_net_notifier *nn;\n\n\tlist_for_each_entry(nn, &dev->net_notifier_list, list) {\n\t\t__unregister_netdevice_notifier_net(dev_net(dev), nn->nb);\n\t\t__register_netdevice_notifier_net(net, nn->nb, true);\n\t}\n}\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/* Run per-netns notifier block chain first, then run the global one.\n\t * Hopefully, one day, the global one is going to be removed after\n\t * all notifier block registrators get converted to be per-netns.\n\t */\n\tret = raw_notifier_call_chain(&net->netdev_chain, val, info);\n\tif (ret & NOTIFY_STOP_MASK)\n\t\treturn ret;\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t\t.extack = extack,\n\t};\n\n\treturn call_netdevice_notifiers_info(val, &info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\treturn call_netdevice_notifiers_extack(val, dev, NULL);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n/**\n *\tcall_netdevice_notifiers_mtu - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@arg: additional u32 argument passed to the notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\nstatic int call_netdevice_notifiers_mtu(unsigned long val,\n\t\t\t\t\tstruct net_device *dev, u32 arg)\n{\n\tstruct netdev_notifier_info_ext info = {\n\t\t.info.dev = dev,\n\t\t.ext.mtu = arg,\n\t};\n\n\tBUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);\n\n\treturn call_netdevice_notifiers_info(val, &info.info);\n}\n\n#ifdef CONFIG_NET_INGRESS\nstatic DEFINE_STATIC_KEY_FALSE(ingress_needed_key);\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_branch_inc(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_branch_dec(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic DEFINE_STATIC_KEY_FALSE(egress_needed_key);\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_branch_inc(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_branch_dec(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nstatic DEFINE_STATIC_KEY_FALSE(netstamp_needed_key);\n#ifdef CONFIG_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_branch_enable(&netstamp_needed_key);\n\telse\n\t\tstatic_branch_disable(&netstamp_needed_key);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 0)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted + 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_inc(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 1)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted - 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_dec(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\t__net_timestamp(skb);\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\t\\\n\tif (static_branch_unlikely(&netstamp_needed_key)) {\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\t\t\\\n\t\t\t__net_timestamp(SKB);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\treturn __is_skb_forwardable(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nstatic int __dev_forward_skb2(struct net_device *dev, struct sk_buff *skb,\n\t\t\t      bool check_mtu)\n{\n\tint ret = ____dev_forward_skb(dev, skb, check_mtu);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, false) ?: netif_rx_internal(skb);\n}\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * dev_nit_active - return true if any network interface taps are in use\n *\n * @dev: network device to check for the presence of taps\n */\nbool dev_nit_active(struct net_device *dev)\n{\n\treturn !list_empty(&ptype_all) || !list_empty(&dev->ptype_all);\n}\nEXPORT_SYMBOL_GPL(dev_nit_active);\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->ignore_outgoing)\n\t\t\tcontinue;\n\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tpr_warn(\"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tpr_warn(\"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\ti, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\t/* walk through the TCs and see if it falls into any of them */\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\t/* didn't find it, just return -1 to indicate no match */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_txq_to_tc);\n\n#ifdef CONFIG_XPS\nstatic struct static_key xps_needed __read_mostly;\nstatic struct static_key xps_rxqs_needed __read_mostly;\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     struct xps_dev_maps *old_maps, int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tif (old_maps)\n\t\t\tRCU_INIT_POINTER(old_maps->attr_map[tci], NULL);\n\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev_maps->num_tc;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, NULL, tci, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void reset_xps_maps(struct net_device *dev,\n\t\t\t   struct xps_dev_maps *dev_maps,\n\t\t\t   enum xps_map_type type)\n{\n\tstatic_key_slow_dec_cpuslocked(&xps_needed);\n\tif (type == XPS_RXQS)\n\t\tstatic_key_slow_dec_cpuslocked(&xps_rxqs_needed);\n\n\tRCU_INIT_POINTER(dev->xps_maps[type], NULL);\n\n\tkfree_rcu(dev_maps, rcu);\n}\n\nstatic void clean_xps_maps(struct net_device *dev, enum xps_map_type type,\n\t\t\t   u16 offset, u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tbool active = false;\n\tint i, j;\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (!dev_maps)\n\t\treturn;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, j, offset, count);\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\n\tif (type == XPS_CPUS) {\n\t\tfor (i = offset + (count - 1); count--; i--)\n\t\t\tnetdev_queue_numa_node_write(\n\t\t\t\tnetdev_get_tx_queue(dev, i), NUMA_NO_NODE);\n\t}\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tif (!static_key_false(&xps_needed))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&xps_map_mutex);\n\n\tif (static_key_false(&xps_rxqs_needed))\n\t\tclean_xps_maps(dev, XPS_RXQS, offset, count);\n\n\tclean_xps_maps(dev, XPS_CPUS, offset, count);\n\n\tmutex_unlock(&xps_map_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,\n\t\t\t\t      u16 index, bool is_rxqs_map)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add tx-queue to this CPU's/rx-queue's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store tx-queue on this CPU's/rx-queue's\n\t *  map\n\t */\n\tif (is_rxqs_map)\n\t\tnew_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);\n\telse\n\t\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t\t       cpu_to_node(attr_index));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\n/* Copy xps maps at a given index */\nstatic void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,\n\t\t\t      struct xps_dev_maps *new_dev_maps, int index,\n\t\t\t      int tc, bool skip_tc)\n{\n\tint i, tci = index * dev_maps->num_tc;\n\tstruct xps_map *map;\n\n\t/* copy maps belonging to foreign traffic classes */\n\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\tif (i == tc && skip_tc)\n\t\t\tcontinue;\n\n\t\t/* fill in the new device map from the old device map */\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n}\n\n/* Must be called under cpus_read_lock */\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL, *old_dev_maps = NULL;\n\tconst unsigned long *online_mask = NULL;\n\tbool active = false, copy = false;\n\tint i, j, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tunsigned int nr_ids;\n\n\tif (dev->num_tc) {\n\t\t/* Do not allow XPS on subordinate device directly */\n\t\tnum_tc = dev->num_tc;\n\t\tif (num_tc < 0)\n\t\t\treturn -EINVAL;\n\n\t\t/* If queue belongs to subordinate dev use its map */\n\t\tdev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;\n\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (type == XPS_RXQS) {\n\t\tmaps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);\n\t\tnr_ids = dev->num_rx_queues;\n\t} else {\n\t\tmaps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);\n\t\tif (num_possible_cpus() > 1)\n\t\t\tonline_mask = cpumask_bits(cpu_online_mask);\n\t\tnr_ids = nr_cpu_ids;\n\t}\n\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\t/* The old dev_maps could be larger or smaller than the one we're\n\t * setting up now, as dev->num_tc or nr_ids could have been updated in\n\t * between. We could try to be smart, but let's be safe instead and only\n\t * copy foreign traffic classes if the two map sizes match.\n\t */\n\tif (dev_maps &&\n\t    dev_maps->num_tc == num_tc && dev_maps->nr_ids == nr_ids)\n\t\tcopy = true;\n\n\t/* allocate memory for queue storage */\n\tfor (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tif (!new_dev_maps) {\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\t\tif (!new_dev_maps) {\n\t\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tnew_dev_maps->nr_ids = nr_ids;\n\t\t\tnew_dev_maps->num_tc = num_tc;\n\t\t}\n\n\t\ttci = j * num_tc + tc;\n\t\tmap = copy ? xmap_dereference(dev_maps->attr_map[tci]) : NULL;\n\n\t\tmap = expand_xps_map(map, j, index, type == XPS_RXQS);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tif (!dev_maps) {\n\t\t/* Increment static keys at most once per type */\n\t\tstatic_key_slow_inc_cpuslocked(&xps_needed);\n\t\tif (type == XPS_RXQS)\n\t\t\tstatic_key_slow_inc_cpuslocked(&xps_rxqs_needed);\n\t}\n\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tbool skip_tc = false;\n\n\t\ttci = j * num_tc + tc;\n\t\tif (netif_attr_test_mask(j, mask, nr_ids) &&\n\t\t    netif_attr_test_online(j, online_mask, nr_ids)) {\n\t\t\t/* add tx-queue to CPU/rx-queue maps */\n\t\t\tint pos = 0;\n\n\t\t\tskip_tc = true;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (type == XPS_CPUS) {\n\t\t\t\tif (numa_node_id == -2)\n\t\t\t\t\tnuma_node_id = cpu_to_node(j);\n\t\t\t\telse if (numa_node_id != cpu_to_node(j))\n\t\t\t\t\tnuma_node_id = -1;\n\t\t\t}\n#endif\n\t\t}\n\n\t\tif (copy)\n\t\t\txps_copy_dev_maps(dev_maps, new_dev_maps, j, tc,\n\t\t\t\t\t  skip_tc);\n\t}\n\n\trcu_assign_pointer(dev->xps_maps[type], new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * dev_maps->num_tc; i--; tci++) {\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tif (!map)\n\t\t\t\tcontinue;\n\n\t\t\tif (copy) {\n\t\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\t\tif (map == new_map)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\told_dev_maps = dev_maps;\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\tif (type == XPS_CPUS)\n\t\t/* update Tx queue numa node */\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t\t     (numa_node_id >= 0) ?\n\t\t\t\t\t     numa_node_id : NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes tx-queue from unused CPUs/rx-queues */\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\ttci = j * dev_maps->num_tc;\n\n\t\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\t\tif (i == tc &&\n\t\t\t    netif_attr_test_mask(j, mask, dev_maps->nr_ids) &&\n\t\t\t    netif_attr_test_online(j, online_mask, dev_maps->nr_ids))\n\t\t\t\tcontinue;\n\n\t\t\tactive |= remove_xps_queue(dev_maps,\n\t\t\t\t\t\t   copy ? old_dev_maps : NULL,\n\t\t\t\t\t\t   tci, index);\n\t\t}\n\t}\n\n\tif (old_dev_maps)\n\t\tkfree_rcu(old_dev_maps, rcu);\n\n\t/* free map if not active */\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = copy ?\n\t\t\t      xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(__netif_set_xps_queue);\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nstatic void netdev_unbind_all_sb_channels(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n\t/* Unbind any subordinate channels */\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev)\n\t\t\tnetdev_unbind_sb_channel(dev, txq->sb_dev);\n\t}\n}\n\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\t/* Reset TC configuration of device */\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(sb_dev, 0);\n#endif\n\tmemset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));\n\tmemset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));\n\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev == sb_dev)\n\t\t\ttxq->sb_dev = NULL;\n\t}\n}\nEXPORT_SYMBOL(netdev_unbind_sb_channel);\n\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset)\n{\n\t/* Make certain the sb_dev and dev are already configured */\n\tif (sb_dev->num_tc >= 0 || tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\t/* We cannot hand out queues we don't have */\n\tif ((offset + count) > dev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\t/* Record the mapping */\n\tsb_dev->tc_to_txq[tc].count = count;\n\tsb_dev->tc_to_txq[tc].offset = offset;\n\n\t/* Provide a way for Tx queue to find the tc_to_txq map or\n\t * XPS map for itself.\n\t */\n\twhile (count--)\n\t\tnetdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_bind_sb_channel_queue);\n\nint netdev_set_sb_channel(struct net_device *dev, u16 channel)\n{\n\t/* Do not use a multiqueue device to represent a subordinate channel */\n\tif (netif_is_multiqueue(dev))\n\t\treturn -ENODEV;\n\n\t/* We allow channels 1 - 32767 to be used for subordinate channels.\n\t * Channel 0 is meant to be \"native\" mode and used only to represent\n\t * the main root device. We allow writing 0 to reset the device back\n\t * to normal mode after being used as a subordinate channel.\n\t */\n\tif (channel > S16_MAX)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = -channel;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_sb_channel);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater than real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tbool disabling;\n\tint rc;\n\n\tdisabling = txq < dev->real_num_tx_queues;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tdev->real_num_tx_queues = txq;\n\n\t\tif (disabling) {\n\t\t\tsynchronize_net();\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t} else {\n\t\tdev->real_num_tx_queues = txq;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * This routine should set an upper limit on the number of RSS queues\n * used by default by multiqueue devices.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\treturn is_kdump_kernel() ?\n\t\t1 : min_t(int, DEFAULT_MAX_NUM_RSS_QUEUES, num_online_cpus());\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!netif_xmit_stopped(txq)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_irq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nstatic u16 skb_tx_hash(const struct net_device *dev,\n\t\t       const struct net_device *sb_dev,\n\t\t       struct sk_buff *skb)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = dev->real_num_tx_queues;\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = sb_dev->tc_to_txq[tc].offset;\n\t\tqcount = sb_dev->tc_to_txq[tc].count;\n\t}\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\tif (hash >= qoffset)\n\t\t\thash -= qoffset;\n\t\twhile (unlikely(hash >= qcount))\n\t\t\thash -= qcount;\n\t\treturn hash + qoffset;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tskb_dump(KERN_WARNING, skb, false);\n\tWARN(1, \"%s: caps=(%pNF, %pNF)\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_is_gso(skb))) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__sum16));\n\tif (ret)\n\t\tgoto out;\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__le32));\n\tif (ret)\n\t\tgoto out;\n\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_not_inet = 0;\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb->data;\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn __vlan_get_protocol(skb, type, depth);\n}\n\n/**\n *\tskb_mac_gso_segment - mac layer segmentation handler.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n */\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_offload *ptype;\n\tint vlan_depth = skb->mac_len;\n\t__be16 type = skb_network_protocol(skb, &vlan_depth);\n\n\tif (unlikely(!type))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__skb_pull(skb, vlan_depth);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &offload_base, list) {\n\t\tif (ptype->type == type && ptype->callbacks.gso_segment) {\n\t\t\tsegs = ptype->callbacks.gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_mac_gso_segment);\n\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL &&\n\t\t       skb->ip_summed != CHECKSUM_UNNECESSARY;\n\n\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_GSO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tstruct sk_buff *segs;\n\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\t/* We're going to init ->check field in TCP or UDP header */\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\t/* Only report GSO partial support if it will enable us to\n\t * support segmentation on this frame without needing additional\n\t * work.\n\t */\n\tif (features & NETIF_F_GSO_PARTIAL) {\n\t\tnetdev_features_t partial_features = NETIF_F_GSO_ROBUST;\n\t\tstruct net_device *dev = skb->dev;\n\n\t\tpartial_features |= dev->features & dev->gso_partial_features;\n\t\tif (!skb_gso_ok(skb, features | partial_features))\n\t\t\tfeatures &= ~NETIF_F_GSO_PARTIAL;\n\t}\n\n\tBUILD_BUG_ON(SKB_GSO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tsegs = skb_mac_gso_segment(skb, features);\n\n\tif (segs != skb && unlikely(skb_needs_check(skb, tx_path) && !IS_ERR(segs)))\n\t\tskb_warn_bad_offload(skb);\n\n\treturn segs;\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tif (net_ratelimit()) {\n\t\tpr_err(\"%s: hw csum failure\\n\", dev ? dev->name : \"<unknown>\");\n\t\tskb_dump(KERN_ERR, skb, true);\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* XXX: check that highmem exists at all on the given machine. */\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, NULL);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > dev->gso_max_segs)\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (!skb_shinfo(skb)->gso_type) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\t}\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (dev_nit_active(dev))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\tPRANDOM_ADD_NOISE(skb, dev, txq, len + jiffies);\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb_mark_not_on_list(skb);\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_tx_queue_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb_csum_is_sctp(skb)))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\tif (features & NETIF_F_HW_CSUM)\n\t\treturn 0;\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tswitch (skb->csum_offset) {\n\t\tcase offsetof(struct tcphdr, check):\n\t\tcase offsetof(struct udphdr, check):\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tskb = sk_validate_xmit_skb(skb, dev);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\tskb = validate_xmit_xfrm(skb, features, again);\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tatomic_long_inc(&dev->tx_dropped);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev, again);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size && skb_transport_header_was_set(skb)) {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\t\tconst struct tcphdr *th;\n\t\t\tstruct tcphdr _tcphdr;\n\n\t\t\tth = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\t\tif (likely(th))\n\t\t\t\thdr_len += __tcp_hdrlen(th);\n\t\t} else {\n\t\t\tstruct udphdr _udphdr;\n\n\t\t\tif (skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\t\thdr_len += sizeof(struct udphdr);\n\t\t}\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\trc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;\n\t\tqdisc_run(q);\n\n\t\tif (unlikely(to_free))\n\t\t\tkfree_skb_list(to_free);\n\t\treturn rc;\n\t}\n\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t */\n\tcontended = qdisc_is_running(q);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\n\t\tqdisc_run_end(q);\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t\tqdisc_run_end(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list(to_free);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tconst struct netprio_map *map;\n\tconst struct sock *sk;\n\tunsigned int prioidx;\n\n\tif (skb->priority)\n\t\treturn;\n\tmap = rcu_dereference_bh(skb->dev->priomap);\n\tif (!map)\n\t\treturn;\n\tsk = skb_to_full_sk(skb);\n\tif (!sk)\n\t\treturn;\n\n\tprioidx = sock_cgroup_prioidx(&sk->sk_cgrp_data);\n\n\tif (prioidx < map->priomap_len)\n\t\tskb->priority = map->priomap[prioidx];\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx_ni(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(dev->miniq_egress);\n\tstruct tcf_result cl_res;\n\n\tif (!miniq)\n\t\treturn skb;\n\n\t/* qdisc_skb_cb(skb)->pkt_len was already set by the caller. */\n\tqdisc_skb_cb(skb)->mru = 0;\n\tqdisc_skb_cb(skb)->post_ct = false;\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\n\tswitch (tcf_classify(skb, miniq->filter_list, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\t*ret = NET_XMIT_DROP;\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn skb;\n}\n#endif /* CONFIG_NET_EGRESS */\n\n#ifdef CONFIG_XPS\nstatic int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct xps_dev_maps *dev_maps, unsigned int tci)\n{\n\tint tc = netdev_get_prio_tc_map(dev, skb->priority);\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\tif (tc >= dev_maps->num_tc || tci >= dev_maps->nr_ids)\n\t\treturn queue_index;\n\n\ttci *= dev_maps->num_tc;\n\ttci += tc;\n\n\tmap = rcu_dereference(dev_maps->attr_map[tci]);\n\tif (map) {\n\t\tif (map->len == 1)\n\t\t\tqueue_index = map->queues[0];\n\t\telse\n\t\t\tqueue_index = map->queues[reciprocal_scale(\n\t\t\t\t\t\tskb_get_hash(skb), map->len)];\n\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\tqueue_index = -1;\n\t}\n\treturn queue_index;\n}\n#endif\n\nstatic int get_xps_queue(struct net_device *dev, struct net_device *sb_dev,\n\t\t\t struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct sock *sk = skb->sk;\n\tint queue_index = -1;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn -1;\n\n\trcu_read_lock();\n\tif (!static_key_false(&xps_rxqs_needed))\n\t\tgoto get_cpus_map;\n\n\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_RXQS]);\n\tif (dev_maps) {\n\t\tint tci = sk_rx_queue_get(sk);\n\n\t\tif (tci >= 0)\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t}\n\nget_cpus_map:\n\tif (queue_index < 0) {\n\t\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_CPUS]);\n\t\tif (dev_maps) {\n\t\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_pick_tx_zero);\n\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev)\n{\n\treturn (u16)raw_smp_processor_id() % dev->real_num_tx_queues;\n}\nEXPORT_SYMBOL(dev_pick_tx_cpu_id);\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tsb_dev = sb_dev ? : dev;\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, sb_dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, sb_dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\nEXPORT_SYMBOL(netdev_pick_tx);\n\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, sb_dev);\n\t\telse\n\t\t\tqueue_index = netdev_pick_tx(dev, skb, sb_dev);\n\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n *\t__dev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\t@sb_dev: suboordinate device used for L2 forwarding offload\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nstatic int __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\tbool again = false;\n\n\tskb_reset_mac_header(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_at_ingress = 0;\n# ifdef CONFIG_NET_EGRESS\n\tif (static_branch_unlikely(&egress_needed_key)) {\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n# endif\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\ttxq = netdev_core_pick_tx(dev, skb, sb_dev);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\t\t\tif (dev_xmit_recursion())\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev, &again);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tPRANDOM_ADD_NOISE(skb, dev, txq, jiffies);\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\tdev_xmit_recursion_inc();\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\tdev_xmit_recursion_dec();\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\n\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\nint dev_queue_xmit_accel(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\treturn __dev_queue_xmit(skb, sb_dev);\n}\nEXPORT_SYMBOL(dev_queue_xmit_accel);\n\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev, &again);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tskb_set_queue_mapping(skb, queue_id);\n\ttxq = skb_get_tx_queue(dev, skb);\n\tPRANDOM_ADD_NOISE(skb, dev, txq, jiffies);\n\n\tlocal_bh_disable();\n\n\tdev_xmit_recursion_inc();\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\tdev_xmit_recursion_dec();\n\n\tlocal_bh_enable();\n\treturn ret;\ndrop:\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\nEXPORT_SYMBOL(__dev_direct_xmit);\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\n/* Must be at least 2 jiffes to guarantee 1 jiffy timeout */\nunsigned int __read_mostly netdev_budget_usecs = 2 * USEC_PER_SEC / HZ;\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n/* Maximum number of GRO_NORMAL skbs to batch up for list-RX */\nint gro_normal_batch __read_mostly = 8;\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tstruct task_struct *thread;\n\n\tif (test_bit(NAPI_STATE_THREADED, &napi->state)) {\n\t\t/* Paired with smp_mb__before_atomic() in\n\t\t * napi_enable()/dev_set_threaded().\n\t\t * Use READ_ONCE() to guarantee a complete\n\t\t * read on napi->thread. Only call\n\t\t * wake_up_process() when it's not NULL.\n\t\t */\n\t\tthread = READ_ONCE(napi->thread);\n\t\tif (thread) {\n\t\t\t/* Avoid doing set_bit() if the thread is in\n\t\t\t * INTERRUPTIBLE state, cause napi_thread_wait()\n\t\t\t * makes sure to proceed with napi polling\n\t\t\t * if the thread is explicitly woken from here.\n\t\t\t */\n\t\t\tif (READ_ONCE(thread->state) != TASK_INTERRUPTIBLE)\n\t\t\t\tset_bit(NAPI_STATE_SCHED_THREADED, &napi->state);\n\t\t\twake_up_process(thread);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key_false rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key_false rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match */\n\t\tident = sock_flow_table->ents[hash & sock_flow_table->mask];\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = READ_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (netdev_max_backlog >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\ndrop:\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\nstatic struct netdev_rx_queue *netif_get_rxqueue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_rx_queue *rxqueue;\n\n\trxqueue = dev->_rx;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\n\t\t\treturn rxqueue; /* Return first rxqueue */\n\t\t}\n\t\trxqueue += index;\n\t}\n\treturn rxqueue;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tvoid *orig_data, *orig_data_end, *hard_start;\n\tstruct netdev_rx_queue *rxqueue;\n\tu32 metalen, act = XDP_DROP;\n\tu32 mac_len, frame_sz;\n\t__be16 orig_eth_type;\n\tstruct ethhdr *eth;\n\tbool orig_bcast;\n\tint off;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_is_redirected(skb))\n\t\treturn XDP_PASS;\n\n\t/* XDP packets must be linear and must have sufficient headroom\n\t * of XDP_PACKET_HEADROOM bytes. This is the guarantee that also\n\t * native XDP provides, thus we need to do it here as well.\n\t */\n\tif (skb_cloned(skb) || skb_is_nonlinear(skb) ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tint hroom = XDP_PACKET_HEADROOM - skb_headroom(skb);\n\t\tint troom = skb->tail + skb->data_len - skb->end;\n\n\t\t/* In case we have to go down the path and also linearize,\n\t\t * then lets do the pskb_expand_head() work just once here.\n\t\t */\n\t\tif (pskb_expand_head(skb,\n\t\t\t\t     hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,\n\t\t\t\t     troom > 0 ? troom + 128 : 0, GFP_ATOMIC))\n\t\t\tgoto do_drop;\n\t\tif (skb_linearize(skb))\n\t\t\tgoto do_drop;\n\t}\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thard_start = skb->data - skb_headroom(skb);\n\n\t/* SKB \"head\" area always have tailroom for skb_shared_info */\n\tframe_sz = (void *)skb_end_pointer(skb) - hard_start;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\trxqueue = netif_get_rxqueue(skb);\n\txdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,\n\t\t\t skb_headlen(skb) + mac_len, true);\n\n\torig_data_end = xdp->data_end;\n\torig_data = xdp->data;\n\teth = (struct ethhdr *)xdp->data;\n\torig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);\n\torig_eth_type = eth->h_proto;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t/* check if bpf_xdp_adjust_head was used */\n\toff = xdp->data - orig_data;\n\tif (off) {\n\t\tif (off > 0)\n\t\t\t__skb_pull(skb, off);\n\t\telse if (off < 0)\n\t\t\t__skb_push(skb, -off);\n\n\t\tskb->mac_header += off;\n\t\tskb_reset_network_header(skb);\n\t}\n\n\t/* check if bpf_xdp_adjust_tail was used */\n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0) {\n\t\tskb_set_tail_pointer(skb, xdp->data_end - xdp->data);\n\t\tskb->len += off; /* positive on grow, negative on shrink */\n\t}\n\n\t/* check if XDP changed eth hdr such SKB needs update */\n\teth = (struct ethhdr *)xdp->data;\n\tif ((orig_eth_type != eth->h_proto) ||\n\t    (orig_bcast != is_multicast_ether_addr_64bits(eth->h_dest))) {\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t}\n\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\tbreak;\n\tcase XDP_PASS:\n\t\tmetalen = xdp->data - xdp->data_meta;\n\t\tif (metalen)\n\t\t\tskb_metadata_set(skb, metalen);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_core_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(generic_xdp_needed_key);\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tstruct xdp_buff xdp;\n\t\tu32 act;\n\t\tint err;\n\n\t\tact = netif_receive_generic_xdp(skb, &xdp, xdp_prog);\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      &xdp, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t\tbreak;\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb(skb);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_rx(skb);\n\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n\treturn ret;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_rx_entry(skb);\n\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_netif_rx_ni_entry(skb);\n\n\tpreempt_disable();\n\terr = netif_rx_internal(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\ttrace_netif_rx_ni_exit(err);\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nint netif_rx_any_context(struct sk_buff *skb)\n{\n\t/*\n\t * If invoked from contexts which do not invoke bottom half\n\t * processing either at return from interrupt or when softrqs are\n\t * reenabled, use netif_rx_ni() which invokes bottomhalf processing\n\t * directly.\n\t */\n\tif (in_interrupt())\n\t\treturn netif_rx(skb);\n\telse\n\t\treturn netif_rx_ni(skb);\n}\nEXPORT_SYMBOL(netif_rx_any_context);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock = NULL;\n\n\t\t\thead = head->next_sched;\n\n\t\t\tif (!(q->flags & TCQ_F_NOLOCK)) {\n\t\t\t\troot_lock = qdisc_lock(q);\n\t\t\t\tspin_lock(root_lock);\n\t\t\t}\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tif (root_lock)\n\t\t\t\tspin_unlock(root_lock);\n\t\t}\n\t}\n\n\txfrm_dev_backlog(sd);\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(skb->dev->miniq_ingress);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!miniq)\n\t\treturn skb;\n\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\tqdisc_skb_cb(skb)->mru = 0;\n\tqdisc_skb_cb(skb)->post_ct = false;\n\tskb->tc_at_ingress = 1;\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\n\tswitch (tcf_classify_ingress(skb, miniq->block, miniq->filter_list,\n\t\t\t\t     &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tif (skb_do_redirect(skb) == -EAGAIN) {\n\t\t\t__skb_pull(skb, skb->mac_len);\n\t\t\t*another = true;\n\t\t\tbreak;\n\t\t}\n\t\treturn NULL;\n\tcase TC_ACT_CONSUMED:\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\tif (dev->priv_flags & IFF_NO_RX_HANDLER)\n\t\treturn -EINVAL;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,\n\t\t\t\t    struct packet_type **ppt_prev)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (static_branch_unlikely(&generic_xdp_needed_key)) {\n\t\tint ret2;\n\n\t\tpreempt_disable();\n\t\tret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\tpreempt_enable();\n\n\t\tif (ret2 != XDP_PASS) {\n\t\t\tret = NET_RX_DROP;\n\t\t\tgoto out;\n\t\t}\n\t\tskb_reset_mac_len(skb);\n\t}\n\n\tif (eth_type_vlan(skb->protocol)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_branch_unlikely(&ingress_needed_key)) {\n\t\tbool another = false;\n\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev,\n\t\t\t\t\t &another);\n\t\tif (another)\n\t\t\tgoto another_round;\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_redirect(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\t\tbreak;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {\ncheck_vlan_id:\n\t\tif (skb_vlan_tag_get_id(skb)) {\n\t\t\t/* Vlan id is non 0 and vlan_do_receive() above couldn't\n\t\t\t * find vlan device.\n\t\t\t */\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t} else if (eth_type_vlan(skb->protocol)) {\n\t\t\t/* Outer header is 802.1P with vlan 0, inner header is\n\t\t\t * 802.1Q or 802.1AD and vlan_do_receive() above could\n\t\t\t * not find vlan dev for vlan id 0.\n\t\t\t */\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb = skb_vlan_untag(skb);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\tif (vlan_do_receive(&skb))\n\t\t\t\t/* After stripping off 802.1P header with vlan 0\n\t\t\t\t * vlan dev is found for inner header.\n\t\t\t\t */\n\t\t\t\tgoto another_round;\n\t\t\telse if (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\telse\n\t\t\t\t/* We have stripped outer 802.1P vlan 0 header.\n\t\t\t\t * But could not find vlan dev.\n\t\t\t\t * check again for vlan id to set OTHERHOST.\n\t\t\t\t */\n\t\t\t\tgoto check_vlan_id;\n\t\t}\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\t*ppt_prev = pt_prev;\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\telse\n\t\t\tatomic_long_inc(&skb->dev->rx_nohandler);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\t/* The invariant here is that if *ppt_prev is not NULL\n\t * then skb should also be non-NULL.\n\t *\n\t * Apparently *ppt_prev assignment above holds this invariant due to\n\t * skb dereferencing near it.\n\t */\n\t*pskb = skb;\n\treturn ret;\n}\n\nstatic int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct net_device *orig_dev = skb->dev;\n\tstruct packet_type *pt_prev = NULL;\n\tint ret;\n\n\tret = __netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\tif (pt_prev)\n\t\tret = INDIRECT_CALL_INET(pt_prev->func, ipv6_rcv, ip_rcv, skb,\n\t\t\t\t\t skb->dev, pt_prev, orig_dev);\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb_core - special purpose version of netif_receive_skb\n *\t@skb: buffer to process\n *\n *\tMore direct receive version of netif_receive_skb().  It should\n *\tonly be used by callers that have a need to skip RPS and Generic XDP.\n *\tCaller must also take care of handling if ``(page_is_)pfmemalloc``.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb_core(struct sk_buff *skb)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __netif_receive_skb_one_core(skb, false);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb_core);\n\nstatic inline void __netif_receive_skb_list_ptype(struct list_head *head,\n\t\t\t\t\t\t  struct packet_type *pt_prev,\n\t\t\t\t\t\t  struct net_device *orig_dev)\n{\n\tstruct sk_buff *skb, *next;\n\n\tif (!pt_prev)\n\t\treturn;\n\tif (list_empty(head))\n\t\treturn;\n\tif (pt_prev->list_func != NULL)\n\t\tINDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,\n\t\t\t\t   ip_list_rcv, head, pt_prev, orig_dev);\n\telse\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tskb_list_del_init(skb);\n\t\t\tpt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t\t}\n}\n\nstatic void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)\n{\n\t/* Fast-path assumptions:\n\t * - There is no RX handler.\n\t * - Only one packet_type matches.\n\t * If either of these fails, we will end up doing some per-packet\n\t * processing in-line, then handling the 'last ptype' for the whole\n\t * sublist.  This can't cause out-of-order delivery to any single ptype,\n\t * because the 'last ptype' must be constant across the sublist, and all\n\t * other ptypes are handled per-packet.\n\t */\n\t/* Current (common) ptype of sublist */\n\tstruct packet_type *pt_curr = NULL;\n\t/* Current (common) orig_dev of sublist */\n\tstruct net_device *od_curr = NULL;\n\tstruct list_head sublist;\n\tstruct sk_buff *skb, *next;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tstruct net_device *orig_dev = skb->dev;\n\t\tstruct packet_type *pt_prev = NULL;\n\n\t\tskb_list_del_init(skb);\n\t\t__netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\t\tif (!pt_prev)\n\t\t\tcontinue;\n\t\tif (pt_curr != pt_prev || od_curr != orig_dev) {\n\t\t\t/* dispatch old sublist */\n\t\t\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n\t\t\t/* start new sublist */\n\t\t\tINIT_LIST_HEAD(&sublist);\n\t\t\tpt_curr = pt_prev;\n\t\t\tod_curr = orig_dev;\n\t\t}\n\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\n\t/* dispatch final sublist */\n\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_one_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_one_core(skb, false);\n\n\treturn ret;\n}\n\nstatic void __netif_receive_skb_list(struct list_head *head)\n{\n\tunsigned long noreclaim_flag = 0;\n\tstruct sk_buff *skb, *next;\n\tbool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tif ((sk_memalloc_socks() && skb_pfmemalloc(skb)) != pfmemalloc) {\n\t\t\tstruct list_head sublist;\n\n\t\t\t/* Handle the previous sublist */\n\t\t\tlist_cut_before(&sublist, head, &skb->list);\n\t\t\tif (!list_empty(&sublist))\n\t\t\t\t__netif_receive_skb_list_core(&sublist, pfmemalloc);\n\t\t\tpfmemalloc = !pfmemalloc;\n\t\t\t/* See comments in __netif_receive_skb */\n\t\t\tif (pfmemalloc)\n\t\t\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\t\telse\n\t\t\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t\t}\n\t}\n\t/* Handle the remaining sublist */\n\tif (!list_empty(head))\n\t\t__netif_receive_skb_list_core(head, pfmemalloc);\n\t/* Restore pflags */\n\tif (pfmemalloc)\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tif (new) {\n\t\tu32 i;\n\n\t\tmutex_lock(&new->aux->used_maps_mutex);\n\n\t\t/* generic XDP does not work with DEVMAPs that can\n\t\t * have a bpf_prog installed on an entry\n\t\t */\n\t\tfor (i = 0; i < new->aux->used_map_cnt; i++) {\n\t\t\tif (dev_map_can_have_prog(new->aux->used_maps[i]) ||\n\t\t\t    cpu_map_prog_allowed(new->aux->used_maps[i])) {\n\t\t\t\tmutex_unlock(&new->aux->used_maps_mutex);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tmutex_unlock(&new->aux->used_maps_mutex);\n\t}\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_branch_dec(&generic_xdp_needed_key);\n\t\t} else if (new && !old) {\n\t\t\tstatic_branch_inc(&generic_xdp_needed_key);\n\t\t\tdev_disable_lro(dev);\n\t\t\tdev_disable_gro_hw(dev);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic void netif_receive_skb_list_internal(struct list_head *head)\n{\n\tstruct sk_buff *skb, *next;\n\tstruct list_head sublist;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\t\tskb_list_del_init(skb);\n\t\tif (!skb_defer_rx_timestamp(skb))\n\t\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\tlist_splice_init(&sublist, head);\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\t\tif (cpu >= 0) {\n\t\t\t\t/* Will be handled, remove from list */\n\t\t\t\tskb_list_del_init(skb);\n\t\t\t\tenqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\t__netif_receive_skb_list(head);\n\trcu_read_unlock();\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_receive_skb_entry(skb);\n\n\tret = netif_receive_skb_internal(skb);\n\ttrace_netif_receive_skb_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/**\n *\tnetif_receive_skb_list - process many receive buffers from network\n *\t@head: list of skbs to process.\n *\n *\tSince return value of netif_receive_skb() is normally ignored, and\n *\twouldn't be meaningful for a list, this function returns void.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n */\nvoid netif_receive_skb_list(struct list_head *head)\n{\n\tstruct sk_buff *skb;\n\n\tif (list_empty(head))\n\t\treturn;\n\tif (trace_netif_receive_skb_list_entry_enabled()) {\n\t\tlist_for_each_entry(skb, head, list)\n\t\t\ttrace_netif_receive_skb_list_entry(skb);\n\t}\n\tnetif_receive_skb_list_internal(head);\n\ttrace_netif_receive_skb_list_exit(0);\n}\nEXPORT_SYMBOL(netif_receive_skb_list);\n\nstatic DEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\tlocal_irq_disable();\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\tlocal_irq_enable();\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic bool flush_required(int cpu)\n{\n#if IS_ENABLED(CONFIG_RPS)\n\tstruct softnet_data *sd = &per_cpu(softnet_data, cpu);\n\tbool do_flush;\n\n\tlocal_irq_disable();\n\trps_lock(sd);\n\n\t/* as insertion into process_queue happens with the rps lock held,\n\t * process_queue access may race only with dequeue\n\t */\n\tdo_flush = !skb_queue_empty(&sd->input_pkt_queue) ||\n\t\t   !skb_queue_empty_lockless(&sd->process_queue);\n\trps_unlock(sd);\n\tlocal_irq_enable();\n\n\treturn do_flush;\n#endif\n\t/* without RPS we can't safely check input_pkt_queue: during a\n\t * concurrent remote skb_queue_splice() we can detect as empty both\n\t * input_pkt_queue and process_queue even if the latter could end-up\n\t * containing a lot of packets.\n\t */\n\treturn true;\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tstatic cpumask_t flush_cpus;\n\tunsigned int cpu;\n\n\t/* since we are under rtnl lock protection we can use static data\n\t * for the cpumask and avoid allocating on stack the possibly\n\t * large mask\n\t */\n\tASSERT_RTNL();\n\n\tget_online_cpus();\n\n\tcpumask_clear(&flush_cpus);\n\tfor_each_online_cpu(cpu) {\n\t\tif (flush_required(cpu)) {\n\t\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\t\t\tcpumask_set_cpu(cpu, &flush_cpus);\n\t\t}\n\t}\n\n\t/* we can have in flight packet[s] on the cpus we are not flushing,\n\t * synchronize_net() in unregister_netdevice_many() will take care of\n\t * them\n\t */\n\tfor_each_cpu(cpu, &flush_cpus)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tput_online_cpus();\n}\n\n/* Pass the currently batched GRO_NORMAL SKBs up to the stack. */\nstatic void gro_normal_list(struct napi_struct *napi)\n{\n\tif (!napi->rx_count)\n\t\treturn;\n\tnetif_receive_skb_list_internal(&napi->rx_list);\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n}\n\n/* Queue one GRO_NORMAL SKB up for list processing. If batch size exceeded,\n * pass the whole batch up to the stack.\n */\nstatic void gro_normal_one(struct napi_struct *napi, struct sk_buff *skb, int segs)\n{\n\tlist_add_tail(&skb->list, &napi->rx_list);\n\tnapi->rx_count += segs;\n\tif (napi->rx_count >= gro_normal_batch)\n\t\tgro_normal_list(napi);\n}\n\nstatic int napi_gro_complete(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint err = -ENOENT;\n\n\tBUILD_BUG_ON(sizeof(struct napi_gro_cb) > sizeof(skb->cb));\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\n\t\terr = INDIRECT_CALL_INET(ptype->callbacks.gro_complete,\n\t\t\t\t\t ipv6_gro_complete, inet_gro_complete,\n\t\t\t\t\t skb, 0);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\tgro_normal_one(napi, skb, NAPI_GRO_CB(skb)->count);\n\treturn NET_RX_SUCCESS;\n}\n\nstatic void __napi_gro_flush_chain(struct napi_struct *napi, u32 index,\n\t\t\t\t   bool flush_old)\n{\n\tstruct list_head *head = &napi->gro_hash[index].list;\n\tstruct sk_buff *skb, *p;\n\n\tlist_for_each_entry_safe_reverse(skb, p, head, list) {\n\t\tif (flush_old && NAPI_GRO_CB(skb)->age == jiffies)\n\t\t\treturn;\n\t\tskb_list_del_init(skb);\n\t\tnapi_gro_complete(napi, skb);\n\t\tnapi->gro_hash[index].count--;\n\t}\n\n\tif (!napi->gro_hash[index].count)\n\t\t__clear_bit(index, &napi->gro_bitmask);\n}\n\n/* napi->gro_hash[].list contains packets ordered by age.\n * youngest packets at the head of it.\n * Complete skbs in reverse order to reduce latencies.\n */\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old)\n{\n\tunsigned long bitmask = napi->gro_bitmask;\n\tunsigned int i, base = ~0U;\n\n\twhile ((i = ffs(bitmask)) != 0) {\n\t\tbitmask >>= i;\n\t\tbase += i;\n\t\t__napi_gro_flush_chain(napi, base, flush_old);\n\t}\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nstatic void gro_list_prepare(const struct list_head *head,\n\t\t\t     const struct sk_buff *skb)\n{\n\tunsigned int maclen = skb->dev->hard_header_len;\n\tu32 hash = skb_get_hash_raw(skb);\n\tstruct sk_buff *p;\n\n\tlist_for_each_entry(p, head, list) {\n\t\tunsigned long diffs;\n\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\n\t\tif (hash != skb_get_hash_raw(p)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= skb_vlan_tag_present(p) ^ skb_vlan_tag_present(skb);\n\t\tif (skb_vlan_tag_present(p))\n\t\t\tdiffs |= skb_vlan_tag_get(p) ^ skb_vlan_tag_get(skb);\n\t\tdiffs |= skb_metadata_dst_cmp(p, skb);\n\t\tdiffs |= skb_metadata_differs(p, skb);\n\t\tif (maclen == ETH_HLEN)\n\t\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t\t      skb_mac_header(skb));\n\t\telse if (!diffs)\n\t\t\tdiffs = memcmp(skb_mac_header(p),\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       maclen);\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t}\n}\n\nstatic void skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *pinfo = skb_shinfo(skb);\n\tconst skb_frag_t *frag0 = &pinfo->frags[0];\n\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (!skb_headlen(skb) && pinfo->nr_frags &&\n\t    !PageHighMem(skb_frag_page(frag0))) {\n\t\tNAPI_GRO_CB(skb)->frag0 = skb_frag_address(frag0);\n\t\tNAPI_GRO_CB(skb)->frag0_len = min_t(unsigned int,\n\t\t\t\t\t\t    skb_frag_size(frag0),\n\t\t\t\t\t\t    skb->end - skb->tail);\n\t}\n}\n\nstatic void gro_pull_from_frag0(struct sk_buff *skb, int grow)\n{\n\tstruct skb_shared_info *pinfo = skb_shinfo(skb);\n\n\tBUG_ON(skb->end - skb->tail < grow);\n\n\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\tskb->data_len -= grow;\n\tskb->tail += grow;\n\n\tskb_frag_off_add(&pinfo->frags[0], grow);\n\tskb_frag_size_sub(&pinfo->frags[0], grow);\n\n\tif (unlikely(!skb_frag_size(&pinfo->frags[0]))) {\n\t\tskb_frag_unref(skb, 0);\n\t\tmemmove(pinfo->frags, pinfo->frags + 1,\n\t\t\t--pinfo->nr_frags * sizeof(pinfo->frags[0]));\n\t}\n}\n\nstatic void gro_flush_oldest(struct napi_struct *napi, struct list_head *head)\n{\n\tstruct sk_buff *oldest;\n\n\toldest = list_last_entry(head, struct sk_buff, list);\n\n\t/* We are called with head length >= MAX_GRO_SKBS, so this is\n\t * impossible.\n\t */\n\tif (WARN_ON_ONCE(!oldest))\n\t\treturn;\n\n\t/* Do not adjust napi->gro_hash[].count, caller is adding a new\n\t * SKB to the chain.\n\t */\n\tskb_list_del_init(oldest);\n\tnapi_gro_complete(napi, oldest);\n}\n\nstatic enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tu32 bucket = skb_get_hash_raw(skb) & (GRO_HASH_BUCKETS - 1);\n\tstruct gro_list *gro_list = &napi->gro_hash[bucket];\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct sk_buff *pp = NULL;\n\tenum gro_result ret;\n\tint same_flow;\n\tint grow;\n\n\tif (netif_elide_gro(skb->dev))\n\t\tgoto normal;\n\n\tgro_list_prepare(&gro_list->list, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = skb_is_gso(skb) || skb_has_frag_list(skb);\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n\t\tNAPI_GRO_CB(skb)->recursion_counter = 0;\n\t\tNAPI_GRO_CB(skb)->is_fou = 0;\n\t\tNAPI_GRO_CB(skb)->is_atomic = 1;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = INDIRECT_CALL_INET(ptype->callbacks.gro_receive,\n\t\t\t\t\tipv6_gro_receive, inet_gro_receive,\n\t\t\t\t\t&gro_list->list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tif (PTR_ERR(pp) == -EINPROGRESS) {\n\t\tret = GRO_CONSUMED;\n\t\tgoto ok;\n\t}\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tskb_list_del_init(pp);\n\t\tnapi_gro_complete(napi, pp);\n\t\tgro_list->count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(gro_list->count >= MAX_GRO_SKBS))\n\t\tgro_flush_oldest(napi, &gro_list->list);\n\telse\n\t\tgro_list->count++;\n\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tlist_add(&skb->list, &gro_list->list);\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\tif (gro_list->count) {\n\t\tif (!test_bit(bucket, &napi->gro_bitmask))\n\t\t\t__set_bit(bucket, &napi->gro_bitmask);\n\t} else if (test_bit(bucket, &napi->gro_bitmask)) {\n\t\t__clear_bit(bucket, &napi->gro_bitmask);\n\t}\n\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\n\nstruct packet_offload *gro_find_receive_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_receive_by_type);\n\nstruct packet_offload *gro_find_complete_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_complete_by_type);\n\nstatic gro_result_t napi_skb_finish(struct napi_struct *napi,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tgro_normal_one(napi, skb, 1);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\t__kfree_skb_defer(skb);\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tgro_result_t ret;\n\n\tskb_mark_napi_id(skb, napi);\n\ttrace_napi_gro_receive_entry(skb);\n\n\tskb_gro_reset_offset(skb);\n\n\tret = napi_skb_finish(napi, skb, dev_gro_receive(napi, skb));\n\ttrace_napi_gro_receive_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tif (unlikely(skb->pfmemalloc)) {\n\t\tconsume_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, skb_headlen(skb));\n\t/* restore the reserve we had after netdev_alloc_skb_ip_align() */\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN - skb_headroom(skb));\n\t__vlan_hwaccel_clear_tag(skb);\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\n\t/* eth_type_trans() assumes pkt_type is PACKET_HOST */\n\tskb->pkt_type = PACKET_HOST;\n\n\tskb->encapsulation = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\tskb_ext_reset(skb);\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = napi_alloc_skb(napi, GRO_MAX_HEAD);\n\t\tif (skb) {\n\t\t\tnapi->skb = skb;\n\t\t\tskb_mark_napi_id(skb, napi);\n\t\t}\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\nstatic gro_result_t napi_frags_finish(struct napi_struct *napi,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tif (ret == GRO_NORMAL)\n\t\t\tgro_normal_one(napi, skb, 1);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/* Upper GRO stack assumes network header starts at gro_offset=0\n * Drivers could call both napi_gro_frags() and napi_gro_receive()\n * We copy ethernet header into skb->data to have a common layout.\n */\nstatic struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnet_warn_ratelimited(\"%s: dropping impossible skb from %s\\n\",\n\t\t\t\t\t     __func__, napi->dev->name);\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\teth = (const struct ethhdr *)skb->data;\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tgro_result_t ret;\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\ttrace_napi_gro_frags_entry(skb);\n\n\tret = napi_frags_finish(napi, skb, dev_gro_receive(napi, skb));\n\ttrace_napi_gro_frags_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/* Compute the checksum from gro_offset and return the folded value\n * after adding in any pseudo checksum.\n */\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum wsum;\n\t__sum16 sum;\n\n\twsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);\n\n\t/* NAPI_GRO_CB(skb)->csum holds pseudo checksum */\n\tsum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));\n\t/* See comments in __skb_checksum_complete(). */\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev, skb);\n\t}\n\n\tNAPI_GRO_CB(skb)->csum = wsum;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_gro_checksum_complete);\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = dev_rx_weight;\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\trps_lock(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock(sd);\n\t\tlocal_irq_enable();\n\t}\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable to\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new, timeout = 0;\n\tbool ret = true;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (work_done) {\n\t\tif (n->gro_bitmask)\n\t\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tn->defer_hard_irqs_count = READ_ONCE(n->dev->napi_defer_hard_irqs);\n\t}\n\tif (n->defer_hard_irqs_count > 0) {\n\t\tn->defer_hard_irqs_count--;\n\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tif (timeout)\n\t\t\tret = false;\n\t}\n\tif (n->gro_bitmask) {\n\t\t/* When the NAPI instance uses a timeout and keeps postponing\n\t\t * it, we need to bound somehow the time packets are kept in\n\t\t * the GRO layer\n\t\t */\n\t\tnapi_gro_flush(n, !!timeout);\n\t}\n\n\tgro_normal_list(n);\n\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |\n\t\t\t      NAPIF_STATE_SCHED_THREADED |\n\t\t\t      NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\tif (timeout)\n\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\nstatic void __busy_poll_stop(struct napi_struct *napi, bool skip_schedule)\n{\n\tif (!skip_schedule) {\n\t\tgro_normal_list(napi);\n\t\t__napi_schedule(napi);\n\t\treturn;\n\t}\n\n\tif (napi->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(napi, HZ >= 1000);\n\t}\n\n\tgro_normal_list(napi);\n\tclear_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock, bool prefer_busy_poll,\n\t\t\t   u16 budget)\n{\n\tbool skip_schedule = false;\n\tunsigned long timeout;\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\tif (prefer_busy_poll) {\n\t\tnapi->defer_hard_irqs_count = READ_ONCE(napi->dev->napi_defer_hard_irqs);\n\t\ttimeout = READ_ONCE(napi->dev->gro_flush_timeout);\n\t\tif (napi->defer_hard_irqs_count && timeout) {\n\t\t\thrtimer_start(&napi->timer, ns_to_ktime(timeout), HRTIMER_MODE_REL_PINNED);\n\t\t\tskip_schedule = true;\n\t\t}\n\t}\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, budget);\n\t/* We can't gro_normal_list() here, because napi->poll() might have\n\t * rearmed the napi (napi_complete_done()) in which case it could\n\t * already be running on another CPU.\n\t */\n\ttrace_napi_poll(napi, rc, budget);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == budget)\n\t\t__busy_poll_stop(napi, skip_schedule);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL)) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, budget);\n\t\ttrace_napi_poll(napi, work, budget);\n\t\tgro_normal_list(napi);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nstatic void napi_hash_del(struct napi_struct *napi)\n{\n\tspin_lock(&napi_hash_lock);\n\n\thlist_del_init_rcu(&napi->napi_hash_node);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (!napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t__napi_schedule_irqoff(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void init_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tINIT_LIST_HEAD(&napi->gro_hash[i].list);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n\tnapi->gro_bitmask = 0;\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded)\n{\n\tstruct napi_struct *napi;\n\tint err = 0;\n\n\tif (dev->threaded == threaded)\n\t\treturn 0;\n\n\tif (threaded) {\n\t\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\t\tif (!napi->thread) {\n\t\t\t\terr = napi_kthread_create(napi);\n\t\t\t\tif (err) {\n\t\t\t\t\tthreaded = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdev->threaded = threaded;\n\n\t/* Make sure kthread is created before THREADED bit\n\t * is set.\n\t */\n\tsmp_mb__before_atomic();\n\n\t/* Setting/unsetting threaded mode on a napi might not immediately\n\t * take effect, if the current napi instance is actively being\n\t * polled. In this case, the switch between threaded mode and\n\t * softirq mode will happen in the next round of napi_schedule().\n\t * This should not cause hiccups/stalls to the live traffic.\n\t */\n\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\tif (threaded)\n\t\t\tset_bit(NAPI_STATE_THREADED, &napi->state);\n\t\telse\n\t\t\tclear_bit(NAPI_STATE_THREADED, &napi->state);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_threaded);\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tif (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tINIT_HLIST_NODE(&napi->napi_hash_node);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tinit_gro_hash(napi);\n\tnapi->skb = NULL;\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tnetdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,\n\t\t\t\tweight);\n\tnapi->weight = weight;\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tset_bit(NAPI_STATE_NPSVC, &napi->state);\n\tlist_add_rcu(&napi->dev_list, &dev->napi_list);\n\tnapi_hash_add(napi);\n\t/* Create kthread for this napi if dev->threaded is set.\n\t * Clear dev->threaded if kthread creation failed so that\n\t * threaded mode will not be enabled in napi_enable().\n\t */\n\tif (dev->threaded && napi_kthread_create(napi))\n\t\tdev->threaded = 0;\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\twhile (test_and_set_bit(NAPI_STATE_NPSVC, &n->state))\n\t\tmsleep(1);\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &n->state);\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n\tclear_bit(NAPI_STATE_THREADED, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: NAPI context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nvoid napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n\tclear_bit(NAPI_STATE_NPSVC, &n->state);\n\tif (n->dev->threaded && n->thread)\n\t\tset_bit(NAPI_STATE_THREADED, &n->state);\n}\nEXPORT_SYMBOL(napi_enable);\n\nstatic void flush_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tstruct sk_buff *skb, *n;\n\n\t\tlist_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)\n\t\t\tkfree_skb(skb);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n}\n\n/* Must be called in process context */\nvoid __netif_napi_del(struct napi_struct *napi)\n{\n\tif (!test_and_clear_bit(NAPI_STATE_LISTED, &napi->state))\n\t\treturn;\n\n\tnapi_hash_del(napi);\n\tlist_del_rcu(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tflush_gro_hash(napi);\n\tnapi->gro_bitmask = 0;\n\n\tif (napi->thread) {\n\t\tkthread_stop(napi->thread);\n\t\tnapi->thread = NULL;\n\t}\n}\nEXPORT_SYMBOL(__netif_napi_del);\n\nstatic int __napi_poll(struct napi_struct *n, bool *repoll)\n{\n\tint work, weight;\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tif (unlikely(work > weight))\n\t\tpr_err_once(\"NAPI poll function %pS returned %d, exceeding its budget of %d.\\n\",\n\t\t\t    n->poll, work, weight);\n\n\tif (likely(work < weight))\n\t\treturn work;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\treturn work;\n\t}\n\n\t/* The NAPI context has more processing work, but busy-polling\n\t * is preferred. Exit early.\n\t */\n\tif (napi_prefer_busy_poll(n)) {\n\t\tif (napi_complete_done(n, work)) {\n\t\t\t/* If timeout is not set, we need to make sure\n\t\t\t * that the NAPI is re-scheduled.\n\t\t\t */\n\t\t\tnapi_schedule(n);\n\t\t}\n\t\treturn work;\n\t}\n\n\tif (n->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\tgro_normal_list(n);\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\treturn work;\n\t}\n\n\t*repoll = true;\n\n\treturn work;\n}\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tbool do_repoll = false;\n\tvoid *have;\n\tint work;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\twork = __napi_poll(n, &do_repoll);\n\n\tif (do_repoll)\n\t\tlist_add_tail(&n->poll_list, repoll);\n\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic int napi_thread_wait(struct napi_struct *napi)\n{\n\tbool woken = false;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\twhile (!kthread_should_stop() && !napi_disable_pending(napi)) {\n\t\t/* Testing SCHED_THREADED bit here to make sure the current\n\t\t * kthread owns this napi and could poll on this napi.\n\t\t * Testing SCHED bit is not enough because SCHED bit might be\n\t\t * set by some other busy poll thread or by napi_disable().\n\t\t */\n\t\tif (test_bit(NAPI_STATE_SCHED_THREADED, &napi->state) || woken) {\n\t\t\tWARN_ON(!list_empty(&napi->poll_list));\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn 0;\n\t\t}\n\n\t\tschedule();\n\t\t/* woken being true indicates this thread owns this napi. */\n\t\twoken = true;\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\treturn -1;\n}\n\nstatic int napi_threaded_poll(void *data)\n{\n\tstruct napi_struct *napi = data;\n\tvoid *have;\n\n\twhile (!napi_thread_wait(napi)) {\n\t\tfor (;;) {\n\t\t\tbool repoll = false;\n\n\t\t\tlocal_bh_disable();\n\n\t\t\thave = netpoll_poll_lock(napi);\n\t\t\t__napi_poll(napi, &repoll);\n\t\t\tnetpoll_poll_unlock(have);\n\n\t\t\tlocal_bh_enable();\n\n\t\t\tif (!repoll)\n\t\t\t\tbreak;\n\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(netdev_budget_usecs);\n\tint budget = netdev_budget;\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\treturn;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* lookup ignore flag */\n\tbool ignore;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int ____netdev_has_upper_dev(struct net_device *upper_dev,\n\t\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct net_device *dev = (struct net_device *)priv->data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t     &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all_rcu - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t       &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nstatic struct net_device *__netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master) && !upper->ignore)\n\t\treturn upper->dev;\n\treturn NULL;\n}\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *__netdev_next_upper_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\t*ignore = upper->ignore;\n\n\treturn upper->dev;\n}\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nstatic int __netdev_walk_all_upper_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = __netdev_next_upper_dev(now, &iter, &ignore);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = netdev_next_upper_dev_rcu(now, &iter);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\nstatic bool __netdev_has_upper_dev(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn __netdev_walk_all_upper_dev(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t   &priv);\n}\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nstatic struct net_device *__netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\t*ignore = lower->ignore;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic int __netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = __netdev_next_lower_dev(now, &iter, &ignore);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_next_lower_dev_rcu);\n\nstatic u8 __netdev_upper_depth(struct net_device *dev)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore);\n\t     udev;\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < udev->upper_level)\n\t\t\tmax_depth = udev->upper_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic u8 __netdev_lower_depth(struct net_device *dev)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore);\n\t     ldev;\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < ldev->lower_level)\n\t\t\tmax_depth = ldev->lower_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic int __netdev_update_upper_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *__unused)\n{\n\tdev->upper_level = __netdev_upper_depth(dev) + 1;\n\treturn 0;\n}\n\nstatic int __netdev_update_lower_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tdev->lower_level = __netdev_lower_depth(dev) + 1;\n\n#ifdef CONFIG_LOCKDEP\n\tif (!priv)\n\t\treturn 0;\n\n\tif (priv->flags & NESTED_SYNC_IMM)\n\t\tdev->nested_level = dev->lower_level - 1;\n\tif (priv->flags & NESTED_SYNC_TODO)\n\t\tnet_unlink_todo(dev);\n#endif\n\treturn 0;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev_rcu(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tadj->ignore = false;\n\tdev_hold(adj_dev);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tkfree(adj);\n\tdev_put(adj_dev);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tdev_put(adj_dev);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info,\n\t\t\t\t   struct netdev_nested_priv *priv,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t\t.extack = extack,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.master = master,\n\t\t.linking = true,\n\t\t.upper_info = upper_info,\n\t};\n\tstruct net_device *master_dev;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)\n\t\treturn -EMLINK;\n\n\tif (!master) {\n\t\tif (__netdev_has_upper_dev(dev, upper_dev))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tmaster_dev = __netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\treturn master_dev == upper_dev ? -EEXIST : -EBUSY;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, false,\n\t\t\t\t       NULL, NULL, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\nstatic void __netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev,\n\t\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.linking = false,\n\t};\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n}\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\t__netdev_upper_dev_unlink(dev, upper_dev, &priv);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\nstatic void __netdev_adjacent_dev_set(struct net_device *upper_dev,\n\t\t\t\t      struct net_device *lower_dev,\n\t\t\t\t      bool val)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(lower_dev, &upper_dev->adj_list.lower);\n\tif (adj)\n\t\tadj->ignore = val;\n\n\tadj = __netdev_find_adj(upper_dev, &lower_dev->adj_list.upper);\n\tif (adj)\n\t\tadj->ignore = val;\n}\n\nstatic void netdev_adjacent_dev_disable(struct net_device *upper_dev,\n\t\t\t\t\tstruct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, true);\n}\n\nstatic void netdev_adjacent_dev_enable(struct net_device *upper_dev,\n\t\t\t\t       struct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, false);\n}\n\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\tint err;\n\n\tif (!new_dev)\n\t\treturn 0;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_disable(dev, old_dev);\n\terr = __netdev_upper_dev_link(new_dev, dev, false, NULL, NULL, &priv,\n\t\t\t\t      extack);\n\tif (err) {\n\t\tif (old_dev && new_dev != old_dev)\n\t\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_adjacent_change_prepare);\n\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev || !old_dev)\n\t\treturn;\n\n\tif (new_dev == old_dev)\n\t\treturn;\n\n\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t__netdev_upper_dev_unlink(old_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_commit);\n\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev)\n\t\treturn;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\n\t__netdev_upper_dev_unlink(new_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_abort);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info info = {\n\t\t.info.dev = dev,\n\t};\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\n/**\n * netdev_get_xmit_slave - Get the xmit slave of master device\n * @dev: device\n * @skb: The packet\n * @all_slaves: assume all the slaves are active\n *\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n * %NULL is returned if no slave is found.\n */\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_xmit_slave)\n\t\treturn NULL;\n\treturn ops->ndo_get_xmit_slave(dev, skb, all_slaves);\n}\nEXPORT_SYMBOL(netdev_get_xmit_slave);\n\nstatic struct net_device *netdev_sk_get_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_sk_get_lower_dev)\n\t\treturn NULL;\n\treturn ops->ndo_sk_get_lower_dev(dev, sk);\n}\n\n/**\n * netdev_sk_get_lowest_dev - Get the lowest device in chain given device and socket\n * @dev: device\n * @sk: the socket\n *\n * %NULL is returned if no lower device is found.\n */\n\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct net_device *lower;\n\n\tlower = netdev_sk_get_lower_dev(dev, sk);\n\twhile (lower) {\n\t\tdev = lower;\n\t\tlower = netdev_sk_get_lower_dev(dev, sk);\n\t}\n\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_sk_get_lowest_dev);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\n/**\n * netdev_lower_state_changed - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info = {\n\t\t.info.dev = lower_dev,\n\t};\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tpr_warn(\"%s: promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(audit_context(), GFP_ATOMIC,\n\t\t\t\t  AUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t  \"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\t  dev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t  (old_flags & IFF_PROMISC),\n\t\t\t\t  from_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\t  from_kuid(&init_user_ns, uid),\n\t\t\t\t  from_kgid(&init_user_ns, gid),\n\t\t\t\t  audit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tpr_warn(\"%s: allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev, extack);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info = {\n\t\t\t\t.dev = dev,\n\t\t\t},\n\t\t\t.flags_changed = changes,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\t@extack: netlink extended ack\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\t/* Pairs with all the lockless reads of dev->mtu in the stack */\n\tWRITE_ONCE(dev->mtu, new_mtu);\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\nint dev_validate_mtu(struct net_device *dev, int new_mtu,\n\t\t     struct netlink_ext_ack *extack)\n{\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu less than device minimum\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu greater than device maximum\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu_ext - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\t@extack: netlink extended ack\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu_ext(struct net_device *dev, int new_mtu,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\terr = dev_validate_mtu(dev, new_mtu, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t   orig_mtu);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t     new_mtu);\n\t\t}\n\t}\n\treturn err;\n}\n\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct netlink_ext_ack extack;\n\tint err;\n\n\tmemset(&extack, 0, sizeof(extack));\n\terr = dev_set_mtu_ext(dev, new_mtu, &extack);\n\tif (err && extack._msg)\n\t\tnet_err_ratelimited(\"%s: %s\\n\", dev->name, extack._msg);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_change_tx_queue_len - Change TX queue length of a netdevice\n *\t@dev: device\n *\t@new_len: new tx queue length\n */\nint dev_change_tx_queue_len(struct net_device *dev, unsigned long new_len)\n{\n\tunsigned int orig_len = dev->tx_queue_len;\n\tint res;\n\n\tif (new_len != (unsigned int)new_len)\n\t\treturn -ERANGE;\n\n\tif (new_len != orig_len) {\n\t\tdev->tx_queue_len = new_len;\n\t\tres = call_netdevice_notifiers(NETDEV_CHANGE_TX_QUEUE_LEN, dev);\n\t\tres = notifier_to_errno(res);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t\tres = dev_qdisc_change_tx_queue_len(dev);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t}\n\n\treturn 0;\n\nerr_rollback:\n\tnetdev_err(dev, \"refused to change device tx_queue_len\\n\");\n\tdev->tx_queue_len = orig_len;\n\treturn res;\n}\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\nEXPORT_SYMBOL(dev_set_group);\n\n/**\n *\tdev_pre_changeaddr_notify - Call NETDEV_PRE_CHANGEADDR.\n *\t@dev: device\n *\t@addr: new address\n *\t@extack: netlink extended ack\n */\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_pre_changeaddr_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.dev_addr = addr,\n\t};\n\tint rc;\n\n\trc = call_netdevice_notifiers_info(NETDEV_PRE_CHANGEADDR, &info.info);\n\treturn notifier_to_errno(rc);\n}\nEXPORT_SYMBOL(dev_pre_changeaddr_notify);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\t@extack: netlink extended ack\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = dev_pre_changeaddr_notify(dev, sa->sa_data, extack);\n\tif (err)\n\t\treturn err;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\nstatic DECLARE_RWSEM(dev_addr_sem);\n\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tdown_write(&dev_addr_sem);\n\tret = dev_set_mac_address(dev, sa, extack);\n\tup_write(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_set_mac_address_user);\n\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name)\n{\n\tsize_t size = sizeof(sa->sa_data);\n\tstruct net_device *dev;\n\tint ret = 0;\n\n\tdown_read(&dev_addr_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_name_rcu(net, dev_name);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!dev->addr_len)\n\t\tmemset(sa->sa_data, 0, size);\n\telse\n\t\tmemcpy(sa->sa_data, dev->dev_addr,\n\t\t       min_t(size_t, size, dev->addr_len));\n\tsa->sa_family = dev->type;\n\nunlock:\n\trcu_read_unlock();\n\tup_read(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\nEXPORT_SYMBOL(dev_change_carrier);\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\nEXPORT_SYMBOL(dev_get_phys_port_id);\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (ops->ndo_get_phys_port_name) {\n\t\terr = ops->ndo_get_phys_port_name(dev, name, len);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\treturn devlink_compat_phys_port_name_get(dev, name, len);\n}\nEXPORT_SYMBOL(dev_get_phys_port_name);\n\n/**\n *\tdev_get_port_parent_id - Get the device's port parent identifier\n *\t@dev: network device\n *\t@ppid: pointer to a storage for the port's parent identifier\n *\t@recurse: allow/disallow recursion to lower devices\n *\n *\tGet the devices's port parent identifier\n */\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid,\n\t\t\t   bool recurse)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct netdev_phys_item_id first = { };\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\tint err;\n\n\tif (ops->ndo_get_port_parent_id) {\n\t\terr = ops->ndo_get_port_parent_id(dev, ppid);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\n\terr = devlink_compat_switch_id_get(dev, ppid);\n\tif (!err || err != -EOPNOTSUPP)\n\t\treturn err;\n\n\tif (!recurse)\n\t\treturn -EOPNOTSUPP;\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter) {\n\t\terr = dev_get_port_parent_id(lower_dev, ppid, recurse);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!first.id_len)\n\t\t\tfirst = *ppid;\n\t\telse if (memcmp(&first, ppid, sizeof(*ppid)))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_get_port_parent_id);\n\n/**\n *\tnetdev_port_same_parent_id - Indicate if two network devices have\n *\tthe same port parent identifier\n *\t@a: first network device\n *\t@b: second network device\n */\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)\n{\n\tstruct netdev_phys_item_id a_id = { };\n\tstruct netdev_phys_item_id b_id = { };\n\n\tif (dev_get_port_parent_id(a, &a_id, true) ||\n\t    dev_get_port_parent_id(b, &b_id, true))\n\t\treturn false;\n\n\treturn netdev_phys_item_id_same(&a_id, &b_id);\n}\nEXPORT_SYMBOL(netdev_port_same_parent_id);\n\n/**\n *\tdev_change_proto_down - update protocol port state information\n *\t@dev: device\n *\t@proto_down: new value\n *\n *\tThis info can be used by switch drivers to set the phys state of the\n *\tport.\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_proto_down(dev, proto_down);\n}\nEXPORT_SYMBOL(dev_change_proto_down);\n\n/**\n *\tdev_change_proto_down_generic - generic implementation for\n * \tndo_change_proto_down that sets carrier according to\n * \tproto_down.\n *\n *\t@dev: device\n *\t@proto_down: new value\n */\nint dev_change_proto_down_generic(struct net_device *dev, bool proto_down)\n{\n\tif (proto_down)\n\t\tnetif_carrier_off(dev);\n\telse\n\t\tnetif_carrier_on(dev);\n\tdev->proto_down = proto_down;\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_change_proto_down_generic);\n\n/**\n *\tdev_change_proto_down_reason - proto down reason\n *\n *\t@dev: device\n *\t@mask: proto down mask\n *\t@value: proto down value\n */\nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value)\n{\n\tint b;\n\n\tif (!mask) {\n\t\tdev->proto_down_reason = value;\n\t} else {\n\t\tfor_each_set_bit(b, &mask, 32) {\n\t\t\tif (value & (1 << b))\n\t\t\t\tdev->proto_down_reason |= BIT(b);\n\t\t\telse\n\t\t\t\tdev->proto_down_reason &= ~BIT(b);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(dev_change_proto_down_reason);\n\nstruct bpf_xdp_link {\n\tstruct bpf_link link;\n\tstruct net_device *dev; /* protected by rtnl_lock, no refcnt held */\n\tint flags;\n};\n\nstatic enum bpf_xdp_mode dev_xdp_mode(struct net_device *dev, u32 flags)\n{\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\treturn XDP_MODE_HW;\n\tif (flags & XDP_FLAGS_DRV_MODE)\n\t\treturn XDP_MODE_DRV;\n\tif (flags & XDP_FLAGS_SKB_MODE)\n\t\treturn XDP_MODE_SKB;\n\treturn dev->netdev_ops->ndo_bpf ? XDP_MODE_DRV : XDP_MODE_SKB;\n}\n\nstatic bpf_op_t dev_xdp_bpf_op(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tswitch (mode) {\n\tcase XDP_MODE_SKB:\n\t\treturn generic_xdp_install;\n\tcase XDP_MODE_DRV:\n\tcase XDP_MODE_HW:\n\t\treturn dev->netdev_ops->ndo_bpf;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_xdp_link *dev_xdp_link(struct net_device *dev,\n\t\t\t\t\t enum bpf_xdp_mode mode)\n{\n\treturn dev->xdp_state[mode].link;\n}\n\nstatic struct bpf_prog *dev_xdp_prog(struct net_device *dev,\n\t\t\t\t     enum bpf_xdp_mode mode)\n{\n\tstruct bpf_xdp_link *link = dev_xdp_link(dev, mode);\n\n\tif (link)\n\t\treturn link->link.prog;\n\treturn dev->xdp_state[mode].prog;\n}\n\nstatic u8 dev_xdp_prog_count(struct net_device *dev)\n{\n\tu8 count = 0;\n\tint i;\n\n\tfor (i = 0; i < __MAX_XDP_MODE; i++)\n\t\tif (dev->xdp_state[i].prog || dev->xdp_state[i].link)\n\t\t\tcount++;\n\treturn count;\n}\n\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tstruct bpf_prog *prog = dev_xdp_prog(dev, mode);\n\n\treturn prog ? prog->aux->id : 0;\n}\n\nstatic void dev_xdp_set_link(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_xdp_link *link)\n{\n\tdev->xdp_state[mode].link = link;\n\tdev->xdp_state[mode].prog = NULL;\n}\n\nstatic void dev_xdp_set_prog(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_prog *prog)\n{\n\tdev->xdp_state[mode].link = NULL;\n\tdev->xdp_state[mode].prog = prog;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t   bpf_op_t bpf_op, struct netlink_ext_ack *extack,\n\t\t\t   u32 flags, struct bpf_prog *prog)\n{\n\tstruct netdev_bpf xdp;\n\tint err;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = mode == XDP_MODE_HW ? XDP_SETUP_PROG_HW : XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\t/* Drivers assume refcnt is already incremented (i.e, prog pointer is\n\t * \"moved\" into driver), so they don't increment it on their own, but\n\t * they do decrement refcnt when program is detached or replaced.\n\t * Given net_device also owns link/prog, we need to bump refcnt here\n\t * to prevent drivers from underflowing it.\n\t */\n\tif (prog)\n\t\tbpf_prog_inc(prog);\n\terr = bpf_op(dev, &xdp);\n\tif (err) {\n\t\tif (prog)\n\t\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\tif (mode != XDP_MODE_HW)\n\t\tbpf_prog_change_xdp(dev_xdp_prog(dev, mode), prog);\n\n\treturn 0;\n}\n\nstatic void dev_xdp_uninstall(struct net_device *dev)\n{\n\tstruct bpf_xdp_link *link;\n\tstruct bpf_prog *prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tfor (mode = XDP_MODE_SKB; mode < __MAX_XDP_MODE; mode++) {\n\t\tprog = dev_xdp_prog(dev, mode);\n\t\tif (!prog)\n\t\t\tcontinue;\n\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op)\n\t\t\tcontinue;\n\n\t\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\n\t\t/* auto-detach link from net device */\n\t\tlink = dev_xdp_link(dev, mode);\n\t\tif (link)\n\t\t\tlink->dev = NULL;\n\t\telse\n\t\t\tbpf_prog_put(prog);\n\n\t\tdev_xdp_set_link(dev, mode, NULL);\n\t}\n}\n\nstatic int dev_xdp_attach(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t\t  struct bpf_xdp_link *link, struct bpf_prog *new_prog,\n\t\t\t  struct bpf_prog *old_prog, u32 flags)\n{\n\tunsigned int num_modes = hweight32(flags & XDP_FLAGS_MODES);\n\tstruct bpf_prog *cur_prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* either link or prog attachment, never both */\n\tif (link && (new_prog || old_prog))\n\t\treturn -EINVAL;\n\t/* link supports only XDP mode flags */\n\tif (link && (flags & ~XDP_FLAGS_MODES)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid XDP flags for BPF link attachment\");\n\t\treturn -EINVAL;\n\t}\n\t/* just one XDP mode bit should be set, zero defaults to drv/skb mode */\n\tif (num_modes > 1) {\n\t\tNL_SET_ERR_MSG(extack, \"Only one XDP mode flag can be set\");\n\t\treturn -EINVAL;\n\t}\n\t/* avoid ambiguity if offload + drv/skb mode progs are both loaded */\n\tif (!num_modes && dev_xdp_prog_count(dev) > 1) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"More than one program loaded, unset mode is ambiguous\");\n\t\treturn -EINVAL;\n\t}\n\t/* old_prog != NULL implies XDP_FLAGS_REPLACE is set */\n\tif (old_prog && !(flags & XDP_FLAGS_REPLACE)) {\n\t\tNL_SET_ERR_MSG(extack, \"XDP_FLAGS_REPLACE is not specified\");\n\t\treturn -EINVAL;\n\t}\n\n\tmode = dev_xdp_mode(dev, flags);\n\t/* can't replace attached link */\n\tif (dev_xdp_link(dev, mode)) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active BPF XDP link\");\n\t\treturn -EBUSY;\n\t}\n\n\tcur_prog = dev_xdp_prog(dev, mode);\n\t/* can't replace attached prog with link */\n\tif (link && cur_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active XDP program with BPF link\");\n\t\treturn -EBUSY;\n\t}\n\tif ((flags & XDP_FLAGS_REPLACE) && cur_prog != old_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Active program does not match expected\");\n\t\treturn -EEXIST;\n\t}\n\n\t/* put effective new program into new_prog */\n\tif (link)\n\t\tnew_prog = link->link.prog;\n\n\tif (new_prog) {\n\t\tbool offload = mode == XDP_MODE_HW;\n\t\tenum bpf_xdp_mode other_mode = mode == XDP_MODE_SKB\n\t\t\t\t\t       ? XDP_MODE_DRV : XDP_MODE_SKB;\n\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) && cur_prog) {\n\t\t\tNL_SET_ERR_MSG(extack, \"XDP program already attached\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (!offload && dev_xdp_prog(dev, other_mode)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Native and generic XDP can't be active at the same time\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (!offload && bpf_prog_is_dev_bound(new_prog->aux)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Using device-bound program without HW_MODE flag is not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_DEVMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_DEVMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_CPUMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_CPUMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* don't call drivers if the effective program didn't change */\n\tif (new_prog != cur_prog) {\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Underlying driver does not support XDP in native mode\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\terr = dev_xdp_install(dev, mode, bpf_op, extack, flags, new_prog);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (link)\n\t\tdev_xdp_set_link(dev, mode, link);\n\telse\n\t\tdev_xdp_set_prog(dev, mode, new_prog);\n\tif (cur_prog)\n\t\tbpf_prog_put(cur_prog);\n\n\treturn 0;\n}\n\nstatic int dev_xdp_attach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\treturn dev_xdp_attach(dev, extack, link, NULL, NULL, link->flags);\n}\n\nstatic int dev_xdp_detach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tmode = dev_xdp_mode(dev, link->flags);\n\tif (dev_xdp_link(dev, mode) != link)\n\t\treturn -EINVAL;\n\n\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\tdev_xdp_set_link(dev, mode, NULL);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\trtnl_lock();\n\n\t/* if racing with net_device's tear down, xdp_link->dev might be\n\t * already NULL, in which case link was already auto-detached\n\t */\n\tif (xdp_link->dev) {\n\t\tWARN_ON(dev_xdp_detach_link(xdp_link->dev, NULL, xdp_link));\n\t\txdp_link->dev = NULL;\n\t}\n\n\trtnl_unlock();\n}\n\nstatic int bpf_xdp_link_detach(struct bpf_link *link)\n{\n\tbpf_xdp_link_release(link);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\tkfree(xdp_link);\n}\n\nstatic void bpf_xdp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t     struct seq_file *seq)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tseq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);\n}\n\nstatic int bpf_xdp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t       struct bpf_link_info *info)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tinfo->xdp.ifindex = ifindex;\n\treturn 0;\n}\n\nstatic int bpf_xdp_link_update(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t       struct bpf_prog *old_prog)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err = 0;\n\n\trtnl_lock();\n\n\t/* link might have been auto-released already, so fail */\n\tif (!xdp_link->dev) {\n\t\terr = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog && link->prog != old_prog) {\n\t\terr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\told_prog = link->prog;\n\tif (old_prog == new_prog) {\n\t\t/* no-op, don't disturb drivers */\n\t\tbpf_prog_put(new_prog);\n\t\tgoto out_unlock;\n\t}\n\n\tmode = dev_xdp_mode(xdp_link->dev, xdp_link->flags);\n\tbpf_op = dev_xdp_bpf_op(xdp_link->dev, mode);\n\terr = dev_xdp_install(xdp_link->dev, mode, bpf_op, NULL,\n\t\t\t      xdp_link->flags, new_prog);\n\tif (err)\n\t\tgoto out_unlock;\n\n\told_prog = xchg(&link->prog, new_prog);\n\tbpf_prog_put(old_prog);\n\nout_unlock:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_xdp_link_lops = {\n\t.release = bpf_xdp_link_release,\n\t.dealloc = bpf_xdp_link_dealloc,\n\t.detach = bpf_xdp_link_detach,\n\t.show_fdinfo = bpf_xdp_link_show_fdinfo,\n\t.fill_link_info = bpf_xdp_link_fill_link_info,\n\t.update_prog = bpf_xdp_link_update,\n};\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_xdp_link *link;\n\tstruct net_device *dev;\n\tint err, fd;\n\n\tdev = dev_get_by_index(net, attr->link_create.target_ifindex);\n\tif (!dev)\n\t\treturn -EINVAL;\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_dev;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_XDP, &bpf_xdp_link_lops, prog);\n\tlink->dev = dev;\n\tlink->flags = attr->link_create.flags;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto out_put_dev;\n\t}\n\n\trtnl_lock();\n\terr = dev_xdp_attach_link(dev, NULL, link);\n\trtnl_unlock();\n\n\tif (err) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\tgoto out_put_dev;\n\t}\n\n\tfd = bpf_link_settle(&link_primer);\n\t/* link itself doesn't hold dev's refcnt to not complicate shutdown */\n\tdev_put(dev);\n\treturn fd;\n\nout_put_dev:\n\tdev_put(dev);\n\treturn err;\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@expected_fd: old program fd that userspace expects to replace or clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags)\n{\n\tenum bpf_xdp_mode mode = dev_xdp_mode(dev, flags);\n\tstruct bpf_prog *new_prog = NULL, *old_prog = NULL;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (fd >= 0) {\n\t\tnew_prog = bpf_prog_get_type_dev(fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(new_prog))\n\t\t\treturn PTR_ERR(new_prog);\n\t}\n\n\tif (expected_fd >= 0) {\n\t\told_prog = bpf_prog_get_type_dev(expected_fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\terr = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = dev_xdp_attach(dev, extack, NULL, new_prog, old_prog, flags);\n\nerr_out:\n\tif (err && new_prog)\n\t\tbpf_prog_put(new_prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\treturn err;\n}\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tdev_net(dev)->dev_unreg_count++;\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\t__netdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t\telse\n\t\t\t\tnetdev_features_change(lower);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\tif (!(features & NETIF_F_RXCSUM)) {\n\t\t/* NETIF_F_GRO_HW implies doing RXCSUM since every packet\n\t\t * successfully merged by hardware must also have the\n\t\t * checksum verified by hardware.  If the user does not\n\t\t * want to enable RXCSUM, logically, we should disable GRO_HW.\n\t\t */\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GRO_HW since no RXCSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\t/* LRO/HW-GRO features cannot be combined with RX-FCS */\n\tif (features & NETIF_F_RXFCS) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_dbg(dev, \"Dropping LRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping HW-GRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif (features & NETIF_F_HW_TLS_TX) {\n\t\tbool ip_csum = (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) ==\n\t\t\t(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\tbool hw_csum = features & NETIF_F_HW_CSUM;\n\n\t\tif (!ip_csum && !hw_csum) {\n\t\t\tnetdev_dbg(dev, \"Dropping TLS TX HW offload feature since no CSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_RX) && !(features & NETIF_F_RXCSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS RX HW offload feature since no RXCSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off on an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_ctag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_ctag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_stag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_stag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (rootdev->operstate == IF_OPER_TESTING)\n\t\tnetif_testing_on(dev);\n\telse\n\t\tnetif_testing_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\tint err = 0;\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++) {\n\t\trx[i].dev = dev;\n\n\t\t/* XDP RX-queue setup */\n\t\terr = xdp_rxq_info_reg(&rx[i].xdp_rxq, dev, i, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_info;\n\t}\n\treturn 0;\n\nerr_rxq_info:\n\t/* Rollback successful reg's and free other resources */\n\twhile (i--)\n\t\txdp_rxq_info_unreg(&rx[i].xdp_rxq);\n\tkvfree(dev->_rx);\n\tdev->_rx = NULL;\n\treturn err;\n}\n\nstatic void netif_free_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\n\t/* netif_alloc_rx_queues alloc failed, resources have been unreg'ed */\n\tif (!dev->_rx)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++)\n\t\txdp_rxq_info_unreg(&dev->_rx[i].xdp_rxq);\n\n\tkvfree(dev->_rx);\n}\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <\n\t\t     NETDEV_FEATURE_COUNT);\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tret = ethtool_check_ops(dev->ethtool_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tdev->name_node = netdev_name_node_head_alloc(dev);\n\tif (!dev->name_node)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto err_free_name;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_uninit;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= (NETIF_F_SOFT_FEATURES | NETIF_F_SOFT_FEATURES_OFF);\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->udp_tunnel_nic_info) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret) {\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t\tgoto err_uninit;\n\t}\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\t/* Expect explicit free_netdev() on failure */\n\t\tdev->needs_free_netdev = false;\n\t\tunregister_netdevice_queue(dev, NULL);\n\t\tgoto out;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\nerr_free_name:\n\tnetdev_name_node_free(dev->name_node);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* napi_busy_loop stats accounting wants this */\n\tdev_net_set(dev, &init_net);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\tif (rtnl_lock_killable())\n\t\treturn -EINTR;\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n#else\n\treturn refcount_read(&dev->dev_refcnt);\n#endif\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n#define WAIT_REFS_MIN_MSECS 1\n#define WAIT_REFS_MAX_MSECS 250\n/**\n * netdev_wait_allrefs - wait until all references are gone.\n * @dev: target net_device\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint wait = 0, refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 1) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tif (!wait) {\n\t\t\trcu_barrier();\n\t\t\twait = WAIT_REFS_MIN_MSECS;\n\t\t} else {\n\t\t\tmsleep(wait);\n\t\t\twait = min(wait << 1, WAIT_REFS_MAX_MSECS);\n\t\t}\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (refcnt != 1 && time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head unlink_list;\n\n\tlist_replace_init(&net_unlink_list, &unlink_list);\n\n\twhile (!list_empty(&unlink_list)) {\n\t\tstruct net_device *dev = list_first_entry(&unlink_list,\n\t\t\t\t\t\t\t  struct net_device,\n\t\t\t\t\t\t\t  unlink_list);\n\t\tlist_del_init(&dev->unlink_list);\n\t\tdev->nested_level = dev->lower_level - 1;\n\t}\n#endif\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tpr_err(\"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev) != 1);\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n#if IS_ENABLED(CONFIG_DECNET)\n\t\tWARN_ON(dev->dn_ptr);\n#endif\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\t/* Report a network device has been unregistered */\n\t\trtnl_lock();\n\t\tdev_net(dev)->dev_unreg_count--;\n\t\t__rtnl_unlock();\n\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n\tBUILD_BUG_ON(sizeof(*stats64) < sizeof(*netdev_stats));\n\tmemcpy(stats64, netdev_stats, sizeof(*netdev_stats));\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + sizeof(*netdev_stats), 0,\n\t       sizeof(*stats64) - sizeof(*netdev_stats));\n#else\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(unsigned long);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n#endif\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += (unsigned long)atomic_long_read(&dev->rx_dropped);\n\tstorage->tx_dropped += (unsigned long)atomic_long_read(&dev->tx_dropped);\n\tstorage->rx_nohandler += (unsigned long)atomic_long_read(&dev->rx_nohandler);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\n/**\n *\tdev_fetch_sw_netstats - get per-cpu network device statistics\n *\t@s: place to store stats\n *\t@netstats: per-cpu network stats to read from\n *\n *\tRead per-cpu network statistics and populate the related fields in @s.\n */\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tconst struct pcpu_sw_netstats *stats;\n\t\tstruct pcpu_sw_netstats tmp;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(netstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&stats->syncp);\n\t\t\ttmp.rx_packets = stats->rx_packets;\n\t\t\ttmp.rx_bytes   = stats->rx_bytes;\n\t\t\ttmp.tx_packets = stats->tx_packets;\n\t\t\ttmp.tx_bytes   = stats->tx_bytes;\n\t\t} while (u64_stats_fetch_retry_irq(&stats->syncp, start));\n\n\t\ts->rx_packets += tmp.rx_packets;\n\t\ts->rx_bytes   += tmp.rx_bytes;\n\t\ts->tx_packets += tmp.tx_packets;\n\t\ts->tx_bytes   += tmp.tx_bytes;\n\t}\n}\nEXPORT_SYMBOL_GPL(dev_fetch_sw_netstats);\n\n/**\n *\tdev_get_tstats64 - ndo_get_stats64 implementation\n *\t@dev: device to get statistics from\n *\t@s: place to store stats\n *\n *\tPopulate @s from dev->stats and dev->tstats. Can be used as\n *\tndo_get_stats64() callback.\n */\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_sw_netstats(s, dev->tstats);\n}\nEXPORT_SYMBOL_GPL(dev_get_tstats64);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tunsigned int alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\tdev_hold(dev);\n#else\n\trefcount_set(&dev->dev_refcnt, 1);\n#endif\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->upper_level = 1;\n\tdev->lower_level = 1;\n#ifdef CONFIG_LOCKDEP\n\tdev->nested_level = 0;\n\tINIT_LIST_HEAD(&dev->unlink_list);\n#endif\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tINIT_LIST_HEAD(&dev->net_notifier_list);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_ingress_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n#endif\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\n\t/* When called immediately after register_netdevice() failed the unwind\n\t * handling may still be dismantling the device. Handle that case by\n\t * deferring the free.\n\t */\n\tif (dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\t\tdev->needs_free_netdev = true;\n\t\treturn;\n\t}\n\n\tnetif_free_tx_queues(dev);\n\tnetif_free_rx_queues(dev);\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n#endif\n\tfree_percpu(dev->xdp_bulkq);\n\tdev->xdp_bulkq = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->unreg_list, &single);\n\t\tunregister_netdevice_many(&single);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\t\tdev_xdp_uninstall(dev);\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL, NULL, 0);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tnetdev_name_node_alt_flush(dev);\n\t\tnetdev_name_node_free(dev->name_node);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tdev_put(dev);\n\t\tnet_set_todo(dev);\n\t}\n\n\tlist_del(head);\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tstruct net *net_old = dev_net(dev);\n\tint err, new_nsid, new_ifindex;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(net_old, net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\terr = dev_get_valid_name(net, dev, pat);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\n\tnew_nsid = peernet2id_alloc(dev_net(dev), net, GFP_KERNEL);\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex))\n\t\tnew_ifindex = dev_new_index(net);\n\telse\n\t\tnew_ifindex = dev->ifindex;\n\n\trtmsg_ifinfo_newnet(RTM_DELLINK, dev, ~0U, GFP_KERNEL, &new_nsid,\n\t\t\t    new_ifindex);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Move per-net netdevice notifiers that are following the netdevice */\n\tmove_netdevice_notifiers_dev_net(dev, net);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\tdev->ifindex = new_ifindex;\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Adapt owner in case owning user namespace of target network\n\t * namespace is different from the original one.\n\t */\n\terr = netdev_change_owner(dev, net_old, net);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t/* send out pending IPI's on offline CPU */\n\tnet_rps_send_ipi(remsd);\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc_array(NETDEV_HASHENTRIES, sizeof(*hash), GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tBUILD_BUG_ON(GRO_HASH_BUCKETS >\n\t\t     8 * sizeof_field(struct napi_struct, gro_bitmask));\n\n\tif (net != &init_net)\n\t\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\tRAW_INIT_NOTIFIER_HEAD(&net->netdev_chain);\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n\tif (net != &init_net)\n\t\tWARN_ON_ONCE(!list_empty(&net->dev_base_head));\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops && !dev->rtnl_link_ops->netns_refund)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\tif (__dev_get_by_name(&init_net, fb_name))\n\t\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%%d\");\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit rtnl_lock_unregistering(struct list_head *net_list)\n{\n\t/* Return with the rtnl_lock held when there are no network\n\t * devices unregistering in any network namespace in net_list.\n\t */\n\tstruct net *net;\n\tbool unregistering;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tadd_wait_queue(&netdev_unregistering_wq, &wait);\n\tfor (;;) {\n\t\tunregistering = false;\n\t\trtnl_lock();\n\t\tlist_for_each_entry(net, net_list, exit_list) {\n\t\t\tif (net->dev_unreg_count > 0) {\n\t\t\t\tunregistering = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!unregistering)\n\t\t\tbreak;\n\t\t__rtnl_unlock();\n\n\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n\t}\n\tremove_wait_queue(&netdev_unregistering_wq, &wait);\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\t/* To prevent network device cleanup code from dereferencing\n\t * loopback devices or network devices that have been freed\n\t * wait here for all pending unregistrations to complete,\n\t * before unregistring the loopback device and allowing the\n\t * network namespace be freed.\n\t *\n\t * The netdev todo list containing all network devices\n\t * unregistrations that happen in default_device_exit_batch\n\t * will run in the rtnl_unlock() at the end of\n\t * default_device_exit_batch.\n\t */\n\trtnl_lock_unregistering(net_list);\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tINIT_LIST_HEAD(&offload_base);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n#ifdef CONFIG_XFRM_OFFLOAD\n\t\tskb_queue_head_init(&sd->xfrm_backlog);\n#endif\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tINIT_CSD(&sd->csd, rps_trigger_softirq, sd);\n\t\tsd->cpu = i;\n#endif\n\n\t\tinit_gro_hash(&sd->backlog);\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"}, "2": {"id": 2, "path": "/src/include/linux/rtnetlink.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_RTNETLINK_H\n#define __LINUX_RTNETLINK_H\n\n\n#include <linux/mutex.h>\n#include <linux/netdevice.h>\n#include <linux/wait.h>\n#include <linux/refcount.h>\n#include <uapi/linux/rtnetlink.h>\n\nextern int rtnetlink_send(struct sk_buff *skb, struct net *net, u32 pid, u32 group, int echo);\nextern int rtnl_unicast(struct sk_buff *skb, struct net *net, u32 pid);\nextern void rtnl_notify(struct sk_buff *skb, struct net *net, u32 pid,\n\t\t\tu32 group, struct nlmsghdr *nlh, gfp_t flags);\nextern void rtnl_set_sk_err(struct net *net, u32 group, int error);\nextern int rtnetlink_put_metrics(struct sk_buff *skb, u32 *metrics);\nextern int rtnl_put_cacheinfo(struct sk_buff *skb, struct dst_entry *dst,\n\t\t\t      u32 id, long expires, u32 error);\n\nvoid rtmsg_ifinfo(int type, struct net_device *dev, unsigned change, gfp_t flags);\nvoid rtmsg_ifinfo_newnet(int type, struct net_device *dev, unsigned int change,\n\t\t\t gfp_t flags, int *new_nsid, int new_ifindex);\nstruct sk_buff *rtmsg_ifinfo_build_skb(int type, struct net_device *dev,\n\t\t\t\t       unsigned change, u32 event,\n\t\t\t\t       gfp_t flags, int *new_nsid,\n\t\t\t\t       int new_ifindex);\nvoid rtmsg_ifinfo_send(struct sk_buff *skb, struct net_device *dev,\n\t\t       gfp_t flags);\n\n\n/* RTNL is used as a global lock for all changes to network configuration  */\nextern void rtnl_lock(void);\nextern void rtnl_unlock(void);\nextern int rtnl_trylock(void);\nextern int rtnl_is_locked(void);\nextern int rtnl_lock_killable(void);\nextern bool refcount_dec_and_rtnl_lock(refcount_t *r);\n\nextern wait_queue_head_t netdev_unregistering_wq;\nextern struct rw_semaphore pernet_ops_rwsem;\nextern struct rw_semaphore net_rwsem;\n\n#ifdef CONFIG_PROVE_LOCKING\nextern bool lockdep_rtnl_is_held(void);\n#else\nstatic inline bool lockdep_rtnl_is_held(void)\n{\n\treturn true;\n}\n#endif /* #ifdef CONFIG_PROVE_LOCKING */\n\n/**\n * rcu_dereference_rtnl - rcu_dereference with debug checking\n * @p: The pointer to read, prior to dereferencing\n *\n * Do an rcu_dereference(p), but check caller either holds rcu_read_lock()\n * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference()\n */\n#define rcu_dereference_rtnl(p)\t\t\t\t\t\\\n\trcu_dereference_check(p, lockdep_rtnl_is_held())\n\n/**\n * rcu_dereference_bh_rtnl - rcu_dereference_bh with debug checking\n * @p: The pointer to read, prior to dereference\n *\n * Do an rcu_dereference_bh(p), but check caller either holds rcu_read_lock_bh()\n * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference_bh()\n */\n#define rcu_dereference_bh_rtnl(p)\t\t\t\t\\\n\trcu_dereference_bh_check(p, lockdep_rtnl_is_held())\n\n/**\n * rtnl_dereference - fetch RCU pointer when updates are prevented by RTNL\n * @p: The pointer to read, prior to dereferencing\n *\n * Return the value of the specified RCU-protected pointer, but omit\n * the READ_ONCE(), because caller holds RTNL.\n */\n#define rtnl_dereference(p)\t\t\t\t\t\\\n\trcu_dereference_protected(p, lockdep_rtnl_is_held())\n\nstatic inline struct netdev_queue *dev_ingress_queue(struct net_device *dev)\n{\n\treturn rtnl_dereference(dev->ingress_queue);\n}\n\nstatic inline struct netdev_queue *dev_ingress_queue_rcu(struct net_device *dev)\n{\n\treturn rcu_dereference(dev->ingress_queue);\n}\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev);\n\n#ifdef CONFIG_NET_INGRESS\nvoid net_inc_ingress_queue(void);\nvoid net_dec_ingress_queue(void);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nvoid net_inc_egress_queue(void);\nvoid net_dec_egress_queue(void);\n#endif\n\nvoid rtnetlink_init(void);\nvoid __rtnl_unlock(void);\nvoid rtnl_kfree_skbs(struct sk_buff *head, struct sk_buff *tail);\n\n#define ASSERT_RTNL() \\\n\tWARN_ONCE(!rtnl_is_locked(), \\\n\t\t  \"RTNL: assertion failed at %s (%d)\\n\", __FILE__,  __LINE__)\n\nextern int ndo_dflt_fdb_dump(struct sk_buff *skb,\n\t\t\t     struct netlink_callback *cb,\n\t\t\t     struct net_device *dev,\n\t\t\t     struct net_device *filter_dev,\n\t\t\t     int *idx);\nextern int ndo_dflt_fdb_add(struct ndmsg *ndm,\n\t\t\t    struct nlattr *tb[],\n\t\t\t    struct net_device *dev,\n\t\t\t    const unsigned char *addr,\n\t\t\t    u16 vid,\n\t\t\t    u16 flags);\nextern int ndo_dflt_fdb_del(struct ndmsg *ndm,\n\t\t\t    struct nlattr *tb[],\n\t\t\t    struct net_device *dev,\n\t\t\t    const unsigned char *addr,\n\t\t\t    u16 vid);\n\nextern int ndo_dflt_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,\n\t\t\t\t   struct net_device *dev, u16 mode,\n\t\t\t\t   u32 flags, u32 mask, int nlflags,\n\t\t\t\t   u32 filter_mask,\n\t\t\t\t   int (*vlan_fill)(struct sk_buff *skb,\n\t\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t\t    u32 filter_mask));\n#endif\t/* __LINUX_RTNETLINK_H */\n"}, "3": {"id": 3, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}, "4": {"id": 4, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n# define likely_notrace(x)\tlikely(x)\n# define unlikely_notrace(x)\tunlikely(x)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "5": {"id": 5, "path": "/src/include/linux/printk.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __KERNEL_PRINTK__\n#define __KERNEL_PRINTK__\n\n#include <stdarg.h>\n#include <linux/init.h>\n#include <linux/kern_levels.h>\n#include <linux/linkage.h>\n#include <linux/cache.h>\n#include <linux/ratelimit_types.h>\n\nextern const char linux_banner[];\nextern const char linux_proc_banner[];\n\nextern int oops_in_progress;\t/* If set, an oops, panic(), BUG() or die() is in progress */\n\n#define PRINTK_MAX_SINGLE_HEADER_LEN 2\n\nstatic inline int printk_get_level(const char *buffer)\n{\n\tif (buffer[0] == KERN_SOH_ASCII && buffer[1]) {\n\t\tswitch (buffer[1]) {\n\t\tcase '0' ... '7':\n\t\tcase 'c':\t/* KERN_CONT */\n\t\t\treturn buffer[1];\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic inline const char *printk_skip_level(const char *buffer)\n{\n\tif (printk_get_level(buffer))\n\t\treturn buffer + 2;\n\n\treturn buffer;\n}\n\nstatic inline const char *printk_skip_headers(const char *buffer)\n{\n\twhile (printk_get_level(buffer))\n\t\tbuffer = printk_skip_level(buffer);\n\n\treturn buffer;\n}\n\n#define CONSOLE_EXT_LOG_MAX\t8192\n\n/* printk's without a loglevel use this.. */\n#define MESSAGE_LOGLEVEL_DEFAULT CONFIG_MESSAGE_LOGLEVEL_DEFAULT\n\n/* We show everything that is MORE important than this.. */\n#define CONSOLE_LOGLEVEL_SILENT  0 /* Mum's the word */\n#define CONSOLE_LOGLEVEL_MIN\t 1 /* Minimum loglevel we let people use */\n#define CONSOLE_LOGLEVEL_DEBUG\t10 /* issue debug messages */\n#define CONSOLE_LOGLEVEL_MOTORMOUTH 15\t/* You can't shut this one up */\n\n/*\n * Default used to be hard-coded at 7, quiet used to be hardcoded at 4,\n * we're now allowing both to be set from kernel config.\n */\n#define CONSOLE_LOGLEVEL_DEFAULT CONFIG_CONSOLE_LOGLEVEL_DEFAULT\n#define CONSOLE_LOGLEVEL_QUIET\t CONFIG_CONSOLE_LOGLEVEL_QUIET\n\nextern int console_printk[];\n\n#define console_loglevel (console_printk[0])\n#define default_message_loglevel (console_printk[1])\n#define minimum_console_loglevel (console_printk[2])\n#define default_console_loglevel (console_printk[3])\n\nstatic inline void console_silent(void)\n{\n\tconsole_loglevel = CONSOLE_LOGLEVEL_SILENT;\n}\n\nstatic inline void console_verbose(void)\n{\n\tif (console_loglevel)\n\t\tconsole_loglevel = CONSOLE_LOGLEVEL_MOTORMOUTH;\n}\n\n/* strlen(\"ratelimit\") + 1 */\n#define DEVKMSG_STR_MAX_SIZE 10\nextern char devkmsg_log_str[];\nstruct ctl_table;\n\nextern int suppress_printk;\n\nstruct va_format {\n\tconst char *fmt;\n\tva_list *va;\n};\n\n/*\n * FW_BUG\n * Add this to a message where you are sure the firmware is buggy or behaves\n * really stupid or out of spec. Be aware that the responsible BIOS developer\n * should be able to fix this issue or at least get a concrete idea of the\n * problem by reading your message without the need of looking at the kernel\n * code.\n *\n * Use it for definite and high priority BIOS bugs.\n *\n * FW_WARN\n * Use it for not that clear (e.g. could the kernel messed up things already?)\n * and medium priority BIOS bugs.\n *\n * FW_INFO\n * Use this one if you want to tell the user or vendor about something\n * suspicious, but generally harmless related to the firmware.\n *\n * Use it for information or very low priority BIOS bugs.\n */\n#define FW_BUG\t\t\"[Firmware Bug]: \"\n#define FW_WARN\t\t\"[Firmware Warn]: \"\n#define FW_INFO\t\t\"[Firmware Info]: \"\n\n/*\n * HW_ERR\n * Add this to a message for hardware errors, so that user can report\n * it to hardware vendor instead of LKML or software vendor.\n */\n#define HW_ERR\t\t\"[Hardware Error]: \"\n\n/*\n * DEPRECATED\n * Add this to a message whenever you want to warn user space about the use\n * of a deprecated aspect of an API so they can stop using it\n */\n#define DEPRECATED\t\"[Deprecated]: \"\n\n/*\n * Dummy printk for disabled debugging statements to use whilst maintaining\n * gcc's format checking.\n */\n#define no_printk(fmt, ...)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\\\n\t\tprintk(fmt, ##__VA_ARGS__);\t\t\\\n\t0;\t\t\t\t\t\t\\\n})\n\n#ifdef CONFIG_EARLY_PRINTK\nextern asmlinkage __printf(1, 2)\nvoid early_printk(const char *fmt, ...);\n#else\nstatic inline __printf(1, 2) __cold\nvoid early_printk(const char *s, ...) { }\n#endif\n\n#ifdef CONFIG_PRINTK_NMI\nextern void printk_nmi_enter(void);\nextern void printk_nmi_exit(void);\nextern void printk_nmi_direct_enter(void);\nextern void printk_nmi_direct_exit(void);\n#else\nstatic inline void printk_nmi_enter(void) { }\nstatic inline void printk_nmi_exit(void) { }\nstatic inline void printk_nmi_direct_enter(void) { }\nstatic inline void printk_nmi_direct_exit(void) { }\n#endif /* PRINTK_NMI */\n\nstruct dev_printk_info;\n\n#ifdef CONFIG_PRINTK\nasmlinkage __printf(4, 0)\nint vprintk_emit(int facility, int level,\n\t\t const struct dev_printk_info *dev_info,\n\t\t const char *fmt, va_list args);\n\nasmlinkage __printf(1, 0)\nint vprintk(const char *fmt, va_list args);\n\nasmlinkage __printf(1, 2) __cold\nint printk(const char *fmt, ...);\n\n/*\n * Special printk facility for scheduler/timekeeping use only, _DO_NOT_USE_ !\n */\n__printf(1, 2) __cold int printk_deferred(const char *fmt, ...);\n\n/*\n * Please don't use printk_ratelimit(), because it shares ratelimiting state\n * with all other unrelated printk_ratelimit() callsites.  Instead use\n * printk_ratelimited() or plain old __ratelimit().\n */\nextern int __printk_ratelimit(const char *func);\n#define printk_ratelimit() __printk_ratelimit(__func__)\nextern bool printk_timed_ratelimit(unsigned long *caller_jiffies,\n\t\t\t\t   unsigned int interval_msec);\n\nextern int printk_delay_msec;\nextern int dmesg_restrict;\n\nextern int\ndevkmsg_sysctl_set_loglvl(struct ctl_table *table, int write, void *buf,\n\t\t\t  size_t *lenp, loff_t *ppos);\n\nextern void wake_up_klogd(void);\n\nchar *log_buf_addr_get(void);\nu32 log_buf_len_get(void);\nvoid log_buf_vmcoreinfo_setup(void);\nvoid __init setup_log_buf(int early);\n__printf(1, 2) void dump_stack_set_arch_desc(const char *fmt, ...);\nvoid dump_stack_print_info(const char *log_lvl);\nvoid show_regs_print_info(const char *log_lvl);\nextern asmlinkage void dump_stack(void) __cold;\nextern void printk_safe_flush(void);\nextern void printk_safe_flush_on_panic(void);\n#else\nstatic inline __printf(1, 0)\nint vprintk(const char *s, va_list args)\n{\n\treturn 0;\n}\nstatic inline __printf(1, 2) __cold\nint printk(const char *s, ...)\n{\n\treturn 0;\n}\nstatic inline __printf(1, 2) __cold\nint printk_deferred(const char *s, ...)\n{\n\treturn 0;\n}\nstatic inline int printk_ratelimit(void)\n{\n\treturn 0;\n}\nstatic inline bool printk_timed_ratelimit(unsigned long *caller_jiffies,\n\t\t\t\t\t  unsigned int interval_msec)\n{\n\treturn false;\n}\n\nstatic inline void wake_up_klogd(void)\n{\n}\n\nstatic inline char *log_buf_addr_get(void)\n{\n\treturn NULL;\n}\n\nstatic inline u32 log_buf_len_get(void)\n{\n\treturn 0;\n}\n\nstatic inline void log_buf_vmcoreinfo_setup(void)\n{\n}\n\nstatic inline void setup_log_buf(int early)\n{\n}\n\nstatic inline __printf(1, 2) void dump_stack_set_arch_desc(const char *fmt, ...)\n{\n}\n\nstatic inline void dump_stack_print_info(const char *log_lvl)\n{\n}\n\nstatic inline void show_regs_print_info(const char *log_lvl)\n{\n}\n\nstatic inline void dump_stack(void)\n{\n}\n\nstatic inline void printk_safe_flush(void)\n{\n}\n\nstatic inline void printk_safe_flush_on_panic(void)\n{\n}\n#endif\n\nextern int kptr_restrict;\n\n/**\n * pr_fmt - used by the pr_*() macros to generate the printk format string\n * @fmt: format string passed from a pr_*() macro\n *\n * This macro can be used to generate a unified format string for pr_*()\n * macros. A common use is to prefix all pr_*() messages in a file with a common\n * string. For example, defining this at the top of a source file:\n *\n *        #define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n *\n * would prefix all pr_info, pr_emerg... messages in the file with the module\n * name.\n */\n#ifndef pr_fmt\n#define pr_fmt(fmt) fmt\n#endif\n\n/**\n * pr_emerg - Print an emergency-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_EMERG loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_emerg(fmt, ...) \\\n\tprintk(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_alert - Print an alert-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_ALERT loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_alert(fmt, ...) \\\n\tprintk(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_crit - Print a critical-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_CRIT loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_crit(fmt, ...) \\\n\tprintk(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_err - Print an error-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_ERR loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_err(fmt, ...) \\\n\tprintk(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_warn - Print a warning-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_WARNING loglevel. It uses pr_fmt()\n * to generate the format string.\n */\n#define pr_warn(fmt, ...) \\\n\tprintk(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_notice - Print a notice-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_NOTICE loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_notice(fmt, ...) \\\n\tprintk(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)\n/**\n * pr_info - Print an info-level message\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_INFO loglevel. It uses pr_fmt() to\n * generate the format string.\n */\n#define pr_info(fmt, ...) \\\n\tprintk(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)\n\n/**\n * pr_cont - Continues a previous log message in the same line.\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_CONT loglevel. It should only be\n * used when continuing a log message with no newline ('\\n') enclosed. Otherwise\n * it defaults back to KERN_DEFAULT loglevel.\n */\n#define pr_cont(fmt, ...) \\\n\tprintk(KERN_CONT fmt, ##__VA_ARGS__)\n\n/**\n * pr_devel - Print a debug-level message conditionally\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to a printk with KERN_DEBUG loglevel if DEBUG is\n * defined. Otherwise it does nothing.\n *\n * It uses pr_fmt() to generate the format string.\n */\n#ifdef DEBUG\n#define pr_devel(fmt, ...) \\\n\tprintk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_devel(fmt, ...) \\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n\n/* If you are writing a driver, please use dev_dbg instead */\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#include <linux/dynamic_debug.h>\n\n/**\n * pr_debug - Print a debug-level message conditionally\n * @fmt: format string\n * @...: arguments for the format string\n *\n * This macro expands to dynamic_pr_debug() if CONFIG_DYNAMIC_DEBUG is\n * set. Otherwise, if DEBUG is defined, it's equivalent to a printk with\n * KERN_DEBUG loglevel. If DEBUG is not defined it does nothing.\n *\n * It uses pr_fmt() to generate the format string (dynamic_pr_debug() uses\n * pr_fmt() internally).\n */\n#define pr_debug(fmt, ...)\t\t\t\\\n\tdynamic_pr_debug(fmt, ##__VA_ARGS__)\n#elif defined(DEBUG)\n#define pr_debug(fmt, ...) \\\n\tprintk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_debug(fmt, ...) \\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/*\n * Print a one-time message (analogous to WARN_ONCE() et al):\n */\n\n#ifdef CONFIG_PRINTK\n#define printk_once(fmt, ...)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __print_once;\t\\\n\tbool __ret_print_once = !__print_once;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (!__print_once) {\t\t\t\t\t\\\n\t\t__print_once = true;\t\t\t\t\\\n\t\tprintk(fmt, ##__VA_ARGS__);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_print_once);\t\t\t\t\\\n})\n#define printk_deferred_once(fmt, ...)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __print_once;\t\\\n\tbool __ret_print_once = !__print_once;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (!__print_once) {\t\t\t\t\t\\\n\t\t__print_once = true;\t\t\t\t\\\n\t\tprintk_deferred(fmt, ##__VA_ARGS__);\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_print_once);\t\t\t\t\\\n})\n#else\n#define printk_once(fmt, ...)\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__)\n#define printk_deferred_once(fmt, ...)\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n#define pr_emerg_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_alert_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_crit_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_err_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_warn_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_notice_once(fmt, ...)\t\t\t\t\\\n\tprintk_once(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_info_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)\n/* no pr_cont_once, don't do that... */\n\n#if defined(DEBUG)\n#define pr_devel_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_devel_once(fmt, ...)\t\t\t\t\t\\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/* If you are writing a driver, please use dev_dbg instead */\n#if defined(DEBUG)\n#define pr_debug_once(fmt, ...)\t\t\t\t\t\\\n\tprintk_once(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_debug_once(fmt, ...)\t\t\t\t\t\\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/*\n * ratelimited messages with local ratelimit_state,\n * no local ratelimit_state used in the !PRINTK case\n */\n#ifdef CONFIG_PRINTK\n#define printk_ratelimited(fmt, ...)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstatic DEFINE_RATELIMIT_STATE(_rs,\t\t\t\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__ratelimit(&_rs))\t\t\t\t\t\t\\\n\t\tprintk(fmt, ##__VA_ARGS__);\t\t\t\t\\\n})\n#else\n#define printk_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tno_printk(fmt, ##__VA_ARGS__)\n#endif\n\n#define pr_emerg_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_EMERG pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_alert_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_ALERT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_crit_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_err_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_ERR pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_warn_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_WARNING pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_notice_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_NOTICE pr_fmt(fmt), ##__VA_ARGS__)\n#define pr_info_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_INFO pr_fmt(fmt), ##__VA_ARGS__)\n/* no pr_cont_ratelimited, don't do that... */\n\n#if defined(DEBUG)\n#define pr_devel_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_devel_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\n/* If you are writing a driver, please use dev_dbg instead */\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n/* descriptor check is first to prevent flooding with \"callbacks suppressed\" */\n#define pr_debug_ratelimited(fmt, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic DEFINE_RATELIMIT_STATE(_rs,\t\t\t\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,\t\\\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\t\t\\\n\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, pr_fmt(fmt));\t\t\\\n\tif (DYNAMIC_DEBUG_BRANCH(descriptor) &&\t\t\t\t\\\n\t    __ratelimit(&_rs))\t\t\t\t\t\t\\\n\t\t__dynamic_pr_debug(&descriptor, pr_fmt(fmt), ##__VA_ARGS__);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define pr_debug_ratelimited(fmt, ...)\t\t\t\t\t\\\n\tprintk_ratelimited(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#else\n#define pr_debug_ratelimited(fmt, ...) \\\n\tno_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)\n#endif\n\nextern const struct file_operations kmsg_fops;\n\nenum {\n\tDUMP_PREFIX_NONE,\n\tDUMP_PREFIX_ADDRESS,\n\tDUMP_PREFIX_OFFSET\n};\nextern int hex_dump_to_buffer(const void *buf, size_t len, int rowsize,\n\t\t\t      int groupsize, char *linebuf, size_t linebuflen,\n\t\t\t      bool ascii);\n#ifdef CONFIG_PRINTK\nextern void print_hex_dump(const char *level, const char *prefix_str,\n\t\t\t   int prefix_type, int rowsize, int groupsize,\n\t\t\t   const void *buf, size_t len, bool ascii);\n#else\nstatic inline void print_hex_dump(const char *level, const char *prefix_str,\n\t\t\t\t  int prefix_type, int rowsize, int groupsize,\n\t\t\t\t  const void *buf, size_t len, bool ascii)\n{\n}\nstatic inline void print_hex_dump_bytes(const char *prefix_str, int prefix_type,\n\t\t\t\t\tconst void *buf, size_t len)\n{\n}\n\n#endif\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#define print_hex_dump_debug(prefix_str, prefix_type, rowsize,\t\\\n\t\t\t     groupsize, buf, len, ascii)\t\\\n\tdynamic_hex_dump(prefix_str, prefix_type, rowsize,\t\\\n\t\t\t groupsize, buf, len, ascii)\n#elif defined(DEBUG)\n#define print_hex_dump_debug(prefix_str, prefix_type, rowsize,\t\t\\\n\t\t\t     groupsize, buf, len, ascii)\t\t\\\n\tprint_hex_dump(KERN_DEBUG, prefix_str, prefix_type, rowsize,\t\\\n\t\t       groupsize, buf, len, ascii)\n#else\nstatic inline void print_hex_dump_debug(const char *prefix_str, int prefix_type,\n\t\t\t\t\tint rowsize, int groupsize,\n\t\t\t\t\tconst void *buf, size_t len, bool ascii)\n{\n}\n#endif\n\n/**\n * print_hex_dump_bytes - shorthand form of print_hex_dump() with default params\n * @prefix_str: string to prefix each line with;\n *  caller supplies trailing spaces for alignment if desired\n * @prefix_type: controls whether prefix of an offset, address, or none\n *  is printed (%DUMP_PREFIX_OFFSET, %DUMP_PREFIX_ADDRESS, %DUMP_PREFIX_NONE)\n * @buf: data blob to dump\n * @len: number of bytes in the @buf\n *\n * Calls print_hex_dump(), with log level of KERN_DEBUG,\n * rowsize of 16, groupsize of 1, and ASCII output included.\n */\n#define print_hex_dump_bytes(prefix_str, prefix_type, buf, len)\t\\\n\tprint_hex_dump_debug(prefix_str, prefix_type, 16, 1, buf, len, true)\n\n#endif\n"}, "6": {"id": 6, "path": "/src/include/linux/netdevice.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/timer.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/prefetch.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/workqueue.h>\n#include <linux/dynamic_queue_limits.h>\n\n#include <net/net_namespace.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n#include <net/netprio_cgroup.h>\n#include <net/xdp.h>\n\n#include <linux/netdev_features.h>\n#include <linux/neighbour.h>\n#include <uapi/linux/netdevice.h>\n#include <uapi/linux/if_bonding.h>\n#include <uapi/linux/pkt_cls.h>\n#include <linux/hashtable.h>\n\nstruct netpoll_info;\nstruct device;\nstruct ethtool_ops;\nstruct phy_device;\nstruct dsa_port;\nstruct ip_tunnel_parm;\nstruct macsec_context;\nstruct macsec_ops;\n\nstruct sfp_bus;\n/* 802.11 specific */\nstruct wireless_dev;\n/* 802.15.4 specific */\nstruct wpan_dev;\nstruct mpls_dev;\n/* UDP Tunnel offloads */\nstruct udp_tunnel_info;\nstruct udp_tunnel_nic_info;\nstruct udp_tunnel_nic;\nstruct bpf_prog;\nstruct xdp_buff;\n\nvoid synchronize_net(void);\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops);\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n#define MAX_NEST_DEV 8\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously; in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), and all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tCompute the worst-case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_HYPERV_NET)\n# define LL_MAX_HEADER 128\n#elif defined(CONFIG_WLAN) || IS_ENABLED(CONFIG_AX25)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !IS_ENABLED(CONFIG_NET_IPIP) && !IS_ENABLED(CONFIG_NET_IPGRE) && \\\n    !IS_ENABLED(CONFIG_IPV6_SIT) && !IS_ENABLED(CONFIG_IPV6_TUNNEL)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\n#ifdef CONFIG_RPS\n#include <linux/static_key.h>\nextern struct static_key_false rps_needed;\nextern struct static_key_false rfs_needed;\n#endif\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_UNICAST\t3\n#define NETDEV_HW_ADDR_T_MULTICAST\t4\n\tbool\t\t\tglobal_use;\n\tint\t\t\tsync_cnt;\n\tint\t\t\trefcount;\n\tint\t\t\tsynced;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tunsigned int\thh_len;\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\n/* Reserve HH_DATA_MOD byte-aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned int len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n\tbool\t(*validate)(const char *ll_header, unsigned int len);\n\t__be16\t(*parse_protocol)(const struct sk_buff *skb);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer; they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n\t__LINK_STATE_TESTING,\n};\n\n\n/*\n * This structure holds boot-time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nint __init netdev_boot_setup(char *str);\n\nstruct gro_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n/*\n * size of gro hash buckets, must less than bit number of\n * napi_struct::gro_bitmask\n */\n#define GRO_HASH_BUCKETS\t8\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-CPU poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tint\t\t\tdefer_hard_irqs_count;\n\tunsigned long\t\tgro_bitmask;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tint\t\t\tpoll_owner;\n#endif\n\tstruct net_device\t*dev;\n\tstruct gro_list\t\tgro_hash[GRO_HASH_BUCKETS];\n\tstruct sk_buff\t\t*skb;\n\tstruct list_head\trx_list; /* Pending GRO_NORMAL skbs */\n\tint\t\t\trx_count; /* length of rx_list */\n\tstruct hrtimer\t\ttimer;\n\tstruct list_head\tdev_list;\n\tstruct hlist_node\tnapi_hash_node;\n\tunsigned int\t\tnapi_id;\n\tstruct task_struct\t*thread;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t\t/* Poll is scheduled */\n\tNAPI_STATE_MISSED,\t\t/* reschedule a napi */\n\tNAPI_STATE_DISABLE,\t\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t\t/* Netpoll - don't dequeue from poll_list */\n\tNAPI_STATE_LISTED,\t\t/* NAPI added to system lists */\n\tNAPI_STATE_NO_BUSY_POLL,\t/* Do not add in napi_hash, no busy polling */\n\tNAPI_STATE_IN_BUSY_POLL,\t/* sk_busy_loop() owns this NAPI */\n\tNAPI_STATE_PREFER_BUSY_POLL,\t/* prefer busy-polling over softirq processing*/\n\tNAPI_STATE_THREADED,\t\t/* The poll is performed inside its own thread*/\n\tNAPI_STATE_SCHED_THREADED,\t/* Napi is currently scheduled in threaded mode */\n};\n\nenum {\n\tNAPIF_STATE_SCHED\t\t= BIT(NAPI_STATE_SCHED),\n\tNAPIF_STATE_MISSED\t\t= BIT(NAPI_STATE_MISSED),\n\tNAPIF_STATE_DISABLE\t\t= BIT(NAPI_STATE_DISABLE),\n\tNAPIF_STATE_NPSVC\t\t= BIT(NAPI_STATE_NPSVC),\n\tNAPIF_STATE_LISTED\t\t= BIT(NAPI_STATE_LISTED),\n\tNAPIF_STATE_NO_BUSY_POLL\t= BIT(NAPI_STATE_NO_BUSY_POLL),\n\tNAPIF_STATE_IN_BUSY_POLL\t= BIT(NAPI_STATE_IN_BUSY_POLL),\n\tNAPIF_STATE_PREFER_BUSY_POLL\t= BIT(NAPI_STATE_PREFER_BUSY_POLL),\n\tNAPIF_STATE_THREADED\t\t= BIT(NAPI_STATE_THREADED),\n\tNAPIF_STATE_SCHED_THREADED\t= BIT(NAPI_STATE_SCHED_THREADED),\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_CONSUMED,\n};\ntypedef enum gro_result gro_result_t;\n\n/*\n * enum rx_handler_result - Possible return values for rx_handlers.\n * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it\n * further.\n * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in\n * case skb->dev was changed by rx_handler.\n * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.\n * @RX_HANDLER_PASS: Do nothing, pass the skb as if no rx_handler was called.\n *\n * rx_handlers are functions called from inside __netif_receive_skb(), to do\n * special processing of the skb, prior to delivery to protocol handlers.\n *\n * Currently, a net_device can only have a single rx_handler registered. Trying\n * to register a second rx_handler will return -EBUSY.\n *\n * To register a rx_handler on a net_device, use netdev_rx_handler_register().\n * To unregister a rx_handler on a net_device, use\n * netdev_rx_handler_unregister().\n *\n * Upon return, rx_handler is expected to tell __netif_receive_skb() what to\n * do with the skb.\n *\n * If the rx_handler consumed the skb in some way, it should return\n * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for\n * the skb to be delivered in some other way.\n *\n * If the rx_handler changed skb->dev, to divert the skb to another\n * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the\n * new device will be called if it exists.\n *\n * If the rx_handler decides the skb should be ignored, it should return\n * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that\n * are registered on exact device (ptype->dev == skb->dev).\n *\n * If the rx_handler didn't change skb->dev, but wants the skb to be normally\n * delivered, it should return RX_HANDLER_PASS.\n *\n * A device without a registered rx_handler will behave as if rx_handler\n * returned RX_HANDLER_PASS.\n */\n\nenum rx_handler_result {\n\tRX_HANDLER_CONSUMED,\n\tRX_HANDLER_ANOTHER,\n\tRX_HANDLER_EXACT,\n\tRX_HANDLER_PASS,\n};\ntypedef enum rx_handler_result rx_handler_result_t;\ntypedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);\n\nvoid __napi_schedule(struct napi_struct *n);\nvoid __napi_schedule_irqoff(struct napi_struct *n);\n\nstatic inline bool napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\nstatic inline bool napi_prefer_busy_poll(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_PREFER_BUSY_POLL, &n->state);\n}\n\nbool napi_schedule_prep(struct napi_struct *n);\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: NAPI context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/**\n *\tnapi_schedule_irqoff - schedule NAPI poll\n *\t@n: NAPI context\n *\n * Variant of napi_schedule(), assuming hard irqs are masked.\n */\nstatic inline void napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule_irqoff(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline bool napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool napi_complete_done(struct napi_struct *n, int work_done);\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: NAPI context\n *\n * Mark NAPI processing as complete.\n * Consider using napi_complete_done() instead.\n * Return false if device should avoid rearming interrupts.\n */\nstatic inline bool napi_complete(struct napi_struct *n)\n{\n\treturn napi_complete_done(n, 0);\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: NAPI context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nvoid napi_disable(struct napi_struct *n);\n\nvoid napi_enable(struct napi_struct *n);\n\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: NAPI context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\tif (IS_ENABLED(CONFIG_SMP))\n\t\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\t\tmsleep(1);\n\telse\n\t\tbarrier();\n}\n\n/**\n *\tnapi_if_scheduled_mark_missed - if napi is running, set the\n *\tNAPIF_STATE_MISSED\n *\t@n: NAPI context\n *\n * If napi is running, set the NAPIF_STATE_MISSED, and return true if\n * NAPI is scheduled.\n **/\nstatic inline bool napi_if_scheduled_mark_missed(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tif (val & NAPIF_STATE_DISABLE)\n\t\t\treturn true;\n\n\t\tif (!(val & NAPIF_STATE_SCHED))\n\t\t\treturn false;\n\n\t\tnew = val | NAPIF_STATE_MISSED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\treturn true;\n}\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_DRV_XOFF,\n\t__QUEUE_STATE_STACK_XOFF,\n\t__QUEUE_STATE_FROZEN,\n};\n\n#define QUEUE_STATE_DRV_XOFF\t(1 << __QUEUE_STATE_DRV_XOFF)\n#define QUEUE_STATE_STACK_XOFF\t(1 << __QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_FROZEN\t(1 << __QUEUE_STATE_FROZEN)\n\n#define QUEUE_STATE_ANY_XOFF\t(QUEUE_STATE_DRV_XOFF | QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_ANY_XOFF_OR_FROZEN (QUEUE_STATE_ANY_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n#define QUEUE_STATE_DRV_XOFF_OR_FROZEN (QUEUE_STATE_DRV_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n\n/*\n * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The\n * netif_tx_* functions below are used to manipulate this flag.  The\n * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit\n * queue independently.  The netif_xmit_*stopped functions below are called\n * to check if the queue has been stopped by the driver or stack (either\n * of the XOFF bits are set in the state).  Drivers should not need to call\n * netif_xmit*stopped functions, they should only be using netif_tx_*.\n */\n\nstruct netdev_queue {\n/*\n * read-mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc __rcu\t*qdisc;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_SYSFS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n\tunsigned long\t\ttx_maxrate;\n\t/*\n\t * Number of TX timeouts for this queue\n\t * (/sys/class/net/DEV/Q/trans_timeout)\n\t */\n\tunsigned long\t\ttrans_timeout;\n\n\t/* Subordinate device that the queue has been assigned to */\n\tstruct net_device\t*sb_dev;\n#ifdef CONFIG_XDP_SOCKETS\n\tstruct xsk_buff_pool    *pool;\n#endif\n/*\n * write-mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * Time (in jiffies) of last Tx\n\t */\n\tunsigned long\t\ttrans_start;\n\n\tunsigned long\t\tstate;\n\n#ifdef CONFIG_BQL\n\tstruct dql\t\tdql;\n#endif\n} ____cacheline_aligned_in_smp;\n\nextern int sysctl_fb_tunnels_only_for_init_net;\nextern int sysctl_devconf_inherit_init_net;\n\n/*\n * sysctl_fb_tunnels_only_for_init_net == 0 : For all netns\n *                                     == 1 : For initns only\n *                                     == 2 : For none.\n */\nstatic inline bool net_has_fallback_tunnels(const struct net *net)\n{\n\treturn !IS_ENABLED(CONFIG_SYSCTL) ||\n\t       !sysctl_fb_tunnels_only_for_init_net ||\n\t       (net == &init_net && sysctl_fb_tunnels_only_for_init_net == 1);\n}\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU, the\n * tail pointer for that CPU's input queue at the time of last enqueue, and\n * a hardware filter index.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 filter;\n\tunsigned int last_qtail;\n};\n#define RPS_NO_FILTER 0xffff\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct rps_dev_flow flows[];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    ((_num) * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n * Each entry is a 32bit value. Upper part is the high-order bits\n * of flow hash, lower part is CPU number.\n * rps_cpu_mask is used to partition the space, depending on number of\n * possible CPUs : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1\n * For example, if 64 CPUs are possible, rps_cpu_mask = 0x3f,\n * meaning we use 32-6=26 bits for the hash.\n */\nstruct rps_sock_flow_table {\n\tu32\tmask;\n\n\tu32\tents[] ____cacheline_aligned_in_smp;\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (offsetof(struct rps_sock_flow_table, ents[_num]))\n\n#define RPS_NO_CPU 0xffff\n\nextern u32 rps_cpu_mask;\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int index = hash & table->mask;\n\t\tu32 val = hash & ~rps_cpu_mask;\n\n\t\t/* We only give a hint, preemption can change CPU under us */\n\t\tval |= raw_smp_processor_id();\n\n\t\tif (table->ents[index] != val)\n\t\t\ttable->ents[index] = val;\n\t}\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id,\n\t\t\t u16 filter_id);\n#endif\n#endif /* CONFIG_RPS */\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n#ifdef CONFIG_RPS\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n#endif\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n\tstruct xdp_rxq_info\t\txdp_rxq;\n#ifdef CONFIG_XDP_SOCKETS\n\tstruct xsk_buff_pool            *pool;\n#endif\n} ____cacheline_aligned_in_smp;\n\n/*\n * RX queue sysfs structures and functions.\n */\nstruct rx_queue_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct netdev_rx_queue *queue, char *buf);\n\tssize_t (*store)(struct netdev_rx_queue *queue,\n\t\t\t const char *buf, size_t len);\n};\n\n/* XPS map type and offset of the xps map within net_device->xps_maps[]. */\nenum xps_map_type {\n\tXPS_CPUS = 0,\n\tXPS_RXQS,\n\tXPS_MAPS_MAX,\n};\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + ((_num) * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_ALIGN(offsetof(struct xps_map, queues[1])) \\\n       - sizeof(struct xps_map)) / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n *\n * We keep track of the number of cpus/rxqs used when the struct is allocated,\n * in nr_ids. This will help not accessing out-of-bound memory.\n *\n * We keep track of the number of traffic classes used when the struct is\n * allocated, in num_tc. This will be used to navigate the maps, to ensure we're\n * not crossing its upper bound, as the original dev->num_tc can be updated in\n * the meantime.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tunsigned int nr_ids;\n\ts16 num_tc;\n\tstruct xps_map __rcu *attr_map[]; /* Either CPUs map or RXQs map */\n};\n\n#define XPS_CPU_DEV_MAPS_SIZE(_tcs) (sizeof(struct xps_dev_maps) +\t\\\n\t(nr_cpu_ids * (_tcs) * sizeof(struct xps_map *)))\n\n#define XPS_RXQ_DEV_MAPS_SIZE(_tcs, _rxqs) (sizeof(struct xps_dev_maps) +\\\n\t(_rxqs * (_tcs) * sizeof(struct xps_map *)))\n\n#endif /* CONFIG_XPS */\n\n#define TC_MAX_QUEUE\t16\n#define TC_BITMASK\t15\n/* HW offloaded queuing disciplines txq count and offset maps */\nstruct netdev_tc_txq {\n\tu16 count;\n\tu16 offset;\n};\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n/*\n * This structure is to hold information about the device\n * configured to run FCoE protocol stack.\n */\nstruct netdev_fcoe_hbainfo {\n\tchar\tmanufacturer[64];\n\tchar\tserial_number[64];\n\tchar\thardware_version[64];\n\tchar\tdriver_version[64];\n\tchar\toptionrom_version[64];\n\tchar\tfirmware_version[64];\n\tchar\tmodel[256];\n\tchar\tmodel_description[256];\n};\n#endif\n\n#define MAX_PHYS_ITEM_ID_LEN 32\n\n/* This structure holds a unique identifier to identify some\n * physical item (port for example) used by a netdevice.\n */\nstruct netdev_phys_item_id {\n\tunsigned char id[MAX_PHYS_ITEM_ID_LEN];\n\tunsigned char id_len;\n};\n\nstatic inline bool netdev_phys_item_id_same(struct netdev_phys_item_id *a,\n\t\t\t\t\t    struct netdev_phys_item_id *b)\n{\n\treturn a->id_len == b->id_len &&\n\t       memcmp(a->id, b->id, a->id_len) == 0;\n}\n\ntypedef u16 (*select_queue_fallback_t)(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct net_device *sb_dev);\n\nenum tc_setup_type {\n\tTC_SETUP_QDISC_MQPRIO,\n\tTC_SETUP_CLSU32,\n\tTC_SETUP_CLSFLOWER,\n\tTC_SETUP_CLSMATCHALL,\n\tTC_SETUP_CLSBPF,\n\tTC_SETUP_BLOCK,\n\tTC_SETUP_QDISC_CBS,\n\tTC_SETUP_QDISC_RED,\n\tTC_SETUP_QDISC_PRIO,\n\tTC_SETUP_QDISC_MQ,\n\tTC_SETUP_QDISC_ETF,\n\tTC_SETUP_ROOT_QDISC,\n\tTC_SETUP_QDISC_GRED,\n\tTC_SETUP_QDISC_TAPRIO,\n\tTC_SETUP_FT,\n\tTC_SETUP_QDISC_ETS,\n\tTC_SETUP_QDISC_TBF,\n\tTC_SETUP_QDISC_FIFO,\n\tTC_SETUP_QDISC_HTB,\n};\n\n/* These structures hold the attributes of bpf state that are being passed\n * to the netdevice through the bpf op.\n */\nenum bpf_netdev_command {\n\t/* Set or clear a bpf program used in the earliest stages of packet\n\t * rx. The prog will have been loaded as BPF_PROG_TYPE_XDP. The callee\n\t * is responsible for calling bpf_prog_put on any old progs that are\n\t * stored. In case of error, the callee need not release the new prog\n\t * reference, but on success it takes ownership and must bpf_prog_put\n\t * when it is no longer used.\n\t */\n\tXDP_SETUP_PROG,\n\tXDP_SETUP_PROG_HW,\n\t/* BPF program for offload callbacks, invoked at program load time. */\n\tBPF_OFFLOAD_MAP_ALLOC,\n\tBPF_OFFLOAD_MAP_FREE,\n\tXDP_SETUP_XSK_POOL,\n};\n\nstruct bpf_prog_offload_ops;\nstruct netlink_ext_ack;\nstruct xdp_umem;\nstruct xdp_dev_bulk_queue;\nstruct bpf_xdp_link;\n\nenum bpf_xdp_mode {\n\tXDP_MODE_SKB = 0,\n\tXDP_MODE_DRV = 1,\n\tXDP_MODE_HW = 2,\n\t__MAX_XDP_MODE\n};\n\nstruct bpf_xdp_entity {\n\tstruct bpf_prog *prog;\n\tstruct bpf_xdp_link *link;\n};\n\nstruct netdev_bpf {\n\tenum bpf_netdev_command command;\n\tunion {\n\t\t/* XDP_SETUP_PROG */\n\t\tstruct {\n\t\t\tu32 flags;\n\t\t\tstruct bpf_prog *prog;\n\t\t\tstruct netlink_ext_ack *extack;\n\t\t};\n\t\t/* BPF_OFFLOAD_MAP_ALLOC, BPF_OFFLOAD_MAP_FREE */\n\t\tstruct {\n\t\t\tstruct bpf_offloaded_map *offmap;\n\t\t};\n\t\t/* XDP_SETUP_XSK_POOL */\n\t\tstruct {\n\t\t\tstruct xsk_buff_pool *pool;\n\t\t\tu16 queue_id;\n\t\t} xsk;\n\t};\n};\n\n/* Flags for ndo_xsk_wakeup. */\n#define XDP_WAKEUP_RX (1 << 0)\n#define XDP_WAKEUP_TX (1 << 1)\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstruct xfrmdev_ops {\n\tint\t(*xdo_dev_state_add) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_delete) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_free) (struct xfrm_state *x);\n\tbool\t(*xdo_dev_offload_ok) (struct sk_buff *skb,\n\t\t\t\t       struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_advance_esn) (struct xfrm_state *x);\n};\n#endif\n\nstruct dev_ifalias {\n\tstruct rcu_head rcuhead;\n\tchar ifalias[];\n};\n\nstruct devlink;\nstruct tlsdev_ops;\n\nstruct netdev_name_node {\n\tstruct hlist_node hlist;\n\tstruct list_head list;\n\tstruct net_device *dev;\n\tconst char *name;\n};\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name);\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name);\n\nstruct netdev_net_notifier {\n\tstruct list_head list;\n\tstruct notifier_block *nb;\n};\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when a network device is registered.\n *     The network device can use this for any late stage initialization\n *     or semantic validation. It can fail with an error code which will\n *     be propagated back to register_netdev.\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when a network device transitions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when a network device transitions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tReturns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop\n *\tthe queue before that can happen; it's for obsolete devices and weird\n *\tcorner cases, but the stack really does a non-trivial amount\n *\tof useless work if you return NETDEV_TX_BUSY.\n *\tRequired; cannot be NULL.\n *\n * netdev_features_t (*ndo_features_check)(struct sk_buff *skb,\n *\t\t\t\t\t   struct net_device *dev\n *\t\t\t\t\t   netdev_features_t features);\n *\tCalled by core transmit path to determine if device is capable of\n *\tperforming offload operations on a given packet. This is to give\n *\tthe device an opportunity to implement any restrictions that cannot\n *\tbe otherwise expressed by feature flags. The check is called with\n *\tthe set of features that the stack has calculated and it returns\n *\tthose the driver believes to be appropriate.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,\n *                         struct net_device *sb_dev);\n *\tCalled to decide which queue to use when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscuous is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\tIf driver handles unicast address filtering, it should set\n *\tIFF_UNICAST_FLT in its priv_flags.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tMAC address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user requests an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctls return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reasons; new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev, unsigned int txqueue);\n *\tCallback used when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * void (*ndo_get_stats64)(struct net_device *dev,\n *                         struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * bool (*ndo_has_offload_stats)(const struct net_device *dev, int attr_id)\n *\tReturn true if this device supports offload stats of this attr_id.\n *\n * int (*ndo_get_offload_stats)(int attr_id, const struct net_device *dev,\n *\tvoid *attr_data)\n *\tGet statistics for offload operations by attr_id. Write it into the\n *\tattr_data pointer.\n *\n * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device supports VLAN filtering this function is called when a\n *\tVLAN id is registered.\n *\n * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device supports VLAN filtering this function is called when a\n *\tVLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan,\n *\t\t\t  u8 qos, __be16 proto);\n * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,\n *\t\t\t  int max_tx_rate);\n * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n *\n *      Enable or disable the VF ability to query its RSS Redirection Table and\n *      Hash Key. This is needed since on some devices VF share this information\n *      with PF and querying it may introduce a theoretical security risk.\n * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n * int (*ndo_setup_tc)(struct net_device *dev, enum tc_setup_type type,\n *\t\t       void *type_data);\n *\tCalled to setup any 'tc' scheduler, classifier or action on @dev.\n *\tThis is always called from the stack with the rtnl lock held and netif\n *\ttx queues stopped. This allows the netdevice to perform queue\n *\tmanagement safely.\n *\n *\tFiber Channel over Ethernet (FCoE) offload functions.\n * int (*ndo_fcoe_enable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to start using LLD for FCoE\n *\tso the underlying device can perform whatever needed configuration or\n *\tinitialization to support acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_disable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to stop using LLD for FCoE\n *\tso the underlying device can perform whatever needed clean-ups to\n *\tstop supporting acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,\n *\t\t\t     struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Initiator wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);\n *\tCalled when the FCoE Initiator/Target is done with the DDPed I/O as\n *\tindicated by the FC exchange id 'xid', so the underlying device can\n *\tclean up and reuse resources for later DDP requests.\n *\n * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,\n *\t\t\t      struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Target wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n *\t\t\t       struct netdev_fcoe_hbainfo *hbainfo);\n *\tCalled when the FCoE Protocol stack wants information on the underlying\n *\tdevice. This information is utilized by the FCoE protocol stack to\n *\tregister attributes with Fiber Channel management service as per the\n *\tFC-GS Fabric Device Management Information(FDMI) specification.\n *\n * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);\n *\tCalled when the underlying device wants to override default World Wide\n *\tName (WWN) generation mechanism in FCoE protocol stack to pass its own\n *\tWorld Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE\n *\tprotocol stack to use.\n *\n *\tRFS acceleration.\n * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,\n *\t\t\t    u16 rxq_index, u32 flow_id);\n *\tSet hardware filter for RFS.  rxq_index is the target queue index;\n *\tflow_id is a flow ID to be passed to rps_may_expire_flow() later.\n *\tReturn the filter ID on success, or a negative error code.\n *\n *\tSlave management functions (for bridge, bonding, etc).\n * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to make another netdev an underling.\n *\n * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to release previously enslaved netdev.\n *\n * struct net_device *(*ndo_get_xmit_slave)(struct net_device *dev,\n *\t\t\t\t\t    struct sk_buff *skb,\n *\t\t\t\t\t    bool all_slaves);\n *\tGet the xmit slave of master device. If all_slaves is true, function\n *\tassume all the slaves can transmit.\n *\n *      Feature/offload setting functions.\n * netdev_features_t (*ndo_fix_features)(struct net_device *dev,\n *\t\tnetdev_features_t features);\n *\tAdjusts the requested feature flags according to device-specific\n *\tconstraints, and returns the resulting flags. Must not modify\n *\tthe device state.\n *\n * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);\n *\tCalled to update device configuration to new features. Passed\n *\tfeature set might be less than what was returned by ndo_fix_features()).\n *\tMust return >0 or -errno if it changed dev->features itself.\n *\n * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid, u16 flags,\n *\t\t      struct netlink_ext_ack *extack);\n *\tAdds an FDB entry to dev for addr.\n * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid)\n *\tDeletes the FDB entry from dev coresponding to addr.\n * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,\n *\t\t       struct net_device *dev, struct net_device *filter_dev,\n *\t\t       int *idx)\n *\tUsed to add FDB entries to dump requests. Implementers should add\n *\tentries to skb and update idx with the number of entries.\n *\n * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags, struct netlink_ext_ack *extack)\n * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,\n *\t\t\t     struct net_device *dev, u32 filter_mask,\n *\t\t\t     int nlflags)\n * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags);\n *\n * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);\n *\tCalled to change device carrier. Soft-devices (like dummy, team, etc)\n *\twhich do not represent real hardware may define this to allow their\n *\tuserspace components to manage their virtual carrier state. Devices\n *\tthat determine carrier state from physical hardware properties (eg\n *\tnetwork cables) or protocol-dependent mechanisms (eg\n *\tUSB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.\n *\n * int (*ndo_get_phys_port_id)(struct net_device *dev,\n *\t\t\t       struct netdev_phys_item_id *ppid);\n *\tCalled to get ID of physical port of this device. If driver does\n *\tnot implement this, it is assumed that the hw is not able to have\n *\tmultiple net devices on single physical port.\n *\n * int (*ndo_get_port_parent_id)(struct net_device *dev,\n *\t\t\t\t struct netdev_phys_item_id *ppid)\n *\tCalled to get the parent ID of the physical port of this device.\n *\n * void* (*ndo_dfwd_add_station)(struct net_device *pdev,\n *\t\t\t\t struct net_device *dev)\n *\tCalled by upper layer devices to accelerate switching or other\n *\tstation functionality into hardware. 'pdev is the lowerdev\n *\tto use for the offload and 'dev' is the net device that will\n *\tback the offload. Returns a pointer to the private structure\n *\tthe upper layer will maintain.\n * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)\n *\tCalled by upper layer device to delete the station created\n *\tby 'ndo_dfwd_add_station'. 'pdev' is the net device backing\n *\tthe station and priv is the structure returned by the add\n *\toperation.\n * int (*ndo_set_tx_maxrate)(struct net_device *dev,\n *\t\t\t     int queue_index, u32 maxrate);\n *\tCalled when a user wants to set a max-rate limitation of specific\n *\tTX queue.\n * int (*ndo_get_iflink)(const struct net_device *dev);\n *\tCalled to get the iflink value of this device.\n * void (*ndo_change_proto_down)(struct net_device *dev,\n *\t\t\t\t bool proto_down);\n *\tThis function is used to pass protocol port error state information\n *\tto the switch driver. The switch driver can react to the proto_down\n *      by doing a phys down on the associated switch port.\n * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);\n *\tThis function is used to get egress tunnel information for given skb.\n *\tThis is useful for retrieving outer tunnel header parameters while\n *\tsampling packet.\n * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);\n *\tThis function is used to specify the headroom that the skb must\n *\tconsider when allocation skb during packet reception. Setting\n *\tappropriate rx headroom value allows avoiding skb head copy on\n *\tforward. Setting a negative value resets the rx headroom to the\n *\tdefault value.\n * int (*ndo_bpf)(struct net_device *dev, struct netdev_bpf *bpf);\n *\tThis function is used to set or query state related to XDP on the\n *\tnetdevice and manage BPF offload. See definition of\n *\tenum bpf_netdev_command for details.\n * int (*ndo_xdp_xmit)(struct net_device *dev, int n, struct xdp_frame **xdp,\n *\t\t\tu32 flags);\n *\tThis function is used to submit @n XDP packets for transmit on a\n *\tnetdevice. Returns number of frames successfully transmitted, frames\n *\tthat got dropped are freed/returned via xdp_return_frame().\n *\tReturns negative number, means general error invoking ndo, meaning\n *\tno frames were xmit'ed and core-caller will free all frames.\n * int (*ndo_xsk_wakeup)(struct net_device *dev, u32 queue_id, u32 flags);\n *      This function is used to wake up the softirq, ksoftirqd or kthread\n *\tresponsible for sending and/or receiving packets on a specific\n *\tqueue id bound to an AF_XDP socket. The flags field specifies if\n *\tonly RX, only Tx, or both should be woken up using the flags\n *\tXDP_WAKEUP_RX and XDP_WAKEUP_TX.\n * struct devlink_port *(*ndo_get_devlink_port)(struct net_device *dev);\n *\tGet devlink port instance associated with a given netdev.\n *\tCalled with a reference on the netdevice and devlink locks only,\n *\trtnl_lock is not held.\n * int (*ndo_tunnel_ctl)(struct net_device *dev, struct ip_tunnel_parm *p,\n *\t\t\t int cmd);\n *\tAdd, change, delete or get information on an IPv4 tunnel.\n * struct net_device *(*ndo_get_peer_dev)(struct net_device *dev);\n *\tIf a device is paired with a peer device, return the peer instance.\n *\tThe caller must be under RCU read context.\n */\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit)(struct sk_buff *skb,\n\t\t\t\t\t\t  struct net_device *dev);\n\tnetdev_features_t\t(*ndo_features_check)(struct sk_buff *skb,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      netdev_features_t features);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct net_device *sb_dev);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev,\n\t\t\t\t\t\t   unsigned int txqueue);\n\n\tvoid\t\t\t(*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t   struct rtnl_link_stats64 *storage);\n\tbool\t\t\t(*ndo_has_offload_stats)(const struct net_device *dev, int attr_id);\n\tint\t\t\t(*ndo_get_offload_stats)(int attr_id,\n\t\t\t\t\t\t\t const struct net_device *dev,\n\t\t\t\t\t\t\t void *attr_data);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tint\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       __be16 proto, u16 vid);\n\tint\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        __be16 proto, u16 vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan,\n\t\t\t\t\t\t   u8 qos, __be16 proto);\n\tint\t\t\t(*ndo_set_vf_rate)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, int min_tx_rate,\n\t\t\t\t\t\t   int max_tx_rate);\n\tint\t\t\t(*ndo_set_vf_spoofchk)(struct net_device *dev,\n\t\t\t\t\t\t       int vf, bool setting);\n\tint\t\t\t(*ndo_set_vf_trust)(struct net_device *dev,\n\t\t\t\t\t\t    int vf, bool setting);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_link_state)(struct net_device *dev,\n\t\t\t\t\t\t\t int vf, int link_state);\n\tint\t\t\t(*ndo_get_vf_stats)(struct net_device *dev,\n\t\t\t\t\t\t    int vf,\n\t\t\t\t\t\t    struct ifla_vf_stats\n\t\t\t\t\t\t    *vf_stats);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n\tint\t\t\t(*ndo_get_vf_guid)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct ifla_vf_guid *node_guid,\n\t\t\t\t\t\t   struct ifla_vf_guid *port_guid);\n\tint\t\t\t(*ndo_set_vf_guid)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, u64 guid,\n\t\t\t\t\t\t   int guid_type);\n\tint\t\t\t(*ndo_set_vf_rss_query_en)(\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   int vf, bool setting);\n\tint\t\t\t(*ndo_setup_tc)(struct net_device *dev,\n\t\t\t\t\t\tenum tc_setup_type type,\n\t\t\t\t\t\tvoid *type_data);\n#if IS_ENABLED(CONFIG_FCOE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n\tint\t\t\t(*ndo_fcoe_ddp_target)(struct net_device *dev,\n\t\t\t\t\t\t       u16 xid,\n\t\t\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t\t\t       unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_fcoe_hbainfo *hbainfo);\n#endif\n\n#if IS_ENABLED(CONFIG_LIBFCOE)\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n\n#ifdef CONFIG_RFS_ACCEL\n\tint\t\t\t(*ndo_rx_flow_steer)(struct net_device *dev,\n\t\t\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t\t\t     u16 rxq_index,\n\t\t\t\t\t\t     u32 flow_id);\n#endif\n\tint\t\t\t(*ndo_add_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev,\n\t\t\t\t\t\t struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_del_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tstruct net_device*\t(*ndo_get_xmit_slave)(struct net_device *dev,\n\t\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t\t      bool all_slaves);\n\tstruct net_device*\t(*ndo_sk_get_lower_dev)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct sock *sk);\n\tnetdev_features_t\t(*ndo_fix_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_set_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_neigh_construct)(struct net_device *dev,\n\t\t\t\t\t\t       struct neighbour *n);\n\tvoid\t\t\t(*ndo_neigh_destroy)(struct net_device *dev,\n\t\t\t\t\t\t     struct neighbour *n);\n\n\tint\t\t\t(*ndo_fdb_add)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid,\n\t\t\t\t\t       u16 flags,\n\t\t\t\t\t       struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_fdb_del)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid);\n\tint\t\t\t(*ndo_fdb_dump)(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\tstruct net_device *filter_dev,\n\t\t\t\t\t\tint *idx);\n\tint\t\t\t(*ndo_fdb_get)(struct sk_buff *skb,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid, u32 portid, u32 seq,\n\t\t\t\t\t       struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_bridge_setlink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags,\n\t\t\t\t\t\t      struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_bridge_getlink)(struct sk_buff *skb,\n\t\t\t\t\t\t      u32 pid, u32 seq,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      u32 filter_mask,\n\t\t\t\t\t\t      int nlflags);\n\tint\t\t\t(*ndo_bridge_dellink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_change_carrier)(struct net_device *dev,\n\t\t\t\t\t\t      bool new_carrier);\n\tint\t\t\t(*ndo_get_phys_port_id)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_port_parent_id)(struct net_device *dev,\n\t\t\t\t\t\t\t  struct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_phys_port_name)(struct net_device *dev,\n\t\t\t\t\t\t\t  char *name, size_t len);\n\tvoid*\t\t\t(*ndo_dfwd_add_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tstruct net_device *dev);\n\tvoid\t\t\t(*ndo_dfwd_del_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tvoid *priv);\n\n\tint\t\t\t(*ndo_set_tx_maxrate)(struct net_device *dev,\n\t\t\t\t\t\t      int queue_index,\n\t\t\t\t\t\t      u32 maxrate);\n\tint\t\t\t(*ndo_get_iflink)(const struct net_device *dev);\n\tint\t\t\t(*ndo_change_proto_down)(struct net_device *dev,\n\t\t\t\t\t\t\t bool proto_down);\n\tint\t\t\t(*ndo_fill_metadata_dst)(struct net_device *dev,\n\t\t\t\t\t\t       struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_set_rx_headroom)(struct net_device *dev,\n\t\t\t\t\t\t       int needed_headroom);\n\tint\t\t\t(*ndo_bpf)(struct net_device *dev,\n\t\t\t\t\t   struct netdev_bpf *bpf);\n\tint\t\t\t(*ndo_xdp_xmit)(struct net_device *dev, int n,\n\t\t\t\t\t\tstruct xdp_frame **xdp,\n\t\t\t\t\t\tu32 flags);\n\tint\t\t\t(*ndo_xsk_wakeup)(struct net_device *dev,\n\t\t\t\t\t\t  u32 queue_id, u32 flags);\n\tstruct devlink_port *\t(*ndo_get_devlink_port)(struct net_device *dev);\n\tint\t\t\t(*ndo_tunnel_ctl)(struct net_device *dev,\n\t\t\t\t\t\t  struct ip_tunnel_parm *p, int cmd);\n\tstruct net_device *\t(*ndo_get_peer_dev)(struct net_device *dev);\n};\n\n/**\n * enum netdev_priv_flags - &struct net_device priv_flags\n *\n * These are the &struct net_device, they are only set internally\n * by drivers and used in the kernel. These flags are invisible to\n * userspace; this means that the order of these flags can change\n * during any kernel release.\n *\n * You should have a pretty good reason to be extending these flags.\n *\n * @IFF_802_1Q_VLAN: 802.1Q VLAN device\n * @IFF_EBRIDGE: Ethernet bridging device\n * @IFF_BONDING: bonding master or slave\n * @IFF_ISATAP: ISATAP interface (RFC4214)\n * @IFF_WAN_HDLC: WAN HDLC device\n * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to\n *\trelease skb->dst\n * @IFF_DONT_BRIDGE: disallow bridging this ether dev\n * @IFF_DISABLE_NETPOLL: disable netpoll at run-time\n * @IFF_MACVLAN_PORT: device used as macvlan port\n * @IFF_BRIDGE_PORT: device used as bridge port\n * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port\n * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit\n * @IFF_UNICAST_FLT: Supports unicast filtering\n * @IFF_TEAM_PORT: device used as team port\n * @IFF_SUPP_NOFCS: device supports sending custom FCS\n * @IFF_LIVE_ADDR_CHANGE: device supports hardware address\n *\tchange when it's running\n * @IFF_MACVLAN: Macvlan device\n * @IFF_XMIT_DST_RELEASE_PERM: IFF_XMIT_DST_RELEASE not taking into account\n *\tunderlying stacked devices\n * @IFF_L3MDEV_MASTER: device is an L3 master device\n * @IFF_NO_QUEUE: device can run without qdisc attached\n * @IFF_OPENVSWITCH: device is a Open vSwitch master\n * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device\n * @IFF_TEAM: device is a team device\n * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured\n * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external\n *\tentity (i.e. the master device for bridged veth)\n * @IFF_MACSEC: device is a MACsec device\n * @IFF_NO_RX_HANDLER: device doesn't support the rx_handler hook\n * @IFF_FAILOVER: device is a failover master device\n * @IFF_FAILOVER_SLAVE: device is lower dev of a failover master device\n * @IFF_L3MDEV_RX_HANDLER: only invoke the rx handler of L3 master device\n * @IFF_LIVE_RENAME_OK: rename is allowed while device is up and running\n * @IFF_TX_SKB_NO_LINEAR: device/driver is capable of xmitting frames with\n *\tskb_headlen(skb) == 0 (data starts from frag0)\n */\nenum netdev_priv_flags {\n\tIFF_802_1Q_VLAN\t\t\t= 1<<0,\n\tIFF_EBRIDGE\t\t\t= 1<<1,\n\tIFF_BONDING\t\t\t= 1<<2,\n\tIFF_ISATAP\t\t\t= 1<<3,\n\tIFF_WAN_HDLC\t\t\t= 1<<4,\n\tIFF_XMIT_DST_RELEASE\t\t= 1<<5,\n\tIFF_DONT_BRIDGE\t\t\t= 1<<6,\n\tIFF_DISABLE_NETPOLL\t\t= 1<<7,\n\tIFF_MACVLAN_PORT\t\t= 1<<8,\n\tIFF_BRIDGE_PORT\t\t\t= 1<<9,\n\tIFF_OVS_DATAPATH\t\t= 1<<10,\n\tIFF_TX_SKB_SHARING\t\t= 1<<11,\n\tIFF_UNICAST_FLT\t\t\t= 1<<12,\n\tIFF_TEAM_PORT\t\t\t= 1<<13,\n\tIFF_SUPP_NOFCS\t\t\t= 1<<14,\n\tIFF_LIVE_ADDR_CHANGE\t\t= 1<<15,\n\tIFF_MACVLAN\t\t\t= 1<<16,\n\tIFF_XMIT_DST_RELEASE_PERM\t= 1<<17,\n\tIFF_L3MDEV_MASTER\t\t= 1<<18,\n\tIFF_NO_QUEUE\t\t\t= 1<<19,\n\tIFF_OPENVSWITCH\t\t\t= 1<<20,\n\tIFF_L3MDEV_SLAVE\t\t= 1<<21,\n\tIFF_TEAM\t\t\t= 1<<22,\n\tIFF_RXFH_CONFIGURED\t\t= 1<<23,\n\tIFF_PHONY_HEADROOM\t\t= 1<<24,\n\tIFF_MACSEC\t\t\t= 1<<25,\n\tIFF_NO_RX_HANDLER\t\t= 1<<26,\n\tIFF_FAILOVER\t\t\t= 1<<27,\n\tIFF_FAILOVER_SLAVE\t\t= 1<<28,\n\tIFF_L3MDEV_RX_HANDLER\t\t= 1<<29,\n\tIFF_LIVE_RENAME_OK\t\t= 1<<30,\n\tIFF_TX_SKB_NO_LINEAR\t\t= 1<<31,\n};\n\n#define IFF_802_1Q_VLAN\t\t\tIFF_802_1Q_VLAN\n#define IFF_EBRIDGE\t\t\tIFF_EBRIDGE\n#define IFF_BONDING\t\t\tIFF_BONDING\n#define IFF_ISATAP\t\t\tIFF_ISATAP\n#define IFF_WAN_HDLC\t\t\tIFF_WAN_HDLC\n#define IFF_XMIT_DST_RELEASE\t\tIFF_XMIT_DST_RELEASE\n#define IFF_DONT_BRIDGE\t\t\tIFF_DONT_BRIDGE\n#define IFF_DISABLE_NETPOLL\t\tIFF_DISABLE_NETPOLL\n#define IFF_MACVLAN_PORT\t\tIFF_MACVLAN_PORT\n#define IFF_BRIDGE_PORT\t\t\tIFF_BRIDGE_PORT\n#define IFF_OVS_DATAPATH\t\tIFF_OVS_DATAPATH\n#define IFF_TX_SKB_SHARING\t\tIFF_TX_SKB_SHARING\n#define IFF_UNICAST_FLT\t\t\tIFF_UNICAST_FLT\n#define IFF_TEAM_PORT\t\t\tIFF_TEAM_PORT\n#define IFF_SUPP_NOFCS\t\t\tIFF_SUPP_NOFCS\n#define IFF_LIVE_ADDR_CHANGE\t\tIFF_LIVE_ADDR_CHANGE\n#define IFF_MACVLAN\t\t\tIFF_MACVLAN\n#define IFF_XMIT_DST_RELEASE_PERM\tIFF_XMIT_DST_RELEASE_PERM\n#define IFF_L3MDEV_MASTER\t\tIFF_L3MDEV_MASTER\n#define IFF_NO_QUEUE\t\t\tIFF_NO_QUEUE\n#define IFF_OPENVSWITCH\t\t\tIFF_OPENVSWITCH\n#define IFF_L3MDEV_SLAVE\t\tIFF_L3MDEV_SLAVE\n#define IFF_TEAM\t\t\tIFF_TEAM\n#define IFF_RXFH_CONFIGURED\t\tIFF_RXFH_CONFIGURED\n#define IFF_PHONY_HEADROOM\t\tIFF_PHONY_HEADROOM\n#define IFF_MACSEC\t\t\tIFF_MACSEC\n#define IFF_NO_RX_HANDLER\t\tIFF_NO_RX_HANDLER\n#define IFF_FAILOVER\t\t\tIFF_FAILOVER\n#define IFF_FAILOVER_SLAVE\t\tIFF_FAILOVER_SLAVE\n#define IFF_L3MDEV_RX_HANDLER\t\tIFF_L3MDEV_RX_HANDLER\n#define IFF_LIVE_RENAME_OK\t\tIFF_LIVE_RENAME_OK\n#define IFF_TX_SKB_NO_LINEAR\t\tIFF_TX_SKB_NO_LINEAR\n\n/* Specifies the type of the struct net_device::ml_priv pointer */\nenum netdev_ml_priv_type {\n\tML_PRIV_NONE,\n\tML_PRIV_CAN,\n};\n\n/**\n *\tstruct net_device - The DEVICE structure.\n *\n *\tActually, this whole structure is a big mistake.  It mixes I/O\n *\tdata with strictly \"high-level\" data, and it has to know about\n *\talmost every data structure used in the INET module.\n *\n *\t@name:\tThis is the first field of the \"visible\" part of this structure\n *\t\t(i.e. as seen by users in the \"Space.c\" file).  It is the name\n *\t\tof the interface.\n *\n *\t@name_node:\tName hashlist node\n *\t@ifalias:\tSNMP alias\n *\t@mem_end:\tShared memory end\n *\t@mem_start:\tShared memory start\n *\t@base_addr:\tDevice I/O address\n *\t@irq:\t\tDevice IRQ number\n *\n *\t@state:\t\tGeneric network queuing layer state, see netdev_state_t\n *\t@dev_list:\tThe global list of network devices\n *\t@napi_list:\tList entry used for polling NAPI devices\n *\t@unreg_list:\tList entry  when we are unregistering the\n *\t\t\tdevice; see the function unregister_netdev\n *\t@close_list:\tList entry used when we are closing the device\n *\t@ptype_all:     Device-specific packet handlers for all protocols\n *\t@ptype_specific: Device-specific, protocol-specific packet handlers\n *\n *\t@adj_list:\tDirectly linked devices, like slaves for bonding\n *\t@features:\tCurrently active device features\n *\t@hw_features:\tUser-changeable features\n *\n *\t@wanted_features:\tUser-requested features\n *\t@vlan_features:\t\tMask of features inheritable by VLAN devices\n *\n *\t@hw_enc_features:\tMask of features inherited by encapsulating devices\n *\t\t\t\tThis field indicates what encapsulation\n *\t\t\t\toffloads the hardware is capable of doing,\n *\t\t\t\tand drivers will need to set them appropriately.\n *\n *\t@mpls_features:\tMask of features inheritable by MPLS\n *\t@gso_partial_features: value(s) from NETIF_F_GSO\\*\n *\n *\t@ifindex:\tinterface index\n *\t@group:\t\tThe group the device belongs to\n *\n *\t@stats:\t\tStatistics struct, which was left as a legacy, use\n *\t\t\trtnl_link_stats64 instead\n *\n *\t@rx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@tx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@rx_nohandler:\tnohandler dropped packets by core network on\n *\t\t\tinactive devices, do not use this in drivers\n *\t@carrier_up_count:\tNumber of times the carrier has been up\n *\t@carrier_down_count:\tNumber of times the carrier has been down\n *\n *\t@wireless_handlers:\tList of functions to handle Wireless Extensions,\n *\t\t\t\tinstead of ioctl,\n *\t\t\t\tsee <net/iw_handler.h> for details.\n *\t@wireless_data:\tInstance data managed by the core of wireless extensions\n *\n *\t@netdev_ops:\tIncludes several pointers to callbacks,\n *\t\t\tif one wants to override the ndo_*() functions\n *\t@ethtool_ops:\tManagement operations\n *\t@l3mdev_ops:\tLayer 3 master device operations\n *\t@ndisc_ops:\tIncludes callbacks for different IPv6 neighbour\n *\t\t\tdiscovery handling. Necessary for e.g. 6LoWPAN.\n *\t@xfrmdev_ops:\tTransformation offload operations\n *\t@tlsdev_ops:\tTransport Layer Security offload operations\n *\t@header_ops:\tIncludes callbacks for creating,parsing,caching,etc\n *\t\t\tof Layer 2 headers.\n *\n *\t@flags:\t\tInterface flags (a la BSD)\n *\t@priv_flags:\tLike 'flags' but invisible to userspace,\n *\t\t\tsee if.h for the definitions\n *\t@gflags:\tGlobal flags ( kept as legacy )\n *\t@padded:\tHow much padding added by alloc_netdev()\n *\t@operstate:\tRFC2863 operstate\n *\t@link_mode:\tMapping policy to operstate\n *\t@if_port:\tSelectable AUI, TP, ...\n *\t@dma:\t\tDMA channel\n *\t@mtu:\t\tInterface MTU value\n *\t@min_mtu:\tInterface Minimum MTU value\n *\t@max_mtu:\tInterface Maximum MTU value\n *\t@type:\t\tInterface hardware type\n *\t@hard_header_len: Maximum hardware header length.\n *\t@min_header_len:  Minimum hardware header length\n *\n *\t@needed_headroom: Extra headroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed\n *\t@needed_tailroom: Extra tailroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed. Some cases also use\n *\t\t\t  LL_MAX_HEADER instead to allocate the skb\n *\n *\tinterface address info:\n *\n * \t@perm_addr:\t\tPermanent hw address\n * \t@addr_assign_type:\tHw address assignment type\n * \t@addr_len:\t\tHardware address length\n *\t@upper_level:\t\tMaximum depth level of upper devices.\n *\t@lower_level:\t\tMaximum depth level of lower devices.\n *\t@neigh_priv_len:\tUsed in neigh_alloc()\n * \t@dev_id:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same link layer address\n * \t@dev_port:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same function\n *\t@addr_list_lock:\tXXX: need comments on this one\n *\t@name_assign_type:\tnetwork interface name assignment type\n *\t@uc_promisc:\t\tCounter that indicates promiscuous mode\n *\t\t\t\thas been enabled due to the need to listen to\n *\t\t\t\tadditional unicast addresses in a device that\n *\t\t\t\tdoes not implement ndo_set_rx_mode()\n *\t@uc:\t\t\tunicast mac addresses\n *\t@mc:\t\t\tmulticast mac addresses\n *\t@dev_addrs:\t\tlist of device hw addresses\n *\t@queues_kset:\t\tGroup of all Kobjects in the Tx and RX queues\n *\t@promiscuity:\t\tNumber of times the NIC is told to work in\n *\t\t\t\tpromiscuous mode; if it becomes 0 the NIC will\n *\t\t\t\texit promiscuous mode\n *\t@allmulti:\t\tCounter, enables or disables allmulticast mode\n *\n *\t@vlan_info:\tVLAN info\n *\t@dsa_ptr:\tdsa specific data\n *\t@tipc_ptr:\tTIPC specific data\n *\t@atalk_ptr:\tAppleTalk link\n *\t@ip_ptr:\tIPv4 specific data\n *\t@dn_ptr:\tDECnet specific data\n *\t@ip6_ptr:\tIPv6 specific data\n *\t@ax25_ptr:\tAX.25 specific data\n *\t@ieee80211_ptr:\tIEEE 802.11 specific data, assign before registering\n *\t@ieee802154_ptr: IEEE 802.15.4 low-rate Wireless Personal Area Network\n *\t\t\t device struct\n *\t@mpls_ptr:\tmpls_dev struct pointer\n *\n *\t@dev_addr:\tHw address (before bcast,\n *\t\t\tbecause most packets are unicast)\n *\n *\t@_rx:\t\t\tArray of RX queues\n *\t@num_rx_queues:\t\tNumber of RX queues\n *\t\t\t\tallocated at register_netdev() time\n *\t@real_num_rx_queues: \tNumber of RX queues currently active in device\n *\t@xdp_prog:\t\tXDP sockets filter program pointer\n *\t@gro_flush_timeout:\ttimeout for GRO layer in NAPI\n *\t@napi_defer_hard_irqs:\tIf not zero, provides a counter that would\n *\t\t\t\tallow to avoid NIC hard IRQ, on busy queues.\n *\n *\t@rx_handler:\t\thandler for received packets\n *\t@rx_handler_data: \tXXX: need comments on this one\n *\t@miniq_ingress:\t\tingress/clsact qdisc specific data for\n *\t\t\t\tingress processing\n *\t@ingress_queue:\t\tXXX: need comments on this one\n *\t@nf_hooks_ingress:\tnetfilter hooks executed for ingress packets\n *\t@broadcast:\t\thw bcast address\n *\n *\t@rx_cpu_rmap:\tCPU reverse-mapping for RX completion interrupts,\n *\t\t\tindexed by RX queue number. Assigned by driver.\n *\t\t\tThis must only be set if the ndo_rx_flow_steer\n *\t\t\toperation is defined\n *\t@index_hlist:\t\tDevice index hash chain\n *\n *\t@_tx:\t\t\tArray of TX queues\n *\t@num_tx_queues:\t\tNumber of TX queues allocated at alloc_netdev_mq() time\n *\t@real_num_tx_queues: \tNumber of TX queues currently active in device\n *\t@qdisc:\t\t\tRoot qdisc from userspace point of view\n *\t@tx_queue_len:\t\tMax frames per queue allowed\n *\t@tx_global_lock: \tXXX: need comments on this one\n *\t@xdp_bulkq:\t\tXDP device bulk queue\n *\t@xps_maps:\t\tall CPUs/RXQs maps for XPS device\n *\n *\t@xps_maps:\tXXX: need comments on this one\n *\t@miniq_egress:\t\tclsact qdisc specific data for\n *\t\t\t\tegress processing\n *\t@qdisc_hash:\t\tqdisc hash table\n *\t@watchdog_timeo:\tRepresents the timeout that is used by\n *\t\t\t\tthe watchdog (see dev_watchdog())\n *\t@watchdog_timer:\tList of timers\n *\n *\t@proto_down_reason:\treason a netdev interface is held down\n *\t@pcpu_refcnt:\t\tNumber of references to this device\n *\t@dev_refcnt:\t\tNumber of references to this device\n *\t@todo_list:\t\tDelayed register/unregister\n *\t@link_watch_list:\tXXX: need comments on this one\n *\n *\t@reg_state:\t\tRegister/unregister state machine\n *\t@dismantle:\t\tDevice is going to be freed\n *\t@rtnl_link_state:\tThis enum represents the phases of creating\n *\t\t\t\ta new link\n *\n *\t@needs_free_netdev:\tShould unregister perform free_netdev?\n *\t@priv_destructor:\tCalled from unregister\n *\t@npinfo:\t\tXXX: need comments on this one\n * \t@nd_net:\t\tNetwork namespace this network device is inside\n *\n * \t@ml_priv:\tMid-layer private\n *\t@ml_priv_type:  Mid-layer private type\n * \t@lstats:\tLoopback statistics\n * \t@tstats:\tTunnel statistics\n * \t@dstats:\tDummy statistics\n * \t@vstats:\tVirtual ethernet statistics\n *\n *\t@garp_port:\tGARP\n *\t@mrp_port:\tMRP\n *\n *\t@dev:\t\tClass/net/name entry\n *\t@sysfs_groups:\tSpace for optional device, statistics and wireless\n *\t\t\tsysfs groups\n *\n *\t@sysfs_rx_queue_group:\tSpace for optional per-rx queue attributes\n *\t@rtnl_link_ops:\tRtnl_link_ops\n *\n *\t@gso_max_size:\tMaximum size of generic segmentation offload\n *\t@gso_max_segs:\tMaximum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\n *\t@dcbnl_ops:\tData Center Bridging netlink ops\n *\t@num_tc:\tNumber of traffic classes in the net device\n *\t@tc_to_txq:\tXXX: need comments on this one\n *\t@prio_tc_map:\tXXX: need comments on this one\n *\n *\t@fcoe_ddp_xid:\tMax exchange id for FCoE LRO by ddp\n *\n *\t@priomap:\tXXX: need comments on this one\n *\t@phydev:\tPhysical device may attach itself\n *\t\t\tfor hardware timestamping\n *\t@sfp_bus:\tattached &struct sfp_bus structure.\n *\n *\t@qdisc_tx_busylock: lockdep class annotating Qdisc->busylock spinlock\n *\t@qdisc_running_key: lockdep class annotating Qdisc->running seqcount\n *\n *\t@proto_down:\tprotocol port state information can be sent to the\n *\t\t\tswitch driver and used to set the phys state of the\n *\t\t\tswitch port.\n *\n *\t@wol_enabled:\tWake-on-LAN is enabled\n *\n *\t@threaded:\tnapi threaded mode is enabled\n *\n *\t@net_notifier_list:\tList of per-net netdev notifier block\n *\t\t\t\tthat follow this device when it is moved\n *\t\t\t\tto another network namespace.\n *\n *\t@macsec_ops:    MACsec offloading ops\n *\n *\t@udp_tunnel_nic_info:\tstatic structure describing the UDP tunnel\n *\t\t\t\toffload capabilities of the device\n *\t@udp_tunnel_nic:\tUDP tunnel offload state\n *\t@xdp_state:\t\tstores info on attached XDP BPF programs\n *\n *\t@nested_level:\tUsed as as a parameter of spin_lock_nested() of\n *\t\t\tdev->addr_list_lock.\n *\t@unlink_list:\tAs netif_addr_lock() can be called recursively,\n *\t\t\tkeep a list of interfaces to be deleted.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\tchar\t\t\tname[IFNAMSIZ];\n\tstruct netdev_name_node\t*name_node;\n\tstruct dev_ifalias\t__rcu *ifalias;\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\n\tunsigned long\t\tmem_start;\n\tunsigned long\t\tbase_addr;\n\n\t/*\n\t *\tSome hardware also needs these fields (state,dev_list,\n\t *\tnapi_list,unreg_list,close_list) but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\tstruct list_head\tclose_list;\n\tstruct list_head\tptype_all;\n\tstruct list_head\tptype_specific;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} adj_list;\n\n\t/* Read-mostly cache-line for fast-path access */\n\tunsigned int\t\tflags;\n\tunsigned int\t\tpriv_flags;\n\tconst struct net_device_ops *netdev_ops;\n\tint\t\t\tifindex;\n\tunsigned short\t\tgflags;\n\tunsigned short\t\thard_header_len;\n\n\t/* Note : dev->mtu is often read without holding a lock.\n\t * Writers usually hold RTNL.\n\t * It is recommended to use READ_ONCE() to annotate the reads,\n\t * and to use WRITE_ONCE() to annotate the writes.\n\t */\n\tunsigned int\t\tmtu;\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\tnetdev_features_t\tfeatures;\n\tnetdev_features_t\thw_features;\n\tnetdev_features_t\twanted_features;\n\tnetdev_features_t\tvlan_features;\n\tnetdev_features_t\thw_enc_features;\n\tnetdev_features_t\tmpls_features;\n\tnetdev_features_t\tgso_partial_features;\n\n\tunsigned int\t\tmin_mtu;\n\tunsigned int\t\tmax_mtu;\n\tunsigned short\t\ttype;\n\tunsigned char\t\tmin_header_len;\n\tunsigned char\t\tname_assign_type;\n\n\tint\t\t\tgroup;\n\n\tstruct net_device_stats\tstats; /* not used by modern drivers */\n\n\tatomic_long_t\t\trx_dropped;\n\tatomic_long_t\t\ttx_dropped;\n\tatomic_long_t\t\trx_nohandler;\n\n\t/* Stats to monitor link on/off, flapping */\n\tatomic_t\t\tcarrier_up_count;\n\tatomic_t\t\tcarrier_down_count;\n\n#ifdef CONFIG_WIRELESS_EXT\n\tconst struct iw_handler_def *wireless_handlers;\n\tstruct iw_public_data\t*wireless_data;\n#endif\n\tconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tconst struct l3mdev_ops\t*l3mdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\n\tconst struct ndisc_ops *ndisc_ops;\n#endif\n\n#ifdef CONFIG_XFRM_OFFLOAD\n\tconst struct xfrmdev_ops *xfrmdev_ops;\n#endif\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\n\tconst struct tlsdev_ops *tlsdev_ops;\n#endif\n\n\tconst struct header_ops *header_ops;\n\n\tunsigned char\t\toperstate;\n\tunsigned char\t\tlink_mode;\n\n\tunsigned char\t\tif_port;\n\tunsigned char\t\tdma;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN];\n\tunsigned char\t\taddr_assign_type;\n\tunsigned char\t\taddr_len;\n\tunsigned char\t\tupper_level;\n\tunsigned char\t\tlower_level;\n\n\tunsigned short\t\tneigh_priv_len;\n\tunsigned short          dev_id;\n\tunsigned short          dev_port;\n\tunsigned short\t\tpadded;\n\n\tspinlock_t\t\taddr_list_lock;\n\tint\t\t\tirq;\n\n\tstruct netdev_hw_addr_list\tuc;\n\tstruct netdev_hw_addr_list\tmc;\n\tstruct netdev_hw_addr_list\tdev_addrs;\n\n#ifdef CONFIG_SYSFS\n\tstruct kset\t\t*queues_kset;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head\tunlink_list;\n#endif\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\tbool\t\t\tuc_promisc;\n#ifdef CONFIG_LOCKDEP\n\tunsigned char\t\tnested_level;\n#endif\n\n\n\t/* Protocol-specific pointers */\n\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\n\tstruct vlan_info __rcu\t*vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tstruct dsa_port\t\t*dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\n\tstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n#if IS_ENABLED(CONFIG_IRDA) || IS_ENABLED(CONFIG_ATALK)\n\tvoid \t\t\t*atalk_ptr;\n#endif\n\tstruct in_device __rcu\t*ip_ptr;\n#if IS_ENABLED(CONFIG_DECNET)\n\tstruct dn_dev __rcu     *dn_ptr;\n#endif\n\tstruct inet6_dev __rcu\t*ip6_ptr;\n#if IS_ENABLED(CONFIG_AX25)\n\tvoid\t\t\t*ax25_ptr;\n#endif\n\tstruct wireless_dev\t*ieee80211_ptr;\n\tstruct wpan_dev\t\t*ieee802154_ptr;\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\n\tstruct mpls_dev __rcu\t*mpls_ptr;\n#endif\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\n\n\tstruct netdev_rx_queue\t*_rx;\n\tunsigned int\t\tnum_rx_queues;\n\tunsigned int\t\treal_num_rx_queues;\n\n\tstruct bpf_prog __rcu\t*xdp_prog;\n\tunsigned long\t\tgro_flush_timeout;\n\tint\t\t\tnapi_defer_hard_irqs;\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc __rcu\t*miniq_ingress;\n#endif\n\tstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\n\tstruct nf_hook_entries __rcu *nf_hooks_ingress;\n#endif\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap\t\t*rx_cpu_rmap;\n#endif\n\tstruct hlist_node\tindex_hlist;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\tunsigned int\t\tnum_tx_queues;\n\tunsigned int\t\treal_num_tx_queues;\n\tstruct Qdisc\t\t*qdisc;\n\tunsigned int\t\ttx_queue_len;\n\tspinlock_t\t\ttx_global_lock;\n\n\tstruct xdp_dev_bulk_queue __percpu *xdp_bulkq;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps[XPS_MAPS_MAX];\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc __rcu\t*miniq_egress;\n#endif\n\n#ifdef CONFIG_NET_SCHED\n\tDECLARE_HASHTABLE\t(qdisc_hash, 4);\n#endif\n\t/* These may be needed for future network-power-down code. */\n\tstruct timer_list\twatchdog_timer;\n\tint\t\t\twatchdog_timeo;\n\n\tu32                     proto_down_reason;\n\n\tstruct list_head\ttodo_list;\n\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint __percpu\t\t*pcpu_refcnt;\n#else\n\trefcount_t\t\tdev_refcnt;\n#endif\n\n\tstruct list_head\tlink_watch_list;\n\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:8;\n\n\tbool dismantle;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\tbool needs_free_netdev;\n\tvoid (*priv_destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info __rcu\t*npinfo;\n#endif\n\n\tpossible_net_t\t\t\tnd_net;\n\n\t/* mid-layer private */\n\tvoid\t\t\t\t*ml_priv;\n\tenum netdev_ml_priv_type\tml_priv_type;\n\n\tunion {\n\t\tstruct pcpu_lstats __percpu\t\t*lstats;\n\t\tstruct pcpu_sw_netstats __percpu\t*tstats;\n\t\tstruct pcpu_dstats __percpu\t\t*dstats;\n\t};\n\n#if IS_ENABLED(CONFIG_GARP)\n\tstruct garp_port __rcu\t*garp_port;\n#endif\n#if IS_ENABLED(CONFIG_MRP)\n\tstruct mrp_port __rcu\t*mrp_port;\n#endif\n\n\tstruct device\t\tdev;\n\tconst struct attribute_group *sysfs_groups[4];\n\tconst struct attribute_group *sysfs_rx_queue_group;\n\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n#define GSO_MAX_SEGS\t\t65535\n\tu16\t\t\tgso_max_segs;\n\n#ifdef CONFIG_DCB\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\ts16\t\t\tnum_tc;\n\tstruct netdev_tc_txq\ttc_to_txq[TC_MAX_QUEUE];\n\tu8\t\t\tprio_tc_map[TC_BITMASK + 1];\n\n#if IS_ENABLED(CONFIG_FCOE)\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\n\tstruct netprio_map __rcu *priomap;\n#endif\n\tstruct phy_device\t*phydev;\n\tstruct sfp_bus\t\t*sfp_bus;\n\tstruct lock_class_key\t*qdisc_tx_busylock;\n\tstruct lock_class_key\t*qdisc_running_key;\n\tbool\t\t\tproto_down;\n\tunsigned\t\twol_enabled:1;\n\tunsigned\t\tthreaded:1;\n\n\tstruct list_head\tnet_notifier_list;\n\n#if IS_ENABLED(CONFIG_MACSEC)\n\t/* MACsec management functions */\n\tconst struct macsec_ops *macsec_ops;\n#endif\n\tconst struct udp_tunnel_nic_info\t*udp_tunnel_nic_info;\n\tstruct udp_tunnel_nic\t*udp_tunnel_nic;\n\n\t/* protected by rtnl_lock */\n\tstruct bpf_xdp_entity\txdp_state[__MAX_XDP_MODE];\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\nstatic inline bool netif_elide_gro(const struct net_device *dev)\n{\n\tif (!(dev->features & NETIF_F_GRO) || dev->xdp_prog)\n\t\treturn true;\n\treturn false;\n}\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nint netdev_get_prio_tc_map(const struct net_device *dev, u32 prio)\n{\n\treturn dev->prio_tc_map[prio & TC_BITMASK];\n}\n\nstatic inline\nint netdev_set_prio_tc_map(struct net_device *dev, u8 prio, u8 tc)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->prio_tc_map[prio & TC_BITMASK] = tc & TC_BITMASK;\n\treturn 0;\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq);\nvoid netdev_reset_tc(struct net_device *dev);\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset);\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc);\n\nstatic inline\nint netdev_get_num_tc(struct net_device *dev)\n{\n\treturn dev->num_tc;\n}\n\nstatic inline void net_prefetch(void *p)\n{\n\tprefetch(p);\n#if L1_CACHE_BYTES < 128\n\tprefetch((u8 *)p + L1_CACHE_BYTES);\n#endif\n}\n\nstatic inline void net_prefetchw(void *p)\n{\n\tprefetchw(p);\n#if L1_CACHE_BYTES < 128\n\tprefetchw((u8 *)p + L1_CACHE_BYTES);\n#endif\n}\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev);\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset);\nint netdev_set_sb_channel(struct net_device *dev, u16 channel);\nstatic inline int netdev_get_sb_channel(struct net_device *dev)\n{\n\treturn max_t(int, -dev->num_tc, 0);\n}\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline struct netdev_queue *skb_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t\t    const struct sk_buff *skb)\n{\n\treturn netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\n#define netdev_lockdep_set_classes(dev)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key qdisc_tx_busylock_key;\t\\\n\tstatic struct lock_class_key qdisc_running_key;\t\t\\\n\tstatic struct lock_class_key qdisc_xmit_lock_key;\t\\\n\tstatic struct lock_class_key dev_addr_list_lock_key;\t\\\n\tunsigned int i;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t(dev)->qdisc_tx_busylock = &qdisc_tx_busylock_key;\t\\\n\t(dev)->qdisc_running_key = &qdisc_running_key;\t\t\\\n\tlockdep_set_class(&(dev)->addr_list_lock,\t\t\\\n\t\t\t  &dev_addr_list_lock_key);\t\t\\\n\tfor (i = 0; i < (dev)->num_tx_queues; i++)\t\t\\\n\t\tlockdep_set_class(&(dev)->_tx[i]._xmit_lock,\t\\\n\t\t\t\t  &qdisc_xmit_lock_key);\t\\\n}\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev);\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev);\n\n/* returns the headroom that the master device needs to take in account\n * when forwarding to this dev\n */\nstatic inline unsigned netdev_get_fwd_headroom(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_PHONY_HEADROOM ? 0 : dev->needed_headroom;\n}\n\nstatic inline void netdev_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tif (dev->netdev_ops->ndo_set_rx_headroom)\n\t\tdev->netdev_ops->ndo_set_rx_headroom(dev, new_hr);\n}\n\n/* set the device rx headroom to the dev's default */\nstatic inline void netdev_reset_rx_headroom(struct net_device *dev)\n{\n\tnetdev_set_rx_headroom(dev, -1);\n}\n\nstatic inline void *netdev_get_ml_priv(struct net_device *dev,\n\t\t\t\t       enum netdev_ml_priv_type type)\n{\n\tif (dev->ml_priv_type != type)\n\t\treturn NULL;\n\n\treturn dev->ml_priv;\n}\n\nstatic inline void netdev_set_ml_priv(struct net_device *dev,\n\t\t\t\t      void *ml_priv,\n\t\t\t\t      enum netdev_ml_priv_type type)\n{\n\tWARN(dev->ml_priv_type && dev->ml_priv_type != type,\n\t     \"Overwriting already set ml_priv_type (%u) with different ml_priv_type (%u)!\\n\",\n\t     dev->ml_priv_type, type);\n\tWARN(!dev->ml_priv_type && dev->ml_priv,\n\t     \"Overwriting already set ml_priv and ml_priv_type is ML_PRIV_NONE!\\n\");\n\n\tdev->ml_priv = ml_priv;\n\tdev->ml_priv_type = type;\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n\twrite_pnet(&dev->nd_net, net);\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fine-grained identification of different network device types. For\n * example Ethernet, Wireless LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/* Default NAPI poll() weight\n * Device drivers are strongly advised to not use bigger value\n */\n#define NAPI_POLL_WEIGHT 64\n\n/**\n *\tnetif_napi_add - initialize a NAPI context\n *\t@dev:  network device\n *\t@napi: NAPI context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a NAPI context prior to calling\n * *any* of the other NAPI-related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *\tnetif_tx_napi_add - initialize a NAPI context\n *\t@dev:  network device\n *\t@napi: NAPI context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * This variant of netif_napi_add() should be used from drivers using NAPI\n * to exclusively poll a TX queue.\n * This will avoid we add it into napi_hash[], thus polluting this hash table.\n */\nstatic inline void netif_tx_napi_add(struct net_device *dev,\n\t\t\t\t     struct napi_struct *napi,\n\t\t\t\t     int (*poll)(struct napi_struct *, int),\n\t\t\t\t     int weight)\n{\n\tset_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state);\n\tnetif_napi_add(dev, napi, poll, weight);\n}\n\n/**\n *  __netif_napi_del - remove a NAPI context\n *  @napi: NAPI context\n *\n * Warning: caller must observe RCU grace period before freeing memory\n * containing @napi. Drivers might want to call this helper to combine\n * all the needed RCU grace periods into a single one.\n */\nvoid __netif_napi_del(struct napi_struct *napi);\n\n/**\n *  netif_napi_del - remove a NAPI context\n *  @napi: NAPI context\n *\n *  netif_napi_del() removes a NAPI context from the network device NAPI list\n */\nstatic inline void netif_napi_del(struct napi_struct *napi)\n{\n\t__netif_napi_del(napi);\n\tsynchronize_net();\n}\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid\t*frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint\tdata_offset;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tu16\tflush;\n\n\t/* Save the IP ID here and check when we get to the transport layer */\n\tu16\tflush_id;\n\n\t/* Number of segments aggregated. */\n\tu16\tcount;\n\n\t/* Start offset for remote checksum offload */\n\tu16\tgro_remcsum_start;\n\n\t/* jiffies when first packet was created/queued */\n\tunsigned long age;\n\n\t/* Used in ipv6_gro_receive() and foo-over-udp */\n\tu16\tproto;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tu8\tsame_flow:1;\n\n\t/* Used in tunnel GRO receive */\n\tu8\tencap_mark:1;\n\n\t/* GRO checksum is valid */\n\tu8\tcsum_valid:1;\n\n\t/* Number of checksums via CHECKSUM_UNNECESSARY */\n\tu8\tcsum_cnt:3;\n\n\t/* Free the skb? */\n\tu8\tfree:2;\n#define NAPI_GRO_FREE\t\t  1\n#define NAPI_GRO_FREE_STOLEN_HEAD 2\n\n\t/* Used in foo-over-udp, set in udp[46]_gro_receive */\n\tu8\tis_ipv6:1;\n\n\t/* Used in GRE, set in fou/gue_gro_receive */\n\tu8\tis_fou:1;\n\n\t/* Used to determine if flush_id can be ignored */\n\tu8\tis_atomic:1;\n\n\t/* Number of gro_receive callbacks this packet already went through */\n\tu8 recursion_counter:4;\n\n\t/* GRO is done by frag_list pointer chaining. */\n\tu8\tis_flist:1;\n\n\t/* used to support CHECKSUM_COMPLETE for tunneling protocols */\n\t__wsum\tcsum;\n\n\t/* used in skb_gro_receive() slow path */\n\tstruct sk_buff *last;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\n#define GRO_RECURSION_LIMIT 15\nstatic inline int gro_recursion_inc_test(struct sk_buff *skb)\n{\n\treturn ++NAPI_GRO_CB(skb)->recursion_counter == GRO_RECURSION_LIMIT;\n}\n\ntypedef struct sk_buff *(*gro_receive_t)(struct list_head *, struct sk_buff *);\nstatic inline struct sk_buff *call_gro_receive(gro_receive_t cb,\n\t\t\t\t\t       struct list_head *head,\n\t\t\t\t\t       struct sk_buff *skb)\n{\n\tif (unlikely(gro_recursion_inc_test(skb))) {\n\t\tNAPI_GRO_CB(skb)->flush |= 1;\n\t\treturn NULL;\n\t}\n\n\treturn cb(head, skb);\n}\n\ntypedef struct sk_buff *(*gro_receive_sk_t)(struct sock *, struct list_head *,\n\t\t\t\t\t    struct sk_buff *);\nstatic inline struct sk_buff *call_gro_receive_sk(gro_receive_sk_t cb,\n\t\t\t\t\t\t  struct sock *sk,\n\t\t\t\t\t\t  struct list_head *head,\n\t\t\t\t\t\t  struct sk_buff *skb)\n{\n\tif (unlikely(gro_recursion_inc_test(skb))) {\n\t\tNAPI_GRO_CB(skb)->flush |= 1;\n\t\treturn NULL;\n\t}\n\n\treturn cb(sk, head, skb);\n}\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tbool\t\t\tignore_outgoing;\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tvoid\t\t\t(*list_func) (struct list_head *,\n\t\t\t\t\t      struct packet_type *,\n\t\t\t\t\t      struct net_device *);\n\tbool\t\t\t(*id_match)(struct packet_type *ptype,\n\t\t\t\t\t    struct sock *sk);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\nstruct offload_callbacks {\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tnetdev_features_t features);\n\tstruct sk_buff\t\t*(*gro_receive)(struct list_head *head,\n\t\t\t\t\t\tstruct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb, int nhoff);\n};\n\nstruct packet_offload {\n\t__be16\t\t\t type;\t/* This is really htons(ether_type). */\n\tu16\t\t\t priority;\n\tstruct offload_callbacks callbacks;\n\tstruct list_head\t list;\n};\n\n/* often modified stats are per-CPU, other are shared (netdev->stats) */\nstruct pcpu_sw_netstats {\n\tu64     rx_packets;\n\tu64     rx_bytes;\n\tu64     tx_packets;\n\tu64     tx_bytes;\n\tstruct u64_stats_sync   syncp;\n} __aligned(4 * sizeof(u64));\n\nstruct pcpu_lstats {\n\tu64_stats_t packets;\n\tu64_stats_t bytes;\n\tstruct u64_stats_sync syncp;\n} __aligned(2 * sizeof(u64));\n\nvoid dev_lstats_read(struct net_device *dev, u64 *packets, u64 *bytes);\n\nstatic inline void dev_sw_netstats_rx_add(struct net_device *dev, unsigned int len)\n{\n\tstruct pcpu_sw_netstats *tstats = this_cpu_ptr(dev->tstats);\n\n\tu64_stats_update_begin(&tstats->syncp);\n\ttstats->rx_bytes += len;\n\ttstats->rx_packets++;\n\tu64_stats_update_end(&tstats->syncp);\n}\n\nstatic inline void dev_sw_netstats_tx_add(struct net_device *dev,\n\t\t\t\t\t  unsigned int packets,\n\t\t\t\t\t  unsigned int len)\n{\n\tstruct pcpu_sw_netstats *tstats = this_cpu_ptr(dev->tstats);\n\n\tu64_stats_update_begin(&tstats->syncp);\n\ttstats->tx_bytes += len;\n\ttstats->tx_packets += packets;\n\tu64_stats_update_end(&tstats->syncp);\n}\n\nstatic inline void dev_lstats_add(struct net_device *dev, unsigned int len)\n{\n\tstruct pcpu_lstats *lstats = this_cpu_ptr(dev->lstats);\n\n\tu64_stats_update_begin(&lstats->syncp);\n\tu64_stats_add(&lstats->bytes, len);\n\tu64_stats_inc(&lstats->packets);\n\tu64_stats_update_end(&lstats->syncp);\n}\n\n#define __netdev_alloc_pcpu_stats(type, gfp)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = alloc_percpu_gfp(type, gfp);\\\n\tif (pcpu_stats)\t{\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\n#define netdev_alloc_pcpu_stats(type)\t\t\t\t\t\\\n\t__netdev_alloc_pcpu_stats(type, GFP_KERNEL)\n\n#define devm_netdev_alloc_pcpu_stats(dev, type)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = devm_alloc_percpu(dev, type);\\\n\tif (pcpu_stats) {\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\nenum netdev_lag_tx_type {\n\tNETDEV_LAG_TX_TYPE_UNKNOWN,\n\tNETDEV_LAG_TX_TYPE_RANDOM,\n\tNETDEV_LAG_TX_TYPE_BROADCAST,\n\tNETDEV_LAG_TX_TYPE_ROUNDROBIN,\n\tNETDEV_LAG_TX_TYPE_ACTIVEBACKUP,\n\tNETDEV_LAG_TX_TYPE_HASH,\n};\n\nenum netdev_lag_hash {\n\tNETDEV_LAG_HASH_NONE,\n\tNETDEV_LAG_HASH_L2,\n\tNETDEV_LAG_HASH_L34,\n\tNETDEV_LAG_HASH_L23,\n\tNETDEV_LAG_HASH_E23,\n\tNETDEV_LAG_HASH_E34,\n\tNETDEV_LAG_HASH_VLAN_SRCMAC,\n\tNETDEV_LAG_HASH_UNKNOWN,\n};\n\nstruct netdev_lag_upper_info {\n\tenum netdev_lag_tx_type tx_type;\n\tenum netdev_lag_hash hash_type;\n};\n\nstruct netdev_lag_lower_state_info {\n\tu8 link_up : 1,\n\t   tx_enabled : 1;\n};\n\n#include <linux/notifier.h>\n\n/* netdevice notifier chain. Please remember to update netdev_cmd_to_name()\n * and the rtnetlink notification exclusion list in rtnetlink_event() when\n * adding new types.\n */\nenum netdev_cmd {\n\tNETDEV_UP\t= 1,\t/* For now you can't veto a device up/down */\n\tNETDEV_DOWN,\n\tNETDEV_REBOOT,\t\t/* Tell a protocol stack a network interface\n\t\t\t\t   detected a hardware crash and restarted\n\t\t\t\t   - we can use this eg to kick tcp sessions\n\t\t\t\t   once done */\n\tNETDEV_CHANGE,\t\t/* Notify device state change */\n\tNETDEV_REGISTER,\n\tNETDEV_UNREGISTER,\n\tNETDEV_CHANGEMTU,\t/* notify after mtu change happened */\n\tNETDEV_CHANGEADDR,\t/* notify after the address change */\n\tNETDEV_PRE_CHANGEADDR,\t/* notify before the address change */\n\tNETDEV_GOING_DOWN,\n\tNETDEV_CHANGENAME,\n\tNETDEV_FEAT_CHANGE,\n\tNETDEV_BONDING_FAILOVER,\n\tNETDEV_PRE_UP,\n\tNETDEV_PRE_TYPE_CHANGE,\n\tNETDEV_POST_TYPE_CHANGE,\n\tNETDEV_POST_INIT,\n\tNETDEV_RELEASE,\n\tNETDEV_NOTIFY_PEERS,\n\tNETDEV_JOIN,\n\tNETDEV_CHANGEUPPER,\n\tNETDEV_RESEND_IGMP,\n\tNETDEV_PRECHANGEMTU,\t/* notify before mtu change happened */\n\tNETDEV_CHANGEINFODATA,\n\tNETDEV_BONDING_INFO,\n\tNETDEV_PRECHANGEUPPER,\n\tNETDEV_CHANGELOWERSTATE,\n\tNETDEV_UDP_TUNNEL_PUSH_INFO,\n\tNETDEV_UDP_TUNNEL_DROP_INFO,\n\tNETDEV_CHANGE_TX_QUEUE_LEN,\n\tNETDEV_CVLAN_FILTER_PUSH_INFO,\n\tNETDEV_CVLAN_FILTER_DROP_INFO,\n\tNETDEV_SVLAN_FILTER_PUSH_INFO,\n\tNETDEV_SVLAN_FILTER_DROP_INFO,\n};\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd);\n\nint register_netdevice_notifier(struct notifier_block *nb);\nint unregister_netdevice_notifier(struct notifier_block *nb);\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb);\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb);\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn);\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn);\n\nstruct netdev_notifier_info {\n\tstruct net_device\t*dev;\n\tstruct netlink_ext_ack\t*extack;\n};\n\nstruct netdev_notifier_info_ext {\n\tstruct netdev_notifier_info info; /* must be first */\n\tunion {\n\t\tu32 mtu;\n\t} ext;\n};\n\nstruct netdev_notifier_change_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tunsigned int flags_changed;\n};\n\nstruct netdev_notifier_changeupper_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct net_device *upper_dev; /* new upper dev */\n\tbool master; /* is upper dev master */\n\tbool linking; /* is the notification for link or unlink */\n\tvoid *upper_info; /* upper dev info */\n};\n\nstruct netdev_notifier_changelowerstate_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tvoid *lower_state_info; /* is lower dev state */\n};\n\nstruct netdev_notifier_pre_changeaddr_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tconst unsigned char *dev_addr;\n};\n\nstatic inline void netdev_notifier_info_init(struct netdev_notifier_info *info,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tinfo->dev = dev;\n\tinfo->extack = NULL;\n}\n\nstatic inline struct net_device *\nnetdev_notifier_info_to_dev(const struct netdev_notifier_info *info)\n{\n\treturn info->dev;\n}\n\nstatic inline struct netlink_ext_ack *\nnetdev_notifier_info_to_extack(const struct netdev_notifier_info *info)\n{\n\treturn info->extack;\n}\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev);\n\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_reverse(net, d)\t\t\\\n\t\tlist_for_each_entry_continue_reverse(d, &(net)->dev_base_head, \\\n\t\t\t\t\t\t     dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_in_bond_rcu(bond, slave)\t\\\n\t\tfor_each_netdev_rcu(&init_net, slave)\t\\\n\t\t\tif (netdev_master_upper_dev_get_rcu(slave) == (bond))\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(list_next_rcu(&dev->dev_list));\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nstatic inline struct net_device *first_net_device_rcu(struct net *net)\n{\n\tstruct list_head *lh = rcu_dereference(list_next_rcu(&net->dev_base_head));\n\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nint netdev_boot_setup_check(struct net_device *dev);\nunsigned long netdev_boot_base(const char *prefix, int unit);\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *hwaddr);\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nvoid dev_add_pack(struct packet_type *pt);\nvoid dev_remove_pack(struct packet_type *pt);\nvoid __dev_remove_pack(struct packet_type *pt);\nvoid dev_add_offload(struct packet_offload *po);\nvoid dev_remove_offload(struct packet_offload *po);\n\nint dev_get_iflink(const struct net_device *dev);\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb);\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short flags,\n\t\t\t\t      unsigned short mask);\nstruct net_device *dev_get_by_name(struct net *net, const char *name);\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name);\nstruct net_device *__dev_get_by_name(struct net *net, const char *name);\nint dev_alloc_name(struct net_device *dev, const char *name);\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack);\nvoid dev_close(struct net_device *dev);\nvoid dev_close_many(struct list_head *head, bool unlink);\nvoid dev_disable_lro(struct net_device *dev);\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *newskb);\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev);\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev);\n\nint dev_queue_xmit(struct sk_buff *skb);\nint dev_queue_xmit_accel(struct sk_buff *skb, struct net_device *sb_dev);\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id);\n\nstatic inline int dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tint ret;\n\n\tret = __dev_direct_xmit(skb, queue_id);\n\tif (!dev_xmit_complete(ret))\n\t\tkfree_skb(skb);\n\treturn ret;\n}\n\nint register_netdevice(struct net_device *dev);\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head);\nvoid unregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nint netdev_refcnt_read(const struct net_device *dev);\nvoid free_netdev(struct net_device *dev);\nvoid netdev_freemem(struct net_device *dev);\nint init_dummy_netdev(struct net_device *dev);\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves);\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk);\nstruct net_device *dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex);\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id);\nint netdev_get_name(struct net *net, char *name, int ifindex);\nint dev_restart(struct net_device *dev);\nint skb_gro_receive(struct sk_buff *p, struct sk_buff *skb);\nint skb_gro_receive_list(struct sk_buff *p, struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void skb_gro_frag0_invalidate(struct sk_buff *skb)\n{\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tif (!pskb_may_pull(skb, hlen))\n\t\treturn NULL;\n\n\tskb_gro_frag0_invalidate(skb);\n\treturn skb->data + offset;\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline void skb_gro_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t\tconst void *start, unsigned int len)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid)\n\t\tNAPI_GRO_CB(skb)->csum = csum_sub(NAPI_GRO_CB(skb)->csum,\n\t\t\t\t\t\t  csum_partial(start, len, 0));\n}\n\n/* GRO checksum functions. These are logical equivalents of the normal\n * checksum functions (in skbuff.h) except that they operate on the GRO\n * offsets and fields in sk_buff.\n */\n\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb);\n\nstatic inline bool skb_at_gro_remcsum_start(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->gro_remcsum_start == skb_gro_offset(skb));\n}\n\nstatic inline bool __skb_gro_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t      bool zero_okay,\n\t\t\t\t\t\t      __sum16 check)\n{\n\treturn ((skb->ip_summed != CHECKSUM_PARTIAL ||\n\t\tskb_checksum_start_offset(skb) <\n\t\t skb_gro_offset(skb)) &&\n\t\t!skb_at_gro_remcsum_start(skb) &&\n\t\tNAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t(!zero_okay || check));\n}\n\nstatic inline __sum16 __skb_gro_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t\t   __wsum psum)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid &&\n\t    !csum_fold(csum_add(psum, NAPI_GRO_CB(skb)->csum)))\n\t\treturn 0;\n\n\tNAPI_GRO_CB(skb)->csum = psum;\n\n\treturn __skb_gro_checksum_complete(skb);\n}\n\nstatic inline void skb_gro_incr_csum_unnecessary(struct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->csum_cnt > 0) {\n\t\t/* Consume a checksum from CHECKSUM_UNNECESSARY */\n\t\tNAPI_GRO_CB(skb)->csum_cnt--;\n\t} else {\n\t\t/* Update skb for CHECKSUM_UNNECESSARY and csum_level when we\n\t\t * verified a new top level checksum or an encapsulated one\n\t\t * during GRO. This saves work if we fallback to normal path.\n\t\t */\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t}\n}\n\n#define __skb_gro_checksum_validate(skb, proto, zero_okay, check,\t\\\n\t\t\t\t    compute_pseudo)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_gro_checksum_validate_complete(skb,\t\\\n\t\t\t\tcompute_pseudo(skb, proto));\t\t\\\n\tif (!__ret)\t\t\t\t\t\t\t\\\n\t\tskb_gro_incr_csum_unnecessary(skb);\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_gro_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, false, 0, compute_pseudo)\n\n#define skb_gro_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t     compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, true, check, compute_pseudo)\n\n#define skb_gro_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_gro_checksum_validate(skb, 0, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_gro_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t!NAPI_GRO_CB(skb)->csum_valid);\n}\n\nstatic inline void __skb_gro_checksum_convert(struct sk_buff *skb,\n\t\t\t\t\t      __wsum pseudo)\n{\n\tNAPI_GRO_CB(skb)->csum = ~pseudo;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n}\n\n#define skb_gro_checksum_try_convert(skb, proto, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_convert_check(skb))\t\t\t\\\n\t\t__skb_gro_checksum_convert(skb, \t\t\t\\\n\t\t\t\t\t   compute_pseudo(skb, proto));\t\\\n} while (0)\n\nstruct gro_remcsum {\n\tint offset;\n\t__wsum delta;\n};\n\nstatic inline void skb_gro_remcsum_init(struct gro_remcsum *grc)\n{\n\tgrc->offset = 0;\n\tgrc->delta = 0;\n}\n\nstatic inline void *skb_gro_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t    unsigned int off, size_t hdrlen,\n\t\t\t\t\t    int start, int offset,\n\t\t\t\t\t    struct gro_remcsum *grc,\n\t\t\t\t\t    bool nopartial)\n{\n\t__wsum delta;\n\tsize_t plen = hdrlen + max_t(size_t, offset + sizeof(u16), start);\n\n\tBUG_ON(!NAPI_GRO_CB(skb)->csum_valid);\n\n\tif (!nopartial) {\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = off + hdrlen + start;\n\t\treturn ptr;\n\t}\n\n\tptr = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, off + plen)) {\n\t\tptr = skb_gro_header_slow(skb, off + plen, off);\n\t\tif (!ptr)\n\t\t\treturn NULL;\n\t}\n\n\tdelta = remcsum_adjust(ptr + hdrlen, NAPI_GRO_CB(skb)->csum,\n\t\t\t       start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tNAPI_GRO_CB(skb)->csum = csum_add(NAPI_GRO_CB(skb)->csum, delta);\n\n\tgrc->offset = off + hdrlen + offset;\n\tgrc->delta = delta;\n\n\treturn ptr;\n}\n\nstatic inline void skb_gro_remcsum_cleanup(struct sk_buff *skb,\n\t\t\t\t\t   struct gro_remcsum *grc)\n{\n\tvoid *ptr;\n\tsize_t plen = grc->offset + sizeof(u16);\n\n\tif (!grc->delta)\n\t\treturn;\n\n\tptr = skb_gro_header_fast(skb, grc->offset);\n\tif (skb_gro_header_hard(skb, grc->offset + sizeof(u16))) {\n\t\tptr = skb_gro_header_slow(skb, plen, grc->offset);\n\t\tif (!ptr)\n\t\t\treturn;\n\t}\n\n\tremcsum_unadjust((__sum16 *)ptr, grc->delta);\n}\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstatic inline void skb_gro_flush_final(struct sk_buff *skb, struct sk_buff *pp, int flush)\n{\n\tif (PTR_ERR(pp) != -EINPROGRESS)\n\t\tNAPI_GRO_CB(skb)->flush |= flush;\n}\nstatic inline void skb_gro_flush_final_remcsum(struct sk_buff *skb,\n\t\t\t\t\t       struct sk_buff *pp,\n\t\t\t\t\t       int flush,\n\t\t\t\t\t       struct gro_remcsum *grc)\n{\n\tif (PTR_ERR(pp) != -EINPROGRESS) {\n\t\tNAPI_GRO_CB(skb)->flush |= flush;\n\t\tskb_gro_remcsum_cleanup(skb, grc);\n\t\tskb->remcsum_offload = 0;\n\t}\n}\n#else\nstatic inline void skb_gro_flush_final(struct sk_buff *skb, struct sk_buff *pp, int flush)\n{\n\tNAPI_GRO_CB(skb)->flush |= flush;\n}\nstatic inline void skb_gro_flush_final_remcsum(struct sk_buff *skb,\n\t\t\t\t\t       struct sk_buff *pp,\n\t\t\t\t\t       int flush,\n\t\t\t\t\t       struct gro_remcsum *grc)\n{\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\tskb_gro_remcsum_cleanup(skb, grc);\n\tskb->remcsum_offload = 0;\n}\n#endif\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned int len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\nstatic inline __be16 dev_parse_header_protocol(const struct sk_buff *skb)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse_protocol)\n\t\treturn 0;\n\treturn dev->header_ops->parse_protocol(skb);\n}\n\n/* ll_header must have at least hard_header_len allocated */\nstatic inline bool dev_validate_header(const struct net_device *dev,\n\t\t\t\t       char *ll_header, int len)\n{\n\tif (likely(len >= dev->hard_header_len))\n\t\treturn true;\n\tif (len < dev->min_header_len)\n\t\treturn false;\n\n\tif (capable(CAP_SYS_RAWIO)) {\n\t\tmemset(ll_header + len, 0, dev->hard_header_len - len);\n\t\treturn true;\n\t}\n\n\tif (dev->header_ops && dev->header_ops->validate)\n\t\treturn dev->header_ops->validate(ll_header, len);\n\n\treturn false;\n}\n\nstatic inline bool dev_has_header(const struct net_device *dev)\n{\n\treturn dev->header_ops && dev->header_ops->create;\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr,\n\t\t\t   int len, int size);\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\n#define FLOW_LIMIT_HISTORY\t(1 << 7)  /* must be ^2 and !overflow buckets */\nstruct sd_flow_limit {\n\tu64\t\t\tcount;\n\tunsigned int\t\tnum_buckets;\n\tunsigned int\t\thistory_head;\n\tu16\t\t\thistory[FLOW_LIMIT_HISTORY];\n\tu8\t\t\tbuckets[];\n};\n\nextern int netdev_flow_limit_table_len;\n#endif /* CONFIG_NET_FLOW_LIMIT */\n\n/*\n * Incoming packets are placed on per-CPU queues\n */\nstruct softnet_data {\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\treceived_rps;\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n#endif\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit __rcu *flow_limit;\n#endif\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct sk_buff\t\t*completion_queue;\n#ifdef CONFIG_XFRM_OFFLOAD\n\tstruct sk_buff_head\txfrm_backlog;\n#endif\n\t/* written and read only by owning cpu: */\n\tstruct {\n\t\tu16 recursion;\n\t\tu8  more;\n\t} xmit;\n#ifdef CONFIG_RPS\n\t/* input_queue_head should be written by cpu owning this struct,\n\t * and only read by other cpus. Worth using a cache line.\n\t */\n\tunsigned int\t\tinput_queue_head ____cacheline_aligned_in_smp;\n\n\t/* Elements below can be accessed between CPUs for RPS/RFS */\n\tcall_single_data_t\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned int\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\nstatic inline int dev_recursion_level(void)\n{\n\treturn this_cpu_read(softnet_data.xmit.recursion);\n}\n\n#define XMIT_RECURSION_LIMIT\t8\nstatic inline bool dev_xmit_recursion(void)\n{\n\treturn unlikely(__this_cpu_read(softnet_data.xmit.recursion) >\n\t\t\tXMIT_RECURSION_LIMIT);\n}\n\nstatic inline void dev_xmit_recursion_inc(void)\n{\n\t__this_cpu_inc(softnet_data.xmit.recursion);\n}\n\nstatic inline void dev_xmit_recursion_dec(void)\n{\n\t__this_cpu_dec(softnet_data.xmit.recursion);\n}\n\nvoid __netif_schedule(struct Qdisc *q);\nvoid netif_schedule_queue(struct netdev_queue *txq);\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic __always_inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue);\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic __always_inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tset_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev);\n\nstatic inline bool netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline bool netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline bool netif_xmit_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF_OR_FROZEN;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_drv_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_DRV_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetdev_queue_set_dql_min_limit - set dql minimum limit\n *\t@dev_queue: pointer to transmit queue\n *\t@min_limit: dql minimum limit\n *\n * Forces xmit_more() to return true until the minimum threshold\n * defined by @min_limit is reached (or until the tx queue is\n * empty). Warning: to be use with care, misuse will impact the\n * latency.\n */\nstatic inline void netdev_queue_set_dql_min_limit(struct netdev_queue *dev_queue,\n\t\t\t\t\t\t  unsigned int min_limit)\n{\n#ifdef CONFIG_BQL\n\tdev_queue->dql.min_limit = min_limit;\n#endif\n}\n\n/**\n *\tnetdev_txq_bql_enqueue_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their ndo_start_xmit(),\n * to give appropriate hint to the CPU.\n */\nstatic inline void netdev_txq_bql_enqueue_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.num_queued);\n#endif\n}\n\n/**\n *\tnetdev_txq_bql_complete_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their TX completion path,\n * to give appropriate hint to the CPU.\n */\nstatic inline void netdev_txq_bql_complete_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.limit);\n#endif\n}\n\nstatic inline void netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\tunsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tdql_queued(&dev_queue->dql, bytes);\n\n\tif (likely(dql_avail(&dev_queue->dql) >= 0))\n\t\treturn;\n\n\tset_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n\n\t/*\n\t * The XOFF flag must be set before checking the dql_avail below,\n\t * because in netdev_tx_completed_queue we update the dql_completed\n\t * before checking the XOFF flag.\n\t */\n\tsmp_mb();\n\n\t/* check again in case another CPU has just made room avail */\n\tif (unlikely(dql_avail(&dev_queue->dql) >= 0))\n\t\tclear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n#endif\n}\n\n/* Variant of netdev_tx_sent_queue() for drivers that are aware\n * that they should not test BQL status themselves.\n * We do want to change __QUEUE_STATE_STACK_XOFF only for the last\n * skb of a batch.\n * Returns true if the doorbell must be used to kick the NIC.\n */\nstatic inline bool __netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t  unsigned int bytes,\n\t\t\t\t\t  bool xmit_more)\n{\n\tif (xmit_more) {\n#ifdef CONFIG_BQL\n\t\tdql_queued(&dev_queue->dql, bytes);\n#endif\n\t\treturn netif_tx_queue_stopped(dev_queue);\n\t}\n\tnetdev_tx_sent_queue(dev_queue, bytes);\n\treturn true;\n}\n\n/**\n * \tnetdev_sent_queue - report the number of bytes queued to hardware\n * \t@dev: network device\n * \t@bytes: number of bytes queued to the hardware device queue\n *\n * \tReport the number of bytes queued for sending/completion to the network\n * \tdevice hardware queue. @bytes should be a good approximation and should\n * \texactly match netdev_completed_queue() @bytes\n */\nstatic inline void netdev_sent_queue(struct net_device *dev, unsigned int bytes)\n{\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes);\n}\n\nstatic inline bool __netdev_sent_queue(struct net_device *dev,\n\t\t\t\t       unsigned int bytes,\n\t\t\t\t       bool xmit_more)\n{\n\treturn __netdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes,\n\t\t\t\t      xmit_more);\n}\n\nstatic inline void netdev_tx_completed_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t     unsigned int pkts, unsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tif (unlikely(!bytes))\n\t\treturn;\n\n\tdql_completed(&dev_queue->dql, bytes);\n\n\t/*\n\t * Without the memory barrier there is a small possiblity that\n\t * netdev_tx_sent_queue will miss the update and cause the queue to\n\t * be stopped forever\n\t */\n\tsmp_mb();\n\n\tif (unlikely(dql_avail(&dev_queue->dql) < 0))\n\t\treturn;\n\n\tif (test_and_clear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state))\n\t\tnetif_schedule_queue(dev_queue);\n#endif\n}\n\n/**\n * \tnetdev_completed_queue - report bytes and packets completed by device\n * \t@dev: network device\n * \t@pkts: actual number of packets sent over the medium\n * \t@bytes: actual number of bytes sent over the medium\n *\n * \tReport the number of bytes and packets transmitted by the network device\n * \thardware queue over the physical medium, @bytes must exactly match the\n * \t@bytes amount passed to netdev_sent_queue()\n */\nstatic inline void netdev_completed_queue(struct net_device *dev,\n\t\t\t\t\t  unsigned int pkts, unsigned int bytes)\n{\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, 0), pkts, bytes);\n}\n\nstatic inline void netdev_tx_reset_queue(struct netdev_queue *q)\n{\n#ifdef CONFIG_BQL\n\tclear_bit(__QUEUE_STATE_STACK_XOFF, &q->state);\n\tdql_reset(&q->dql);\n#endif\n}\n\n/**\n * \tnetdev_reset_queue - reset the packets and bytes count of a network device\n * \t@dev_queue: network device\n *\n * \tReset the bytes and packet count of a network device and clear the\n * \tsoftware flow control OFF bit for this network device\n */\nstatic inline void netdev_reset_queue(struct net_device *dev_queue)\n{\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));\n}\n\n/**\n * \tnetdev_cap_txqueue - check if selected tx queue exceeds device queues\n * \t@dev: network device\n * \t@queue_index: given tx queue index\n *\n * \tReturns 0 if given tx queue index >= number of device tx queues,\n * \totherwise returns the originally passed tx queue index.\n */\nstatic inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tnet_warn_ratelimited(\"%s selects TX queue %d, but real number of TX queues is %d\\n\",\n\t\t\t\t     dev->name, queue_index,\n\t\t\t\t     dev->real_num_tx_queues);\n\t\treturn 0;\n\t}\n\n\treturn queue_index;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline bool netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start,\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\t__netif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline bool __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t    u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@skb: sub queue buffer pointer\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline bool netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_wake_queue(txq);\n}\n\n#ifdef CONFIG_XPS\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index);\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type);\n\n/**\n *\tnetif_attr_test_mask - Test a CPU or Rx queue set in a mask\n *\t@j: CPU/Rx queue index\n *\t@mask: bitmask of all cpus/rx queues\n *\t@nr_bits: number of bits in the bitmask\n *\n * Test if a CPU or Rx queue index is set in a mask of all CPU/Rx queues.\n */\nstatic inline bool netif_attr_test_mask(unsigned long j,\n\t\t\t\t\tconst unsigned long *mask,\n\t\t\t\t\tunsigned int nr_bits)\n{\n\tcpu_max_bits_warn(j, nr_bits);\n\treturn test_bit(j, mask);\n}\n\n/**\n *\tnetif_attr_test_online - Test for online CPU/Rx queue\n *\t@j: CPU/Rx queue index\n *\t@online_mask: bitmask for CPUs/Rx queues that are online\n *\t@nr_bits: number of bits in the bitmask\n *\n * Returns true if a CPU/Rx queue is online.\n */\nstatic inline bool netif_attr_test_online(unsigned long j,\n\t\t\t\t\t  const unsigned long *online_mask,\n\t\t\t\t\t  unsigned int nr_bits)\n{\n\tcpu_max_bits_warn(j, nr_bits);\n\n\tif (online_mask)\n\t\treturn test_bit(j, online_mask);\n\n\treturn (j < nr_bits);\n}\n\n/**\n *\tnetif_attrmask_next - get the next CPU/Rx queue in a cpu/Rx queues mask\n *\t@n: CPU/Rx queue index\n *\t@srcp: the cpumask/Rx queue mask pointer\n *\t@nr_bits: number of bits in the bitmask\n *\n * Returns >= nr_bits if no further CPUs/Rx queues set.\n */\nstatic inline unsigned int netif_attrmask_next(int n, const unsigned long *srcp,\n\t\t\t\t\t       unsigned int nr_bits)\n{\n\t/* -1 is a legal arg here. */\n\tif (n != -1)\n\t\tcpu_max_bits_warn(n, nr_bits);\n\n\tif (srcp)\n\t\treturn find_next_bit(srcp, nr_bits, n + 1);\n\n\treturn n + 1;\n}\n\n/**\n *\tnetif_attrmask_next_and - get the next CPU/Rx queue in \\*src1p & \\*src2p\n *\t@n: CPU/Rx queue index\n *\t@src1p: the first CPUs/Rx queues mask pointer\n *\t@src2p: the second CPUs/Rx queues mask pointer\n *\t@nr_bits: number of bits in the bitmask\n *\n * Returns >= nr_bits if no further CPUs/Rx queues set in both.\n */\nstatic inline int netif_attrmask_next_and(int n, const unsigned long *src1p,\n\t\t\t\t\t  const unsigned long *src2p,\n\t\t\t\t\t  unsigned int nr_bits)\n{\n\t/* -1 is a legal arg here. */\n\tif (n != -1)\n\t\tcpu_max_bits_warn(n, nr_bits);\n\n\tif (src1p && src2p)\n\t\treturn find_next_and_bit(src1p, src2p, nr_bits, n + 1);\n\telse if (src1p)\n\t\treturn find_next_bit(src1p, nr_bits, n + 1);\n\telse if (src2p)\n\t\treturn find_next_bit(src2p, nr_bits, n + 1);\n\n\treturn n + 1;\n}\n#else\nstatic inline int netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t      const struct cpumask *mask,\n\t\t\t\t      u16 index)\n{\n\treturn 0;\n}\n\nstatic inline int __netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t\tconst unsigned long *mask,\n\t\t\t\t\tu16 index, enum xps_map_type type)\n{\n\treturn 0;\n}\n#endif\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline bool netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq);\n\n#ifdef CONFIG_SYSFS\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxqs)\n{\n\tdev->real_num_rx_queues = rxqs;\n\treturn 0;\n}\n#endif\n\nstatic inline struct netdev_rx_queue *\n__netif_get_rx_queue(struct net_device *dev, unsigned int rxq)\n{\n\treturn dev->_rx + rxq;\n}\n\n#ifdef CONFIG_SYSFS\nstatic inline unsigned int get_netdev_rx_queue_index(\n\t\tstruct netdev_rx_queue *queue)\n{\n\tstruct net_device *dev = queue->dev;\n\tint index = queue - dev->_rx;\n\n\tBUG_ON(index >= dev->num_rx_queues);\n\treturn index;\n}\n#endif\n\n#define DEFAULT_MAX_NUM_RSS_QUEUES\t(8)\nint netif_get_num_default_rss_queues(void);\n\nenum skb_free_reason {\n\tSKB_REASON_CONSUMED,\n\tSKB_REASON_DROPPED,\n};\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason);\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason);\n\n/*\n * It is not allowed to call kfree_skb() or consume_skb() from hardware\n * interrupt context or with hardware interrupts being disabled.\n * (in_irq() || irqs_disabled())\n *\n * We provide four helpers that can be used in following contexts :\n *\n * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.\n *  Typically used in place of consume_skb(skb) in TX completion path\n *\n * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_any(skb) when caller doesn't know its current irq context,\n *  and consumed a packet. Used in place of consume_skb(skb)\n */\nstatic inline void dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_CONSUMED);\n}\n\nstatic inline void dev_kfree_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_CONSUMED);\n}\n\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog);\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb);\nint netif_rx(struct sk_buff *skb);\nint netif_rx_ni(struct sk_buff *skb);\nint netif_rx_any_context(struct sk_buff *skb);\nint netif_receive_skb(struct sk_buff *skb);\nint netif_receive_skb_core(struct sk_buff *skb);\nvoid netif_receive_skb_list(struct list_head *head);\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb);\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old);\nstruct sk_buff *napi_get_frags(struct napi_struct *napi);\ngro_result_t napi_gro_frags(struct napi_struct *napi);\nstruct packet_offload *gro_find_receive_by_type(__be16 type);\nstruct packet_offload *gro_find_complete_by_type(__be16 type);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nbool netdev_is_rx_handler_busy(struct net_device *dev);\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data);\nvoid netdev_rx_handler_unregister(struct net_device *dev);\n\nbool dev_valid_name(const char *name);\nint dev_ioctl(struct net *net, unsigned int cmd, struct ifreq *ifr,\n\t\tbool *need_copyout);\nint dev_ifconf(struct net *net, struct ifconf *, int);\nint dev_ethtool(struct net *net, struct ifreq *);\nunsigned int dev_get_flags(const struct net_device *);\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack);\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack);\nvoid __dev_notify_flags(struct net_device *, unsigned int old_flags,\n\t\t\tunsigned int gchanges);\nint dev_change_name(struct net_device *, const char *);\nint dev_set_alias(struct net_device *, const char *, size_t);\nint dev_get_alias(const struct net_device *, char *, size_t);\nint dev_change_net_namespace(struct net_device *, struct net *, const char *);\nint __dev_set_mtu(struct net_device *, int);\nint dev_validate_mtu(struct net_device *dev, int mtu,\n\t\t     struct netlink_ext_ack *extack);\nint dev_set_mtu_ext(struct net_device *dev, int mtu,\n\t\t    struct netlink_ext_ack *extack);\nint dev_set_mtu(struct net_device *, int);\nint dev_change_tx_queue_len(struct net_device *, unsigned long);\nvoid dev_set_group(struct net_device *, int);\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack);\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack);\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack);\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name);\nint dev_change_carrier(struct net_device *, bool new_carrier);\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid);\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len);\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid, bool recurse);\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b);\nint dev_change_proto_down(struct net_device *dev, bool proto_down);\nint dev_change_proto_down_generic(struct net_device *dev, bool proto_down);\nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value);\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again);\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret);\n\ntypedef int (*bpf_op_t)(struct net_device *dev, struct netdev_bpf *bpf);\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags);\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog);\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb);\nbool is_skb_forwardable(const struct net_device *dev,\n\t\t\tconst struct sk_buff *skb);\n\nstatic __always_inline bool __is_skb_forwardable(const struct net_device *dev,\n\t\t\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t\t\t const bool check_mtu)\n{\n\tconst u32 vlan_hdr_len = 4; /* VLAN_HLEN */\n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tif (!check_mtu)\n\t\treturn true;\n\n\tlen = dev->mtu + dev->hard_header_len + vlan_hdr_len;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t/* if TSO is enabled, we don't care about the length as the packet\n\t * could be forwarded without being segmented before\n\t */\n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic __always_inline int ____dev_forward_skb(struct net_device *dev,\n\t\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t\t       const bool check_mtu)\n{\n\tif (skb_orphan_frags(skb, GFP_ATOMIC) ||\n\t    unlikely(!__is_skb_forwardable(dev, skb, check_mtu))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tskb_scrub_packet(skb, true);\n\tskb->priority = 0;\n\treturn 0;\n}\n\nbool dev_nit_active(struct net_device *dev);\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev);\n\nextern int\t\tnetdev_budget;\nextern unsigned int\tnetdev_budget_usecs;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nvoid netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tthis_cpu_dec(*dev->pcpu_refcnt);\n#else\n\trefcount_dec(&dev->dev_refcnt);\n#endif\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tthis_cpu_inc(*dev->pcpu_refcnt);\n#else\n\trefcount_inc(&dev->dev_refcnt);\n#endif\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nvoid linkwatch_init_dev(struct net_device *dev);\nvoid linkwatch_fire_event(struct net_device *dev);\nvoid linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nunsigned long dev_trans_start(struct net_device *dev);\n\nvoid __netdev_watchdog_up(struct net_device *dev);\n\nvoid netif_carrier_on(struct net_device *dev);\n\nvoid netif_carrier_off(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if device is dormant\n *\t@dev: network device\n *\n * Check if device is dormant.\n */\nstatic inline bool netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_testing_on - mark device as under test.\n *\t@dev: network device\n *\n * Mark device as under test (as per RFC2863).\n *\n * The testing state indicates that some test(s) must be performed on\n * the interface. After completion, of the test, the interface state\n * will change to up, dormant, or down, as appropriate.\n */\nstatic inline void netif_testing_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_TESTING, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_testing_off - set device as not under test.\n *\t@dev: network device\n *\n * Device is not in testing state.\n */\nstatic inline void netif_testing_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_TESTING, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_testing - test if device is under test\n *\t@dev: network device\n *\n * Check if device is under test\n */\nstatic inline bool netif_testing(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_TESTING, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline bool netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline bool netif_device_present(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nvoid netif_device_detach(struct net_device *dev);\n\nvoid netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n\nenum {\n\tNETIF_MSG_DRV_BIT,\n\tNETIF_MSG_PROBE_BIT,\n\tNETIF_MSG_LINK_BIT,\n\tNETIF_MSG_TIMER_BIT,\n\tNETIF_MSG_IFDOWN_BIT,\n\tNETIF_MSG_IFUP_BIT,\n\tNETIF_MSG_RX_ERR_BIT,\n\tNETIF_MSG_TX_ERR_BIT,\n\tNETIF_MSG_TX_QUEUED_BIT,\n\tNETIF_MSG_INTR_BIT,\n\tNETIF_MSG_TX_DONE_BIT,\n\tNETIF_MSG_RX_STATUS_BIT,\n\tNETIF_MSG_PKTDATA_BIT,\n\tNETIF_MSG_HW_BIT,\n\tNETIF_MSG_WOL_BIT,\n\n\t/* When you add a new bit above, update netif_msg_class_names array\n\t * in net/ethtool/common.c\n\t */\n\tNETIF_MSG_CLASS_COUNT,\n};\n/* Both ethtool_ops interface and internal driver implementation use u32 */\nstatic_assert(NETIF_MSG_CLASS_COUNT <= 32);\n\n#define __NETIF_MSG_BIT(bit)\t((u32)1 << (bit))\n#define __NETIF_MSG(name)\t__NETIF_MSG_BIT(NETIF_MSG_ ## name ## _BIT)\n\n#define NETIF_MSG_DRV\t\t__NETIF_MSG(DRV)\n#define NETIF_MSG_PROBE\t\t__NETIF_MSG(PROBE)\n#define NETIF_MSG_LINK\t\t__NETIF_MSG(LINK)\n#define NETIF_MSG_TIMER\t\t__NETIF_MSG(TIMER)\n#define NETIF_MSG_IFDOWN\t__NETIF_MSG(IFDOWN)\n#define NETIF_MSG_IFUP\t\t__NETIF_MSG(IFUP)\n#define NETIF_MSG_RX_ERR\t__NETIF_MSG(RX_ERR)\n#define NETIF_MSG_TX_ERR\t__NETIF_MSG(TX_ERR)\n#define NETIF_MSG_TX_QUEUED\t__NETIF_MSG(TX_QUEUED)\n#define NETIF_MSG_INTR\t\t__NETIF_MSG(INTR)\n#define NETIF_MSG_TX_DONE\t__NETIF_MSG(TX_DONE)\n#define NETIF_MSG_RX_STATUS\t__NETIF_MSG(RX_STATUS)\n#define NETIF_MSG_PKTDATA\t__NETIF_MSG(PKTDATA)\n#define NETIF_MSG_HW\t\t__NETIF_MSG(HW)\n#define NETIF_MSG_WOL\t\t__NETIF_MSG(WOL)\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1U << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline bool __netif_tx_acquire(struct netdev_queue *txq)\n{\n\t__acquire(&txq->_xmit_lock);\n\treturn true;\n}\n\nstatic inline void __netif_tx_release(struct netdev_queue *txq)\n{\n\t__release(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline bool __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tbool ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/* legacy drivers only, netdev_start_xmit() sets txq->trans_start */\nstatic inline void netif_trans_update(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, 0);\n\n\tif (txq->trans_start != jiffies)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_acquire(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_TRYLOCK(dev, txq)\t\t\t\\\n\t(((dev->features & NETIF_F_LLTX) == 0) ?\t\\\n\t\t__netif_tx_trylock(txq) :\t\t\\\n\t\t__netif_tx_acquire(txq))\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_release(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tspin_lock(&dev->tx_global_lock);\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tunsigned char nest_level = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tnest_level = dev->nested_level;\n#endif\n\tspin_lock_nested(&dev->addr_list_lock, nest_level);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tunsigned char nest_level = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tnest_level = dev->nested_level;\n#endif\n\tlocal_bh_disable();\n\tspin_lock_nested(&dev->addr_list_lock, nest_level);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nvoid ether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t    unsigned char name_assign_type,\n\t\t\t\t    void (*setup)(struct net_device *),\n\t\t\t\t    unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, name_assign_type, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, count, \\\n\t\t\t count)\n\nint register_netdev(struct net_device *dev);\nvoid unregister_netdev(struct net_device *dev);\n\nint devm_register_netdev(struct device *dev, struct net_device *ndev);\n\n/* General hardware address lists handling functions */\nint __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t   struct netdev_hw_addr_list *from_list, int addr_len);\nvoid __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t      struct netdev_hw_addr_list *from_list, int addr_len);\nint __hw_addr_sync_dev(struct netdev_hw_addr_list *list,\n\t\t       struct net_device *dev,\n\t\t       int (*sync)(struct net_device *, const unsigned char *),\n\t\t       int (*unsync)(struct net_device *,\n\t\t\t\t     const unsigned char *));\nint __hw_addr_ref_sync_dev(struct netdev_hw_addr_list *list,\n\t\t\t   struct net_device *dev,\n\t\t\t   int (*sync)(struct net_device *,\n\t\t\t\t       const unsigned char *, int),\n\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t const unsigned char *, int));\nvoid __hw_addr_ref_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t      struct net_device *dev,\n\t\t\t      int (*unsync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *, int));\nvoid __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t  struct net_device *dev,\n\t\t\t  int (*unsync)(struct net_device *,\n\t\t\t\t\tconst unsigned char *));\nvoid __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nint dev_addr_add(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nint dev_addr_del(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nvoid dev_addr_flush(struct net_device *dev);\nint dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nint dev_uc_add(struct net_device *dev, const unsigned char *addr);\nint dev_uc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_uc_del(struct net_device *dev, const unsigned char *addr);\nint dev_uc_sync(struct net_device *to, struct net_device *from);\nint dev_uc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_uc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_uc_flush(struct net_device *dev);\nvoid dev_uc_init(struct net_device *dev);\n\n/**\n *  __dev_uc_sync - Synchonize device's unicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n */\nstatic inline int __dev_uc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->uc, dev, sync, unsync);\n}\n\n/**\n *  __dev_uc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_uc_sync().\n */\nstatic inline void __dev_uc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->uc, dev, unsync);\n}\n\n/* Functions used for multicast addresses handling */\nint dev_mc_add(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_sync(struct net_device *to, struct net_device *from);\nint dev_mc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_mc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_mc_flush(struct net_device *dev);\nvoid dev_mc_init(struct net_device *dev);\n\n/**\n *  __dev_mc_sync - Synchonize device's multicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n */\nstatic inline int __dev_mc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->mc, dev, sync, unsync);\n}\n\n/**\n *  __dev_mc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_mc_sync().\n */\nstatic inline void __dev_mc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->mc, dev, unsync);\n}\n\n/* Functions used for secondary unicast and multicast support */\nvoid dev_set_rx_mode(struct net_device *dev);\nvoid __dev_set_rx_mode(struct net_device *dev);\nint dev_set_promiscuity(struct net_device *dev, int inc);\nint dev_set_allmulti(struct net_device *dev, int inc);\nvoid netdev_state_change(struct net_device *dev);\nvoid __netdev_notify_peers(struct net_device *dev);\nvoid netdev_notify_peers(struct net_device *dev);\nvoid netdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nvoid dev_load(struct net *net, const char *name);\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage);\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats);\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats);\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tdev_weight_rx_bias;\nextern int\t\tdev_weight_tx_bias;\nextern int\t\tdev_rx_weight;\nextern int\t\tdev_tx_weight;\nextern int\t\tgro_normal_batch;\n\nenum {\n\tNESTED_SYNC_IMM_BIT,\n\tNESTED_SYNC_TODO_BIT,\n};\n\n#define __NESTED_SYNC_BIT(bit)\t((u32)1 << (bit))\n#define __NESTED_SYNC(name)\t__NESTED_SYNC_BIT(NESTED_SYNC_ ## name ## _BIT)\n\n#define NESTED_SYNC_IMM\t\t__NESTED_SYNC(IMM)\n#define NESTED_SYNC_TODO\t__NESTED_SYNC(TODO)\n\nstruct netdev_nested_priv {\n\tunsigned char flags;\n\tvoid *data;\n};\n\nbool netdev_has_upper_dev(struct net_device *dev, struct net_device *upper_dev);\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\n\n#ifdef CONFIG_LOCKDEP\nstatic LIST_HEAD(net_unlink_list);\n\nstatic inline void net_unlink_todo(struct net_device *dev)\n{\n\tif (list_empty(&dev->unlink_list))\n\t\tlist_add_tail(&dev->unlink_list, &net_unlink_list);\n}\n#endif\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->adj_list.upper, \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)))\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *upper_dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv);\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev);\n\nbool netdev_has_any_upper_dev(struct net_device *dev);\n\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter);\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_private(dev, priv, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)))\n\n#define netdev_for_each_lower_private_rcu(dev, priv, iter) \\\n\tfor (iter = &(dev)->adj_list.lower, \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next(struct net_device *dev,\n\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_dev(dev, ldev, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)); \\\n\t     ldev; \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)))\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter);\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv);\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv);\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list);\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev);\nint netdev_upper_dev_link(struct net_device *dev, struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack);\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack);\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev);\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack);\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev);\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev);\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname);\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev);\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info);\n\n/* RSS keys are 40 or 52 bytes long */\n#define NETDEV_RSS_KEY_LEN 52\nextern u8 netdev_rss_key[NETDEV_RSS_KEY_LEN] __read_mostly;\nvoid netdev_rss_key_fill(void *buffer, size_t len);\n\nint skb_checksum_help(struct sk_buff *skb);\nint skb_crc32c_csum_help(struct sk_buff *skb);\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features);\n\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path);\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features);\n\nstruct netdev_bonding_info {\n\tifslave\tslave;\n\tifbond\tmaster;\n};\n\nstruct netdev_notifier_bonding_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct netdev_bonding_info  bonding_info;\n};\n\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info);\n\n#if IS_ENABLED(CONFIG_ETHTOOL_NETLINK)\nvoid ethtool_notify(struct net_device *dev, unsigned int cmd, const void *data);\n#else\nstatic inline void ethtool_notify(struct net_device *dev, unsigned int cmd,\n\t\t\t\t  const void *data)\n{\n}\n#endif\n\nstatic inline\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn __skb_gso_segment(skb, features, true);\n}\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth);\n\nstatic inline bool can_checksum_protocol(netdev_features_t features,\n\t\t\t\t\t __be16 protocol)\n{\n\tif (protocol == htons(ETH_P_FCOE))\n\t\treturn !!(features & NETIF_F_FCOE_CRC);\n\n\t/* Assume this is an IP checksum (not SCTP CRC) */\n\n\tif (features & NETIF_F_HW_CSUM) {\n\t\t/* Can checksum everything */\n\t\treturn true;\n\t}\n\n\tswitch (protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn !!(features & NETIF_F_IP_CSUM);\n\tcase htons(ETH_P_IPV6):\n\t\treturn !!(features & NETIF_F_IPV6_CSUM);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n}\n#endif\n/* rx skb timestamps */\nvoid net_enable_timestamp(void);\nvoid net_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nint __init dev_proc_init(void);\n#else\n#define dev_proc_init() 0\n#endif\n\nstatic inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,\n\t\t\t\t\t      struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t      bool more)\n{\n\t__this_cpu_write(softnet_data.xmit.more, more);\n\treturn ops->ndo_start_xmit(skb, dev);\n}\n\nstatic inline bool netdev_xmit_more(void)\n{\n\treturn __this_cpu_read(softnet_data.xmit.more);\n}\n\nstatic inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq, bool more)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tnetdev_tx_t rc;\n\n\trc = __netdev_start_xmit(ops, skb, dev, more);\n\tif (rc == NETDEV_TX_OK)\n\t\ttxq_trans_update(txq);\n\n\treturn rc;\n}\n\nint netdev_class_create_file_ns(const struct class_attribute *class_attr,\n\t\t\t\tconst void *ns);\nvoid netdev_class_remove_file_ns(const struct class_attribute *class_attr,\n\t\t\t\t const void *ns);\n\nextern const struct kobj_ns_type_operations net_ns_type_operations;\n\nconst char *netdev_drivername(const struct net_device *dev);\n\nvoid linkwatch_run_queue(void);\n\nstatic inline netdev_features_t netdev_intersect_features(netdev_features_t f1,\n\t\t\t\t\t\t\t  netdev_features_t f2)\n{\n\tif ((f1 ^ f2) & NETIF_F_HW_CSUM) {\n\t\tif (f1 & NETIF_F_HW_CSUM)\n\t\t\tf1 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t\telse\n\t\t\tf2 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\treturn f1 & f2;\n}\n\nstatic inline netdev_features_t netdev_get_wanted_features(\n\tstruct net_device *dev)\n{\n\treturn (dev->features & ~dev->hw_features) | dev->wanted_features;\n}\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask);\n\n/* Allow TSO being used on stacked device :\n * Performing the GSO segmentation before last device\n * is a performance improvement.\n */\nstatic inline netdev_features_t netdev_add_tso_features(netdev_features_t features,\n\t\t\t\t\t\t\tnetdev_features_t mask)\n{\n\treturn netdev_increment_features(features, NETIF_F_ALL_TSO, mask);\n}\n\nint __netdev_update_features(struct net_device *dev);\nvoid netdev_update_features(struct net_device *dev);\nvoid netdev_change_features(struct net_device *dev);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features);\nnetdev_features_t netif_skb_features(struct sk_buff *skb);\n\nstatic inline bool net_gso_ok(netdev_features_t features, int gso_type)\n{\n\tnetdev_features_t feature = (netdev_features_t)gso_type << NETIF_F_GSO_SHIFT;\n\n\t/* check flags correspondence */\n\tBUILD_BUG_ON(SKB_GSO_TCPV4   != (NETIF_F_TSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_DODGY   != (NETIF_F_GSO_ROBUST >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_ECN != (NETIF_F_TSO_ECN >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_FIXEDID != (NETIF_F_TSO_MANGLEID >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCPV6   != (NETIF_F_TSO6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FCOE    != (NETIF_F_FSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE     != (NETIF_F_GSO_GRE >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE_CSUM != (NETIF_F_GSO_GRE_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP4  != (NETIF_F_GSO_IPXIP4 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP6  != (NETIF_F_GSO_IPXIP6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL != (NETIF_F_GSO_UDP_TUNNEL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL_CSUM != (NETIF_F_GSO_UDP_TUNNEL_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_PARTIAL != (NETIF_F_GSO_PARTIAL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TUNNEL_REMCSUM != (NETIF_F_GSO_TUNNEL_REMCSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_SCTP    != (NETIF_F_GSO_SCTP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_ESP != (NETIF_F_GSO_ESP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP != (NETIF_F_GSO_UDP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_L4 != (NETIF_F_GSO_UDP_L4 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FRAGLIST != (NETIF_F_GSO_FRAGLIST >> NETIF_F_GSO_SHIFT));\n\n\treturn (features & feature) == feature;\n}\n\nstatic inline bool skb_gso_ok(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline bool netif_needs_gso(struct sk_buff *skb,\n\t\t\t\t   netdev_features_t features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely((skb->ip_summed != CHECKSUM_PARTIAL) &&\n\t\t\t (skb->ip_summed != CHECKSUM_UNNECESSARY)));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nstatic inline void skb_gso_error_unwind(struct sk_buff *skb, __be16 protocol,\n\t\t\t\t\tint pulled_hlen, u16 mac_offset,\n\t\t\t\t\tint mac_len)\n{\n\tskb->protocol = protocol;\n\tskb->encapsulation = 1;\n\tskb_push(skb, pulled_hlen);\n\tskb_reset_transport_header(skb);\n\tskb->mac_header = mac_offset;\n\tskb->network_header = skb->mac_header + mac_len;\n\tskb->mac_len = mac_len;\n}\n\nstatic inline bool netif_is_macsec(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nstatic inline bool netif_is_macvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN;\n}\n\nstatic inline bool netif_is_macvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN_PORT;\n}\n\nstatic inline bool netif_is_bond_master(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_MASTER && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_is_bond_slave(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_SLAVE && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_supports_nofcs(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_SUPP_NOFCS;\n}\n\nstatic inline bool netif_has_l3_rx_handler(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_RX_HANDLER;\n}\n\nstatic inline bool netif_is_l3_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_MASTER;\n}\n\nstatic inline bool netif_is_l3_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_SLAVE;\n}\n\nstatic inline bool netif_is_bridge_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_EBRIDGE;\n}\n\nstatic inline bool netif_is_bridge_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_BRIDGE_PORT;\n}\n\nstatic inline bool netif_is_ovs_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OPENVSWITCH;\n}\n\nstatic inline bool netif_is_ovs_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OVS_DATAPATH;\n}\n\nstatic inline bool netif_is_any_bridge_port(const struct net_device *dev)\n{\n\treturn netif_is_bridge_port(dev) || netif_is_ovs_port(dev);\n}\n\nstatic inline bool netif_is_team_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM;\n}\n\nstatic inline bool netif_is_team_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM_PORT;\n}\n\nstatic inline bool netif_is_lag_master(const struct net_device *dev)\n{\n\treturn netif_is_bond_master(dev) || netif_is_team_master(dev);\n}\n\nstatic inline bool netif_is_lag_port(const struct net_device *dev)\n{\n\treturn netif_is_bond_slave(dev) || netif_is_team_port(dev);\n}\n\nstatic inline bool netif_is_rxfh_configured(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_RXFH_CONFIGURED;\n}\n\nstatic inline bool netif_is_failover(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_FAILOVER;\n}\n\nstatic inline bool netif_is_failover_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_FAILOVER_SLAVE;\n}\n\n/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */\nstatic inline void netif_keep_dst(struct net_device *dev)\n{\n\tdev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM);\n}\n\n/* return true if dev can't cope with mtu frames that need vlan tag insertion */\nstatic inline bool netif_reduces_vlan_mtu(struct net_device *dev)\n{\n\t/* TODO: reserve and use an additional IFF bit, if we get more users */\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (!dev->name[0] || strchr(dev->name, '%'))\n\t\treturn \"(unnamed net_device)\";\n\treturn dev->name;\n}\n\nstatic inline bool netdev_unregistering(const struct net_device *dev)\n{\n\treturn dev->reg_state == NETREG_UNREGISTERING;\n}\n\nstatic inline const char *netdev_reg_state(const struct net_device *dev)\n{\n\tswitch (dev->reg_state) {\n\tcase NETREG_UNINITIALIZED: return \" (uninitialized)\";\n\tcase NETREG_REGISTERED: return \"\";\n\tcase NETREG_UNREGISTERING: return \" (unregistering)\";\n\tcase NETREG_UNREGISTERED: return \" (unregistered)\";\n\tcase NETREG_RELEASED: return \" (released)\";\n\tcase NETREG_DUMMY: return \" (dummy)\";\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown reg_state %d\\n\", dev->name, dev->reg_state);\n\treturn \" (unknown)\";\n}\n\n__printf(3, 4) __cold\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_emerg(const struct net_device *dev, const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_alert(const struct net_device *dev, const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_crit(const struct net_device *dev, const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_err(const struct net_device *dev, const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_warn(const struct net_device *dev, const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_notice(const struct net_device *dev, const char *format, ...);\n__printf(2, 3) __cold\nvoid netdev_info(const struct net_device *dev, const char *format, ...);\n\n#define netdev_level_once(level, dev, fmt, ...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tstatic bool __print_once __read_mostly;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (!__print_once) {\t\t\t\t\t\\\n\t\t__print_once = true;\t\t\t\t\\\n\t\tnetdev_printk(level, dev, fmt, ##__VA_ARGS__);\t\\\n\t}\t\t\t\t\t\t\t\\\n} while (0)\n\n#define netdev_emerg_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_EMERG, dev, fmt, ##__VA_ARGS__)\n#define netdev_alert_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_ALERT, dev, fmt, ##__VA_ARGS__)\n#define netdev_crit_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_CRIT, dev, fmt, ##__VA_ARGS__)\n#define netdev_err_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_ERR, dev, fmt, ##__VA_ARGS__)\n#define netdev_warn_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_WARNING, dev, fmt, ##__VA_ARGS__)\n#define netdev_notice_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_NOTICE, dev, fmt, ##__VA_ARGS__)\n#define netdev_info_once(dev, fmt, ...) \\\n\tnetdev_level_once(KERN_INFO, dev, fmt, ##__VA_ARGS__)\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_netdev_dbg(__dev, format, ##args);\t\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s%s: \" format, netdev_name(dev),\t\\\n\t     netdev_reg_state(dev), ##args)\n\n#define netdev_WARN_ONCE(dev, format, args...)\t\t\t\t\\\n\tWARN_ONCE(1, \"netdevice: %s%s: \" format, netdev_name(dev),\t\\\n\t\t  netdev_reg_state(dev), ##args)\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_netdev_dbg(netdev, format, ##args);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n/* if @cond then downgrade to debug, else print at @level */\n#define netif_cond_dbg(priv, type, netdev, cond, level, fmt, args...)     \\\n\tdo {                                                              \\\n\t\tif (cond)                                                 \\\n\t\t\tnetif_dbg(priv, type, netdev, fmt, ##args);       \\\n\t\telse                                                      \\\n\t\t\tnetif_ ## level(priv, type, netdev, fmt, ##args); \\\n\t} while (0)\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *\t\t0800\tIP\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\nextern struct list_head ptype_all __read_mostly;\nextern struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\n\nextern struct net_device *blackhole_netdev;\n\n#endif\t/* _LINUX_NETDEVICE_H */\n"}, "0": {"id": 0, "path": "/src/include/net/net_namespace.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Operations on the network namespace\n */\n#ifndef __NET_NET_NAMESPACE_H\n#define __NET_NET_NAMESPACE_H\n\n#include <linux/atomic.h>\n#include <linux/refcount.h>\n#include <linux/workqueue.h>\n#include <linux/list.h>\n#include <linux/sysctl.h>\n#include <linux/uidgid.h>\n\n#include <net/flow.h>\n#include <net/netns/core.h>\n#include <net/netns/mib.h>\n#include <net/netns/unix.h>\n#include <net/netns/packet.h>\n#include <net/netns/ipv4.h>\n#include <net/netns/ipv6.h>\n#include <net/netns/nexthop.h>\n#include <net/netns/ieee802154_6lowpan.h>\n#include <net/netns/sctp.h>\n#include <net/netns/dccp.h>\n#include <net/netns/netfilter.h>\n#include <net/netns/x_tables.h>\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n#include <net/netns/conntrack.h>\n#endif\n#include <net/netns/nftables.h>\n#include <net/netns/xfrm.h>\n#include <net/netns/mpls.h>\n#include <net/netns/can.h>\n#include <net/netns/xdp.h>\n#include <net/netns/bpf.h>\n#include <linux/ns_common.h>\n#include <linux/idr.h>\n#include <linux/skbuff.h>\n#include <linux/notifier.h>\n\nstruct user_namespace;\nstruct proc_dir_entry;\nstruct net_device;\nstruct sock;\nstruct ctl_table_header;\nstruct net_generic;\nstruct uevent_sock;\nstruct netns_ipvs;\nstruct bpf_prog;\n\n\n#define NETDEV_HASHBITS    8\n#define NETDEV_HASHENTRIES (1 << NETDEV_HASHBITS)\n\nstruct net {\n\t/* First cache line can be often dirtied.\n\t * Do not place here read-mostly fields.\n\t */\n\trefcount_t\t\tpassive;\t/* To decide when the network\n\t\t\t\t\t\t * namespace should be freed.\n\t\t\t\t\t\t */\n\tspinlock_t\t\trules_mod_lock;\n\n\tunsigned int\t\tdev_unreg_count;\n\n\tunsigned int\t\tdev_base_seq;\t/* protected by rtnl_mutex */\n\tint\t\t\tifindex;\n\n\tspinlock_t\t\tnsid_lock;\n\tatomic_t\t\tfnhe_genid;\n\n\tstruct list_head\tlist;\t\t/* list of network namespaces */\n\tstruct list_head\texit_list;\t/* To linked to call pernet exit\n\t\t\t\t\t\t * methods on dead net (\n\t\t\t\t\t\t * pernet_ops_rwsem read locked),\n\t\t\t\t\t\t * or to unregister pernet ops\n\t\t\t\t\t\t * (pernet_ops_rwsem write locked).\n\t\t\t\t\t\t */\n\tstruct llist_node\tcleanup_list;\t/* namespaces on death row */\n\n#ifdef CONFIG_KEYS\n\tstruct key_tag\t\t*key_domain;\t/* Key domain of operation tag */\n#endif\n\tstruct user_namespace   *user_ns;\t/* Owning user namespace */\n\tstruct ucounts\t\t*ucounts;\n\tstruct idr\t\tnetns_ids;\n\n\tstruct ns_common\tns;\n\n\tstruct list_head \tdev_base_head;\n\tstruct proc_dir_entry \t*proc_net;\n\tstruct proc_dir_entry \t*proc_net_stat;\n\n#ifdef CONFIG_SYSCTL\n\tstruct ctl_table_set\tsysctls;\n#endif\n\n\tstruct sock \t\t*rtnl;\t\t\t/* rtnetlink socket */\n\tstruct sock\t\t*genl_sock;\n\n\tstruct uevent_sock\t*uevent_sock;\t\t/* uevent socket */\n\n\tstruct hlist_head \t*dev_name_head;\n\tstruct hlist_head\t*dev_index_head;\n\tstruct raw_notifier_head\tnetdev_chain;\n\n\t/* Note that @hash_mix can be read millions times per second,\n\t * it is critical that it is on a read_mostly cache line.\n\t */\n\tu32\t\t\thash_mix;\n\n\tstruct net_device       *loopback_dev;          /* The loopback */\n\n\t/* core fib_rules */\n\tstruct list_head\trules_ops;\n\n\tstruct netns_core\tcore;\n\tstruct netns_mib\tmib;\n\tstruct netns_packet\tpacket;\n\tstruct netns_unix\tunx;\n\tstruct netns_nexthop\tnexthop;\n\tstruct netns_ipv4\tipv4;\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct netns_ipv6\tipv6;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\n\tstruct netns_ieee802154_lowpan\tieee802154_lowpan;\n#endif\n#if defined(CONFIG_IP_SCTP) || defined(CONFIG_IP_SCTP_MODULE)\n\tstruct netns_sctp\tsctp;\n#endif\n#if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)\n\tstruct netns_dccp\tdccp;\n#endif\n#ifdef CONFIG_NETFILTER\n\tstruct netns_nf\t\tnf;\n\tstruct netns_xt\t\txt;\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tstruct netns_ct\t\tct;\n#endif\n#if defined(CONFIG_NF_TABLES) || defined(CONFIG_NF_TABLES_MODULE)\n\tstruct netns_nftables\tnft;\n#endif\n#if IS_ENABLED(CONFIG_NF_DEFRAG_IPV6)\n\tstruct netns_nf_frag\tnf_frag;\n\tstruct ctl_table_header *nf_frag_frags_hdr;\n#endif\n\tstruct sock\t\t*nfnl;\n\tstruct sock\t\t*nfnl_stash;\n#if IS_ENABLED(CONFIG_NF_CT_NETLINK_TIMEOUT)\n\tstruct list_head\tnfct_timeout_list;\n#endif\n#endif\n#ifdef CONFIG_WEXT_CORE\n\tstruct sk_buff_head\twext_nlevents;\n#endif\n\tstruct net_generic __rcu\t*gen;\n\n\t/* Used to store attached BPF programs */\n\tstruct netns_bpf\tbpf;\n\n\t/* Note : following structs are cache line aligned */\n#ifdef CONFIG_XFRM\n\tstruct netns_xfrm\txfrm;\n#endif\n\n\tu64\t\t\tnet_cookie; /* written once */\n\n#if IS_ENABLED(CONFIG_IP_VS)\n\tstruct netns_ipvs\t*ipvs;\n#endif\n#if IS_ENABLED(CONFIG_MPLS)\n\tstruct netns_mpls\tmpls;\n#endif\n#if IS_ENABLED(CONFIG_CAN)\n\tstruct netns_can\tcan;\n#endif\n#ifdef CONFIG_XDP_SOCKETS\n\tstruct netns_xdp\txdp;\n#endif\n#if IS_ENABLED(CONFIG_CRYPTO_USER)\n\tstruct sock\t\t*crypto_nlsk;\n#endif\n\tstruct sock\t\t*diag_nlsk;\n} __randomize_layout;\n\n#include <linux/seq_file_net.h>\n\n/* Init's network namespace */\nextern struct net init_net;\n\n#ifdef CONFIG_NET_NS\nstruct net *copy_net_ns(unsigned long flags, struct user_namespace *user_ns,\n\t\t\tstruct net *old_net);\n\nvoid net_ns_get_ownership(const struct net *net, kuid_t *uid, kgid_t *gid);\n\nvoid net_ns_barrier(void);\n#else /* CONFIG_NET_NS */\n#include <linux/sched.h>\n#include <linux/nsproxy.h>\nstatic inline struct net *copy_net_ns(unsigned long flags,\n\tstruct user_namespace *user_ns, struct net *old_net)\n{\n\tif (flags & CLONE_NEWNET)\n\t\treturn ERR_PTR(-EINVAL);\n\treturn old_net;\n}\n\nstatic inline void net_ns_get_ownership(const struct net *net,\n\t\t\t\t\tkuid_t *uid, kgid_t *gid)\n{\n\t*uid = GLOBAL_ROOT_UID;\n\t*gid = GLOBAL_ROOT_GID;\n}\n\nstatic inline void net_ns_barrier(void) {}\n#endif /* CONFIG_NET_NS */\n\n\nextern struct list_head net_namespace_list;\n\nstruct net *get_net_ns_by_pid(pid_t pid);\nstruct net *get_net_ns_by_fd(int fd);\n\n#ifdef CONFIG_SYSCTL\nvoid ipx_register_sysctl(void);\nvoid ipx_unregister_sysctl(void);\n#else\n#define ipx_register_sysctl()\n#define ipx_unregister_sysctl()\n#endif\n\n#ifdef CONFIG_NET_NS\nvoid __put_net(struct net *net);\n\nstatic inline struct net *get_net(struct net *net)\n{\n\trefcount_inc(&net->ns.count);\n\treturn net;\n}\n\nstatic inline struct net *maybe_get_net(struct net *net)\n{\n\t/* Used when we know struct net exists but we\n\t * aren't guaranteed a previous reference count\n\t * exists.  If the reference count is zero this\n\t * function fails and returns NULL.\n\t */\n\tif (!refcount_inc_not_zero(&net->ns.count))\n\t\tnet = NULL;\n\treturn net;\n}\n\nstatic inline void put_net(struct net *net)\n{\n\tif (refcount_dec_and_test(&net->ns.count))\n\t\t__put_net(net);\n}\n\nstatic inline\nint net_eq(const struct net *net1, const struct net *net2)\n{\n\treturn net1 == net2;\n}\n\nstatic inline int check_net(const struct net *net)\n{\n\treturn refcount_read(&net->ns.count) != 0;\n}\n\nvoid net_drop_ns(void *);\n\n#else\n\nstatic inline struct net *get_net(struct net *net)\n{\n\treturn net;\n}\n\nstatic inline void put_net(struct net *net)\n{\n}\n\nstatic inline struct net *maybe_get_net(struct net *net)\n{\n\treturn net;\n}\n\nstatic inline\nint net_eq(const struct net *net1, const struct net *net2)\n{\n\treturn 1;\n}\n\nstatic inline int check_net(const struct net *net)\n{\n\treturn 1;\n}\n\n#define net_drop_ns NULL\n#endif\n\n\ntypedef struct {\n#ifdef CONFIG_NET_NS\n\tstruct net *net;\n#endif\n} possible_net_t;\n\nstatic inline void write_pnet(possible_net_t *pnet, struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\tpnet->net = net;\n#endif\n}\n\nstatic inline struct net *read_pnet(const possible_net_t *pnet)\n{\n#ifdef CONFIG_NET_NS\n\treturn pnet->net;\n#else\n\treturn &init_net;\n#endif\n}\n\n/* Protected by net_rwsem */\n#define for_each_net(VAR)\t\t\t\t\\\n\tlist_for_each_entry(VAR, &net_namespace_list, list)\n#define for_each_net_continue_reverse(VAR)\t\t\\\n\tlist_for_each_entry_continue_reverse(VAR, &net_namespace_list, list)\n#define for_each_net_rcu(VAR)\t\t\t\t\\\n\tlist_for_each_entry_rcu(VAR, &net_namespace_list, list)\n\n#ifdef CONFIG_NET_NS\n#define __net_init\n#define __net_exit\n#define __net_initdata\n#define __net_initconst\n#else\n#define __net_init\t__init\n#define __net_exit\t__ref\n#define __net_initdata\t__initdata\n#define __net_initconst\t__initconst\n#endif\n\nint peernet2id_alloc(struct net *net, struct net *peer, gfp_t gfp);\nint peernet2id(const struct net *net, struct net *peer);\nbool peernet_has_id(const struct net *net, struct net *peer);\nstruct net *get_net_ns_by_id(const struct net *net, int id);\n\nstruct pernet_operations {\n\tstruct list_head list;\n\t/*\n\t * Below methods are called without any exclusive locks.\n\t * More than one net may be constructed and destructed\n\t * in parallel on several cpus. Every pernet_operations\n\t * have to keep in mind all other pernet_operations and\n\t * to introduce a locking, if they share common resources.\n\t *\n\t * The only time they are called with exclusive lock is\n\t * from register_pernet_subsys(), unregister_pernet_subsys()\n\t * register_pernet_device() and unregister_pernet_device().\n\t *\n\t * Exit methods using blocking RCU primitives, such as\n\t * synchronize_rcu(), should be implemented via exit_batch.\n\t * Then, destruction of a group of net requires single\n\t * synchronize_rcu() related to these pernet_operations,\n\t * instead of separate synchronize_rcu() for every net.\n\t * Please, avoid synchronize_rcu() at all, where it's possible.\n\t *\n\t * Note that a combination of pre_exit() and exit() can\n\t * be used, since a synchronize_rcu() is guaranteed between\n\t * the calls.\n\t */\n\tint (*init)(struct net *net);\n\tvoid (*pre_exit)(struct net *net);\n\tvoid (*exit)(struct net *net);\n\tvoid (*exit_batch)(struct list_head *net_exit_list);\n\tunsigned int *id;\n\tsize_t size;\n};\n\n/*\n * Use these carefully.  If you implement a network device and it\n * needs per network namespace operations use device pernet operations,\n * otherwise use pernet subsys operations.\n *\n * Network interfaces need to be removed from a dying netns _before_\n * subsys notifiers can be called, as most of the network code cleanup\n * (which is done from subsys notifiers) runs with the assumption that\n * dev_remove_pack has been called so no new packets will arrive during\n * and after the cleanup functions have been called.  dev_remove_pack\n * is not per namespace so instead the guarantee of no more packets\n * arriving in a network namespace is provided by ensuring that all\n * network devices and all sockets have left the network namespace\n * before the cleanup methods are called.\n *\n * For the longest time the ipv4 icmp code was registered as a pernet\n * device which caused kernel oops, and panics during network\n * namespace cleanup.   So please don't get this wrong.\n */\nint register_pernet_subsys(struct pernet_operations *);\nvoid unregister_pernet_subsys(struct pernet_operations *);\nint register_pernet_device(struct pernet_operations *);\nvoid unregister_pernet_device(struct pernet_operations *);\n\nstruct ctl_table;\nstruct ctl_table_header;\n\n#ifdef CONFIG_SYSCTL\nint net_sysctl_init(void);\nstruct ctl_table_header *register_net_sysctl(struct net *net, const char *path,\n\t\t\t\t\t     struct ctl_table *table);\nvoid unregister_net_sysctl_table(struct ctl_table_header *header);\n#else\nstatic inline int net_sysctl_init(void) { return 0; }\nstatic inline struct ctl_table_header *register_net_sysctl(struct net *net,\n\tconst char *path, struct ctl_table *table)\n{\n\treturn NULL;\n}\nstatic inline void unregister_net_sysctl_table(struct ctl_table_header *header)\n{\n}\n#endif\n\nstatic inline int rt_genid_ipv4(const struct net *net)\n{\n\treturn atomic_read(&net->ipv4.rt_genid);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic inline int rt_genid_ipv6(const struct net *net)\n{\n\treturn atomic_read(&net->ipv6.fib6_sernum);\n}\n#endif\n\nstatic inline void rt_genid_bump_ipv4(struct net *net)\n{\n\tatomic_inc(&net->ipv4.rt_genid);\n}\n\nextern void (*__fib6_flush_trees)(struct net *net);\nstatic inline void rt_genid_bump_ipv6(struct net *net)\n{\n\tif (__fib6_flush_trees)\n\t\t__fib6_flush_trees(net);\n}\n\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\nstatic inline struct netns_ieee802154_lowpan *\nnet_ieee802154_lowpan(struct net *net)\n{\n\treturn &net->ieee802154_lowpan;\n}\n#endif\n\n/* For callers who don't really care about whether it's IPv4 or IPv6 */\nstatic inline void rt_genid_bump_all(struct net *net)\n{\n\trt_genid_bump_ipv4(net);\n\trt_genid_bump_ipv6(net);\n}\n\nstatic inline int fnhe_genid(const struct net *net)\n{\n\treturn atomic_read(&net->fnhe_genid);\n}\n\nstatic inline void fnhe_genid_bump(struct net *net)\n{\n\tatomic_inc(&net->fnhe_genid);\n}\n\n#endif /* __NET_NET_NAMESPACE_H */\n"}}, "reports": [{"events": [{"location": {"col": 6, "file": 1, "line": 8246}, "message": "Assuming 'new_dev' is non-null"}, {"location": {"col": 2, "file": 1, "line": 8246}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 1, "line": 8249}, "message": "Assuming 'old_dev' is null"}, {"location": {"col": 14, "file": 1, "line": 8249}, "message": "Left side of '&&' is false"}, {"location": {"col": 37, "file": 1, "line": 8252}, "message": "Passing value via 2nd parameter 'upper_dev'"}, {"location": {"col": 2, "file": 1, "line": 8252}, "message": "Calling '__netdev_upper_dev_unlink'"}, {"location": {"col": 2, "file": 1, "line": 8122}, "message": "Assuming the condition is false"}, {"location": {"col": 12, "file": 2, "line": 110}, "message": "expanded from macro 'ASSERT_RTNL'"}, {"location": {"col": 27, "file": 3, "line": 157}, "message": "expanded from macro 'WARN_ONCE'"}, {"location": {"col": 2, "file": 1, "line": 8122}, "message": "'__ret_warn_once' is 0"}, {"location": {"col": 2, "file": 2, "line": 110}, "message": "expanded from macro 'ASSERT_RTNL'"}, {"location": {"col": 15, "file": 3, "line": 159}, "message": "expanded from macro 'WARN_ONCE'"}, {"location": {"col": 42, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 1, "line": 8122}, "message": "Left side of '&&' is false"}, {"location": {"col": 2, "file": 2, "line": 110}, "message": "expanded from macro 'ASSERT_RTNL'"}, {"location": {"col": 31, "file": 3, "line": 159}, "message": "expanded from macro 'WARN_ONCE'"}, {"location": {"col": 2, "file": 1, "line": 8122}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 2, "line": 110}, "message": "expanded from macro 'ASSERT_RTNL'"}, {"location": {"col": 2, "file": 3, "line": 159}, "message": "expanded from macro 'WARN_ONCE'"}, {"location": {"col": 28, "file": 1, "line": 8124}, "message": "Assuming pointer value is null"}, {"location": {"col": 46, "file": 1, "line": 8129}, "message": "Passing null pointer value via 2nd parameter 'upper_dev'"}, {"location": {"col": 2, "file": 1, "line": 8129}, "message": "Calling '__netdev_adjacent_dev_unlink_neighbour'"}, {"location": {"col": 42, "file": 1, "line": 7979}, "message": "Passing null pointer value via 2nd parameter 'upper_dev'"}, {"location": {"col": 2, "file": 1, "line": 7979}, "message": "Calling '__netdev_adjacent_dev_unlink_lists'"}, {"location": {"col": 36, "file": 1, "line": 7962}, "message": "Passing null pointer value via 2nd parameter 'adj_dev'"}, {"location": {"col": 2, "file": 1, "line": 7962}, "message": "Calling '__netdev_adjacent_dev_remove'"}, {"location": {"col": 2, "file": 1, "line": 7900}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 5, "line": 430}, "message": "expanded from macro 'pr_debug'"}, {"location": {"col": 2, "file": 5, "line": 139}, "message": "expanded from macro 'no_printk'"}, {"location": {"col": 6, "file": 1, "line": 7905}, "message": "Assuming 'adj' is non-null"}, {"location": {"col": 2, "file": 1, "line": 7905}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 1, "line": 7912}, "message": "Assuming 'ref_nr' is >= field 'ref_nr'"}, {"location": {"col": 2, "file": 1, "line": 7912}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 1, "line": 7920}, "message": "Assuming field 'master' is false"}, {"location": {"col": 2, "file": 1, "line": 7920}, "message": "Taking false branch"}, {"location": {"col": 41, "file": 1, "line": 7923}, "message": "Passing null pointer value via 2nd parameter 'adj_dev'"}, {"location": {"col": 6, "file": 1, "line": 7923}, "message": "Calling 'netdev_adjacent_is_neigh_list'"}, {"location": {"col": 43, "file": 1, "line": 7826}, "message": "Left side of '||' is true"}, {"location": {"col": 32, "file": 1, "line": 7828}, "message": "Passing null pointer value via 1st parameter 'dev'"}, {"location": {"col": 24, "file": 1, "line": 7828}, "message": "Calling 'dev_net'"}, {"location": {"col": 19, "file": 6, "line": 2355}, "message": "Passing null pointer value via 1st parameter 'pnet'"}, {"location": {"col": 9, "file": 6, "line": 2355}, "message": "Calling 'read_pnet'"}, {"location": {"col": 9, "file": 0, "line": 322}, "message": "Access to field 'net' results in a dereference of a null pointer (loaded from variable 'pnet')"}, {"location": {"col": 9, "file": 0, "line": 322}, "message": "Access to field 'net' results in a dereference of a null pointer (loaded from variable 'pnet')"}], "macros": [], "notes": [], "path": "/src/include/net/net_namespace.h", "reportHash": "9167bcc01341e5db155473c7b52912ed", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
