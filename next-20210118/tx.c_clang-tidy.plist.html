<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/net/mac80211/tx.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Copyright 2002-2005, Instant802 Networks, Inc.\n * Copyright 2005-2006, Devicescape Software, Inc.\n * Copyright 2006-2007\tJiri Benc <jbenc@suse.cz>\n * Copyright 2007\tJohannes Berg <johannes@sipsolutions.net>\n * Copyright 2013-2014  Intel Mobile Communications GmbH\n * Copyright (C) 2018-2020 Intel Corporation\n *\n * Transmit and frame generation functions.\n */\n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/skbuff.h>\n#include <linux/if_vlan.h>\n#include <linux/etherdevice.h>\n#include <linux/bitmap.h>\n#include <linux/rcupdate.h>\n#include <linux/export.h>\n#include <net/net_namespace.h>\n#include <net/ieee80211_radiotap.h>\n#include <net/cfg80211.h>\n#include <net/mac80211.h>\n#include <net/codel.h>\n#include <net/codel_impl.h>\n#include <asm/unaligned.h>\n#include <net/fq_impl.h>\n\n#include \"ieee80211_i.h\"\n#include \"driver-ops.h\"\n#include \"led.h\"\n#include \"mesh.h\"\n#include \"wep.h\"\n#include \"wpa.h\"\n#include \"wme.h\"\n#include \"rate.h\"\n\n/* misc utils */\n\nstatic __le16 ieee80211_duration(struct ieee80211_tx_data *tx,\n\t\t\t\t struct sk_buff *skb, int group_addr,\n\t\t\t\t int next_frag_len)\n{\n\tint rate, mrate, erp, dur, i, shift = 0;\n\tstruct ieee80211_rate *txrate;\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_supported_band *sband;\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tu32 rate_flags = 0;\n\n\t/* assume HW handles this */\n\tif (tx->rate.flags & (IEEE80211_TX_RC_MCS | IEEE80211_TX_RC_VHT_MCS))\n\t\treturn 0;\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(tx->sdata->vif.chanctx_conf);\n\tif (chanctx_conf) {\n\t\tshift = ieee80211_chandef_get_shift(&chanctx_conf->def);\n\t\trate_flags = ieee80211_chandef_rate_flags(&chanctx_conf->def);\n\t}\n\trcu_read_unlock();\n\n\t/* uh huh? */\n\tif (WARN_ON_ONCE(tx->rate.idx < 0))\n\t\treturn 0;\n\n\tsband = local->hw.wiphy->bands[info->band];\n\ttxrate = &sband->bitrates[tx->rate.idx];\n\n\terp = txrate->flags & IEEE80211_RATE_ERP_G;\n\n\t/* device is expected to do this */\n\tif (sband->band == NL80211_BAND_S1GHZ)\n\t\treturn 0;\n\n\t/*\n\t * data and mgmt (except PS Poll):\n\t * - during CFP: 32768\n\t * - during contention period:\n\t *   if addr1 is group address: 0\n\t *   if more fragments = 0 and addr1 is individual address: time to\n\t *      transmit one ACK plus SIFS\n\t *   if more fragments = 1 and addr1 is individual address: time to\n\t *      transmit next fragment plus 2 x ACK plus 3 x SIFS\n\t *\n\t * IEEE 802.11, 9.6:\n\t * - control response frame (CTS or ACK) shall be transmitted using the\n\t *   same rate as the immediately previous frame in the frame exchange\n\t *   sequence, if this rate belongs to the PHY mandatory rates, or else\n\t *   at the highest possible rate belonging to the PHY rates in the\n\t *   BSSBasicRateSet\n\t */\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tif (ieee80211_is_ctl(hdr->frame_control)) {\n\t\t/* TODO: These control frames are not currently sent by\n\t\t * mac80211, but should they be implemented, this function\n\t\t * needs to be updated to support duration field calculation.\n\t\t *\n\t\t * RTS: time needed to transmit pending data/mgmt frame plus\n\t\t *    one CTS frame plus one ACK frame plus 3 x SIFS\n\t\t * CTS: duration of immediately previous RTS minus time\n\t\t *    required to transmit CTS and its SIFS\n\t\t * ACK: 0 if immediately previous directed data/mgmt had\n\t\t *    more=0, with more=1 duration in ACK frame is duration\n\t\t *    from previous frame minus time needed to transmit ACK\n\t\t *    and its SIFS\n\t\t * PS Poll: BIT(15) | BIT(14) | aid\n\t\t */\n\t\treturn 0;\n\t}\n\n\t/* data/mgmt */\n\tif (0 /* FIX: data/mgmt during CFP */)\n\t\treturn cpu_to_le16(32768);\n\n\tif (group_addr) /* Group address as the destination - no ACK */\n\t\treturn 0;\n\n\t/* Individual destination address:\n\t * IEEE 802.11, Ch. 9.6 (after IEEE 802.11g changes)\n\t * CTS and ACK frames shall be transmitted using the highest rate in\n\t * basic rate set that is less than or equal to the rate of the\n\t * immediately previous frame and that is using the same modulation\n\t * (CCK or OFDM). If no basic rate set matches with these requirements,\n\t * the highest mandatory rate of the PHY that is less than or equal to\n\t * the rate of the previous frame is used.\n\t * Mandatory rates for IEEE 802.11g PHY: 1, 2, 5.5, 11, 6, 12, 24 Mbps\n\t */\n\trate = -1;\n\t/* use lowest available if everything fails */\n\tmrate = sband->bitrates[0].bitrate;\n\tfor (i = 0; i < sband->n_bitrates; i++) {\n\t\tstruct ieee80211_rate *r = &sband->bitrates[i];\n\n\t\tif (r->bitrate > txrate->bitrate)\n\t\t\tbreak;\n\n\t\tif ((rate_flags & r->flags) != rate_flags)\n\t\t\tcontinue;\n\n\t\tif (tx->sdata->vif.bss_conf.basic_rates & BIT(i))\n\t\t\trate = DIV_ROUND_UP(r->bitrate, 1 << shift);\n\n\t\tswitch (sband->band) {\n\t\tcase NL80211_BAND_2GHZ: {\n\t\t\tu32 flag;\n\t\t\tif (tx->sdata->flags & IEEE80211_SDATA_OPERATING_GMODE)\n\t\t\t\tflag = IEEE80211_RATE_MANDATORY_G;\n\t\t\telse\n\t\t\t\tflag = IEEE80211_RATE_MANDATORY_B;\n\t\t\tif (r->flags & flag)\n\t\t\t\tmrate = r->bitrate;\n\t\t\tbreak;\n\t\t}\n\t\tcase NL80211_BAND_5GHZ:\n\t\tcase NL80211_BAND_6GHZ:\n\t\t\tif (r->flags & IEEE80211_RATE_MANDATORY_A)\n\t\t\t\tmrate = r->bitrate;\n\t\t\tbreak;\n\t\tcase NL80211_BAND_S1GHZ:\n\t\tcase NL80211_BAND_60GHZ:\n\t\t\t/* TODO, for now fall through */\n\t\tcase NUM_NL80211_BANDS:\n\t\t\tWARN_ON(1);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (rate == -1) {\n\t\t/* No matching basic rate found; use highest suitable mandatory\n\t\t * PHY rate */\n\t\trate = DIV_ROUND_UP(mrate, 1 << shift);\n\t}\n\n\t/* Don't calculate ACKs for QoS Frames with NoAck Policy set */\n\tif (ieee80211_is_data_qos(hdr->frame_control) &&\n\t    *(ieee80211_get_qos_ctl(hdr)) & IEEE80211_QOS_CTL_ACK_POLICY_NOACK)\n\t\tdur = 0;\n\telse\n\t\t/* Time needed to transmit ACK\n\t\t * (10 bytes + 4-byte FCS = 112 bits) plus SIFS; rounded up\n\t\t * to closest integer */\n\t\tdur = ieee80211_frame_duration(sband->band, 10, rate, erp,\n\t\t\t\ttx->sdata->vif.bss_conf.use_short_preamble,\n\t\t\t\tshift);\n\n\tif (next_frag_len) {\n\t\t/* Frame is fragmented: duration increases with time needed to\n\t\t * transmit next fragment plus ACK and 2 x SIFS. */\n\t\tdur *= 2; /* ACK + SIFS */\n\t\t/* next fragment */\n\t\tdur += ieee80211_frame_duration(sband->band, next_frag_len,\n\t\t\t\ttxrate->bitrate, erp,\n\t\t\t\ttx->sdata->vif.bss_conf.use_short_preamble,\n\t\t\t\tshift);\n\t}\n\n\treturn cpu_to_le16(dur);\n}\n\n/* tx handlers */\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_dynamic_ps(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_if_managed *ifmgd;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\n\t/* driver doesn't support power save */\n\tif (!ieee80211_hw_check(&local->hw, SUPPORTS_PS))\n\t\treturn TX_CONTINUE;\n\n\t/* hardware does dynamic power save */\n\tif (ieee80211_hw_check(&local->hw, SUPPORTS_DYNAMIC_PS))\n\t\treturn TX_CONTINUE;\n\n\t/* dynamic power save disabled */\n\tif (local->hw.conf.dynamic_ps_timeout <= 0)\n\t\treturn TX_CONTINUE;\n\n\t/* we are scanning, don't enable power save */\n\tif (local->scanning)\n\t\treturn TX_CONTINUE;\n\n\tif (!local->ps_sdata)\n\t\treturn TX_CONTINUE;\n\n\t/* No point if we're going to suspend */\n\tif (local->quiescing)\n\t\treturn TX_CONTINUE;\n\n\t/* dynamic ps is supported only in managed mode */\n\tif (tx->sdata->vif.type != NL80211_IFTYPE_STATION)\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(info->flags & IEEE80211_TX_INTFL_OFFCHAN_TX_OK))\n\t\treturn TX_CONTINUE;\n\n\tifmgd = &tx->sdata->u.mgd;\n\n\t/*\n\t * Don't wakeup from power save if u-apsd is enabled, voip ac has\n\t * u-apsd enabled and the frame is in voip class. This effectively\n\t * means that even if all access categories have u-apsd enabled, in\n\t * practise u-apsd is only used with the voip ac. This is a\n\t * workaround for the case when received voip class packets do not\n\t * have correct qos tag for some reason, due the network or the\n\t * peer application.\n\t *\n\t * Note: ifmgd->uapsd_queues access is racy here. If the value is\n\t * changed via debugfs, user needs to reassociate manually to have\n\t * everything in sync.\n\t */\n\tif ((ifmgd->flags & IEEE80211_STA_UAPSD_ENABLED) &&\n\t    (ifmgd->uapsd_queues & IEEE80211_WMM_IE_STA_QOSINFO_AC_VO) &&\n\t    skb_get_queue_mapping(tx->skb) == IEEE80211_AC_VO)\n\t\treturn TX_CONTINUE;\n\n\tif (local->hw.conf.flags & IEEE80211_CONF_PS) {\n\t\tieee80211_stop_queues_by_reason(&local->hw,\n\t\t\t\t\t\tIEEE80211_MAX_QUEUE_MAP,\n\t\t\t\t\t\tIEEE80211_QUEUE_STOP_REASON_PS,\n\t\t\t\t\t\tfalse);\n\t\tifmgd->flags &= ~IEEE80211_STA_NULLFUNC_ACKED;\n\t\tieee80211_queue_work(&local->hw,\n\t\t\t\t     &local->dynamic_ps_disable_work);\n\t}\n\n\t/* Don't restart the timer if we're not disassociated */\n\tif (!ifmgd->associated)\n\t\treturn TX_CONTINUE;\n\n\tmod_timer(&local->dynamic_ps_timer, jiffies +\n\t\t  msecs_to_jiffies(local->hw.conf.dynamic_ps_timeout));\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_check_assoc(struct ieee80211_tx_data *tx)\n{\n\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tbool assoc = false;\n\n\tif (unlikely(info->flags & IEEE80211_TX_CTL_INJECTED))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(test_bit(SCAN_SW_SCANNING, &tx->local->scanning)) &&\n\t    test_bit(SDATA_STATE_OFFCHANNEL, &tx->sdata->state) &&\n\t    !ieee80211_is_probe_req(hdr->frame_control) &&\n\t    !ieee80211_is_any_nullfunc(hdr->frame_control))\n\t\t/*\n\t\t * When software scanning only nullfunc frames (to notify\n\t\t * the sleep state to the AP) and probe requests (for the\n\t\t * active scan) are allowed, all other frames should not be\n\t\t * sent and we should not get here, but if we do\n\t\t * nonetheless, drop them to avoid sending them\n\t\t * off-channel. See the link below and\n\t\t * ieee80211_start_scan() for more.\n\t\t *\n\t\t * http://article.gmane.org/gmane.linux.kernel.wireless.general/30089\n\t\t */\n\t\treturn TX_DROP;\n\n\tif (tx->sdata->vif.type == NL80211_IFTYPE_OCB)\n\t\treturn TX_CONTINUE;\n\n\tif (tx->flags & IEEE80211_TX_PS_BUFFERED)\n\t\treturn TX_CONTINUE;\n\n\tif (tx->sta)\n\t\tassoc = test_sta_flag(tx->sta, WLAN_STA_ASSOC);\n\n\tif (likely(tx->flags & IEEE80211_TX_UNICAST)) {\n\t\tif (unlikely(!assoc &&\n\t\t\t     ieee80211_is_data(hdr->frame_control))) {\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\t\t\tsdata_info(tx->sdata,\n\t\t\t\t   \"dropped data frame to not associated station %pM\\n\",\n\t\t\t\t   hdr->addr1);\n#endif\n\t\t\tI802_DEBUG_INC(tx->local->tx_handlers_drop_not_assoc);\n\t\t\treturn TX_DROP;\n\t\t}\n\t} else if (unlikely(ieee80211_is_data(hdr->frame_control) &&\n\t\t\t    ieee80211_vif_get_num_mcast_if(tx->sdata) == 0)) {\n\t\t/*\n\t\t * No associated STAs - no need to send multicast\n\t\t * frames.\n\t\t */\n\t\treturn TX_DROP;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\n/* This function is called whenever the AP is about to exceed the maximum limit\n * of buffered frames for power saving STAs. This situation should not really\n * happen often during normal operation, so dropping the oldest buffered packet\n * from each queue should be OK to make some room for new frames. */\nstatic void purge_old_ps_buffers(struct ieee80211_local *local)\n{\n\tint total = 0, purged = 0;\n\tstruct sk_buff *skb;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct sta_info *sta;\n\n\tlist_for_each_entry_rcu(sdata, &local->interfaces, list) {\n\t\tstruct ps_data *ps;\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tps = &sdata->u.ap.ps;\n\t\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\t\tps = &sdata->u.mesh.ps;\n\t\telse\n\t\t\tcontinue;\n\n\t\tskb = skb_dequeue(&ps->bc_buf);\n\t\tif (skb) {\n\t\t\tpurged++;\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t}\n\t\ttotal += skb_queue_len(&ps->bc_buf);\n\t}\n\n\t/*\n\t * Drop one frame from each station from the lowest-priority\n\t * AC that has frames at all.\n\t */\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tint ac;\n\n\t\tfor (ac = IEEE80211_AC_BK; ac >= IEEE80211_AC_VO; ac--) {\n\t\t\tskb = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\ttotal += skb_queue_len(&sta->ps_tx_buf[ac]);\n\t\t\tif (skb) {\n\t\t\t\tpurged++;\n\t\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tlocal->total_ps_buffered = total;\n\tps_dbg_hw(&local->hw, \"PS buffers full - purged %d frames\\n\", purged);\n}\n\nstatic ieee80211_tx_result\nieee80211_tx_h_multicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tstruct ps_data *ps;\n\n\t/*\n\t * broadcast/multicast frame\n\t *\n\t * If any of the associated/peer stations is in power save mode,\n\t * the frame is buffered to be sent after DTIM beacon frame.\n\t * This is done either by the hardware or us.\n\t */\n\n\t/* powersaving STAs currently only in AP/VLAN/mesh mode */\n\tif (tx->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t    tx->sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\tif (!tx->sdata->bss)\n\t\t\treturn TX_CONTINUE;\n\n\t\tps = &tx->sdata->bss->ps;\n\t} else if (ieee80211_vif_is_mesh(&tx->sdata->vif)) {\n\t\tps = &tx->sdata->u.mesh.ps;\n\t} else {\n\t\treturn TX_CONTINUE;\n\t}\n\n\n\t/* no buffering for ordered frames */\n\tif (ieee80211_has_order(hdr->frame_control))\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_is_probe_req(hdr->frame_control))\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_hw_check(&tx->local->hw, QUEUE_CONTROL))\n\t\tinfo->hw_queue = tx->sdata->vif.cab_queue;\n\n\t/* no stations in PS mode and no buffered packets */\n\tif (!atomic_read(&ps->num_sta_ps) && skb_queue_empty(&ps->bc_buf))\n\t\treturn TX_CONTINUE;\n\n\tinfo->flags |= IEEE80211_TX_CTL_SEND_AFTER_DTIM;\n\n\t/* device releases frame after DTIM beacon */\n\tif (!ieee80211_hw_check(&tx->local->hw, HOST_BROADCAST_PS_BUFFERING))\n\t\treturn TX_CONTINUE;\n\n\t/* buffered in mac80211 */\n\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\tpurge_old_ps_buffers(tx->local);\n\n\tif (skb_queue_len(&ps->bc_buf) >= AP_MAX_BC_BUFFER) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"BC TX buffer full - dropping the oldest frame\\n\");\n\t\tieee80211_free_txskb(&tx->local->hw, skb_dequeue(&ps->bc_buf));\n\t} else\n\t\ttx->local->total_ps_buffered++;\n\n\tskb_queue_tail(&ps->bc_buf, tx->skb);\n\n\treturn TX_QUEUED;\n}\n\nstatic int ieee80211_use_mfp(__le16 fc, struct sta_info *sta,\n\t\t\t     struct sk_buff *skb)\n{\n\tif (!ieee80211_is_mgmt(fc))\n\t\treturn 0;\n\n\tif (sta == NULL || !test_sta_flag(sta, WLAN_STA_MFP))\n\t\treturn 0;\n\n\tif (!ieee80211_is_robust_mgmt_frame(skb))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic ieee80211_tx_result\nieee80211_tx_h_unicast_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tstruct sta_info *sta = tx->sta;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tstruct ieee80211_local *local = tx->local;\n\n\tif (unlikely(!sta))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely((test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DRIVER) ||\n\t\t      test_sta_flag(sta, WLAN_STA_PS_DELIVER)) &&\n\t\t     !(info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER))) {\n\t\tint ac = skb_get_queue_mapping(tx->skb);\n\n\t\tif (ieee80211_is_mgmt(hdr->frame_control) &&\n\t\t    !ieee80211_is_bufferable_mmpdu(hdr->frame_control)) {\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_NO_PS_BUFFER;\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tps_dbg(sta->sdata, \"STA %pM aid %d: PS buffer for AC %d\\n\",\n\t\t       sta->sta.addr, sta->sta.aid, ac);\n\t\tif (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)\n\t\t\tpurge_old_ps_buffers(tx->local);\n\n\t\t/* sync with ieee80211_sta_ps_deliver_wakeup */\n\t\tspin_lock(&sta->ps_lock);\n\t\t/*\n\t\t * STA woke up the meantime and all the frames on ps_tx_buf have\n\t\t * been queued to pending queue. No reordering can happen, go\n\t\t * ahead and Tx the packet.\n\t\t */\n\t\tif (!test_sta_flag(sta, WLAN_STA_PS_STA) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DRIVER) &&\n\t\t    !test_sta_flag(sta, WLAN_STA_PS_DELIVER)) {\n\t\t\tspin_unlock(&sta->ps_lock);\n\t\t\treturn TX_CONTINUE;\n\t\t}\n\n\t\tif (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {\n\t\t\tstruct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);\n\t\t\tps_dbg(tx->sdata,\n\t\t\t       \"STA %pM TX buffer for AC %d full - dropping oldest frame\\n\",\n\t\t\t       sta->sta.addr, ac);\n\t\t\tieee80211_free_txskb(&local->hw, old);\n\t\t} else\n\t\t\ttx->local->total_ps_buffered++;\n\n\t\tinfo->control.jiffies = jiffies;\n\t\tinfo->control.vif = &tx->sdata->vif;\n\t\tinfo->control.flags |= IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\tskb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);\n\t\tspin_unlock(&sta->ps_lock);\n\n\t\tif (!timer_pending(&local->sta_cleanup))\n\t\t\tmod_timer(&local->sta_cleanup,\n\t\t\t\t  round_jiffies(jiffies +\n\t\t\t\t\t\tSTA_INFO_CLEANUP_INTERVAL));\n\n\t\t/*\n\t\t * We queued up some frames, so the TIM bit might\n\t\t * need to be set, recalculate it.\n\t\t */\n\t\tsta_info_recalc_tim(sta);\n\n\t\treturn TX_QUEUED;\n\t} else if (unlikely(test_sta_flag(sta, WLAN_STA_PS_STA))) {\n\t\tps_dbg(tx->sdata,\n\t\t       \"STA %pM in PS mode, but polling/in SP -> send frame\\n\",\n\t\t       sta->sta.addr);\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_ps_buf(struct ieee80211_tx_data *tx)\n{\n\tif (unlikely(tx->flags & IEEE80211_TX_PS_BUFFERED))\n\t\treturn TX_CONTINUE;\n\n\tif (tx->flags & IEEE80211_TX_UNICAST)\n\t\treturn ieee80211_tx_h_unicast_ps_buf(tx);\n\telse\n\t\treturn ieee80211_tx_h_multicast_ps_buf(tx);\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_check_control_port_protocol(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\n\tif (unlikely(tx->sdata->control_port_protocol == tx->skb->protocol)) {\n\t\tif (tx->sdata->control_port_no_encrypt)\n\t\t\tinfo->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\t\tinfo->control.flags |= IEEE80211_TX_CTRL_PORT_CTRL_PROTO;\n\t\tinfo->flags |= IEEE80211_TX_CTL_USE_MINRATE;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_select_key(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_key *key;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\n\tif (unlikely(info->flags & IEEE80211_TX_INTFL_DONT_ENCRYPT)) {\n\t\ttx->key = NULL;\n\t\treturn TX_CONTINUE;\n\t}\n\n\tif (tx->sta &&\n\t    (key = rcu_dereference(tx->sta->ptk[tx->sta->ptk_idx])))\n\t\ttx->key = key;\n\telse if (ieee80211_is_group_privacy_action(tx->skb) &&\n\t\t(key = rcu_dereference(tx->sdata->default_multicast_key)))\n\t\ttx->key = key;\n\telse if (ieee80211_is_mgmt(hdr->frame_control) &&\n\t\t is_multicast_ether_addr(hdr->addr1) &&\n\t\t ieee80211_is_robust_mgmt_frame(tx->skb) &&\n\t\t (key = rcu_dereference(tx->sdata->default_mgmt_key)))\n\t\ttx->key = key;\n\telse if (is_multicast_ether_addr(hdr->addr1) &&\n\t\t (key = rcu_dereference(tx->sdata->default_multicast_key)))\n\t\ttx->key = key;\n\telse if (!is_multicast_ether_addr(hdr->addr1) &&\n\t\t (key = rcu_dereference(tx->sdata->default_unicast_key)))\n\t\ttx->key = key;\n\telse\n\t\ttx->key = NULL;\n\n\tif (tx->key) {\n\t\tbool skip_hw = false;\n\n\t\t/* TODO: add threshold stuff again */\n\n\t\tswitch (tx->key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_WEP40:\n\t\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\t\tif (!ieee80211_is_data_present(hdr->frame_control))\n\t\t\t\ttx->key = NULL;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tif (!ieee80211_is_data_present(hdr->frame_control) &&\n\t\t\t    !ieee80211_use_mfp(hdr->frame_control, tx->sta,\n\t\t\t\t\t       tx->skb) &&\n\t\t\t    !ieee80211_is_group_privacy_action(tx->skb))\n\t\t\t\ttx->key = NULL;\n\t\t\telse\n\t\t\t\tskip_hw = (tx->key->conf.flags &\n\t\t\t\t\t   IEEE80211_KEY_FLAG_SW_MGMT_TX) &&\n\t\t\t\t\tieee80211_is_mgmt(hdr->frame_control);\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\t\tif (!ieee80211_is_mgmt(hdr->frame_control))\n\t\t\t\ttx->key = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(tx->key && tx->key->flags & KEY_FLAG_TAINTED &&\n\t\t\t     !ieee80211_is_deauth(hdr->frame_control)))\n\t\t\treturn TX_DROP;\n\n\t\tif (!skip_hw && tx->key &&\n\t\t    tx->key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE)\n\t\t\tinfo->control.hw_key = &tx->key->conf;\n\t} else if (ieee80211_is_data_present(hdr->frame_control) && tx->sta &&\n\t\t   test_sta_flag(tx->sta, WLAN_STA_USES_ENCRYPTION)) {\n\t\treturn TX_DROP;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_rate_ctrl(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (void *)tx->skb->data;\n\tstruct ieee80211_supported_band *sband;\n\tu32 len;\n\tstruct ieee80211_tx_rate_control txrc;\n\tstruct ieee80211_sta_rates *ratetbl = NULL;\n\tbool assoc = false;\n\n\tmemset(&txrc, 0, sizeof(txrc));\n\n\tsband = tx->local->hw.wiphy->bands[info->band];\n\n\tlen = min_t(u32, tx->skb->len + FCS_LEN,\n\t\t\t tx->local->hw.wiphy->frag_threshold);\n\n\t/* set up the tx rate control struct we give the RC algo */\n\ttxrc.hw = &tx->local->hw;\n\ttxrc.sband = sband;\n\ttxrc.bss_conf = &tx->sdata->vif.bss_conf;\n\ttxrc.skb = tx->skb;\n\ttxrc.reported_rate.idx = -1;\n\ttxrc.rate_idx_mask = tx->sdata->rc_rateidx_mask[info->band];\n\n\tif (tx->sdata->rc_has_mcs_mask[info->band])\n\t\ttxrc.rate_idx_mcs_mask =\n\t\t\ttx->sdata->rc_rateidx_mcs_mask[info->band];\n\n\ttxrc.bss = (tx->sdata->vif.type == NL80211_IFTYPE_AP ||\n\t\t    tx->sdata->vif.type == NL80211_IFTYPE_MESH_POINT ||\n\t\t    tx->sdata->vif.type == NL80211_IFTYPE_ADHOC ||\n\t\t    tx->sdata->vif.type == NL80211_IFTYPE_OCB);\n\n\t/* set up RTS protection if desired */\n\tif (len > tx->local->hw.wiphy->rts_threshold) {\n\t\ttxrc.rts = true;\n\t}\n\n\tinfo->control.use_rts = txrc.rts;\n\tinfo->control.use_cts_prot = tx->sdata->vif.bss_conf.use_cts_prot;\n\n\t/*\n\t * Use short preamble if the BSS can handle it, but not for\n\t * management frames unless we know the receiver can handle\n\t * that -- the management frame might be to a station that\n\t * just wants a probe response.\n\t */\n\tif (tx->sdata->vif.bss_conf.use_short_preamble &&\n\t    (ieee80211_is_data(hdr->frame_control) ||\n\t     (tx->sta && test_sta_flag(tx->sta, WLAN_STA_SHORT_PREAMBLE))))\n\t\ttxrc.short_preamble = true;\n\n\tinfo->control.short_preamble = txrc.short_preamble;\n\n\t/* don't ask rate control when rate already injected via radiotap */\n\tif (info->control.flags & IEEE80211_TX_CTRL_RATE_INJECT)\n\t\treturn TX_CONTINUE;\n\n\tif (tx->sta)\n\t\tassoc = test_sta_flag(tx->sta, WLAN_STA_ASSOC);\n\n\t/*\n\t * Lets not bother rate control if we're associated and cannot\n\t * talk to the sta. This should not happen.\n\t */\n\tif (WARN(test_bit(SCAN_SW_SCANNING, &tx->local->scanning) && assoc &&\n\t\t !rate_usable_index_exists(sband, &tx->sta->sta),\n\t\t \"%s: Dropped data frame as no usable bitrate found while \"\n\t\t \"scanning and associated. Target station: \"\n\t\t \"%pM on %d GHz band\\n\",\n\t\t tx->sdata->name, hdr->addr1,\n\t\t info->band ? 5 : 2))\n\t\treturn TX_DROP;\n\n\t/*\n\t * If we're associated with the sta at this point we know we can at\n\t * least send the frame at the lowest bit rate.\n\t */\n\trate_control_get_rate(tx->sdata, tx->sta, &txrc);\n\n\tif (tx->sta && !info->control.skip_table)\n\t\tratetbl = rcu_dereference(tx->sta->sta.rates);\n\n\tif (unlikely(info->control.rates[0].idx < 0)) {\n\t\tif (ratetbl) {\n\t\t\tstruct ieee80211_tx_rate rate = {\n\t\t\t\t.idx = ratetbl->rate[0].idx,\n\t\t\t\t.flags = ratetbl->rate[0].flags,\n\t\t\t\t.count = ratetbl->rate[0].count\n\t\t\t};\n\n\t\t\tif (ratetbl->rate[0].idx < 0)\n\t\t\t\treturn TX_DROP;\n\n\t\t\ttx->rate = rate;\n\t\t} else {\n\t\t\treturn TX_DROP;\n\t\t}\n\t} else {\n\t\ttx->rate = info->control.rates[0];\n\t}\n\n\tif (txrc.reported_rate.idx < 0) {\n\t\ttxrc.reported_rate = tx->rate;\n\t\tif (tx->sta && ieee80211_is_data(hdr->frame_control))\n\t\t\ttx->sta->tx_stats.last_rate = txrc.reported_rate;\n\t} else if (tx->sta)\n\t\ttx->sta->tx_stats.last_rate = txrc.reported_rate;\n\n\tif (ratetbl)\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(!info->control.rates[0].count))\n\t\tinfo->control.rates[0].count = 1;\n\n\tif (WARN_ON_ONCE((info->control.rates[0].count > 1) &&\n\t\t\t (info->flags & IEEE80211_TX_CTL_NO_ACK)))\n\t\tinfo->control.rates[0].count = 1;\n\n\treturn TX_CONTINUE;\n}\n\nstatic __le16 ieee80211_tx_next_seq(struct sta_info *sta, int tid)\n{\n\tu16 *seq = &sta->tid_seq[tid];\n\t__le16 ret = cpu_to_le16(*seq);\n\n\t/* Increase the sequence number. */\n\t*seq = (*seq + 0x10) & IEEE80211_SCTL_SEQ;\n\n\treturn ret;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_sequence(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)tx->skb->data;\n\tint tid;\n\n\t/*\n\t * Packet injection may want to control the sequence\n\t * number, if we have no matching interface then we\n\t * neither assign one ourselves nor ask the driver to.\n\t */\n\tif (unlikely(info->control.vif->type == NL80211_IFTYPE_MONITOR))\n\t\treturn TX_CONTINUE;\n\n\tif (unlikely(ieee80211_is_ctl(hdr->frame_control)))\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_hdrlen(hdr->frame_control) < 24)\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_is_qos_nullfunc(hdr->frame_control))\n\t\treturn TX_CONTINUE;\n\n\tif (info->control.flags & IEEE80211_TX_CTRL_NO_SEQNO)\n\t\treturn TX_CONTINUE;\n\n\t/*\n\t * Anything but QoS data that has a sequence number field\n\t * (is long enough) gets a sequence number from the global\n\t * counter.  QoS data frames with a multicast destination\n\t * also use the global counter (802.11-2012 9.3.2.10).\n\t */\n\tif (!ieee80211_is_data_qos(hdr->frame_control) ||\n\t    is_multicast_ether_addr(hdr->addr1)) {\n\t\t/* driver should assign sequence number */\n\t\tinfo->flags |= IEEE80211_TX_CTL_ASSIGN_SEQ;\n\t\t/* for pure STA mode without beacons, we can do it */\n\t\thdr->seq_ctrl = cpu_to_le16(tx->sdata->sequence_number);\n\t\ttx->sdata->sequence_number += 0x10;\n\t\tif (tx->sta)\n\t\t\ttx->sta->tx_stats.msdu[IEEE80211_NUM_TIDS]++;\n\t\treturn TX_CONTINUE;\n\t}\n\n\t/*\n\t * This should be true for injected/management frames only, for\n\t * management frames we have set the IEEE80211_TX_CTL_ASSIGN_SEQ\n\t * above since they are not QoS-data frames.\n\t */\n\tif (!tx->sta)\n\t\treturn TX_CONTINUE;\n\n\t/* include per-STA, per-TID sequence counter */\n\ttid = ieee80211_get_tid(hdr);\n\ttx->sta->tx_stats.msdu[tid]++;\n\n\thdr->seq_ctrl = ieee80211_tx_next_seq(tx->sta, tid);\n\n\treturn TX_CONTINUE;\n}\n\nstatic int ieee80211_fragment(struct ieee80211_tx_data *tx,\n\t\t\t      struct sk_buff *skb, int hdrlen,\n\t\t\t      int frag_threshold)\n{\n\tstruct ieee80211_local *local = tx->local;\n\tstruct ieee80211_tx_info *info;\n\tstruct sk_buff *tmp;\n\tint per_fragm = frag_threshold - hdrlen - FCS_LEN;\n\tint pos = hdrlen + per_fragm;\n\tint rem = skb->len - hdrlen - per_fragm;\n\n\tif (WARN_ON(rem < 0))\n\t\treturn -EINVAL;\n\n\t/* first fragment was already added to queue by caller */\n\n\twhile (rem) {\n\t\tint fraglen = per_fragm;\n\n\t\tif (fraglen > rem)\n\t\t\tfraglen = rem;\n\t\trem -= fraglen;\n\t\ttmp = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t    frag_threshold +\n\t\t\t\t    tx->sdata->encrypt_headroom +\n\t\t\t\t    IEEE80211_ENCRYPT_TAILROOM);\n\t\tif (!tmp)\n\t\t\treturn -ENOMEM;\n\n\t\t__skb_queue_tail(&tx->skbs, tmp);\n\n\t\tskb_reserve(tmp,\n\t\t\t    local->tx_headroom + tx->sdata->encrypt_headroom);\n\n\t\t/* copy control information */\n\t\tmemcpy(tmp->cb, skb->cb, sizeof(tmp->cb));\n\n\t\tinfo = IEEE80211_SKB_CB(tmp);\n\t\tinfo->flags &= ~(IEEE80211_TX_CTL_CLEAR_PS_FILT |\n\t\t\t\t IEEE80211_TX_CTL_FIRST_FRAGMENT);\n\n\t\tif (rem)\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_MORE_FRAMES;\n\n\t\tskb_copy_queue_mapping(tmp, skb);\n\t\ttmp->priority = skb->priority;\n\t\ttmp->dev = skb->dev;\n\n\t\t/* copy header and data */\n\t\tskb_put_data(tmp, skb->data, hdrlen);\n\t\tskb_put_data(tmp, skb->data + pos, fraglen);\n\n\t\tpos += fraglen;\n\t}\n\n\t/* adjust first fragment's length */\n\tskb_trim(skb, hdrlen + per_fragm);\n\treturn 0;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_fragment(struct ieee80211_tx_data *tx)\n{\n\tstruct sk_buff *skb = tx->skb;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tint frag_threshold = tx->local->hw.wiphy->frag_threshold;\n\tint hdrlen;\n\tint fragnum;\n\n\t/* no matter what happens, tx->skb moves to tx->skbs */\n\t__skb_queue_tail(&tx->skbs, skb);\n\ttx->skb = NULL;\n\n\tif (info->flags & IEEE80211_TX_CTL_DONTFRAG)\n\t\treturn TX_CONTINUE;\n\n\tif (ieee80211_hw_check(&tx->local->hw, SUPPORTS_TX_FRAG))\n\t\treturn TX_CONTINUE;\n\n\t/*\n\t * Warn when submitting a fragmented A-MPDU frame and drop it.\n\t * This scenario is handled in ieee80211_tx_prepare but extra\n\t * caution taken here as fragmented ampdu may cause Tx stop.\n\t */\n\tif (WARN_ON(info->flags & IEEE80211_TX_CTL_AMPDU))\n\t\treturn TX_DROP;\n\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\t/* internal error, why isn't DONTFRAG set? */\n\tif (WARN_ON(skb->len + FCS_LEN <= frag_threshold))\n\t\treturn TX_DROP;\n\n\t/*\n\t * Now fragment the frame. This will allocate all the fragments and\n\t * chain them (using skb as the first fragment) to skb->next.\n\t * During transmission, we will remove the successfully transmitted\n\t * fragments from this list. When the low-level driver rejects one\n\t * of the fragments then we will simply pretend to accept the skb\n\t * but store it away as pending.\n\t */\n\tif (ieee80211_fragment(tx, skb, hdrlen, frag_threshold))\n\t\treturn TX_DROP;\n\n\t/* update duration/seq/flags of fragments */\n\tfragnum = 0;\n\n\tskb_queue_walk(&tx->skbs, skb) {\n\t\tconst __le16 morefrags = cpu_to_le16(IEEE80211_FCTL_MOREFRAGS);\n\n\t\thdr = (void *)skb->data;\n\t\tinfo = IEEE80211_SKB_CB(skb);\n\n\t\tif (!skb_queue_is_last(&tx->skbs, skb)) {\n\t\t\thdr->frame_control |= morefrags;\n\t\t\t/*\n\t\t\t * No multi-rate retries for fragmented frames, that\n\t\t\t * would completely throw off the NAV at other STAs.\n\t\t\t */\n\t\t\tinfo->control.rates[1].idx = -1;\n\t\t\tinfo->control.rates[2].idx = -1;\n\t\t\tinfo->control.rates[3].idx = -1;\n\t\t\tBUILD_BUG_ON(IEEE80211_TX_MAX_RATES != 4);\n\t\t\tinfo->flags &= ~IEEE80211_TX_CTL_RATE_CTRL_PROBE;\n\t\t} else {\n\t\t\thdr->frame_control &= ~morefrags;\n\t\t}\n\t\thdr->seq_ctrl |= cpu_to_le16(fragnum & IEEE80211_SCTL_FRAG);\n\t\tfragnum++;\n\t}\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_stats(struct ieee80211_tx_data *tx)\n{\n\tstruct sk_buff *skb;\n\tint ac = -1;\n\n\tif (!tx->sta)\n\t\treturn TX_CONTINUE;\n\n\tskb_queue_walk(&tx->skbs, skb) {\n\t\tac = skb_get_queue_mapping(skb);\n\t\ttx->sta->tx_stats.bytes[ac] += skb->len;\n\t}\n\tif (ac >= 0)\n\t\ttx->sta->tx_stats.packets[ac]++;\n\n\treturn TX_CONTINUE;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_encrypt(struct ieee80211_tx_data *tx)\n{\n\tif (!tx->key)\n\t\treturn TX_CONTINUE;\n\n\tswitch (tx->key->conf.cipher) {\n\tcase WLAN_CIPHER_SUITE_WEP40:\n\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\treturn ieee80211_crypto_wep_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\treturn ieee80211_crypto_tkip_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\treturn ieee80211_crypto_ccmp_encrypt(\n\t\t\ttx, IEEE80211_CCMP_MIC_LEN);\n\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\treturn ieee80211_crypto_ccmp_encrypt(\n\t\t\ttx, IEEE80211_CCMP_256_MIC_LEN);\n\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\treturn ieee80211_crypto_aes_cmac_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\treturn ieee80211_crypto_aes_cmac_256_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\treturn ieee80211_crypto_aes_gmac_encrypt(tx);\n\tcase WLAN_CIPHER_SUITE_GCMP:\n\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\treturn ieee80211_crypto_gcmp_encrypt(tx);\n\tdefault:\n\t\treturn ieee80211_crypto_hw_encrypt(tx);\n\t}\n\n\treturn TX_DROP;\n}\n\nstatic ieee80211_tx_result debug_noinline\nieee80211_tx_h_calculate_duration(struct ieee80211_tx_data *tx)\n{\n\tstruct sk_buff *skb;\n\tstruct ieee80211_hdr *hdr;\n\tint next_len;\n\tbool group_addr;\n\n\tskb_queue_walk(&tx->skbs, skb) {\n\t\thdr = (void *) skb->data;\n\t\tif (unlikely(ieee80211_is_pspoll(hdr->frame_control)))\n\t\t\tbreak; /* must not overwrite AID */\n\t\tif (!skb_queue_is_last(&tx->skbs, skb)) {\n\t\t\tstruct sk_buff *next = skb_queue_next(&tx->skbs, skb);\n\t\t\tnext_len = next->len;\n\t\t} else\n\t\t\tnext_len = 0;\n\t\tgroup_addr = is_multicast_ether_addr(hdr->addr1);\n\n\t\thdr->duration_id =\n\t\t\tieee80211_duration(tx, skb, group_addr, next_len);\n\t}\n\n\treturn TX_CONTINUE;\n}\n\n/* actual transmit path */\n\nstatic bool ieee80211_tx_prep_agg(struct ieee80211_tx_data *tx,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct ieee80211_tx_info *info,\n\t\t\t\t  struct tid_ampdu_tx *tid_tx,\n\t\t\t\t  int tid)\n{\n\tbool queued = false;\n\tbool reset_agg_timer = false;\n\tstruct sk_buff *purge_skb = NULL;\n\n\tif (test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state)) {\n\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\t\treset_agg_timer = true;\n\t} else if (test_bit(HT_AGG_STATE_WANT_START, &tid_tx->state)) {\n\t\t/*\n\t\t * nothing -- this aggregation session is being started\n\t\t * but that might still fail with the driver\n\t\t */\n\t} else if (!tx->sta->sta.txq[tid]) {\n\t\tspin_lock(&tx->sta->lock);\n\t\t/*\n\t\t * Need to re-check now, because we may get here\n\t\t *\n\t\t *  1) in the window during which the setup is actually\n\t\t *     already done, but not marked yet because not all\n\t\t *     packets are spliced over to the driver pending\n\t\t *     queue yet -- if this happened we acquire the lock\n\t\t *     either before or after the splice happens, but\n\t\t *     need to recheck which of these cases happened.\n\t\t *\n\t\t *  2) during session teardown, if the OPERATIONAL bit\n\t\t *     was cleared due to the teardown but the pointer\n\t\t *     hasn't been assigned NULL yet (or we loaded it\n\t\t *     before it was assigned) -- in this case it may\n\t\t *     now be NULL which means we should just let the\n\t\t *     packet pass through because splicing the frames\n\t\t *     back is already done.\n\t\t */\n\t\ttid_tx = rcu_dereference_protected_tid_tx(tx->sta, tid);\n\n\t\tif (!tid_tx) {\n\t\t\t/* do nothing, let packet pass through */\n\t\t} else if (test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state)) {\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\t\t\treset_agg_timer = true;\n\t\t} else {\n\t\t\tqueued = true;\n\t\t\tif (info->flags & IEEE80211_TX_CTL_NO_PS_BUFFER) {\n\t\t\t\tclear_sta_flag(tx->sta, WLAN_STA_SP);\n\t\t\t\tps_dbg(tx->sta->sdata,\n\t\t\t\t       \"STA %pM aid %d: SP frame queued, close the SP w/o telling the peer\\n\",\n\t\t\t\t       tx->sta->sta.addr, tx->sta->sta.aid);\n\t\t\t}\n\t\t\tinfo->control.vif = &tx->sdata->vif;\n\t\t\tinfo->control.flags |= IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\t\t\tinfo->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;\n\t\t\t__skb_queue_tail(&tid_tx->pending, skb);\n\t\t\tif (skb_queue_len(&tid_tx->pending) > STA_MAX_TX_BUFFER)\n\t\t\t\tpurge_skb = __skb_dequeue(&tid_tx->pending);\n\t\t}\n\t\tspin_unlock(&tx->sta->lock);\n\n\t\tif (purge_skb)\n\t\t\tieee80211_free_txskb(&tx->local->hw, purge_skb);\n\t}\n\n\t/* reset session timer */\n\tif (reset_agg_timer)\n\t\ttid_tx->last_tx = jiffies;\n\n\treturn queued;\n}\n\n/*\n * initialises @tx\n * pass %NULL for the station if unknown, a valid pointer if known\n * or an ERR_PTR() if the station is known not to exist\n */\nstatic ieee80211_tx_result\nieee80211_tx_prepare(struct ieee80211_sub_if_data *sdata,\n\t\t     struct ieee80211_tx_data *tx,\n\t\t     struct sta_info *sta, struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tint tid;\n\n\tmemset(tx, 0, sizeof(*tx));\n\ttx->skb = skb;\n\ttx->local = local;\n\ttx->sdata = sdata;\n\t__skb_queue_head_init(&tx->skbs);\n\n\t/*\n\t * If this flag is set to true anywhere, and we get here,\n\t * we are doing the needed processing, so remove the flag\n\t * now.\n\t */\n\tinfo->control.flags &= ~IEEE80211_TX_INTCFL_NEED_TXPROCESSING;\n\n\thdr = (struct ieee80211_hdr *) skb->data;\n\n\tif (likely(sta)) {\n\t\tif (!IS_ERR(sta))\n\t\t\ttx->sta = sta;\n\t} else {\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN) {\n\t\t\ttx->sta = rcu_dereference(sdata->u.vlan.sta);\n\t\t\tif (!tx->sta && sdata->wdev.use_4addr)\n\t\t\t\treturn TX_DROP;\n\t\t} else if (info->flags & (IEEE80211_TX_INTFL_NL80211_FRAME_TX |\n\t\t\t\t\t  IEEE80211_TX_CTL_INJECTED) ||\n\t\t\t   tx->sdata->control_port_protocol == tx->skb->protocol) {\n\t\t\ttx->sta = sta_info_get_bss(sdata, hdr->addr1);\n\t\t}\n\t\tif (!tx->sta && !is_multicast_ether_addr(hdr->addr1))\n\t\t\ttx->sta = sta_info_get(sdata, hdr->addr1);\n\t}\n\n\tif (tx->sta && ieee80211_is_data_qos(hdr->frame_control) &&\n\t    !ieee80211_is_qos_nullfunc(hdr->frame_control) &&\n\t    ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION) &&\n\t    !ieee80211_hw_check(&local->hw, TX_AMPDU_SETUP_IN_HW)) {\n\t\tstruct tid_ampdu_tx *tid_tx;\n\n\t\ttid = ieee80211_get_tid(hdr);\n\n\t\ttid_tx = rcu_dereference(tx->sta->ampdu_mlme.tid_tx[tid]);\n\t\tif (tid_tx) {\n\t\t\tbool queued;\n\n\t\t\tqueued = ieee80211_tx_prep_agg(tx, skb, info,\n\t\t\t\t\t\t       tid_tx, tid);\n\n\t\t\tif (unlikely(queued))\n\t\t\t\treturn TX_QUEUED;\n\t\t}\n\t}\n\n\tif (is_multicast_ether_addr(hdr->addr1)) {\n\t\ttx->flags &= ~IEEE80211_TX_UNICAST;\n\t\tinfo->flags |= IEEE80211_TX_CTL_NO_ACK;\n\t} else\n\t\ttx->flags |= IEEE80211_TX_UNICAST;\n\n\tif (!(info->flags & IEEE80211_TX_CTL_DONTFRAG)) {\n\t\tif (!(tx->flags & IEEE80211_TX_UNICAST) ||\n\t\t    skb->len + FCS_LEN <= local->hw.wiphy->frag_threshold ||\n\t\t    info->flags & IEEE80211_TX_CTL_AMPDU)\n\t\t\tinfo->flags |= IEEE80211_TX_CTL_DONTFRAG;\n\t}\n\n\tif (!tx->sta)\n\t\tinfo->flags |= IEEE80211_TX_CTL_CLEAR_PS_FILT;\n\telse if (test_and_clear_sta_flag(tx->sta, WLAN_STA_CLEAR_PS_FILT)) {\n\t\tinfo->flags |= IEEE80211_TX_CTL_CLEAR_PS_FILT;\n\t\tieee80211_check_fast_xmit(tx->sta);\n\t}\n\n\tinfo->flags |= IEEE80211_TX_CTL_FIRST_FRAGMENT;\n\n\treturn TX_CONTINUE;\n}\n\nstatic struct txq_info *ieee80211_get_txq(struct ieee80211_local *local,\n\t\t\t\t\t  struct ieee80211_vif *vif,\n\t\t\t\t\t  struct sta_info *sta,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_txq *txq = NULL;\n\n\tif ((info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM) ||\n\t    (info->control.flags & IEEE80211_TX_CTRL_PS_RESPONSE))\n\t\treturn NULL;\n\n\tif (!(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) &&\n\t    unlikely(!ieee80211_is_data_present(hdr->frame_control))) {\n\t\tif ((!ieee80211_is_mgmt(hdr->frame_control) ||\n\t\t     ieee80211_is_bufferable_mmpdu(hdr->frame_control) ||\n\t\t     vif->type == NL80211_IFTYPE_STATION) &&\n\t\t    sta && sta->uploaded) {\n\t\t\t/*\n\t\t\t * This will be NULL if the driver didn't set the\n\t\t\t * opt-in hardware flag.\n\t\t\t */\n\t\t\ttxq = sta->sta.txq[IEEE80211_NUM_TIDS];\n\t\t}\n\t} else if (sta) {\n\t\tu8 tid = skb->priority & IEEE80211_QOS_CTL_TID_MASK;\n\n\t\tif (!sta->uploaded)\n\t\t\treturn NULL;\n\n\t\ttxq = sta->sta.txq[tid];\n\t} else if (vif) {\n\t\ttxq = vif->txq;\n\t}\n\n\tif (!txq)\n\t\treturn NULL;\n\n\treturn to_txq_info(txq);\n}\n\nstatic void ieee80211_set_skb_enqueue_time(struct sk_buff *skb)\n{\n\tIEEE80211_SKB_CB(skb)->control.enqueue_time = codel_get_time();\n}\n\nstatic u32 codel_skb_len_func(const struct sk_buff *skb)\n{\n\treturn skb->len;\n}\n\nstatic codel_time_t codel_skb_time_func(const struct sk_buff *skb)\n{\n\tconst struct ieee80211_tx_info *info;\n\n\tinfo = (const struct ieee80211_tx_info *)skb->cb;\n\treturn info->control.enqueue_time;\n}\n\nstatic struct sk_buff *codel_dequeue_func(struct codel_vars *cvars,\n\t\t\t\t\t  void *ctx)\n{\n\tstruct ieee80211_local *local;\n\tstruct txq_info *txqi;\n\tstruct fq *fq;\n\tstruct fq_flow *flow;\n\n\ttxqi = ctx;\n\tlocal = vif_to_sdata(txqi->txq.vif)->local;\n\tfq = &local->fq;\n\n\tif (cvars == &txqi->def_cvars)\n\t\tflow = &txqi->def_flow;\n\telse\n\t\tflow = &fq->flows[cvars - local->cvars];\n\n\treturn fq_flow_dequeue(fq, flow);\n}\n\nstatic void codel_drop_func(struct sk_buff *skb,\n\t\t\t    void *ctx)\n{\n\tstruct ieee80211_local *local;\n\tstruct ieee80211_hw *hw;\n\tstruct txq_info *txqi;\n\n\ttxqi = ctx;\n\tlocal = vif_to_sdata(txqi->txq.vif)->local;\n\thw = &local->hw;\n\n\tieee80211_free_txskb(hw, skb);\n}\n\nstatic struct sk_buff *fq_tin_dequeue_func(struct fq *fq,\n\t\t\t\t\t   struct fq_tin *tin,\n\t\t\t\t\t   struct fq_flow *flow)\n{\n\tstruct ieee80211_local *local;\n\tstruct txq_info *txqi;\n\tstruct codel_vars *cvars;\n\tstruct codel_params *cparams;\n\tstruct codel_stats *cstats;\n\n\tlocal = container_of(fq, struct ieee80211_local, fq);\n\ttxqi = container_of(tin, struct txq_info, tin);\n\tcstats = &txqi->cstats;\n\n\tif (txqi->txq.sta) {\n\t\tstruct sta_info *sta = container_of(txqi->txq.sta,\n\t\t\t\t\t\t    struct sta_info, sta);\n\t\tcparams = &sta->cparams;\n\t} else {\n\t\tcparams = &local->cparams;\n\t}\n\n\tif (flow == &txqi->def_flow)\n\t\tcvars = &txqi->def_cvars;\n\telse\n\t\tcvars = &local->cvars[flow - fq->flows];\n\n\treturn codel_dequeue(txqi,\n\t\t\t     &flow->backlog,\n\t\t\t     cparams,\n\t\t\t     cvars,\n\t\t\t     cstats,\n\t\t\t     codel_skb_len_func,\n\t\t\t     codel_skb_time_func,\n\t\t\t     codel_drop_func,\n\t\t\t     codel_dequeue_func);\n}\n\nstatic void fq_skb_free_func(struct fq *fq,\n\t\t\t     struct fq_tin *tin,\n\t\t\t     struct fq_flow *flow,\n\t\t\t     struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local;\n\n\tlocal = container_of(fq, struct ieee80211_local, fq);\n\tieee80211_free_txskb(&local->hw, skb);\n}\n\nstatic struct fq_flow *fq_flow_get_default_func(struct fq *fq,\n\t\t\t\t\t\tstruct fq_tin *tin,\n\t\t\t\t\t\tint idx,\n\t\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct txq_info *txqi;\n\n\ttxqi = container_of(tin, struct txq_info, tin);\n\treturn &txqi->def_flow;\n}\n\nstatic void ieee80211_txq_enqueue(struct ieee80211_local *local,\n\t\t\t\t  struct txq_info *txqi,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin = &txqi->tin;\n\tu32 flow_idx = fq_flow_idx(fq, skb);\n\n\tieee80211_set_skb_enqueue_time(skb);\n\n\tspin_lock_bh(&fq->lock);\n\tfq_tin_enqueue(fq, tin, flow_idx, skb,\n\t\t       fq_skb_free_func,\n\t\t       fq_flow_get_default_func);\n\tspin_unlock_bh(&fq->lock);\n}\n\nstatic bool fq_vlan_filter_func(struct fq *fq, struct fq_tin *tin,\n\t\t\t\tstruct fq_flow *flow, struct sk_buff *skb,\n\t\t\t\tvoid *data)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\treturn info->control.vif == data;\n}\n\nvoid ieee80211_txq_remove_vlan(struct ieee80211_local *local,\n\t\t\t       struct ieee80211_sub_if_data *sdata)\n{\n\tstruct fq *fq = &local->fq;\n\tstruct txq_info *txqi;\n\tstruct fq_tin *tin;\n\tstruct ieee80211_sub_if_data *ap;\n\n\tif (WARN_ON(sdata->vif.type != NL80211_IFTYPE_AP_VLAN))\n\t\treturn;\n\n\tap = container_of(sdata->bss, struct ieee80211_sub_if_data, u.ap);\n\n\tif (!ap->vif.txq)\n\t\treturn;\n\n\ttxqi = to_txq_info(ap->vif.txq);\n\ttin = &txqi->tin;\n\n\tspin_lock_bh(&fq->lock);\n\tfq_tin_filter(fq, tin, fq_vlan_filter_func, &sdata->vif,\n\t\t      fq_skb_free_func);\n\tspin_unlock_bh(&fq->lock);\n}\n\nvoid ieee80211_txq_init(struct ieee80211_sub_if_data *sdata,\n\t\t\tstruct sta_info *sta,\n\t\t\tstruct txq_info *txqi, int tid)\n{\n\tfq_tin_init(&txqi->tin);\n\tfq_flow_init(&txqi->def_flow);\n\tcodel_vars_init(&txqi->def_cvars);\n\tcodel_stats_init(&txqi->cstats);\n\t__skb_queue_head_init(&txqi->frags);\n\tINIT_LIST_HEAD(&txqi->schedule_order);\n\n\ttxqi->txq.vif = &sdata->vif;\n\n\tif (!sta) {\n\t\tsdata->vif.txq = &txqi->txq;\n\t\ttxqi->txq.tid = 0;\n\t\ttxqi->txq.ac = IEEE80211_AC_BE;\n\n\t\treturn;\n\t}\n\n\tif (tid == IEEE80211_NUM_TIDS) {\n\t\tif (sdata->vif.type == NL80211_IFTYPE_STATION) {\n\t\t\t/* Drivers need to opt in to the management MPDU TXQ */\n\t\t\tif (!ieee80211_hw_check(&sdata->local->hw,\n\t\t\t\t\t\tSTA_MMPDU_TXQ))\n\t\t\t\treturn;\n\t\t} else if (!ieee80211_hw_check(&sdata->local->hw,\n\t\t\t\t\t       BUFF_MMPDU_TXQ)) {\n\t\t\t/* Drivers need to opt in to the bufferable MMPDU TXQ */\n\t\t\treturn;\n\t\t}\n\t\ttxqi->txq.ac = IEEE80211_AC_VO;\n\t} else {\n\t\ttxqi->txq.ac = ieee80211_ac_from_tid(tid);\n\t}\n\n\ttxqi->txq.sta = &sta->sta;\n\ttxqi->txq.tid = tid;\n\tsta->sta.txq[tid] = &txqi->txq;\n}\n\nvoid ieee80211_txq_purge(struct ieee80211_local *local,\n\t\t\t struct txq_info *txqi)\n{\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin = &txqi->tin;\n\n\tspin_lock_bh(&fq->lock);\n\tfq_tin_reset(fq, tin, fq_skb_free_func);\n\tieee80211_purge_tx_queue(&local->hw, &txqi->frags);\n\tspin_unlock_bh(&fq->lock);\n\n\tspin_lock_bh(&local->active_txq_lock[txqi->txq.ac]);\n\tlist_del_init(&txqi->schedule_order);\n\tspin_unlock_bh(&local->active_txq_lock[txqi->txq.ac]);\n}\n\nvoid ieee80211_txq_set_params(struct ieee80211_local *local)\n{\n\tif (local->hw.wiphy->txq_limit)\n\t\tlocal->fq.limit = local->hw.wiphy->txq_limit;\n\telse\n\t\tlocal->hw.wiphy->txq_limit = local->fq.limit;\n\n\tif (local->hw.wiphy->txq_memory_limit)\n\t\tlocal->fq.memory_limit = local->hw.wiphy->txq_memory_limit;\n\telse\n\t\tlocal->hw.wiphy->txq_memory_limit = local->fq.memory_limit;\n\n\tif (local->hw.wiphy->txq_quantum)\n\t\tlocal->fq.quantum = local->hw.wiphy->txq_quantum;\n\telse\n\t\tlocal->hw.wiphy->txq_quantum = local->fq.quantum;\n}\n\nint ieee80211_txq_setup_flows(struct ieee80211_local *local)\n{\n\tstruct fq *fq = &local->fq;\n\tint ret;\n\tint i;\n\tbool supp_vht = false;\n\tenum nl80211_band band;\n\n\tif (!local->ops->wake_tx_queue)\n\t\treturn 0;\n\n\tret = fq_init(fq, 4096);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If the hardware doesn't support VHT, it is safe to limit the maximum\n\t * queue size. 4 Mbytes is 64 max-size aggregates in 802.11n.\n\t */\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband;\n\n\t\tsband = local->hw.wiphy->bands[band];\n\t\tif (!sband)\n\t\t\tcontinue;\n\n\t\tsupp_vht = supp_vht || sband->vht_cap.vht_supported;\n\t}\n\n\tif (!supp_vht)\n\t\tfq->memory_limit = 4 << 20; /* 4 Mbytes */\n\n\tcodel_params_init(&local->cparams);\n\tlocal->cparams.interval = MS2TIME(100);\n\tlocal->cparams.target = MS2TIME(20);\n\tlocal->cparams.ecn = true;\n\n\tlocal->cvars = kcalloc(fq->flows_cnt, sizeof(local->cvars[0]),\n\t\t\t       GFP_KERNEL);\n\tif (!local->cvars) {\n\t\tspin_lock_bh(&fq->lock);\n\t\tfq_reset(fq, fq_skb_free_func);\n\t\tspin_unlock_bh(&fq->lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < fq->flows_cnt; i++)\n\t\tcodel_vars_init(&local->cvars[i]);\n\n\tieee80211_txq_set_params(local);\n\n\treturn 0;\n}\n\nvoid ieee80211_txq_teardown_flows(struct ieee80211_local *local)\n{\n\tstruct fq *fq = &local->fq;\n\n\tif (!local->ops->wake_tx_queue)\n\t\treturn;\n\n\tkfree(local->cvars);\n\tlocal->cvars = NULL;\n\n\tspin_lock_bh(&fq->lock);\n\tfq_reset(fq, fq_skb_free_func);\n\tspin_unlock_bh(&fq->lock);\n}\n\nstatic bool ieee80211_queue_skb(struct ieee80211_local *local,\n\t\t\t\tstruct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct sta_info *sta,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct ieee80211_vif *vif;\n\tstruct txq_info *txqi;\n\n\tif (!local->ops->wake_tx_queue ||\n\t    sdata->vif.type == NL80211_IFTYPE_MONITOR)\n\t\treturn false;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\n\tvif = &sdata->vif;\n\ttxqi = ieee80211_get_txq(local, vif, sta, skb);\n\n\tif (!txqi)\n\t\treturn false;\n\n\tieee80211_txq_enqueue(local, txqi, skb);\n\n\tschedule_and_wake_txq(local, txqi);\n\n\treturn true;\n}\n\nstatic bool ieee80211_tx_frags(struct ieee80211_local *local,\n\t\t\t       struct ieee80211_vif *vif,\n\t\t\t       struct sta_info *sta,\n\t\t\t       struct sk_buff_head *skbs,\n\t\t\t       bool txpending)\n{\n\tstruct ieee80211_tx_control control = {};\n\tstruct sk_buff *skb, *tmp;\n\tunsigned long flags;\n\n\tskb_queue_walk_safe(skbs, skb, tmp) {\n\t\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\t\tint q = info->hw_queue;\n\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\t\tif (WARN_ON_ONCE(q >= local->hw.queues)) {\n\t\t\t__skb_unlink(skb, skbs);\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\tspin_lock_irqsave(&local->queue_stop_reason_lock, flags);\n\t\tif (local->queue_stop_reasons[q] ||\n\t\t    (!txpending && !skb_queue_empty(&local->pending[q]))) {\n\t\t\tif (unlikely(info->flags &\n\t\t\t\t     IEEE80211_TX_INTFL_OFFCHAN_TX_OK)) {\n\t\t\t\tif (local->queue_stop_reasons[q] &\n\t\t\t\t    ~BIT(IEEE80211_QUEUE_STOP_REASON_OFFCHANNEL)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Drop off-channel frames if queues\n\t\t\t\t\t * are stopped for any reason other\n\t\t\t\t\t * than off-channel operation. Never\n\t\t\t\t\t * queue them.\n\t\t\t\t\t */\n\t\t\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t\t&local->queue_stop_reason_lock,\n\t\t\t\t\t\tflags);\n\t\t\t\t\tieee80211_purge_tx_queue(&local->hw,\n\t\t\t\t\t\t\t\t skbs);\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t} else {\n\n\t\t\t\t/*\n\t\t\t\t * Since queue is stopped, queue up frames for\n\t\t\t\t * later transmission from the tx-pending\n\t\t\t\t * tasklet when the queue is woken again.\n\t\t\t\t */\n\t\t\t\tif (txpending)\n\t\t\t\t\tskb_queue_splice_init(skbs,\n\t\t\t\t\t\t\t      &local->pending[q]);\n\t\t\t\telse\n\t\t\t\t\tskb_queue_splice_tail_init(skbs,\n\t\t\t\t\t\t\t\t   &local->pending[q]);\n\n\t\t\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\t\tinfo->control.vif = vif;\n\t\tcontrol.sta = sta ? &sta->sta : NULL;\n\n\t\t__skb_unlink(skb, skbs);\n\t\tdrv_tx(local, &control, skb);\n\t}\n\n\treturn true;\n}\n\n/*\n * Returns false if the frame couldn't be transmitted but was queued instead.\n */\nstatic bool __ieee80211_tx(struct ieee80211_local *local,\n\t\t\t   struct sk_buff_head *skbs, int led_len,\n\t\t\t   struct sta_info *sta, bool txpending)\n{\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_vif *vif;\n\tstruct sk_buff *skb;\n\tbool result = true;\n\t__le16 fc;\n\n\tif (WARN_ON(skb_queue_empty(skbs)))\n\t\treturn true;\n\n\tskb = skb_peek(skbs);\n\tfc = ((struct ieee80211_hdr *)skb->data)->frame_control;\n\tinfo = IEEE80211_SKB_CB(skb);\n\tsdata = vif_to_sdata(info->control.vif);\n\tif (sta && !sta->uploaded)\n\t\tsta = NULL;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_MONITOR:\n\t\tif (sdata->u.mntr.flags & MONITOR_FLAG_ACTIVE) {\n\t\t\tvif = &sdata->vif;\n\t\t\tbreak;\n\t\t}\n\t\tsdata = rcu_dereference(local->monitor_sdata);\n\t\tif (sdata) {\n\t\t\tvif = &sdata->vif;\n\t\t\tinfo->hw_queue =\n\t\t\t\tvif->hw_queue[skb_get_queue_mapping(skb)];\n\t\t} else if (ieee80211_hw_check(&local->hw, QUEUE_CONTROL)) {\n\t\t\tieee80211_purge_tx_queue(&local->hw, skbs);\n\t\t\treturn true;\n\t\t} else\n\t\t\tvif = NULL;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\t\tfallthrough;\n\tdefault:\n\t\tvif = &sdata->vif;\n\t\tbreak;\n\t}\n\n\tresult = ieee80211_tx_frags(local, vif, sta, skbs, txpending);\n\n\tieee80211_tpt_led_trig_tx(local, fc, led_len);\n\n\tWARN_ON_ONCE(!skb_queue_empty(skbs));\n\n\treturn result;\n}\n\n/*\n * Invoke TX handlers, return 0 on success and non-zero if the\n * frame was dropped or queued.\n *\n * The handlers are split into an early and late part. The latter is everything\n * that can be sensitive to reordering, and will be deferred to after packets\n * are dequeued from the intermediate queues (when they are enabled).\n */\nstatic int invoke_tx_handlers_early(struct ieee80211_tx_data *tx)\n{\n\tieee80211_tx_result res = TX_DROP;\n\n#define CALL_TXH(txh) \\\n\tdo {\t\t\t\t\\\n\t\tres = txh(tx);\t\t\\\n\t\tif (res != TX_CONTINUE)\t\\\n\t\t\tgoto txh_done;\t\\\n\t} while (0)\n\n\tCALL_TXH(ieee80211_tx_h_dynamic_ps);\n\tCALL_TXH(ieee80211_tx_h_check_assoc);\n\tCALL_TXH(ieee80211_tx_h_ps_buf);\n\tCALL_TXH(ieee80211_tx_h_check_control_port_protocol);\n\tCALL_TXH(ieee80211_tx_h_select_key);\n\tif (!ieee80211_hw_check(&tx->local->hw, HAS_RATE_CONTROL))\n\t\tCALL_TXH(ieee80211_tx_h_rate_ctrl);\n\n txh_done:\n\tif (unlikely(res == TX_DROP)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_drop);\n\t\tif (tx->skb)\n\t\t\tieee80211_free_txskb(&tx->local->hw, tx->skb);\n\t\telse\n\t\t\tieee80211_purge_tx_queue(&tx->local->hw, &tx->skbs);\n\t\treturn -1;\n\t} else if (unlikely(res == TX_QUEUED)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_queued);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Late handlers can be called while the sta lock is held. Handlers that can\n * cause packets to be generated will cause deadlock!\n */\nstatic int invoke_tx_handlers_late(struct ieee80211_tx_data *tx)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(tx->skb);\n\tieee80211_tx_result res = TX_CONTINUE;\n\n\tif (unlikely(info->flags & IEEE80211_TX_INTFL_RETRANSMISSION)) {\n\t\t__skb_queue_tail(&tx->skbs, tx->skb);\n\t\ttx->skb = NULL;\n\t\tgoto txh_done;\n\t}\n\n\tCALL_TXH(ieee80211_tx_h_michael_mic_add);\n\tCALL_TXH(ieee80211_tx_h_sequence);\n\tCALL_TXH(ieee80211_tx_h_fragment);\n\t/* handlers after fragment must be aware of tx info fragmentation! */\n\tCALL_TXH(ieee80211_tx_h_stats);\n\tCALL_TXH(ieee80211_tx_h_encrypt);\n\tif (!ieee80211_hw_check(&tx->local->hw, HAS_RATE_CONTROL))\n\t\tCALL_TXH(ieee80211_tx_h_calculate_duration);\n#undef CALL_TXH\n\n txh_done:\n\tif (unlikely(res == TX_DROP)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_drop);\n\t\tif (tx->skb)\n\t\t\tieee80211_free_txskb(&tx->local->hw, tx->skb);\n\t\telse\n\t\t\tieee80211_purge_tx_queue(&tx->local->hw, &tx->skbs);\n\t\treturn -1;\n\t} else if (unlikely(res == TX_QUEUED)) {\n\t\tI802_DEBUG_INC(tx->local->tx_handlers_queued);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int invoke_tx_handlers(struct ieee80211_tx_data *tx)\n{\n\tint r = invoke_tx_handlers_early(tx);\n\n\tif (r)\n\t\treturn r;\n\treturn invoke_tx_handlers_late(tx);\n}\n\nbool ieee80211_tx_prepare_skb(struct ieee80211_hw *hw,\n\t\t\t      struct ieee80211_vif *vif, struct sk_buff *skb,\n\t\t\t      int band, struct ieee80211_sta **sta)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_tx_data tx;\n\tstruct sk_buff *skb2;\n\n\tif (ieee80211_tx_prepare(sdata, &tx, NULL, skb) == TX_DROP)\n\t\treturn false;\n\n\tinfo->band = band;\n\tinfo->control.vif = vif;\n\tinfo->hw_queue = vif->hw_queue[skb_get_queue_mapping(skb)];\n\n\tif (invoke_tx_handlers(&tx))\n\t\treturn false;\n\n\tif (sta) {\n\t\tif (tx.sta)\n\t\t\t*sta = &tx.sta->sta;\n\t\telse\n\t\t\t*sta = NULL;\n\t}\n\n\t/* this function isn't suitable for fragmented data frames */\n\tskb2 = __skb_dequeue(&tx.skbs);\n\tif (WARN_ON(skb2 != skb || !skb_queue_empty(&tx.skbs))) {\n\t\tieee80211_free_txskb(hw, skb2);\n\t\tieee80211_purge_tx_queue(hw, &tx.skbs);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(ieee80211_tx_prepare_skb);\n\n/*\n * Returns false if the frame couldn't be transmitted but was queued instead.\n */\nstatic bool ieee80211_tx(struct ieee80211_sub_if_data *sdata,\n\t\t\t struct sta_info *sta, struct sk_buff *skb,\n\t\t\t bool txpending)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_data tx;\n\tieee80211_tx_result res_prepare;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tbool result = true;\n\tint led_len;\n\n\tif (unlikely(skb->len < 10)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn true;\n\t}\n\n\t/* initialises tx */\n\tled_len = skb->len;\n\tres_prepare = ieee80211_tx_prepare(sdata, &tx, sta, skb);\n\n\tif (unlikely(res_prepare == TX_DROP)) {\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t\treturn true;\n\t} else if (unlikely(res_prepare == TX_QUEUED)) {\n\t\treturn true;\n\t}\n\n\t/* set up hw_queue value early */\n\tif (!(info->flags & IEEE80211_TX_CTL_TX_OFFCHAN) ||\n\t    !ieee80211_hw_check(&local->hw, QUEUE_CONTROL))\n\t\tinfo->hw_queue =\n\t\t\tsdata->vif.hw_queue[skb_get_queue_mapping(skb)];\n\n\tif (invoke_tx_handlers_early(&tx))\n\t\treturn true;\n\n\tif (ieee80211_queue_skb(local, sdata, tx.sta, tx.skb))\n\t\treturn true;\n\n\tif (!invoke_tx_handlers_late(&tx))\n\t\tresult = __ieee80211_tx(local, &tx.skbs, led_len,\n\t\t\t\t\ttx.sta, txpending);\n\n\treturn result;\n}\n\n/* device xmit handlers */\n\nenum ieee80211_encrypt {\n\tENCRYPT_NO,\n\tENCRYPT_MGMT,\n\tENCRYPT_DATA,\n};\n\nstatic int ieee80211_skb_resize(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tint head_need,\n\t\t\t\tenum ieee80211_encrypt encrypt)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tbool enc_tailroom;\n\tint tail_need = 0;\n\n\tenc_tailroom = encrypt == ENCRYPT_MGMT ||\n\t\t       (encrypt == ENCRYPT_DATA &&\n\t\t\tsdata->crypto_tx_tailroom_needed_cnt);\n\n\tif (enc_tailroom) {\n\t\ttail_need = IEEE80211_ENCRYPT_TAILROOM;\n\t\ttail_need -= skb_tailroom(skb);\n\t\ttail_need = max_t(int, tail_need, 0);\n\t}\n\n\tif (skb_cloned(skb) &&\n\t    (!ieee80211_hw_check(&local->hw, SUPPORTS_CLONED_SKBS) ||\n\t     !skb_clone_writable(skb, ETH_HLEN) || enc_tailroom))\n\t\tI802_DEBUG_INC(local->tx_expand_skb_head_cloned);\n\telse if (head_need || tail_need)\n\t\tI802_DEBUG_INC(local->tx_expand_skb_head);\n\telse\n\t\treturn 0;\n\n\tif (pskb_expand_head(skb, head_need, tail_need, GFP_ATOMIC)) {\n\t\twiphy_debug(local->hw.wiphy,\n\t\t\t    \"failed to reallocate TX buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nvoid ieee80211_xmit(struct ieee80211_sub_if_data *sdata,\n\t\t    struct sta_info *sta, struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *) skb->data;\n\tint headroom;\n\tenum ieee80211_encrypt encrypt;\n\n\tif (info->flags & IEEE80211_TX_INTFL_DONT_ENCRYPT)\n\t\tencrypt = ENCRYPT_NO;\n\telse if (ieee80211_is_mgmt(hdr->frame_control))\n\t\tencrypt = ENCRYPT_MGMT;\n\telse\n\t\tencrypt = ENCRYPT_DATA;\n\n\theadroom = local->tx_headroom;\n\tif (encrypt != ENCRYPT_NO)\n\t\theadroom += sdata->encrypt_headroom;\n\theadroom -= skb_headroom(skb);\n\theadroom = max_t(int, 0, headroom);\n\n\tif (ieee80211_skb_resize(sdata, skb, headroom, encrypt)) {\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t\treturn;\n\t}\n\n\t/* reload after potential resize */\n\thdr = (struct ieee80211_hdr *) skb->data;\n\tinfo->control.vif = &sdata->vif;\n\n\tif (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tif (ieee80211_is_data(hdr->frame_control) &&\n\t\t    is_unicast_ether_addr(hdr->addr1)) {\n\t\t\tif (mesh_nexthop_resolve(sdata, skb))\n\t\t\t\treturn; /* skb queued: don't free */\n\t\t} else {\n\t\t\tieee80211_mps_set_frame_flags(sdata, NULL, hdr);\n\t\t}\n\t}\n\n\tieee80211_set_qos_hdr(sdata, skb);\n\tieee80211_tx(sdata, sta, skb, false);\n}\n\nbool ieee80211_parse_tx_radiotap(struct sk_buff *skb,\n\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_radiotap_iterator iterator;\n\tstruct ieee80211_radiotap_header *rthdr =\n\t\t(struct ieee80211_radiotap_header *) skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_supported_band *sband =\n\t\tlocal->hw.wiphy->bands[info->band];\n\tint ret = ieee80211_radiotap_iterator_init(&iterator, rthdr, skb->len,\n\t\t\t\t\t\t   NULL);\n\tu16 txflags;\n\tu16 rate = 0;\n\tbool rate_found = false;\n\tu8 rate_retries = 0;\n\tu16 rate_flags = 0;\n\tu8 mcs_known, mcs_flags, mcs_bw;\n\tu16 vht_known;\n\tu8 vht_mcs = 0, vht_nss = 0;\n\tint i;\n\n\t/* check for not even having the fixed radiotap header part */\n\tif (unlikely(skb->len < sizeof(struct ieee80211_radiotap_header)))\n\t\treturn false; /* too short to be possibly valid */\n\n\t/* is it a header version we can trust to find length from? */\n\tif (unlikely(rthdr->it_version))\n\t\treturn false; /* only version 0 is supported */\n\n\t/* does the skb contain enough to deliver on the alleged length? */\n\tif (unlikely(skb->len < ieee80211_get_radiotap_len(skb->data)))\n\t\treturn false; /* skb too short for claimed rt header extent */\n\n\tinfo->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT |\n\t\t       IEEE80211_TX_CTL_DONTFRAG;\n\n\t/*\n\t * for every radiotap entry that is present\n\t * (ieee80211_radiotap_iterator_next returns -ENOENT when no more\n\t * entries present, or -EINVAL on error)\n\t */\n\n\twhile (!ret) {\n\t\tret = ieee80211_radiotap_iterator_next(&iterator);\n\n\t\tif (ret)\n\t\t\tcontinue;\n\n\t\t/* see if this argument is something we can use */\n\t\tswitch (iterator.this_arg_index) {\n\t\t/*\n\t\t * You must take care when dereferencing iterator.this_arg\n\t\t * for multibyte types... the pointer is not aligned.  Use\n\t\t * get_unaligned((type *)iterator.this_arg) to dereference\n\t\t * iterator.this_arg for type \"type\" safely on all arches.\n\t\t*/\n\t\tcase IEEE80211_RADIOTAP_FLAGS:\n\t\t\tif (*iterator.this_arg & IEEE80211_RADIOTAP_F_FCS) {\n\t\t\t\t/*\n\t\t\t\t * this indicates that the skb we have been\n\t\t\t\t * handed has the 32-bit FCS CRC at the end...\n\t\t\t\t * we should react to that by snipping it off\n\t\t\t\t * because it will be recomputed and added\n\t\t\t\t * on transmission\n\t\t\t\t */\n\t\t\t\tif (skb->len < (iterator._max_length + FCS_LEN))\n\t\t\t\t\treturn false;\n\n\t\t\t\tskb_trim(skb, skb->len - FCS_LEN);\n\t\t\t}\n\t\t\tif (*iterator.this_arg & IEEE80211_RADIOTAP_F_WEP)\n\t\t\t\tinfo->flags &= ~IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\t\t\tif (*iterator.this_arg & IEEE80211_RADIOTAP_F_FRAG)\n\t\t\t\tinfo->flags &= ~IEEE80211_TX_CTL_DONTFRAG;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_TX_FLAGS:\n\t\t\ttxflags = get_unaligned_le16(iterator.this_arg);\n\t\t\tif (txflags & IEEE80211_RADIOTAP_F_TX_NOACK)\n\t\t\t\tinfo->flags |= IEEE80211_TX_CTL_NO_ACK;\n\t\t\tif (txflags & IEEE80211_RADIOTAP_F_TX_NOSEQNO)\n\t\t\t\tinfo->control.flags |= IEEE80211_TX_CTRL_NO_SEQNO;\n\t\t\tif (txflags & IEEE80211_RADIOTAP_F_TX_ORDER)\n\t\t\t\tinfo->control.flags |=\n\t\t\t\t\tIEEE80211_TX_CTRL_DONT_REORDER;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_RATE:\n\t\t\trate = *iterator.this_arg;\n\t\t\trate_flags = 0;\n\t\t\trate_found = true;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_DATA_RETRIES:\n\t\t\trate_retries = *iterator.this_arg;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_MCS:\n\t\t\tmcs_known = iterator.this_arg[0];\n\t\t\tmcs_flags = iterator.this_arg[1];\n\t\t\tif (!(mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_MCS))\n\t\t\t\tbreak;\n\n\t\t\trate_found = true;\n\t\t\trate = iterator.this_arg[2];\n\t\t\trate_flags = IEEE80211_TX_RC_MCS;\n\n\t\t\tif (mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_GI &&\n\t\t\t    mcs_flags & IEEE80211_RADIOTAP_MCS_SGI)\n\t\t\t\trate_flags |= IEEE80211_TX_RC_SHORT_GI;\n\n\t\t\tmcs_bw = mcs_flags & IEEE80211_RADIOTAP_MCS_BW_MASK;\n\t\t\tif (mcs_known & IEEE80211_RADIOTAP_MCS_HAVE_BW &&\n\t\t\t    mcs_bw == IEEE80211_RADIOTAP_MCS_BW_40)\n\t\t\t\trate_flags |= IEEE80211_TX_RC_40_MHZ_WIDTH;\n\t\t\tbreak;\n\n\t\tcase IEEE80211_RADIOTAP_VHT:\n\t\t\tvht_known = get_unaligned_le16(iterator.this_arg);\n\t\t\trate_found = true;\n\n\t\t\trate_flags = IEEE80211_TX_RC_VHT_MCS;\n\t\t\tif ((vht_known & IEEE80211_RADIOTAP_VHT_KNOWN_GI) &&\n\t\t\t    (iterator.this_arg[2] &\n\t\t\t     IEEE80211_RADIOTAP_VHT_FLAG_SGI))\n\t\t\t\trate_flags |= IEEE80211_TX_RC_SHORT_GI;\n\t\t\tif (vht_known &\n\t\t\t    IEEE80211_RADIOTAP_VHT_KNOWN_BANDWIDTH) {\n\t\t\t\tif (iterator.this_arg[3] == 1)\n\t\t\t\t\trate_flags |=\n\t\t\t\t\t\tIEEE80211_TX_RC_40_MHZ_WIDTH;\n\t\t\t\telse if (iterator.this_arg[3] == 4)\n\t\t\t\t\trate_flags |=\n\t\t\t\t\t\tIEEE80211_TX_RC_80_MHZ_WIDTH;\n\t\t\t\telse if (iterator.this_arg[3] == 11)\n\t\t\t\t\trate_flags |=\n\t\t\t\t\t\tIEEE80211_TX_RC_160_MHZ_WIDTH;\n\t\t\t}\n\n\t\t\tvht_mcs = iterator.this_arg[4] >> 4;\n\t\t\tvht_nss = iterator.this_arg[4] & 0xF;\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Please update the file\n\t\t * Documentation/networking/mac80211-injection.rst\n\t\t * when parsing new fields here.\n\t\t */\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret != -ENOENT) /* ie, if we didn't simply run out of fields */\n\t\treturn false;\n\n\tif (rate_found) {\n\t\tinfo->control.flags |= IEEE80211_TX_CTRL_RATE_INJECT;\n\n\t\tfor (i = 0; i < IEEE80211_TX_MAX_RATES; i++) {\n\t\t\tinfo->control.rates[i].idx = -1;\n\t\t\tinfo->control.rates[i].flags = 0;\n\t\t\tinfo->control.rates[i].count = 0;\n\t\t}\n\n\t\tif (rate_flags & IEEE80211_TX_RC_MCS) {\n\t\t\tinfo->control.rates[0].idx = rate;\n\t\t} else if (rate_flags & IEEE80211_TX_RC_VHT_MCS) {\n\t\t\tieee80211_rate_set_vht(info->control.rates, vht_mcs,\n\t\t\t\t\t       vht_nss);\n\t\t} else {\n\t\t\tfor (i = 0; i < sband->n_bitrates; i++) {\n\t\t\t\tif (rate * 5 != sband->bitrates[i].bitrate)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tinfo->control.rates[0].idx = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (info->control.rates[0].idx < 0)\n\t\t\tinfo->control.flags &= ~IEEE80211_TX_CTRL_RATE_INJECT;\n\n\t\tinfo->control.rates[0].flags = rate_flags;\n\t\tinfo->control.rates[0].count = min_t(u8, rate_retries + 1,\n\t\t\t\t\t\t     local->hw.max_rate_tries);\n\t}\n\n\treturn true;\n}\n\nnetdev_tx_t ieee80211_monitor_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *dev)\n{\n\tstruct ieee80211_local *local = wdev_priv(dev->ieee80211_ptr);\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *tmp_sdata, *sdata;\n\tstruct cfg80211_chan_def *chandef;\n\tu16 len_rthdr;\n\tint hdrlen;\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t      IEEE80211_TX_CTL_INJECTED;\n\n\t/* Sanity-check and process the injection radiotap header */\n\tif (!ieee80211_parse_tx_radiotap(skb, dev))\n\t\tgoto fail;\n\n\t/* we now know there is a radiotap header with a length we can use */\n\tlen_rthdr = ieee80211_get_radiotap_len(skb->data);\n\n\t/*\n\t * fix up the pointers accounting for the radiotap\n\t * header still being in there.  We are being given\n\t * a precooked IEEE80211 header so no need for\n\t * normal processing\n\t */\n\tskb_set_mac_header(skb, len_rthdr);\n\t/*\n\t * these are just fixed to the end of the rt area since we\n\t * don't have any better information and at this point, nobody cares\n\t */\n\tskb_set_network_header(skb, len_rthdr);\n\tskb_set_transport_header(skb, len_rthdr);\n\n\tif (skb->len < len_rthdr + 2)\n\t\tgoto fail;\n\n\thdr = (struct ieee80211_hdr *)(skb->data + len_rthdr);\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\n\tif (skb->len < len_rthdr + hdrlen)\n\t\tgoto fail;\n\n\t/*\n\t * Initialize skb->protocol if the injected frame is a data frame\n\t * carrying a rfc1042 header\n\t */\n\tif (ieee80211_is_data(hdr->frame_control) &&\n\t    skb->len >= len_rthdr + hdrlen + sizeof(rfc1042_header) + 2) {\n\t\tu8 *payload = (u8 *)hdr + hdrlen;\n\n\t\tif (ether_addr_equal(payload, rfc1042_header))\n\t\t\tskb->protocol = cpu_to_be16((payload[6] << 8) |\n\t\t\t\t\t\t    payload[7]);\n\t}\n\n\t/* Initialize skb->priority for QoS frames. If the DONT_REORDER flag\n\t * is set, stick to the default value for skb->priority to assure\n\t * frames injected with this flag are not reordered relative to each\n\t * other.\n\t */\n\tif (ieee80211_is_data_qos(hdr->frame_control) &&\n\t    !(info->control.flags & IEEE80211_TX_CTRL_DONT_REORDER)) {\n\t\tu8 *p = ieee80211_get_qos_ctl(hdr);\n\t\tskb->priority = *p & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t}\n\n\trcu_read_lock();\n\n\t/*\n\t * We process outgoing injected frames that have a local address\n\t * we handle as though they are non-injected frames.\n\t * This code here isn't entirely correct, the local MAC address\n\t * isn't always enough to find the interface to use; for proper\n\t * VLAN support we have an nl80211-based mechanism.\n\t *\n\t * This is necessary, for example, for old hostapd versions that\n\t * don't use nl80211-based management TX/RX.\n\t */\n\tsdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\n\tlist_for_each_entry_rcu(tmp_sdata, &local->interfaces, list) {\n\t\tif (!ieee80211_sdata_running(tmp_sdata))\n\t\t\tcontinue;\n\t\tif (tmp_sdata->vif.type == NL80211_IFTYPE_MONITOR ||\n\t\t    tmp_sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\t\tcontinue;\n\t\tif (ether_addr_equal(tmp_sdata->vif.addr, hdr->addr2)) {\n\t\t\tsdata = tmp_sdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\ttmp_sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tmp_sdata)\n\t\t\tchanctx_conf =\n\t\t\t\trcu_dereference(tmp_sdata->vif.chanctx_conf);\n\t}\n\n\tif (chanctx_conf)\n\t\tchandef = &chanctx_conf->def;\n\telse if (!local->use_chanctx)\n\t\tchandef = &local->_oper_chandef;\n\telse\n\t\tgoto fail_rcu;\n\n\t/*\n\t * Frame injection is not allowed if beaconing is not allowed\n\t * or if we need radar detection. Beaconing is usually not allowed when\n\t * the mode or operation (Adhoc, AP, Mesh) does not support DFS.\n\t * Passive scan is also used in world regulatory domains where\n\t * your country is not known and as such it should be treated as\n\t * NO TX unless the channel is explicitly allowed in which case\n\t * your current regulatory domain would not have the passive scan\n\t * flag.\n\t *\n\t * Since AP mode uses monitor interfaces to inject/TX management\n\t * frames we can make AP mode the exception to this rule once it\n\t * supports radar detection as its implementation can deal with\n\t * radar detection by itself. We can do that later by adding a\n\t * monitor flag interfaces used for AP support.\n\t */\n\tif (!cfg80211_reg_can_beacon(local->hw.wiphy, chandef,\n\t\t\t\t     sdata->vif.type))\n\t\tgoto fail_rcu;\n\n\tinfo->band = chandef->chan->band;\n\n\t/* remove the injection radiotap header */\n\tskb_pull(skb, len_rthdr);\n\n\tieee80211_xmit(sdata, NULL, skb);\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n\nfail_rcu:\n\trcu_read_unlock();\nfail:\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK; /* meaning, we dealt with the skb */\n}\n\nstatic inline bool ieee80211_is_tdls_setup(struct sk_buff *skb)\n{\n\tu16 ethertype = (skb->data[12] << 8) | skb->data[13];\n\n\treturn ethertype == ETH_P_TDLS &&\n\t       skb->len > 14 &&\n\t       skb->data[14] == WLAN_TDLS_SNAP_RFTYPE;\n}\n\nint ieee80211_lookup_ra_sta(struct ieee80211_sub_if_data *sdata,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct sta_info **sta_out)\n{\n\tstruct sta_info *sta;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tsta = rcu_dereference(sdata->u.vlan.sta);\n\t\tif (sta) {\n\t\t\t*sta_out = sta;\n\t\t\treturn 0;\n\t\t} else if (sdata->wdev.use_4addr) {\n\t\t\treturn -ENOLINK;\n\t\t}\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_OCB:\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tif (is_multicast_ether_addr(skb->data)) {\n\t\t\t*sta_out = ERR_PTR(-ENOENT);\n\t\t\treturn 0;\n\t\t}\n\t\tsta = sta_info_get_bss(sdata, skb->data);\n\t\tbreak;\n#ifdef CONFIG_MAC80211_MESH\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\t/* determined much later */\n\t\t*sta_out = NULL;\n\t\treturn 0;\n#endif\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (sdata->wdev.wiphy->flags & WIPHY_FLAG_SUPPORTS_TDLS) {\n\t\t\tsta = sta_info_get(sdata, skb->data);\n\t\t\tif (sta && test_sta_flag(sta, WLAN_STA_TDLS_PEER)) {\n\t\t\t\tif (test_sta_flag(sta,\n\t\t\t\t\t\t  WLAN_STA_TDLS_PEER_AUTH)) {\n\t\t\t\t\t*sta_out = sta;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * TDLS link during setup - throw out frames to\n\t\t\t\t * peer. Allow TDLS-setup frames to unauthorized\n\t\t\t\t * peers for the special case of a link teardown\n\t\t\t\t * after a TDLS sta is removed due to being\n\t\t\t\t * unreachable.\n\t\t\t\t */\n\t\t\t\tif (!ieee80211_is_tdls_setup(skb))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t}\n\n\t\tsta = sta_info_get(sdata, sdata->u.mgd.bssid);\n\t\tif (!sta)\n\t\t\treturn -ENOLINK;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t*sta_out = sta ?: ERR_PTR(-ENOENT);\n\treturn 0;\n}\n\nstatic u16 ieee80211_store_ack_skb(struct ieee80211_local *local,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   u32 *info_flags,\n\t\t\t\t   u64 *cookie)\n{\n\tstruct sk_buff *ack_skb;\n\tu16 info_id = 0;\n\n\tif (skb->sk)\n\t\tack_skb = skb_clone_sk(skb);\n\telse\n\t\tack_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (ack_skb) {\n\t\tunsigned long flags;\n\t\tint id;\n\n\t\tspin_lock_irqsave(&local->ack_status_lock, flags);\n\t\tid = idr_alloc(&local->ack_status_frames, ack_skb,\n\t\t\t       1, 0x2000, GFP_ATOMIC);\n\t\tspin_unlock_irqrestore(&local->ack_status_lock, flags);\n\n\t\tif (id >= 0) {\n\t\t\tinfo_id = id;\n\t\t\t*info_flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n\t\t\tif (cookie) {\n\t\t\t\t*cookie = ieee80211_mgmt_tx_cookie(local);\n\t\t\t\tIEEE80211_SKB_CB(ack_skb)->ack.cookie = *cookie;\n\t\t\t}\n\t\t} else {\n\t\t\tkfree_skb(ack_skb);\n\t\t}\n\t}\n\n\treturn info_id;\n}\n\n/**\n * ieee80211_build_hdr - build 802.11 header in the given frame\n * @sdata: virtual interface to build the header for\n * @skb: the skb to build the header in\n * @info_flags: skb flags to set\n * @sta: the station pointer\n * @ctrl_flags: info control flags to set\n * @cookie: cookie pointer to fill (if not %NULL)\n *\n * This function takes the skb with 802.3 header and reformats the header to\n * the appropriate IEEE 802.11 header based on which interface the packet is\n * being transmitted on.\n *\n * Note that this function also takes care of the TX status request and\n * potential unsharing of the SKB - this needs to be interleaved with the\n * header building.\n *\n * The function requires the read-side RCU lock held\n *\n * Returns: the (possibly reallocated) skb or an ERR_PTR() code\n */\nstatic struct sk_buff *ieee80211_build_hdr(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t   struct sk_buff *skb, u32 info_flags,\n\t\t\t\t\t   struct sta_info *sta, u32 ctrl_flags,\n\t\t\t\t\t   u64 *cookie)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_info *info;\n\tint head_need;\n\tu16 ethertype, hdrlen,  meshhdrlen = 0;\n\t__le16 fc;\n\tstruct ieee80211_hdr hdr;\n\tstruct ieee80211s_hdr mesh_hdr __maybe_unused;\n\tstruct mesh_path __maybe_unused *mppath = NULL, *mpath = NULL;\n\tconst u8 *encaps_data;\n\tint encaps_len, skip_header_bytes;\n\tbool wme_sta = false, authorized = false;\n\tbool tdls_peer;\n\tbool multicast;\n\tu16 info_id = 0;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tstruct ieee80211_sub_if_data *ap_sdata;\n\tenum nl80211_band band;\n\tint ret;\n\n\tif (IS_ERR(sta))\n\t\tsta = NULL;\n\n#ifdef CONFIG_MAC80211_DEBUGFS\n\tif (local->force_tx_status)\n\t\tinfo_flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n#endif\n\n\t/* convert Ethernet header to proper 802.11 header (based on\n\t * operation mode) */\n\tethertype = (skb->data[12] << 8) | skb->data[13];\n\tfc = cpu_to_le16(IEEE80211_FTYPE_DATA | IEEE80211_STYPE_DATA);\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (sdata->wdev.use_4addr) {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS | IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr.addr1, sta->sta.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, skb->data, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr4, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\thdrlen = 30;\n\t\t\tauthorized = test_sta_flag(sta, WLAN_STA_AUTHORIZED);\n\t\t\twme_sta = sta->sta.wme;\n\t\t}\n\t\tap_sdata = container_of(sdata->bss, struct ieee80211_sub_if_data,\n\t\t\t\t\tu.ap);\n\t\tchanctx_conf = rcu_dereference(ap_sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tif (sdata->wdev.use_4addr)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS);\n\t\t/* DA BSSID SA */\n\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tmemcpy(hdr.addr2, sdata->vif.addr, ETH_ALEN);\n\t\tmemcpy(hdr.addr3, skb->data + ETH_ALEN, ETH_ALEN);\n\t\thdrlen = 24;\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n#ifdef CONFIG_MAC80211_MESH\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tif (!is_multicast_ether_addr(skb->data)) {\n\t\t\tstruct sta_info *next_hop;\n\t\t\tbool mpp_lookup = true;\n\n\t\t\tmpath = mesh_path_lookup(sdata, skb->data);\n\t\t\tif (mpath) {\n\t\t\t\tmpp_lookup = false;\n\t\t\t\tnext_hop = rcu_dereference(mpath->next_hop);\n\t\t\t\tif (!next_hop ||\n\t\t\t\t    !(mpath->flags & (MESH_PATH_ACTIVE |\n\t\t\t\t\t\t      MESH_PATH_RESOLVING)))\n\t\t\t\t\tmpp_lookup = true;\n\t\t\t}\n\n\t\t\tif (mpp_lookup) {\n\t\t\t\tmppath = mpp_path_lookup(sdata, skb->data);\n\t\t\t\tif (mppath)\n\t\t\t\t\tmppath->exp_time = jiffies;\n\t\t\t}\n\n\t\t\tif (mppath && mpath)\n\t\t\t\tmesh_path_del(sdata, mpath->dst);\n\t\t}\n\n\t\t/*\n\t\t * Use address extension if it is a packet from\n\t\t * another interface or if we know the destination\n\t\t * is being proxied by a portal (i.e. portal address\n\t\t * differs from proxied address)\n\t\t */\n\t\tif (ether_addr_equal(sdata->vif.addr, skb->data + ETH_ALEN) &&\n\t\t    !(mppath && !ether_addr_equal(mppath->mpp, skb->data))) {\n\t\t\thdrlen = ieee80211_fill_mesh_addresses(&hdr, &fc,\n\t\t\t\t\tskb->data, skb->data + ETH_ALEN);\n\t\t\tmeshhdrlen = ieee80211_new_mesh_header(sdata, &mesh_hdr,\n\t\t\t\t\t\t\t       NULL, NULL);\n\t\t} else {\n\t\t\t/* DS -> MBSS (802.11-2012 13.11.3.3).\n\t\t\t * For unicast with unknown forwarding information,\n\t\t\t * destination might be in the MBSS or if that fails\n\t\t\t * forwarded to another mesh gate. In either case\n\t\t\t * resolution will be handled in ieee80211_xmit(), so\n\t\t\t * leave the original DA. This also works for mcast */\n\t\t\tconst u8 *mesh_da = skb->data;\n\n\t\t\tif (mppath)\n\t\t\t\tmesh_da = mppath->mpp;\n\t\t\telse if (mpath)\n\t\t\t\tmesh_da = mpath->dst;\n\n\t\t\thdrlen = ieee80211_fill_mesh_addresses(&hdr, &fc,\n\t\t\t\t\tmesh_da, sdata->vif.addr);\n\t\t\tif (is_multicast_ether_addr(mesh_da))\n\t\t\t\t/* DA TA mSA AE:SA */\n\t\t\t\tmeshhdrlen = ieee80211_new_mesh_header(\n\t\t\t\t\t\tsdata, &mesh_hdr,\n\t\t\t\t\t\tskb->data + ETH_ALEN, NULL);\n\t\t\telse\n\t\t\t\t/* RA TA mDA mSA AE:DA SA */\n\t\t\t\tmeshhdrlen = ieee80211_new_mesh_header(\n\t\t\t\t\t\tsdata, &mesh_hdr, skb->data,\n\t\t\t\t\t\tskb->data + ETH_ALEN);\n\n\t\t}\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\n\t\t/* For injected frames, fill RA right away as nexthop lookup\n\t\t * will be skipped.\n\t\t */\n\t\tif ((ctrl_flags & IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP) &&\n\t\t    is_zero_ether_addr(hdr.addr1))\n\t\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tbreak;\n#endif\n\tcase NL80211_IFTYPE_STATION:\n\t\t/* we already did checks when looking up the RA STA */\n\t\ttdls_peer = test_sta_flag(sta, WLAN_STA_TDLS_PEER);\n\n\t\tif (tdls_peer) {\n\t\t\t/* DA SA BSSID */\n\t\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\thdrlen = 24;\n\t\t}  else if (sdata->u.mgd.use_4addr &&\n\t\t\t    cpu_to_be16(ethertype) != sdata->control_port_protocol) {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t  IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr.addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, skb->data, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr4, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\thdrlen = 30;\n\t\t} else {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_TODS);\n\t\t\t/* BSSID SA DA */\n\t\t\tmemcpy(hdr.addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\t\tmemcpy(hdr.addr3, skb->data, ETH_ALEN);\n\t\t\thdrlen = 24;\n\t\t}\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n\tcase NL80211_IFTYPE_OCB:\n\t\t/* DA SA BSSID */\n\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\teth_broadcast_addr(hdr.addr3);\n\t\thdrlen = 24;\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\t/* DA SA BSSID */\n\t\tmemcpy(hdr.addr1, skb->data, ETH_ALEN);\n\t\tmemcpy(hdr.addr2, skb->data + ETH_ALEN, ETH_ALEN);\n\t\tmemcpy(hdr.addr3, sdata->u.ibss.bssid, ETH_ALEN);\n\t\thdrlen = 24;\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (!chanctx_conf) {\n\t\t\tret = -ENOTCONN;\n\t\t\tgoto free;\n\t\t}\n\t\tband = chanctx_conf->def.chan->band;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto free;\n\t}\n\n\tmulticast = is_multicast_ether_addr(hdr.addr1);\n\n\t/* sta is always NULL for mesh */\n\tif (sta) {\n\t\tauthorized = test_sta_flag(sta, WLAN_STA_AUTHORIZED);\n\t\twme_sta = sta->sta.wme;\n\t} else if (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\t/* For mesh, the use of the QoS header is mandatory */\n\t\twme_sta = true;\n\t}\n\n\t/* receiver does QoS (which also means we do) use it */\n\tif (wme_sta) {\n\t\tfc |= cpu_to_le16(IEEE80211_STYPE_QOS_DATA);\n\t\thdrlen += 2;\n\t}\n\n\t/*\n\t * Drop unicast frames to unauthorised stations unless they are\n\t * EAPOL frames from the local station.\n\t */\n\tif (unlikely(!ieee80211_vif_is_mesh(&sdata->vif) &&\n\t\t     (sdata->vif.type != NL80211_IFTYPE_OCB) &&\n\t\t     !multicast && !authorized &&\n\t\t     (cpu_to_be16(ethertype) != sdata->control_port_protocol ||\n\t\t      !ether_addr_equal(sdata->vif.addr, skb->data + ETH_ALEN)))) {\n#ifdef CONFIG_MAC80211_VERBOSE_DEBUG\n\t\tnet_info_ratelimited(\"%s: dropped frame to %pM (unauthorized port)\\n\",\n\t\t\t\t    sdata->name, hdr.addr1);\n#endif\n\n\t\tI802_DEBUG_INC(local->tx_handlers_drop_unauth_port);\n\n\t\tret = -EPERM;\n\t\tgoto free;\n\t}\n\n\tif (unlikely(!multicast && ((skb->sk &&\n\t\t     skb_shinfo(skb)->tx_flags & SKBTX_WIFI_STATUS) ||\n\t\t     ctrl_flags & IEEE80211_TX_CTL_REQ_TX_STATUS)))\n\t\tinfo_id = ieee80211_store_ack_skb(local, skb, &info_flags,\n\t\t\t\t\t\t  cookie);\n\n\t/*\n\t * If the skb is shared we need to obtain our own copy.\n\t */\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *tmp_skb = skb;\n\n\t\t/* can't happen -- skb is a clone if info_id != 0 */\n\t\tWARN_ON(info_id);\n\n\t\tskb = skb_clone(skb, GFP_ATOMIC);\n\t\tkfree_skb(tmp_skb);\n\n\t\tif (!skb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\thdr.frame_control = fc;\n\thdr.duration_id = 0;\n\thdr.seq_ctrl = 0;\n\n\tskip_header_bytes = ETH_HLEN;\n\tif (ethertype == ETH_P_AARP || ethertype == ETH_P_IPX) {\n\t\tencaps_data = bridge_tunnel_header;\n\t\tencaps_len = sizeof(bridge_tunnel_header);\n\t\tskip_header_bytes -= 2;\n\t} else if (ethertype >= ETH_P_802_3_MIN) {\n\t\tencaps_data = rfc1042_header;\n\t\tencaps_len = sizeof(rfc1042_header);\n\t\tskip_header_bytes -= 2;\n\t} else {\n\t\tencaps_data = NULL;\n\t\tencaps_len = 0;\n\t}\n\n\tskb_pull(skb, skip_header_bytes);\n\thead_need = hdrlen + encaps_len + meshhdrlen - skb_headroom(skb);\n\n\t/*\n\t * So we need to modify the skb header and hence need a copy of\n\t * that. The head_need variable above doesn't, so far, include\n\t * the needed header space that we don't need right away. If we\n\t * can, then we don't reallocate right now but only after the\n\t * frame arrives at the master device (if it does...)\n\t *\n\t * If we cannot, however, then we will reallocate to include all\n\t * the ever needed space. Also, if we need to reallocate it anyway,\n\t * make it big enough for everything we may ever need.\n\t */\n\n\tif (head_need > 0 || skb_cloned(skb)) {\n\t\thead_need += sdata->encrypt_headroom;\n\t\thead_need += local->tx_headroom;\n\t\thead_need = max_t(int, 0, head_need);\n\t\tif (ieee80211_skb_resize(sdata, skb, head_need, ENCRYPT_DATA)) {\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tskb = NULL;\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t}\n\n\tif (encaps_data)\n\t\tmemcpy(skb_push(skb, encaps_len), encaps_data, encaps_len);\n\n#ifdef CONFIG_MAC80211_MESH\n\tif (meshhdrlen > 0)\n\t\tmemcpy(skb_push(skb, meshhdrlen), &mesh_hdr, meshhdrlen);\n#endif\n\n\tif (ieee80211_is_data_qos(fc)) {\n\t\t__le16 *qos_control;\n\n\t\tqos_control = skb_push(skb, 2);\n\t\tmemcpy(skb_push(skb, hdrlen - 2), &hdr, hdrlen - 2);\n\t\t/*\n\t\t * Maybe we could actually set some fields here, for now just\n\t\t * initialise to zero to indicate no special operation.\n\t\t */\n\t\t*qos_control = 0;\n\t} else\n\t\tmemcpy(skb_push(skb, hdrlen), &hdr, hdrlen);\n\n\tskb_reset_mac_header(skb);\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\tmemset(info, 0, sizeof(*info));\n\n\tinfo->flags = info_flags;\n\tinfo->ack_frame_id = info_id;\n\tinfo->band = band;\n\tinfo->control.flags = ctrl_flags;\n\n\treturn skb;\n free:\n\tkfree_skb(skb);\n\treturn ERR_PTR(ret);\n}\n\n/*\n * fast-xmit overview\n *\n * The core idea of this fast-xmit is to remove per-packet checks by checking\n * them out of band. ieee80211_check_fast_xmit() implements the out-of-band\n * checks that are needed to get the sta->fast_tx pointer assigned, after which\n * much less work can be done per packet. For example, fragmentation must be\n * disabled or the fast_tx pointer will not be set. All the conditions are seen\n * in the code here.\n *\n * Once assigned, the fast_tx data structure also caches the per-packet 802.11\n * header and other data to aid packet processing in ieee80211_xmit_fast().\n *\n * The most difficult part of this is that when any of these assumptions\n * change, an external trigger (i.e. a call to ieee80211_clear_fast_xmit(),\n * ieee80211_check_fast_xmit() or friends) is required to reset the data,\n * since the per-packet code no longer checks the conditions. This is reflected\n * by the calls to these functions throughout the rest of the code, and must be\n * maintained if any of the TX path checks change.\n */\n\nvoid ieee80211_check_fast_xmit(struct sta_info *sta)\n{\n\tstruct ieee80211_fast_tx build = {}, *fast_tx = NULL, *old;\n\tstruct ieee80211_local *local = sta->local;\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_hdr *hdr = (void *)build.hdr;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\t__le16 fc;\n\n\tif (!ieee80211_hw_check(&local->hw, SUPPORT_FAST_XMIT))\n\t\treturn;\n\n\t/* Locking here protects both the pointer itself, and against concurrent\n\t * invocations winning data access races to, e.g., the key pointer that\n\t * is used.\n\t * Without it, the invocation of this function right after the key\n\t * pointer changes wouldn't be sufficient, as another CPU could access\n\t * the pointer, then stall, and then do the cache update after the CPU\n\t * that invalidated the key.\n\t * With the locking, such scenarios cannot happen as the check for the\n\t * key and the fast-tx assignment are done atomically, so the CPU that\n\t * modifies the key will either wait or other one will see the key\n\t * cleared/changed already.\n\t */\n\tspin_lock_bh(&sta->lock);\n\tif (ieee80211_hw_check(&local->hw, SUPPORTS_PS) &&\n\t    !ieee80211_hw_check(&local->hw, SUPPORTS_DYNAMIC_PS) &&\n\t    sdata->vif.type == NL80211_IFTYPE_STATION)\n\t\tgoto out;\n\n\tif (!test_sta_flag(sta, WLAN_STA_AUTHORIZED))\n\t\tgoto out;\n\n\tif (test_sta_flag(sta, WLAN_STA_PS_STA) ||\n\t    test_sta_flag(sta, WLAN_STA_PS_DRIVER) ||\n\t    test_sta_flag(sta, WLAN_STA_PS_DELIVER) ||\n\t    test_sta_flag(sta, WLAN_STA_CLEAR_PS_FILT))\n\t\tgoto out;\n\n\tif (sdata->noack_map)\n\t\tgoto out;\n\n\t/* fast-xmit doesn't handle fragmentation at all */\n\tif (local->hw.wiphy->frag_threshold != (u32)-1 &&\n\t    !ieee80211_hw_check(&local->hw, SUPPORTS_TX_FRAG))\n\t\tgoto out;\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\tif (!chanctx_conf) {\n\t\trcu_read_unlock();\n\t\tgoto out;\n\t}\n\tbuild.band = chanctx_conf->def.chan->band;\n\trcu_read_unlock();\n\n\tfc = cpu_to_le16(IEEE80211_FTYPE_DATA | IEEE80211_STYPE_DATA);\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_ADHOC:\n\t\t/* DA SA BSSID */\n\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\tmemcpy(hdr->addr3, sdata->u.ibss.bssid, ETH_ALEN);\n\t\tbuild.hdr_len = 24;\n\t\tbreak;\n\tcase NL80211_IFTYPE_STATION:\n\t\tif (test_sta_flag(sta, WLAN_STA_TDLS_PEER)) {\n\t\t\t/* DA SA BSSID */\n\t\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\t\tmemcpy(hdr->addr3, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tbuild.hdr_len = 24;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (sdata->u.mgd.use_4addr) {\n\t\t\t/* non-regular ethertype cannot use the fastpath */\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t  IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr->addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\t\tmemcpy(hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr4);\n\t\t\tbuild.hdr_len = 30;\n\t\t\tbreak;\n\t\t}\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_TODS);\n\t\t/* BSSID SA DA */\n\t\tmemcpy(hdr->addr1, sdata->u.mgd.bssid, ETH_ALEN);\n\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr2);\n\t\tbuild.hdr_len = 24;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (sdata->wdev.use_4addr) {\n\t\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS |\n\t\t\t\t\t  IEEE80211_FCTL_TODS);\n\t\t\t/* RA TA DA SA */\n\t\t\tmemcpy(hdr->addr1, sta->sta.addr, ETH_ALEN);\n\t\t\tmemcpy(hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr4);\n\t\t\tbuild.hdr_len = 30;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_FROMDS);\n\t\t/* DA BSSID SA */\n\t\tbuild.da_offs = offsetof(struct ieee80211_hdr, addr1);\n\t\tmemcpy(hdr->addr2, sdata->vif.addr, ETH_ALEN);\n\t\tbuild.sa_offs = offsetof(struct ieee80211_hdr, addr3);\n\t\tbuild.hdr_len = 24;\n\t\tbreak;\n\tdefault:\n\t\t/* not handled on fast-xmit */\n\t\tgoto out;\n\t}\n\n\tif (sta->sta.wme) {\n\t\tbuild.hdr_len += 2;\n\t\tfc |= cpu_to_le16(IEEE80211_STYPE_QOS_DATA);\n\t}\n\n\t/* We store the key here so there's no point in using rcu_dereference()\n\t * but that's fine because the code that changes the pointers will call\n\t * this function after doing so. For a single CPU that would be enough,\n\t * for multiple see the comment above.\n\t */\n\tbuild.key = rcu_access_pointer(sta->ptk[sta->ptk_idx]);\n\tif (!build.key)\n\t\tbuild.key = rcu_access_pointer(sdata->default_unicast_key);\n\tif (build.key) {\n\t\tbool gen_iv, iv_spc, mmic;\n\n\t\tgen_iv = build.key->conf.flags & IEEE80211_KEY_FLAG_GENERATE_IV;\n\t\tiv_spc = build.key->conf.flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE;\n\t\tmmic = build.key->conf.flags &\n\t\t\t(IEEE80211_KEY_FLAG_GENERATE_MMIC |\n\t\t\t IEEE80211_KEY_FLAG_PUT_MIC_SPACE);\n\n\t\t/* don't handle software crypto */\n\t\tif (!(build.key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE))\n\t\t\tgoto out;\n\n\t\t/* Key is being removed */\n\t\tif (build.key->flags & KEY_FLAG_TAINTED)\n\t\t\tgoto out;\n\n\t\tswitch (build.key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\t\tif (gen_iv)\n\t\t\t\tbuild.pn_offs = build.hdr_len;\n\t\t\tif (gen_iv || iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_CCMP_HDR_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tif (gen_iv)\n\t\t\t\tbuild.pn_offs = build.hdr_len;\n\t\t\tif (gen_iv || iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_GCMP_HDR_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_TKIP:\n\t\t\t/* cannot handle MMIC or IV generation in xmit-fast */\n\t\t\tif (mmic || gen_iv)\n\t\t\t\tgoto out;\n\t\t\tif (iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_TKIP_IV_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_WEP40:\n\t\tcase WLAN_CIPHER_SUITE_WEP104:\n\t\t\t/* cannot handle IV generation in fast-xmit */\n\t\t\tif (gen_iv)\n\t\t\t\tgoto out;\n\t\t\tif (iv_spc)\n\t\t\t\tbuild.hdr_len += IEEE80211_WEP_IV_LEN;\n\t\t\tbreak;\n\t\tcase WLAN_CIPHER_SUITE_AES_CMAC:\n\t\tcase WLAN_CIPHER_SUITE_BIP_CMAC_256:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_128:\n\t\tcase WLAN_CIPHER_SUITE_BIP_GMAC_256:\n\t\t\tWARN(1,\n\t\t\t     \"management cipher suite 0x%x enabled for data\\n\",\n\t\t\t     build.key->conf.cipher);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\t/* we don't know how to generate IVs for this at all */\n\t\t\tif (WARN_ON(gen_iv))\n\t\t\t\tgoto out;\n\t\t\t/* pure hardware keys are OK, of course */\n\t\t\tif (!(build.key->flags & KEY_FLAG_CIPHER_SCHEME))\n\t\t\t\tbreak;\n\t\t\t/* cipher scheme might require space allocation */\n\t\t\tif (iv_spc &&\n\t\t\t    build.key->conf.iv_len > IEEE80211_FAST_XMIT_MAX_IV)\n\t\t\t\tgoto out;\n\t\t\tif (iv_spc)\n\t\t\t\tbuild.hdr_len += build.key->conf.iv_len;\n\t\t}\n\n\t\tfc |= cpu_to_le16(IEEE80211_FCTL_PROTECTED);\n\t}\n\n\thdr->frame_control = fc;\n\n\tmemcpy(build.hdr + build.hdr_len,\n\t       rfc1042_header,  sizeof(rfc1042_header));\n\tbuild.hdr_len += sizeof(rfc1042_header);\n\n\tfast_tx = kmemdup(&build, sizeof(build), GFP_ATOMIC);\n\t/* if the kmemdup fails, continue w/o fast_tx */\n\tif (!fast_tx)\n\t\tgoto out;\n\n out:\n\t/* we might have raced against another call to this function */\n\told = rcu_dereference_protected(sta->fast_tx,\n\t\t\t\t\tlockdep_is_held(&sta->lock));\n\trcu_assign_pointer(sta->fast_tx, fast_tx);\n\tif (old)\n\t\tkfree_rcu(old, rcu_head);\n\tspin_unlock_bh(&sta->lock);\n}\n\nvoid ieee80211_check_fast_xmit_all(struct ieee80211_local *local)\n{\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list)\n\t\tieee80211_check_fast_xmit(sta);\n\trcu_read_unlock();\n}\n\nvoid ieee80211_check_fast_xmit_iface(struct ieee80211_sub_if_data *sdata)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tif (sdata != sta->sdata &&\n\t\t    (!sta->sdata->bss || sta->sdata->bss != sdata->bss))\n\t\t\tcontinue;\n\t\tieee80211_check_fast_xmit(sta);\n\t}\n\n\trcu_read_unlock();\n}\n\nvoid ieee80211_clear_fast_xmit(struct sta_info *sta)\n{\n\tstruct ieee80211_fast_tx *fast_tx;\n\n\tspin_lock_bh(&sta->lock);\n\tfast_tx = rcu_dereference_protected(sta->fast_tx,\n\t\t\t\t\t    lockdep_is_held(&sta->lock));\n\tRCU_INIT_POINTER(sta->fast_tx, NULL);\n\tspin_unlock_bh(&sta->lock);\n\n\tif (fast_tx)\n\t\tkfree_rcu(fast_tx, rcu_head);\n}\n\nstatic bool ieee80211_amsdu_realloc_pad(struct ieee80211_local *local,\n\t\t\t\t\tstruct sk_buff *skb, int headroom)\n{\n\tif (skb_headroom(skb) < headroom) {\n\t\tI802_DEBUG_INC(local->tx_expand_skb_head);\n\n\t\tif (pskb_expand_head(skb, headroom, 0, GFP_ATOMIC)) {\n\t\t\twiphy_debug(local->hw.wiphy,\n\t\t\t\t    \"failed to reallocate TX buffer\\n\");\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool ieee80211_amsdu_prepare_head(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\t struct ieee80211_fast_tx *fast_tx,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr;\n\tstruct ethhdr *amsdu_hdr;\n\tint hdr_len = fast_tx->hdr_len - sizeof(rfc1042_header);\n\tint subframe_len = skb->len - hdr_len;\n\tvoid *data;\n\tu8 *qc, *h_80211_src, *h_80211_dst;\n\tconst u8 *bssid;\n\n\tif (info->flags & IEEE80211_TX_CTL_RATE_CTRL_PROBE)\n\t\treturn false;\n\n\tif (info->control.flags & IEEE80211_TX_CTRL_AMSDU)\n\t\treturn true;\n\n\tif (!ieee80211_amsdu_realloc_pad(local, skb, sizeof(*amsdu_hdr)))\n\t\treturn false;\n\n\tdata = skb_push(skb, sizeof(*amsdu_hdr));\n\tmemmove(data, data + sizeof(*amsdu_hdr), hdr_len);\n\thdr = data;\n\tamsdu_hdr = data + hdr_len;\n\t/* h_80211_src/dst is addr* field within hdr */\n\th_80211_src = data + fast_tx->sa_offs;\n\th_80211_dst = data + fast_tx->da_offs;\n\n\tamsdu_hdr->h_proto = cpu_to_be16(subframe_len);\n\tether_addr_copy(amsdu_hdr->h_source, h_80211_src);\n\tether_addr_copy(amsdu_hdr->h_dest, h_80211_dst);\n\n\t/* according to IEEE 802.11-2012 8.3.2 table 8-19, the outer SA/DA\n\t * fields needs to be changed to BSSID for A-MSDU frames depending\n\t * on FromDS/ToDS values.\n\t */\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\t\tbssid = sdata->u.mgd.bssid;\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tbssid = sdata->vif.addr;\n\t\tbreak;\n\tdefault:\n\t\tbssid = NULL;\n\t}\n\n\tif (bssid && ieee80211_has_fromds(hdr->frame_control))\n\t\tether_addr_copy(h_80211_src, bssid);\n\n\tif (bssid && ieee80211_has_tods(hdr->frame_control))\n\t\tether_addr_copy(h_80211_dst, bssid);\n\n\tqc = ieee80211_get_qos_ctl(hdr);\n\t*qc |= IEEE80211_QOS_CTL_A_MSDU_PRESENT;\n\n\tinfo->control.flags |= IEEE80211_TX_CTRL_AMSDU;\n\n\treturn true;\n}\n\nstatic bool ieee80211_amsdu_aggregate(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t      struct sta_info *sta,\n\t\t\t\t      struct ieee80211_fast_tx *fast_tx,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin;\n\tstruct fq_flow *flow;\n\tu8 tid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\tstruct ieee80211_txq *txq = sta->sta.txq[tid];\n\tstruct txq_info *txqi;\n\tstruct sk_buff **frag_tail, *head;\n\tint subframe_len = skb->len - ETH_ALEN;\n\tu8 max_subframes = sta->sta.max_amsdu_subframes;\n\tint max_frags = local->hw.max_tx_fragments;\n\tint max_amsdu_len = sta->sta.max_amsdu_len;\n\tint orig_truesize;\n\tu32 flow_idx;\n\t__be16 len;\n\tvoid *data;\n\tbool ret = false;\n\tunsigned int orig_len;\n\tint n = 2, nfrags, pad = 0;\n\tu16 hdrlen;\n\n\tif (!ieee80211_hw_check(&local->hw, TX_AMSDU))\n\t\treturn false;\n\n\tif (skb_is_gso(skb))\n\t\treturn false;\n\n\tif (!txq)\n\t\treturn false;\n\n\ttxqi = to_txq_info(txq);\n\tif (test_bit(IEEE80211_TXQ_NO_AMSDU, &txqi->flags))\n\t\treturn false;\n\n\tif (sta->sta.max_rc_amsdu_len)\n\t\tmax_amsdu_len = min_t(int, max_amsdu_len,\n\t\t\t\t      sta->sta.max_rc_amsdu_len);\n\n\tif (sta->sta.max_tid_amsdu_len[tid])\n\t\tmax_amsdu_len = min_t(int, max_amsdu_len,\n\t\t\t\t      sta->sta.max_tid_amsdu_len[tid]);\n\n\tflow_idx = fq_flow_idx(fq, skb);\n\n\tspin_lock_bh(&fq->lock);\n\n\t/* TODO: Ideally aggregation should be done on dequeue to remain\n\t * responsive to environment changes.\n\t */\n\n\ttin = &txqi->tin;\n\tflow = fq_flow_classify(fq, tin, flow_idx, skb,\n\t\t\t\tfq_flow_get_default_func);\n\thead = skb_peek_tail(&flow->queue);\n\tif (!head || skb_is_gso(head))\n\t\tgoto out;\n\n\torig_truesize = head->truesize;\n\torig_len = head->len;\n\n\tif (skb->len + head->len > max_amsdu_len)\n\t\tgoto out;\n\n\tnfrags = 1 + skb_shinfo(skb)->nr_frags;\n\tnfrags += 1 + skb_shinfo(head)->nr_frags;\n\tfrag_tail = &skb_shinfo(head)->frag_list;\n\twhile (*frag_tail) {\n\t\tnfrags += 1 + skb_shinfo(*frag_tail)->nr_frags;\n\t\tfrag_tail = &(*frag_tail)->next;\n\t\tn++;\n\t}\n\n\tif (max_subframes && n > max_subframes)\n\t\tgoto out;\n\n\tif (max_frags && nfrags > max_frags)\n\t\tgoto out;\n\n\tif (!drv_can_aggregate_in_amsdu(local, head, skb))\n\t\tgoto out;\n\n\tif (!ieee80211_amsdu_prepare_head(sdata, fast_tx, head))\n\t\tgoto out;\n\n\t/*\n\t * Pad out the previous subframe to a multiple of 4 by adding the\n\t * padding to the next one, that's being added. Note that head->len\n\t * is the length of the full A-MSDU, but that works since each time\n\t * we add a new subframe we pad out the previous one to a multiple\n\t * of 4 and thus it no longer matters in the next round.\n\t */\n\thdrlen = fast_tx->hdr_len - sizeof(rfc1042_header);\n\tif ((head->len - hdrlen) & 3)\n\t\tpad = 4 - ((head->len - hdrlen) & 3);\n\n\tif (!ieee80211_amsdu_realloc_pad(local, skb, sizeof(rfc1042_header) +\n\t\t\t\t\t\t     2 + pad))\n\t\tgoto out_recalc;\n\n\tret = true;\n\tdata = skb_push(skb, ETH_ALEN + 2);\n\tmemmove(data, data + ETH_ALEN + 2, 2 * ETH_ALEN);\n\n\tdata += 2 * ETH_ALEN;\n\tlen = cpu_to_be16(subframe_len);\n\tmemcpy(data, &len, 2);\n\tmemcpy(data + 2, rfc1042_header, sizeof(rfc1042_header));\n\n\tmemset(skb_push(skb, pad), 0, pad);\n\n\thead->len += skb->len;\n\thead->data_len += skb->len;\n\t*frag_tail = skb;\n\nout_recalc:\n\tfq->memory_usage += head->truesize - orig_truesize;\n\tif (head->len != orig_len) {\n\t\tflow->backlog += head->len - orig_len;\n\t\ttin->backlog_bytes += head->len - orig_len;\n\n\t\tfq_recalc_backlog(fq, tin, flow);\n\t}\nout:\n\tspin_unlock_bh(&fq->lock);\n\n\treturn ret;\n}\n\n/*\n * Can be called while the sta lock is held. Anything that can cause packets to\n * be generated will cause deadlock!\n */\nstatic void ieee80211_xmit_fast_finish(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t       struct sta_info *sta, u8 pn_offs,\n\t\t\t\t       struct ieee80211_key *key,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tu8 tid = IEEE80211_NUM_TIDS;\n\n\tif (key)\n\t\tinfo->control.hw_key = &key->conf;\n\n\tdev_sw_netstats_tx_add(skb->dev, 1, skb->len);\n\n\tif (hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) {\n\t\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t\thdr->seq_ctrl = ieee80211_tx_next_seq(sta, tid);\n\t} else {\n\t\tinfo->flags |= IEEE80211_TX_CTL_ASSIGN_SEQ;\n\t\thdr->seq_ctrl = cpu_to_le16(sdata->sequence_number);\n\t\tsdata->sequence_number += 0x10;\n\t}\n\n\tif (skb_shinfo(skb)->gso_size)\n\t\tsta->tx_stats.msdu[tid] +=\n\t\t\tDIV_ROUND_UP(skb->len, skb_shinfo(skb)->gso_size);\n\telse\n\t\tsta->tx_stats.msdu[tid]++;\n\n\tinfo->hw_queue = sdata->vif.hw_queue[skb_get_queue_mapping(skb)];\n\n\t/* statistics normally done by ieee80211_tx_h_stats (but that\n\t * has to consider fragmentation, so is more complex)\n\t */\n\tsta->tx_stats.bytes[skb_get_queue_mapping(skb)] += skb->len;\n\tsta->tx_stats.packets[skb_get_queue_mapping(skb)]++;\n\n\tif (pn_offs) {\n\t\tu64 pn;\n\t\tu8 *crypto_hdr = skb->data + pn_offs;\n\n\t\tswitch (key->conf.cipher) {\n\t\tcase WLAN_CIPHER_SUITE_CCMP:\n\t\tcase WLAN_CIPHER_SUITE_CCMP_256:\n\t\tcase WLAN_CIPHER_SUITE_GCMP:\n\t\tcase WLAN_CIPHER_SUITE_GCMP_256:\n\t\t\tpn = atomic64_inc_return(&key->conf.tx_pn);\n\t\t\tcrypto_hdr[0] = pn;\n\t\t\tcrypto_hdr[1] = pn >> 8;\n\t\t\tcrypto_hdr[3] = 0x20 | (key->conf.keyidx << 6);\n\t\t\tcrypto_hdr[4] = pn >> 16;\n\t\t\tcrypto_hdr[5] = pn >> 24;\n\t\t\tcrypto_hdr[6] = pn >> 32;\n\t\t\tcrypto_hdr[7] = pn >> 40;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic bool ieee80211_xmit_fast(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct sta_info *sta,\n\t\t\t\tstruct ieee80211_fast_tx *fast_tx,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tu16 ethertype = (skb->data[12] << 8) | skb->data[13];\n\tint extra_head = fast_tx->hdr_len - (ETH_HLEN - 2);\n\tint hw_headroom = sdata->local->hw.extra_tx_headroom;\n\tstruct ethhdr eth;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_hdr *hdr = (void *)fast_tx->hdr;\n\tstruct ieee80211_tx_data tx;\n\tieee80211_tx_result r;\n\tstruct tid_ampdu_tx *tid_tx = NULL;\n\tu8 tid = IEEE80211_NUM_TIDS;\n\n\t/* control port protocol needs a lot of special handling */\n\tif (cpu_to_be16(ethertype) == sdata->control_port_protocol)\n\t\treturn false;\n\n\t/* only RFC 1042 SNAP */\n\tif (ethertype < ETH_P_802_3_MIN)\n\t\treturn false;\n\n\t/* don't handle TX status request here either */\n\tif (skb->sk && skb_shinfo(skb)->tx_flags & SKBTX_WIFI_STATUS)\n\t\treturn false;\n\n\tif (hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) {\n\t\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t\ttid_tx = rcu_dereference(sta->ampdu_mlme.tid_tx[tid]);\n\t\tif (tid_tx) {\n\t\t\tif (!test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state))\n\t\t\t\treturn false;\n\t\t\tif (tid_tx->timeout)\n\t\t\t\ttid_tx->last_tx = jiffies;\n\t\t}\n\t}\n\n\t/* after this point (skb is modified) we cannot return false */\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *tmp_skb = skb;\n\n\t\tskb = skb_clone(skb, GFP_ATOMIC);\n\t\tkfree_skb(tmp_skb);\n\n\t\tif (!skb)\n\t\t\treturn true;\n\t}\n\n\tif ((hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) &&\n\t    ieee80211_amsdu_aggregate(sdata, sta, fast_tx, skb))\n\t\treturn true;\n\n\t/* will not be crypto-handled beyond what we do here, so use false\n\t * as the may-encrypt argument for the resize to not account for\n\t * more room than we already have in 'extra_head'\n\t */\n\tif (unlikely(ieee80211_skb_resize(sdata, skb,\n\t\t\t\t\t  max_t(int, extra_head + hw_headroom -\n\t\t\t\t\t\t     skb_headroom(skb), 0),\n\t\t\t\t\t  ENCRYPT_NO))) {\n\t\tkfree_skb(skb);\n\t\treturn true;\n\t}\n\n\tmemcpy(&eth, skb->data, ETH_HLEN - 2);\n\thdr = skb_push(skb, extra_head);\n\tmemcpy(skb->data, fast_tx->hdr, fast_tx->hdr_len);\n\tmemcpy(skb->data + fast_tx->da_offs, eth.h_dest, ETH_ALEN);\n\tmemcpy(skb->data + fast_tx->sa_offs, eth.h_source, ETH_ALEN);\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\tmemset(info, 0, sizeof(*info));\n\tinfo->band = fast_tx->band;\n\tinfo->control.vif = &sdata->vif;\n\tinfo->flags = IEEE80211_TX_CTL_FIRST_FRAGMENT |\n\t\t      IEEE80211_TX_CTL_DONTFRAG |\n\t\t      (tid_tx ? IEEE80211_TX_CTL_AMPDU : 0);\n\tinfo->control.flags = IEEE80211_TX_CTRL_FAST_XMIT;\n\n#ifdef CONFIG_MAC80211_DEBUGFS\n\tif (local->force_tx_status)\n\t\tinfo->flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n#endif\n\n\tif (hdr->frame_control & cpu_to_le16(IEEE80211_STYPE_QOS_DATA)) {\n\t\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\t\t*ieee80211_get_qos_ctl(hdr) = tid;\n\t}\n\n\t__skb_queue_head_init(&tx.skbs);\n\n\ttx.flags = IEEE80211_TX_UNICAST;\n\ttx.local = local;\n\ttx.sdata = sdata;\n\ttx.sta = sta;\n\ttx.key = fast_tx->key;\n\n\tif (!ieee80211_hw_check(&local->hw, HAS_RATE_CONTROL)) {\n\t\ttx.skb = skb;\n\t\tr = ieee80211_tx_h_rate_ctrl(&tx);\n\t\tskb = tx.skb;\n\t\ttx.skb = NULL;\n\n\t\tif (r != TX_CONTINUE) {\n\t\t\tif (r != TX_QUEUED)\n\t\t\t\tkfree_skb(skb);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (ieee80211_queue_skb(local, sdata, sta, skb))\n\t\treturn true;\n\n\tieee80211_xmit_fast_finish(sdata, sta, fast_tx->pn_offs,\n\t\t\t\t   fast_tx->key, skb);\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\n\t__skb_queue_tail(&tx.skbs, skb);\n\tieee80211_tx_frags(local, &sdata->vif, sta, &tx.skbs, false);\n\treturn true;\n}\n\nstruct sk_buff *ieee80211_tx_dequeue(struct ieee80211_hw *hw,\n\t\t\t\t     struct ieee80211_txq *txq)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct txq_info *txqi = container_of(txq, struct txq_info, txq);\n\tstruct ieee80211_hdr *hdr;\n\tstruct sk_buff *skb = NULL;\n\tstruct fq *fq = &local->fq;\n\tstruct fq_tin *tin = &txqi->tin;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_tx_data tx;\n\tieee80211_tx_result r;\n\tstruct ieee80211_vif *vif = txq->vif;\n\n\tWARN_ON_ONCE(softirq_count() == 0);\n\n\tif (!ieee80211_txq_airtime_check(hw, txq))\n\t\treturn NULL;\n\nbegin:\n\tspin_lock_bh(&fq->lock);\n\n\tif (test_bit(IEEE80211_TXQ_STOP, &txqi->flags) ||\n\t    test_bit(IEEE80211_TXQ_STOP_NETIF_TX, &txqi->flags))\n\t\tgoto out;\n\n\tif (vif->txqs_stopped[ieee80211_ac_from_tid(txq->tid)]) {\n\t\tset_bit(IEEE80211_TXQ_STOP_NETIF_TX, &txqi->flags);\n\t\tgoto out;\n\t}\n\n\t/* Make sure fragments stay together. */\n\tskb = __skb_dequeue(&txqi->frags);\n\tif (skb)\n\t\tgoto out;\n\n\tskb = fq_tin_dequeue(fq, tin, fq_tin_dequeue_func);\n\tif (!skb)\n\t\tgoto out;\n\n\tspin_unlock_bh(&fq->lock);\n\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\tmemset(&tx, 0, sizeof(tx));\n\t__skb_queue_head_init(&tx.skbs);\n\ttx.local = local;\n\ttx.skb = skb;\n\ttx.sdata = vif_to_sdata(info->control.vif);\n\n\tif (txq->sta) {\n\t\ttx.sta = container_of(txq->sta, struct sta_info, sta);\n\t\t/*\n\t\t * Drop unicast frames to unauthorised stations unless they are\n\t\t * injected frames or EAPOL frames from the local station.\n\t\t */\n\t\tif (unlikely(!(info->flags & IEEE80211_TX_CTL_INJECTED) &&\n\t\t\t     ieee80211_is_data(hdr->frame_control) &&\n\t\t\t     !ieee80211_vif_is_mesh(&tx.sdata->vif) &&\n\t\t\t     tx.sdata->vif.type != NL80211_IFTYPE_OCB &&\n\t\t\t     !is_multicast_ether_addr(hdr->addr1) &&\n\t\t\t     !test_sta_flag(tx.sta, WLAN_STA_AUTHORIZED) &&\n\t\t\t     (!(info->control.flags &\n\t\t\t\tIEEE80211_TX_CTRL_PORT_CTRL_PROTO) ||\n\t\t\t      !ether_addr_equal(tx.sdata->vif.addr,\n\t\t\t\t\t\thdr->addr2)))) {\n\t\t\tI802_DEBUG_INC(local->tx_handlers_drop_unauth_port);\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\n\t/*\n\t * The key can be removed while the packet was queued, so need to call\n\t * this here to get the current key.\n\t */\n\tr = ieee80211_tx_h_select_key(&tx);\n\tif (r != TX_CONTINUE) {\n\t\tieee80211_free_txskb(&local->hw, skb);\n\t\tgoto begin;\n\t}\n\n\tif (test_bit(IEEE80211_TXQ_AMPDU, &txqi->flags))\n\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\telse\n\t\tinfo->flags &= ~IEEE80211_TX_CTL_AMPDU;\n\n\tif (info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP)\n\t\tgoto encap_out;\n\n\tif (info->control.flags & IEEE80211_TX_CTRL_FAST_XMIT) {\n\t\tstruct sta_info *sta = container_of(txq->sta, struct sta_info,\n\t\t\t\t\t\t    sta);\n\t\tu8 pn_offs = 0;\n\n\t\tif (tx.key &&\n\t\t    (tx.key->conf.flags & IEEE80211_KEY_FLAG_GENERATE_IV))\n\t\t\tpn_offs = ieee80211_hdrlen(hdr->frame_control);\n\n\t\tieee80211_xmit_fast_finish(sta->sdata, sta, pn_offs,\n\t\t\t\t\t   tx.key, skb);\n\t} else {\n\t\tif (invoke_tx_handlers_late(&tx))\n\t\t\tgoto begin;\n\n\t\tskb = __skb_dequeue(&tx.skbs);\n\n\t\tif (!skb_queue_empty(&tx.skbs)) {\n\t\t\tspin_lock_bh(&fq->lock);\n\t\t\tskb_queue_splice_tail(&tx.skbs, &txqi->frags);\n\t\t\tspin_unlock_bh(&fq->lock);\n\t\t}\n\t}\n\n\tif (skb_has_frag_list(skb) &&\n\t    !ieee80211_hw_check(&local->hw, TX_FRAG_LIST)) {\n\t\tif (skb_linearize(skb)) {\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\n\tswitch (tx.sdata->vif.type) {\n\tcase NL80211_IFTYPE_MONITOR:\n\t\tif (tx.sdata->u.mntr.flags & MONITOR_FLAG_ACTIVE) {\n\t\t\tvif = &tx.sdata->vif;\n\t\t\tbreak;\n\t\t}\n\t\ttx.sdata = rcu_dereference(local->monitor_sdata);\n\t\tif (tx.sdata) {\n\t\t\tvif = &tx.sdata->vif;\n\t\t\tinfo->hw_queue =\n\t\t\t\tvif->hw_queue[skb_get_queue_mapping(skb)];\n\t\t} else if (ieee80211_hw_check(&local->hw, QUEUE_CONTROL)) {\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\tgoto begin;\n\t\t} else {\n\t\t\tvif = NULL;\n\t\t}\n\t\tbreak;\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\ttx.sdata = container_of(tx.sdata->bss,\n\t\t\t\t\tstruct ieee80211_sub_if_data, u.ap);\n\t\tfallthrough;\n\tdefault:\n\t\tvif = &tx.sdata->vif;\n\t\tbreak;\n\t}\n\nencap_out:\n\tIEEE80211_SKB_CB(skb)->control.vif = vif;\n\n\tif (vif &&\n\t    wiphy_ext_feature_isset(local->hw.wiphy, NL80211_EXT_FEATURE_AQL)) {\n\t\tbool ampdu = txq->ac != IEEE80211_AC_VO;\n\t\tu32 airtime;\n\n\t\tairtime = ieee80211_calc_expected_tx_airtime(hw, vif, txq->sta,\n\t\t\t\t\t\t\t     skb->len, ampdu);\n\t\tif (airtime) {\n\t\t\tairtime = ieee80211_info_set_tx_time_est(info, airtime);\n\t\t\tieee80211_sta_update_pending_airtime(local, tx.sta,\n\t\t\t\t\t\t\t     txq->ac,\n\t\t\t\t\t\t\t     airtime,\n\t\t\t\t\t\t\t     false);\n\t\t}\n\t}\n\n\treturn skb;\n\nout:\n\tspin_unlock_bh(&fq->lock);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_tx_dequeue);\n\nstruct ieee80211_txq *ieee80211_next_txq(struct ieee80211_hw *hw, u8 ac)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_txq *ret = NULL;\n\tstruct txq_info *txqi = NULL, *head = NULL;\n\tbool found_eligible_txq = false;\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\n begin:\n\ttxqi = list_first_entry_or_null(&local->active_txqs[ac],\n\t\t\t\t\tstruct txq_info,\n\t\t\t\t\tschedule_order);\n\tif (!txqi)\n\t\tgoto out;\n\n\tif (txqi == head) {\n\t\tif (!found_eligible_txq)\n\t\t\tgoto out;\n\t\telse\n\t\t\tfound_eligible_txq = false;\n\t}\n\n\tif (!head)\n\t\thead = txqi;\n\n\tif (txqi->txq.sta) {\n\t\tstruct sta_info *sta = container_of(txqi->txq.sta,\n\t\t\t\t\t\t    struct sta_info, sta);\n\t\tbool aql_check = ieee80211_txq_airtime_check(hw, &txqi->txq);\n\t\ts64 deficit = sta->airtime[txqi->txq.ac].deficit;\n\n\t\tif (aql_check)\n\t\t\tfound_eligible_txq = true;\n\n\t\tif (deficit < 0)\n\t\t\tsta->airtime[txqi->txq.ac].deficit +=\n\t\t\t\tsta->airtime_weight;\n\n\t\tif (deficit < 0 || !aql_check) {\n\t\t\tlist_move_tail(&txqi->schedule_order,\n\t\t\t\t       &local->active_txqs[txqi->txq.ac]);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\n\n\tif (txqi->schedule_round == local->schedule_round[ac])\n\t\tgoto out;\n\n\tlist_del_init(&txqi->schedule_order);\n\ttxqi->schedule_round = local->schedule_round[ac];\n\tret = &txqi->txq;\n\nout:\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n\treturn ret;\n}\nEXPORT_SYMBOL(ieee80211_next_txq);\n\nvoid __ieee80211_schedule_txq(struct ieee80211_hw *hw,\n\t\t\t      struct ieee80211_txq *txq,\n\t\t\t      bool force)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct txq_info *txqi = to_txq_info(txq);\n\n\tspin_lock_bh(&local->active_txq_lock[txq->ac]);\n\n\tif (list_empty(&txqi->schedule_order) &&\n\t    (force || !skb_queue_empty(&txqi->frags) ||\n\t     txqi->tin.backlog_packets)) {\n\t\t/* If airtime accounting is active, always enqueue STAs at the\n\t\t * head of the list to ensure that they only get moved to the\n\t\t * back by the airtime DRR scheduler once they have a negative\n\t\t * deficit. A station that already has a negative deficit will\n\t\t * get immediately moved to the back of the list on the next\n\t\t * call to ieee80211_next_txq().\n\t\t */\n\t\tif (txqi->txq.sta && local->airtime_flags &&\n\t\t    wiphy_ext_feature_isset(local->hw.wiphy,\n\t\t\t\t\t    NL80211_EXT_FEATURE_AIRTIME_FAIRNESS))\n\t\t\tlist_add(&txqi->schedule_order,\n\t\t\t\t &local->active_txqs[txq->ac]);\n\t\telse\n\t\t\tlist_add_tail(&txqi->schedule_order,\n\t\t\t\t      &local->active_txqs[txq->ac]);\n\t}\n\n\tspin_unlock_bh(&local->active_txq_lock[txq->ac]);\n}\nEXPORT_SYMBOL(__ieee80211_schedule_txq);\n\nbool ieee80211_txq_airtime_check(struct ieee80211_hw *hw,\n\t\t\t\t struct ieee80211_txq *txq)\n{\n\tstruct sta_info *sta;\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\n\tif (!wiphy_ext_feature_isset(local->hw.wiphy, NL80211_EXT_FEATURE_AQL))\n\t\treturn true;\n\n\tif (!txq->sta)\n\t\treturn true;\n\n\tsta = container_of(txq->sta, struct sta_info, sta);\n\tif (atomic_read(&sta->airtime[txq->ac].aql_tx_pending) <\n\t    sta->airtime[txq->ac].aql_limit_low)\n\t\treturn true;\n\n\tif (atomic_read(&local->aql_total_pending_airtime) <\n\t    local->aql_threshold &&\n\t    atomic_read(&sta->airtime[txq->ac].aql_tx_pending) <\n\t    sta->airtime[txq->ac].aql_limit_high)\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL(ieee80211_txq_airtime_check);\n\nbool ieee80211_txq_may_transmit(struct ieee80211_hw *hw,\n\t\t\t\tstruct ieee80211_txq *txq)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct txq_info *iter, *tmp, *txqi = to_txq_info(txq);\n\tstruct sta_info *sta;\n\tu8 ac = txq->ac;\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\n\tif (!txqi->txq.sta)\n\t\tgoto out;\n\n\tif (list_empty(&txqi->schedule_order))\n\t\tgoto out;\n\n\tlist_for_each_entry_safe(iter, tmp, &local->active_txqs[ac],\n\t\t\t\t schedule_order) {\n\t\tif (iter == txqi)\n\t\t\tbreak;\n\n\t\tif (!iter->txq.sta) {\n\t\t\tlist_move_tail(&iter->schedule_order,\n\t\t\t\t       &local->active_txqs[ac]);\n\t\t\tcontinue;\n\t\t}\n\t\tsta = container_of(iter->txq.sta, struct sta_info, sta);\n\t\tif (sta->airtime[ac].deficit < 0)\n\t\t\tsta->airtime[ac].deficit += sta->airtime_weight;\n\t\tlist_move_tail(&iter->schedule_order, &local->active_txqs[ac]);\n\t}\n\n\tsta = container_of(txqi->txq.sta, struct sta_info, sta);\n\tif (sta->airtime[ac].deficit >= 0)\n\t\tgoto out;\n\n\tsta->airtime[ac].deficit += sta->airtime_weight;\n\tlist_move_tail(&txqi->schedule_order, &local->active_txqs[ac]);\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n\n\treturn false;\nout:\n\tif (!list_empty(&txqi->schedule_order))\n\t\tlist_del_init(&txqi->schedule_order);\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n\n\treturn true;\n}\nEXPORT_SYMBOL(ieee80211_txq_may_transmit);\n\nvoid ieee80211_txq_schedule_start(struct ieee80211_hw *hw, u8 ac)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\n\tspin_lock_bh(&local->active_txq_lock[ac]);\n\tlocal->schedule_round[ac]++;\n\tspin_unlock_bh(&local->active_txq_lock[ac]);\n}\nEXPORT_SYMBOL(ieee80211_txq_schedule_start);\n\nvoid __ieee80211_subif_start_xmit(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev,\n\t\t\t\t  u32 info_flags,\n\t\t\t\t  u32 ctrl_flags,\n\t\t\t\t  u64 *cookie)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sta_info *sta;\n\tstruct sk_buff *next;\n\n\tif (unlikely(skb->len < ETH_HLEN)) {\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta))\n\t\tgoto out_free;\n\n\tif (IS_ERR(sta))\n\t\tsta = NULL;\n\n\tif (local->ops->wake_tx_queue) {\n\t\tu16 queue = __ieee80211_select_queue(sdata, sta, skb);\n\t\tskb_set_queue_mapping(skb, queue);\n\t\tskb_get_hash(skb);\n\t}\n\n\tif (sta) {\n\t\tstruct ieee80211_fast_tx *fast_tx;\n\n\t\tsk_pacing_shift_update(skb->sk, sdata->local->hw.tx_sk_pacing_shift);\n\n\t\tfast_tx = rcu_dereference(sta->fast_tx);\n\n\t\tif (fast_tx &&\n\t\t    ieee80211_xmit_fast(sdata, sta, fast_tx, skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, 0);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_free;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\t/* we cannot process non-linear frames on this path */\n\t\tif (skb_linearize(skb)) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* the frame could be fragmented, software-encrypted, and other\n\t\t * things so we cannot really handle checksum offload with it -\n\t\t * fix it up in software before we handle anything else.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_checksum_help(skb))\n\t\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tskb_list_walk_safe(skb, skb, next) {\n\t\tskb_mark_not_on_list(skb);\n\n\t\tif (skb->protocol == sdata->control_port_protocol)\n\t\t\tctrl_flags |= IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP;\n\n\t\tskb = ieee80211_build_hdr(sdata, skb, info_flags,\n\t\t\t\t\t  sta, ctrl_flags, cookie);\n\t\tif (IS_ERR(skb)) {\n\t\t\tkfree_skb_list(next);\n\t\t\tgoto out;\n\t\t}\n\n\t\tdev_sw_netstats_tx_add(dev, 1, skb->len);\n\n\t\tieee80211_xmit(sdata, sta, skb);\n\t}\n\tgoto out;\n out_free:\n\tkfree_skb(skb);\n out:\n\trcu_read_unlock();\n}\n\nstatic int ieee80211_change_da(struct sk_buff *skb, struct sta_info *sta)\n{\n\tstruct ethhdr *eth;\n\tint err;\n\n\terr = skb_ensure_writable(skb, ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\teth = (void *)skb->data;\n\tether_addr_copy(eth->h_dest, sta->sta.addr);\n\n\treturn 0;\n}\n\nstatic bool ieee80211_multicast_to_unicast(struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tconst struct ethhdr *eth = (void *)skb->data;\n\tconst struct vlan_ethhdr *ethvlan = (void *)skb->data;\n\t__be16 ethertype;\n\n\tif (likely(!is_multicast_ether_addr(eth->h_dest)))\n\t\treturn false;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tif (sdata->u.vlan.sta)\n\t\t\treturn false;\n\t\tif (sdata->wdev.use_4addr)\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase NL80211_IFTYPE_AP:\n\t\t/* check runtime toggle for this bss */\n\t\tif (!sdata->bss->multicast_to_unicast)\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t/* multicast to unicast conversion only for some payload */\n\tethertype = eth->h_proto;\n\tif (ethertype == htons(ETH_P_8021Q) && skb->len >= VLAN_ETH_HLEN)\n\t\tethertype = ethvlan->h_vlan_encapsulated_proto;\n\tswitch (ethertype) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void\nieee80211_convert_to_unicast(struct sk_buff *skb, struct net_device *dev,\n\t\t\t     struct sk_buff_head *queue)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tconst struct ethhdr *eth = (struct ethhdr *)skb->data;\n\tstruct sta_info *sta, *first = NULL;\n\tstruct sk_buff *cloned_skb;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(sta, &local->sta_list, list) {\n\t\tif (sdata != sta->sdata)\n\t\t\t/* AP-VLAN mismatch */\n\t\t\tcontinue;\n\t\tif (unlikely(ether_addr_equal(eth->h_source, sta->sta.addr)))\n\t\t\t/* do not send back to source */\n\t\t\tcontinue;\n\t\tif (!first) {\n\t\t\tfirst = sta;\n\t\t\tcontinue;\n\t\t}\n\t\tcloned_skb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!cloned_skb)\n\t\t\tgoto multicast;\n\t\tif (unlikely(ieee80211_change_da(cloned_skb, sta))) {\n\t\t\tdev_kfree_skb(cloned_skb);\n\t\t\tgoto multicast;\n\t\t}\n\t\t__skb_queue_tail(queue, cloned_skb);\n\t}\n\n\tif (likely(first)) {\n\t\tif (unlikely(ieee80211_change_da(skb, first)))\n\t\t\tgoto multicast;\n\t\t__skb_queue_tail(queue, skb);\n\t} else {\n\t\t/* no STA connected, drop */\n\t\tkfree_skb(skb);\n\t\tskb = NULL;\n\t}\n\n\tgoto out;\nmulticast:\n\t__skb_queue_purge(queue);\n\t__skb_queue_tail(queue, skb);\nout:\n\trcu_read_unlock();\n}\n\n/**\n * ieee80211_subif_start_xmit - netif start_xmit function for 802.3 vifs\n * @skb: packet to be sent\n * @dev: incoming interface\n *\n * On failure skb will be freed.\n */\nnetdev_tx_t ieee80211_subif_start_xmit(struct sk_buff *skb,\n\t\t\t\t       struct net_device *dev)\n{\n\tif (unlikely(ieee80211_multicast_to_unicast(skb, dev))) {\n\t\tstruct sk_buff_head queue;\n\n\t\t__skb_queue_head_init(&queue);\n\t\tieee80211_convert_to_unicast(skb, dev, &queue);\n\t\twhile ((skb = __skb_dequeue(&queue)))\n\t\t\t__ieee80211_subif_start_xmit(skb, dev, 0, 0, NULL);\n\t} else {\n\t\t__ieee80211_subif_start_xmit(skb, dev, 0, 0, NULL);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic bool ieee80211_tx_8023(struct ieee80211_sub_if_data *sdata,\n\t\t\t      struct sk_buff *skb, int led_len,\n\t\t\t      struct sta_info *sta,\n\t\t\t      bool txpending)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct ieee80211_tx_control control = {};\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_sta *pubsta = NULL;\n\tunsigned long flags;\n\tint q = info->hw_queue;\n\n\tif (ieee80211_queue_skb(local, sdata, sta, skb))\n\t\treturn true;\n\n\tspin_lock_irqsave(&local->queue_stop_reason_lock, flags);\n\n\tif (local->queue_stop_reasons[q] ||\n\t    (!txpending && !skb_queue_empty(&local->pending[q]))) {\n\t\tif (txpending)\n\t\t\tskb_queue_head(&local->pending[q], skb);\n\t\telse\n\t\t\tskb_queue_tail(&local->pending[q], skb);\n\n\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\t\treturn false;\n\t}\n\n\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\tif (sta && sta->uploaded)\n\t\tpubsta = &sta->sta;\n\n\tcontrol.sta = pubsta;\n\n\tdrv_tx(local, &control, skb);\n\n\treturn true;\n}\n\nstatic void ieee80211_8023_xmit(struct ieee80211_sub_if_data *sdata,\n\t\t\t\tstruct net_device *dev, struct sta_info *sta,\n\t\t\t\tstruct ieee80211_key *key, struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct tid_ampdu_tx *tid_tx;\n\tu8 tid;\n\n\tif (local->ops->wake_tx_queue) {\n\t\tu16 queue = __ieee80211_select_queue(sdata, sta, skb);\n\t\tskb_set_queue_mapping(skb, queue);\n\t\tskb_get_hash(skb);\n\t}\n\n\tif (unlikely(test_bit(SCAN_SW_SCANNING, &local->scanning)) &&\n\t    test_bit(SDATA_STATE_OFFCHANNEL, &sdata->state))\n\t\tgoto out_free;\n\n\tmemset(info, 0, sizeof(*info));\n\n\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\ttid_tx = rcu_dereference(sta->ampdu_mlme.tid_tx[tid]);\n\tif (tid_tx) {\n\t\tif (!test_bit(HT_AGG_STATE_OPERATIONAL, &tid_tx->state)) {\n\t\t\t/* fall back to non-offload slow path */\n\t\t\t__ieee80211_subif_start_xmit(skb, dev, 0, 0, NULL);\n\t\t\treturn;\n\t\t}\n\n\t\tinfo->flags |= IEEE80211_TX_CTL_AMPDU;\n\t\tif (tid_tx->timeout)\n\t\t\ttid_tx->last_tx = jiffies;\n\t}\n\n\tif (unlikely(skb->sk &&\n\t\t     skb_shinfo(skb)->tx_flags & SKBTX_WIFI_STATUS))\n\t\tinfo->ack_frame_id = ieee80211_store_ack_skb(local, skb,\n\t\t\t\t\t\t\t     &info->flags, NULL);\n\n\tinfo->hw_queue = sdata->vif.hw_queue[skb_get_queue_mapping(skb)];\n\n\tdev_sw_netstats_tx_add(dev, 1, skb->len);\n\n\tsta->tx_stats.bytes[skb_get_queue_mapping(skb)] += skb->len;\n\tsta->tx_stats.packets[skb_get_queue_mapping(skb)]++;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP_VLAN)\n\t\tsdata = container_of(sdata->bss,\n\t\t\t\t     struct ieee80211_sub_if_data, u.ap);\n\n\tinfo->flags |= IEEE80211_TX_CTL_HW_80211_ENCAP;\n\tinfo->control.vif = &sdata->vif;\n\n\tif (key)\n\t\tinfo->control.hw_key = &key->conf;\n\n\tieee80211_tx_8023(sdata, skb, skb->len, sta, false);\n\n\treturn;\n\nout_free:\n\tkfree_skb(skb);\n}\n\nnetdev_tx_t ieee80211_subif_start_xmit_8023(struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ethhdr *ehdr = (struct ethhdr *)skb->data;\n\tstruct ieee80211_key *key;\n\tstruct sta_info *sta;\n\n\tif (unlikely(skb->len < ETH_HLEN)) {\n\t\tkfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta)) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(IS_ERR_OR_NULL(sta) || !sta->uploaded ||\n\t    !test_sta_flag(sta, WLAN_STA_AUTHORIZED) ||\n\t    sdata->control_port_protocol == ehdr->h_proto))\n\t\tgoto skip_offload;\n\n\tkey = rcu_dereference(sta->ptk[sta->ptk_idx]);\n\tif (!key)\n\t\tkey = rcu_dereference(sdata->default_unicast_key);\n\n\tif (key && (!(key->flags & KEY_FLAG_UPLOADED_TO_HARDWARE) ||\n\t\t    key->conf.cipher == WLAN_CIPHER_SUITE_TKIP))\n\t\tgoto skip_offload;\n\n\tieee80211_8023_xmit(sdata, dev, sta, key, skb);\n\tgoto out;\n\nskip_offload:\n\tieee80211_subif_start_xmit(skb, dev);\nout:\n\trcu_read_unlock();\n\n\treturn NETDEV_TX_OK;\n}\n\nstruct sk_buff *\nieee80211_build_data_template(struct ieee80211_sub_if_data *sdata,\n\t\t\t      struct sk_buff *skb, u32 info_flags)\n{\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_tx_data tx = {\n\t\t.local = sdata->local,\n\t\t.sdata = sdata,\n\t};\n\tstruct sta_info *sta;\n\n\trcu_read_lock();\n\n\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta)) {\n\t\tkfree_skb(skb);\n\t\tskb = ERR_PTR(-EINVAL);\n\t\tgoto out;\n\t}\n\n\tskb = ieee80211_build_hdr(sdata, skb, info_flags, sta, 0, NULL);\n\tif (IS_ERR(skb))\n\t\tgoto out;\n\n\thdr = (void *)skb->data;\n\ttx.sta = sta_info_get(sdata, hdr->addr1);\n\ttx.skb = skb;\n\n\tif (ieee80211_tx_h_select_key(&tx) != TX_CONTINUE) {\n\t\trcu_read_unlock();\n\t\tkfree_skb(skb);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\nout:\n\trcu_read_unlock();\n\treturn skb;\n}\n\n/*\n * ieee80211_clear_tx_pending may not be called in a context where\n * it is possible that it packets could come in again.\n */\nvoid ieee80211_clear_tx_pending(struct ieee80211_local *local)\n{\n\tstruct sk_buff *skb;\n\tint i;\n\n\tfor (i = 0; i < local->hw.queues; i++) {\n\t\twhile ((skb = skb_dequeue(&local->pending[i])) != NULL)\n\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t}\n}\n\n/*\n * Returns false if the frame couldn't be transmitted but was queued instead,\n * which in this case means re-queued -- take as an indication to stop sending\n * more pending frames.\n */\nstatic bool ieee80211_tx_pending_skb(struct ieee80211_local *local,\n\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct sta_info *sta;\n\tstruct ieee80211_hdr *hdr;\n\tbool result;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\n\tsdata = vif_to_sdata(info->control.vif);\n\n\tif (info->control.flags & IEEE80211_TX_INTCFL_NEED_TXPROCESSING) {\n\t\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\t\tif (unlikely(!chanctx_conf)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn true;\n\t\t}\n\t\tinfo->band = chanctx_conf->def.chan->band;\n\t\tresult = ieee80211_tx(sdata, NULL, skb, true);\n\t} else if (info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) {\n\t\tif (ieee80211_lookup_ra_sta(sdata, skb, &sta)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn true;\n\t\t}\n\n\t\tif (IS_ERR(sta) || (sta && !sta->uploaded))\n\t\t\tsta = NULL;\n\n\t\tresult = ieee80211_tx_8023(sdata, skb, skb->len, sta, true);\n\t} else {\n\t\tstruct sk_buff_head skbs;\n\n\t\t__skb_queue_head_init(&skbs);\n\t\t__skb_queue_tail(&skbs, skb);\n\n\t\thdr = (struct ieee80211_hdr *)skb->data;\n\t\tsta = sta_info_get(sdata, hdr->addr1);\n\n\t\tresult = __ieee80211_tx(local, &skbs, skb->len, sta, true);\n\t}\n\n\treturn result;\n}\n\n/*\n * Transmit all pending packets. Called from tasklet.\n */\nvoid ieee80211_tx_pending(struct tasklet_struct *t)\n{\n\tstruct ieee80211_local *local = from_tasklet(local, t,\n\t\t\t\t\t\t     tx_pending_tasklet);\n\tunsigned long flags;\n\tint i;\n\tbool txok;\n\n\trcu_read_lock();\n\n\tspin_lock_irqsave(&local->queue_stop_reason_lock, flags);\n\tfor (i = 0; i < local->hw.queues; i++) {\n\t\t/*\n\t\t * If queue is stopped by something other than due to pending\n\t\t * frames, or we have no pending frames, proceed to next queue.\n\t\t */\n\t\tif (local->queue_stop_reasons[i] ||\n\t\t    skb_queue_empty(&local->pending[i]))\n\t\t\tcontinue;\n\n\t\twhile (!skb_queue_empty(&local->pending[i])) {\n\t\t\tstruct sk_buff *skb = __skb_dequeue(&local->pending[i]);\n\t\t\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\t\t\tif (WARN_ON(!info->control.vif)) {\n\t\t\t\tieee80211_free_txskb(&local->hw, skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tspin_unlock_irqrestore(&local->queue_stop_reason_lock,\n\t\t\t\t\t\tflags);\n\n\t\t\ttxok = ieee80211_tx_pending_skb(local, skb);\n\t\t\tspin_lock_irqsave(&local->queue_stop_reason_lock,\n\t\t\t\t\t  flags);\n\t\t\tif (!txok)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (skb_queue_empty(&local->pending[i]))\n\t\t\tieee80211_propagate_queue_wake(local, i);\n\t}\n\tspin_unlock_irqrestore(&local->queue_stop_reason_lock, flags);\n\n\trcu_read_unlock();\n}\n\n/* functions for drivers to get certain frames */\n\nstatic void __ieee80211_beacon_add_tim(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t       struct ps_data *ps, struct sk_buff *skb,\n\t\t\t\t       bool is_template)\n{\n\tu8 *pos, *tim;\n\tint aid0 = 0;\n\tint i, have_bits = 0, n1, n2;\n\n\t/* Generate bitmap for TIM only if there are any STAs in power save\n\t * mode. */\n\tif (atomic_read(&ps->num_sta_ps) > 0)\n\t\t/* in the hope that this is faster than\n\t\t * checking byte-for-byte */\n\t\thave_bits = !bitmap_empty((unsigned long *)ps->tim,\n\t\t\t\t\t  IEEE80211_MAX_AID+1);\n\tif (!is_template) {\n\t\tif (ps->dtim_count == 0)\n\t\t\tps->dtim_count = sdata->vif.bss_conf.dtim_period - 1;\n\t\telse\n\t\t\tps->dtim_count--;\n\t}\n\n\ttim = pos = skb_put(skb, 6);\n\t*pos++ = WLAN_EID_TIM;\n\t*pos++ = 4;\n\t*pos++ = ps->dtim_count;\n\t*pos++ = sdata->vif.bss_conf.dtim_period;\n\n\tif (ps->dtim_count == 0 && !skb_queue_empty(&ps->bc_buf))\n\t\taid0 = 1;\n\n\tps->dtim_bc_mc = aid0 == 1;\n\n\tif (have_bits) {\n\t\t/* Find largest even number N1 so that bits numbered 1 through\n\t\t * (N1 x 8) - 1 in the bitmap are 0 and number N2 so that bits\n\t\t * (N2 + 1) x 8 through 2007 are 0. */\n\t\tn1 = 0;\n\t\tfor (i = 0; i < IEEE80211_MAX_TIM_LEN; i++) {\n\t\t\tif (ps->tim[i]) {\n\t\t\t\tn1 = i & 0xfe;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tn2 = n1;\n\t\tfor (i = IEEE80211_MAX_TIM_LEN - 1; i >= n1; i--) {\n\t\t\tif (ps->tim[i]) {\n\t\t\t\tn2 = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Bitmap control */\n\t\t*pos++ = n1 | aid0;\n\t\t/* Part Virt Bitmap */\n\t\tskb_put(skb, n2 - n1);\n\t\tmemcpy(pos, ps->tim + n1, n2 - n1 + 1);\n\n\t\ttim[1] = n2 - n1 + 4;\n\t} else {\n\t\t*pos++ = aid0; /* Bitmap control */\n\t\t*pos++ = 0; /* Part Virt Bitmap */\n\t}\n}\n\nstatic int ieee80211_beacon_add_tim(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t    struct ps_data *ps, struct sk_buff *skb,\n\t\t\t\t    bool is_template)\n{\n\tstruct ieee80211_local *local = sdata->local;\n\n\t/*\n\t * Not very nice, but we want to allow the driver to call\n\t * ieee80211_beacon_get() as a response to the set_tim()\n\t * callback. That, however, is already invoked under the\n\t * sta_lock to guarantee consistent and race-free update\n\t * of the tim bitmap in mac80211 and the driver.\n\t */\n\tif (local->tim_in_locked_section) {\n\t\t__ieee80211_beacon_add_tim(sdata, ps, skb, is_template);\n\t} else {\n\t\tspin_lock_bh(&local->tim_lock);\n\t\t__ieee80211_beacon_add_tim(sdata, ps, skb, is_template);\n\t\tspin_unlock_bh(&local->tim_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic void ieee80211_set_beacon_cntdwn(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t\tstruct beacon_data *beacon)\n{\n\tstruct probe_resp *resp;\n\tu8 *beacon_data;\n\tsize_t beacon_data_len;\n\tint i;\n\tu8 count = beacon->cntdwn_current_counter;\n\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_AP:\n\t\tbeacon_data = beacon->tail;\n\t\tbeacon_data_len = beacon->tail_len;\n\t\tbreak;\n\tcase NL80211_IFTYPE_ADHOC:\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t\tbreak;\n\tcase NL80211_IFTYPE_MESH_POINT:\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\trcu_read_lock();\n\tfor (i = 0; i < IEEE80211_MAX_CNTDWN_COUNTERS_NUM; ++i) {\n\t\tresp = rcu_dereference(sdata->u.ap.probe_resp);\n\n\t\tif (beacon->cntdwn_counter_offsets[i]) {\n\t\t\tif (WARN_ON_ONCE(beacon->cntdwn_counter_offsets[i] >=\n\t\t\t\t\t beacon_data_len)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tbeacon_data[beacon->cntdwn_counter_offsets[i]] = count;\n\t\t}\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP && resp)\n\t\t\tresp->data[resp->cntdwn_counter_offsets[i]] = count;\n\t}\n\trcu_read_unlock();\n}\n\nstatic u8 __ieee80211_beacon_update_cntdwn(struct beacon_data *beacon)\n{\n\tbeacon->cntdwn_current_counter--;\n\n\t/* the counter should never reach 0 */\n\tWARN_ON_ONCE(!beacon->cntdwn_current_counter);\n\n\treturn beacon->cntdwn_current_counter;\n}\n\nu8 ieee80211_beacon_update_cntdwn(struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct beacon_data *beacon = NULL;\n\tu8 count = 0;\n\n\trcu_read_lock();\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\tbeacon = rcu_dereference(sdata->u.ap.beacon);\n\telse if (sdata->vif.type == NL80211_IFTYPE_ADHOC)\n\t\tbeacon = rcu_dereference(sdata->u.ibss.presp);\n\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tbeacon = rcu_dereference(sdata->u.mesh.beacon);\n\n\tif (!beacon)\n\t\tgoto unlock;\n\n\tcount = __ieee80211_beacon_update_cntdwn(beacon);\n\nunlock:\n\trcu_read_unlock();\n\treturn count;\n}\nEXPORT_SYMBOL(ieee80211_beacon_update_cntdwn);\n\nvoid ieee80211_beacon_set_cntdwn(struct ieee80211_vif *vif, u8 counter)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct beacon_data *beacon = NULL;\n\n\trcu_read_lock();\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\tbeacon = rcu_dereference(sdata->u.ap.beacon);\n\telse if (sdata->vif.type == NL80211_IFTYPE_ADHOC)\n\t\tbeacon = rcu_dereference(sdata->u.ibss.presp);\n\telse if (ieee80211_vif_is_mesh(&sdata->vif))\n\t\tbeacon = rcu_dereference(sdata->u.mesh.beacon);\n\n\tif (!beacon)\n\t\tgoto unlock;\n\n\tif (counter < beacon->cntdwn_current_counter)\n\t\tbeacon->cntdwn_current_counter = counter;\n\nunlock:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(ieee80211_beacon_set_cntdwn);\n\nbool ieee80211_beacon_cntdwn_is_complete(struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\tstruct beacon_data *beacon = NULL;\n\tu8 *beacon_data;\n\tsize_t beacon_data_len;\n\tint ret = false;\n\n\tif (!ieee80211_sdata_running(sdata))\n\t\treturn false;\n\n\trcu_read_lock();\n\tif (vif->type == NL80211_IFTYPE_AP) {\n\t\tstruct ieee80211_if_ap *ap = &sdata->u.ap;\n\n\t\tbeacon = rcu_dereference(ap->beacon);\n\t\tif (WARN_ON(!beacon || !beacon->tail))\n\t\t\tgoto out;\n\t\tbeacon_data = beacon->tail;\n\t\tbeacon_data_len = beacon->tail_len;\n\t} else if (vif->type == NL80211_IFTYPE_ADHOC) {\n\t\tstruct ieee80211_if_ibss *ifibss = &sdata->u.ibss;\n\n\t\tbeacon = rcu_dereference(ifibss->presp);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t} else if (vif->type == NL80211_IFTYPE_MESH_POINT) {\n\t\tstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\n\n\t\tbeacon = rcu_dereference(ifmsh->beacon);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tbeacon_data = beacon->head;\n\t\tbeacon_data_len = beacon->head_len;\n\t} else {\n\t\tWARN_ON(1);\n\t\tgoto out;\n\t}\n\n\tif (!beacon->cntdwn_counter_offsets[0])\n\t\tgoto out;\n\n\tif (WARN_ON_ONCE(beacon->cntdwn_counter_offsets[0] > beacon_data_len))\n\t\tgoto out;\n\n\tif (beacon_data[beacon->cntdwn_counter_offsets[0]] == 1)\n\t\tret = true;\n\n out:\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ieee80211_beacon_cntdwn_is_complete);\n\nstatic int ieee80211_beacon_protect(struct sk_buff *skb,\n\t\t\t\t    struct ieee80211_local *local,\n\t\t\t\t    struct ieee80211_sub_if_data *sdata)\n{\n\tieee80211_tx_result res;\n\tstruct ieee80211_tx_data tx;\n\tstruct sk_buff *check_skb;\n\n\tmemset(&tx, 0, sizeof(tx));\n\ttx.key = rcu_dereference(sdata->default_beacon_key);\n\tif (!tx.key)\n\t\treturn 0;\n\ttx.local = local;\n\ttx.sdata = sdata;\n\t__skb_queue_head_init(&tx.skbs);\n\t__skb_queue_tail(&tx.skbs, skb);\n\tres = ieee80211_tx_h_encrypt(&tx);\n\tcheck_skb = __skb_dequeue(&tx.skbs);\n\t/* we may crash after this, but it'd be a bug in crypto */\n\tWARN_ON(check_skb != skb);\n\tif (WARN_ON_ONCE(res != TX_CONTINUE))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic struct sk_buff *\n__ieee80211_beacon_get(struct ieee80211_hw *hw,\n\t\t       struct ieee80211_vif *vif,\n\t\t       struct ieee80211_mutable_offsets *offs,\n\t\t       bool is_template)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct beacon_data *beacon = NULL;\n\tstruct sk_buff *skb = NULL;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_sub_if_data *sdata = NULL;\n\tenum nl80211_band band;\n\tstruct ieee80211_tx_rate_control txrc;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\tint csa_off_base = 0;\n\n\trcu_read_lock();\n\n\tsdata = vif_to_sdata(vif);\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\n\tif (!ieee80211_sdata_running(sdata) || !chanctx_conf)\n\t\tgoto out;\n\n\tif (offs)\n\t\tmemset(offs, 0, sizeof(*offs));\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP) {\n\t\tstruct ieee80211_if_ap *ap = &sdata->u.ap;\n\n\t\tbeacon = rcu_dereference(ap->beacon);\n\t\tif (beacon) {\n\t\t\tif (beacon->cntdwn_counter_offsets[0]) {\n\t\t\t\tif (!is_template)\n\t\t\t\t\tieee80211_beacon_update_cntdwn(vif);\n\n\t\t\t\tieee80211_set_beacon_cntdwn(sdata, beacon);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * headroom, head length,\n\t\t\t * tail length and maximum TIM length\n\t\t\t */\n\t\t\tskb = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t\t    beacon->head_len +\n\t\t\t\t\t    beacon->tail_len + 256 +\n\t\t\t\t\t    local->hw.extra_beacon_tailroom);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tskb_reserve(skb, local->tx_headroom);\n\t\t\tskb_put_data(skb, beacon->head, beacon->head_len);\n\n\t\t\tieee80211_beacon_add_tim(sdata, &ap->ps, skb,\n\t\t\t\t\t\t is_template);\n\n\t\t\tif (offs) {\n\t\t\t\toffs->tim_offset = beacon->head_len;\n\t\t\t\toffs->tim_length = skb->len - beacon->head_len;\n\n\t\t\t\t/* for AP the csa offsets are from tail */\n\t\t\t\tcsa_off_base = skb->len;\n\t\t\t}\n\n\t\t\tif (beacon->tail)\n\t\t\t\tskb_put_data(skb, beacon->tail,\n\t\t\t\t\t     beacon->tail_len);\n\n\t\t\tif (ieee80211_beacon_protect(skb, local, sdata) < 0)\n\t\t\t\tgoto out;\n\t\t} else\n\t\t\tgoto out;\n\t} else if (sdata->vif.type == NL80211_IFTYPE_ADHOC) {\n\t\tstruct ieee80211_if_ibss *ifibss = &sdata->u.ibss;\n\t\tstruct ieee80211_hdr *hdr;\n\n\t\tbeacon = rcu_dereference(ifibss->presp);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tif (beacon->cntdwn_counter_offsets[0]) {\n\t\t\tif (!is_template)\n\t\t\t\t__ieee80211_beacon_update_cntdwn(beacon);\n\n\t\t\tieee80211_set_beacon_cntdwn(sdata, beacon);\n\t\t}\n\n\t\tskb = dev_alloc_skb(local->tx_headroom + beacon->head_len +\n\t\t\t\t    local->hw.extra_beacon_tailroom);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tskb_reserve(skb, local->tx_headroom);\n\t\tskb_put_data(skb, beacon->head, beacon->head_len);\n\n\t\thdr = (struct ieee80211_hdr *) skb->data;\n\t\thdr->frame_control = cpu_to_le16(IEEE80211_FTYPE_MGMT |\n\t\t\t\t\t\t IEEE80211_STYPE_BEACON);\n\t} else if (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\n\n\t\tbeacon = rcu_dereference(ifmsh->beacon);\n\t\tif (!beacon)\n\t\t\tgoto out;\n\n\t\tif (beacon->cntdwn_counter_offsets[0]) {\n\t\t\tif (!is_template)\n\t\t\t\t/* TODO: For mesh csa_counter is in TU, so\n\t\t\t\t * decrementing it by one isn't correct, but\n\t\t\t\t * for now we leave it consistent with overall\n\t\t\t\t * mac80211's behavior.\n\t\t\t\t */\n\t\t\t\t__ieee80211_beacon_update_cntdwn(beacon);\n\n\t\t\tieee80211_set_beacon_cntdwn(sdata, beacon);\n\t\t}\n\n\t\tif (ifmsh->sync_ops)\n\t\t\tifmsh->sync_ops->adjust_tsf(sdata, beacon);\n\n\t\tskb = dev_alloc_skb(local->tx_headroom +\n\t\t\t\t    beacon->head_len +\n\t\t\t\t    256 + /* TIM IE */\n\t\t\t\t    beacon->tail_len +\n\t\t\t\t    local->hw.extra_beacon_tailroom);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tskb_reserve(skb, local->tx_headroom);\n\t\tskb_put_data(skb, beacon->head, beacon->head_len);\n\t\tieee80211_beacon_add_tim(sdata, &ifmsh->ps, skb, is_template);\n\n\t\tif (offs) {\n\t\t\toffs->tim_offset = beacon->head_len;\n\t\t\toffs->tim_length = skb->len - beacon->head_len;\n\t\t}\n\n\t\tskb_put_data(skb, beacon->tail, beacon->tail_len);\n\t} else {\n\t\tWARN_ON(1);\n\t\tgoto out;\n\t}\n\n\t/* CSA offsets */\n\tif (offs && beacon) {\n\t\tint i;\n\n\t\tfor (i = 0; i < IEEE80211_MAX_CNTDWN_COUNTERS_NUM; i++) {\n\t\t\tu16 csa_off = beacon->cntdwn_counter_offsets[i];\n\n\t\t\tif (!csa_off)\n\t\t\t\tcontinue;\n\n\t\t\toffs->cntdwn_counter_offs[i] = csa_off_base + csa_off;\n\t\t}\n\t}\n\n\tband = chanctx_conf->def.chan->band;\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\tinfo->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\tinfo->flags |= IEEE80211_TX_CTL_NO_ACK;\n\tinfo->band = band;\n\n\tmemset(&txrc, 0, sizeof(txrc));\n\ttxrc.hw = hw;\n\ttxrc.sband = local->hw.wiphy->bands[band];\n\ttxrc.bss_conf = &sdata->vif.bss_conf;\n\ttxrc.skb = skb;\n\ttxrc.reported_rate.idx = -1;\n\tif (sdata->beacon_rate_set && sdata->beacon_rateidx_mask[band])\n\t\ttxrc.rate_idx_mask = sdata->beacon_rateidx_mask[band];\n\telse\n\t\ttxrc.rate_idx_mask = sdata->rc_rateidx_mask[band];\n\ttxrc.bss = true;\n\trate_control_get_rate(sdata, NULL, &txrc);\n\n\tinfo->control.vif = vif;\n\n\tinfo->flags |= IEEE80211_TX_CTL_CLEAR_PS_FILT |\n\t\t\tIEEE80211_TX_CTL_ASSIGN_SEQ |\n\t\t\tIEEE80211_TX_CTL_FIRST_FRAGMENT;\n out:\n\trcu_read_unlock();\n\treturn skb;\n\n}\n\nstruct sk_buff *\nieee80211_beacon_get_template(struct ieee80211_hw *hw,\n\t\t\t      struct ieee80211_vif *vif,\n\t\t\t      struct ieee80211_mutable_offsets *offs)\n{\n\treturn __ieee80211_beacon_get(hw, vif, offs, true);\n}\nEXPORT_SYMBOL(ieee80211_beacon_get_template);\n\nstruct sk_buff *ieee80211_beacon_get_tim(struct ieee80211_hw *hw,\n\t\t\t\t\t struct ieee80211_vif *vif,\n\t\t\t\t\t u16 *tim_offset, u16 *tim_length)\n{\n\tstruct ieee80211_mutable_offsets offs = {};\n\tstruct sk_buff *bcn = __ieee80211_beacon_get(hw, vif, &offs, false);\n\tstruct sk_buff *copy;\n\tstruct ieee80211_supported_band *sband;\n\tint shift;\n\n\tif (!bcn)\n\t\treturn bcn;\n\n\tif (tim_offset)\n\t\t*tim_offset = offs.tim_offset;\n\n\tif (tim_length)\n\t\t*tim_length = offs.tim_length;\n\n\tif (ieee80211_hw_check(hw, BEACON_TX_STATUS) ||\n\t    !hw_to_local(hw)->monitors)\n\t\treturn bcn;\n\n\t/* send a copy to monitor interfaces */\n\tcopy = skb_copy(bcn, GFP_ATOMIC);\n\tif (!copy)\n\t\treturn bcn;\n\n\tshift = ieee80211_vif_get_shift(vif);\n\tsband = ieee80211_get_sband(vif_to_sdata(vif));\n\tif (!sband)\n\t\treturn bcn;\n\n\tieee80211_tx_monitor(hw_to_local(hw), copy, sband, 1, shift, false,\n\t\t\t     NULL);\n\n\treturn bcn;\n}\nEXPORT_SYMBOL(ieee80211_beacon_get_tim);\n\nstruct sk_buff *ieee80211_proberesp_get(struct ieee80211_hw *hw,\n\t\t\t\t\tstruct ieee80211_vif *vif)\n{\n\tstruct ieee80211_if_ap *ap = NULL;\n\tstruct sk_buff *skb = NULL;\n\tstruct probe_resp *presp = NULL;\n\tstruct ieee80211_hdr *hdr;\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\n\tap = &sdata->u.ap;\n\tpresp = rcu_dereference(ap->probe_resp);\n\tif (!presp)\n\t\tgoto out;\n\n\tskb = dev_alloc_skb(presp->len);\n\tif (!skb)\n\t\tgoto out;\n\n\tskb_put_data(skb, presp->data, presp->len);\n\n\thdr = (struct ieee80211_hdr *) skb->data;\n\tmemset(hdr->addr1, 0, sizeof(hdr->addr1));\n\nout:\n\trcu_read_unlock();\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_proberesp_get);\n\nstruct sk_buff *ieee80211_get_fils_discovery_tmpl(struct ieee80211_hw *hw,\n\t\t\t\t\t\t  struct ieee80211_vif *vif)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct fils_discovery_data *tmpl = NULL;\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttmpl = rcu_dereference(sdata->u.ap.fils_discovery);\n\tif (!tmpl) {\n\t\trcu_read_unlock();\n\t\treturn NULL;\n\t}\n\n\tskb = dev_alloc_skb(sdata->local->hw.extra_tx_headroom + tmpl->len);\n\tif (skb) {\n\t\tskb_reserve(skb, sdata->local->hw.extra_tx_headroom);\n\t\tskb_put_data(skb, tmpl->data, tmpl->len);\n\t}\n\n\trcu_read_unlock();\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_get_fils_discovery_tmpl);\n\nstruct sk_buff *\nieee80211_get_unsol_bcast_probe_resp_tmpl(struct ieee80211_hw *hw,\n\t\t\t\t\t  struct ieee80211_vif *vif)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct unsol_bcast_probe_resp_data *tmpl = NULL;\n\tstruct ieee80211_sub_if_data *sdata = vif_to_sdata(vif);\n\n\tif (sdata->vif.type != NL80211_IFTYPE_AP)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttmpl = rcu_dereference(sdata->u.ap.unsol_bcast_probe_resp);\n\tif (!tmpl) {\n\t\trcu_read_unlock();\n\t\treturn NULL;\n\t}\n\n\tskb = dev_alloc_skb(sdata->local->hw.extra_tx_headroom + tmpl->len);\n\tif (skb) {\n\t\tskb_reserve(skb, sdata->local->hw.extra_tx_headroom);\n\t\tskb_put_data(skb, tmpl->data, tmpl->len);\n\t}\n\n\trcu_read_unlock();\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_get_unsol_bcast_probe_resp_tmpl);\n\nstruct sk_buff *ieee80211_pspoll_get(struct ieee80211_hw *hw,\n\t\t\t\t     struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_if_managed *ifmgd;\n\tstruct ieee80211_pspoll *pspoll;\n\tstruct ieee80211_local *local;\n\tstruct sk_buff *skb;\n\n\tif (WARN_ON(vif->type != NL80211_IFTYPE_STATION))\n\t\treturn NULL;\n\n\tsdata = vif_to_sdata(vif);\n\tifmgd = &sdata->u.mgd;\n\tlocal = sdata->local;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + sizeof(*pspoll));\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\tpspoll = skb_put_zero(skb, sizeof(*pspoll));\n\tpspoll->frame_control = cpu_to_le16(IEEE80211_FTYPE_CTL |\n\t\t\t\t\t    IEEE80211_STYPE_PSPOLL);\n\tpspoll->aid = cpu_to_le16(sdata->vif.bss_conf.aid);\n\n\t/* aid in PS-Poll has its two MSBs each set to 1 */\n\tpspoll->aid |= cpu_to_le16(1 << 15 | 1 << 14);\n\n\tmemcpy(pspoll->bssid, ifmgd->bssid, ETH_ALEN);\n\tmemcpy(pspoll->ta, vif->addr, ETH_ALEN);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_pspoll_get);\n\nstruct sk_buff *ieee80211_nullfunc_get(struct ieee80211_hw *hw,\n\t\t\t\t       struct ieee80211_vif *vif,\n\t\t\t\t       bool qos_ok)\n{\n\tstruct ieee80211_hdr_3addr *nullfunc;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ieee80211_if_managed *ifmgd;\n\tstruct ieee80211_local *local;\n\tstruct sk_buff *skb;\n\tbool qos = false;\n\n\tif (WARN_ON(vif->type != NL80211_IFTYPE_STATION))\n\t\treturn NULL;\n\n\tsdata = vif_to_sdata(vif);\n\tifmgd = &sdata->u.mgd;\n\tlocal = sdata->local;\n\n\tif (qos_ok) {\n\t\tstruct sta_info *sta;\n\n\t\trcu_read_lock();\n\t\tsta = sta_info_get(sdata, ifmgd->bssid);\n\t\tqos = sta && sta->sta.wme;\n\t\trcu_read_unlock();\n\t}\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom +\n\t\t\t    sizeof(*nullfunc) + 2);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\tnullfunc = skb_put_zero(skb, sizeof(*nullfunc));\n\tnullfunc->frame_control = cpu_to_le16(IEEE80211_FTYPE_DATA |\n\t\t\t\t\t      IEEE80211_STYPE_NULLFUNC |\n\t\t\t\t\t      IEEE80211_FCTL_TODS);\n\tif (qos) {\n\t\t__le16 qoshdr = cpu_to_le16(7);\n\n\t\tBUILD_BUG_ON((IEEE80211_STYPE_QOS_NULLFUNC |\n\t\t\t      IEEE80211_STYPE_NULLFUNC) !=\n\t\t\t     IEEE80211_STYPE_QOS_NULLFUNC);\n\t\tnullfunc->frame_control |=\n\t\t\tcpu_to_le16(IEEE80211_STYPE_QOS_NULLFUNC);\n\t\tskb->priority = 7;\n\t\tskb_set_queue_mapping(skb, IEEE80211_AC_VO);\n\t\tskb_put_data(skb, &qoshdr, sizeof(qoshdr));\n\t}\n\n\tmemcpy(nullfunc->addr1, ifmgd->bssid, ETH_ALEN);\n\tmemcpy(nullfunc->addr2, vif->addr, ETH_ALEN);\n\tmemcpy(nullfunc->addr3, ifmgd->bssid, ETH_ALEN);\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_nullfunc_get);\n\nstruct sk_buff *ieee80211_probereq_get(struct ieee80211_hw *hw,\n\t\t\t\t       const u8 *src_addr,\n\t\t\t\t       const u8 *ssid, size_t ssid_len,\n\t\t\t\t       size_t tailroom)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct ieee80211_hdr_3addr *hdr;\n\tstruct sk_buff *skb;\n\tsize_t ie_ssid_len;\n\tu8 *pos;\n\n\tie_ssid_len = 2 + ssid_len;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + sizeof(*hdr) +\n\t\t\t    ie_ssid_len + tailroom);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\n\thdr = skb_put_zero(skb, sizeof(*hdr));\n\thdr->frame_control = cpu_to_le16(IEEE80211_FTYPE_MGMT |\n\t\t\t\t\t IEEE80211_STYPE_PROBE_REQ);\n\teth_broadcast_addr(hdr->addr1);\n\tmemcpy(hdr->addr2, src_addr, ETH_ALEN);\n\teth_broadcast_addr(hdr->addr3);\n\n\tpos = skb_put(skb, ie_ssid_len);\n\t*pos++ = WLAN_EID_SSID;\n\t*pos++ = ssid_len;\n\tif (ssid_len)\n\t\tmemcpy(pos, ssid, ssid_len);\n\tpos += ssid_len;\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_probereq_get);\n\nvoid ieee80211_rts_get(struct ieee80211_hw *hw, struct ieee80211_vif *vif,\n\t\t       const void *frame, size_t frame_len,\n\t\t       const struct ieee80211_tx_info *frame_txctl,\n\t\t       struct ieee80211_rts *rts)\n{\n\tconst struct ieee80211_hdr *hdr = frame;\n\n\trts->frame_control =\n\t    cpu_to_le16(IEEE80211_FTYPE_CTL | IEEE80211_STYPE_RTS);\n\trts->duration = ieee80211_rts_duration(hw, vif, frame_len,\n\t\t\t\t\t       frame_txctl);\n\tmemcpy(rts->ra, hdr->addr1, sizeof(rts->ra));\n\tmemcpy(rts->ta, hdr->addr2, sizeof(rts->ta));\n}\nEXPORT_SYMBOL(ieee80211_rts_get);\n\nvoid ieee80211_ctstoself_get(struct ieee80211_hw *hw, struct ieee80211_vif *vif,\n\t\t\t     const void *frame, size_t frame_len,\n\t\t\t     const struct ieee80211_tx_info *frame_txctl,\n\t\t\t     struct ieee80211_cts *cts)\n{\n\tconst struct ieee80211_hdr *hdr = frame;\n\n\tcts->frame_control =\n\t    cpu_to_le16(IEEE80211_FTYPE_CTL | IEEE80211_STYPE_CTS);\n\tcts->duration = ieee80211_ctstoself_duration(hw, vif,\n\t\t\t\t\t\t     frame_len, frame_txctl);\n\tmemcpy(cts->ra, hdr->addr1, sizeof(cts->ra));\n}\nEXPORT_SYMBOL(ieee80211_ctstoself_get);\n\nstruct sk_buff *\nieee80211_get_buffered_bc(struct ieee80211_hw *hw,\n\t\t\t  struct ieee80211_vif *vif)\n{\n\tstruct ieee80211_local *local = hw_to_local(hw);\n\tstruct sk_buff *skb = NULL;\n\tstruct ieee80211_tx_data tx;\n\tstruct ieee80211_sub_if_data *sdata;\n\tstruct ps_data *ps;\n\tstruct ieee80211_tx_info *info;\n\tstruct ieee80211_chanctx_conf *chanctx_conf;\n\n\tsdata = vif_to_sdata(vif);\n\n\trcu_read_lock();\n\tchanctx_conf = rcu_dereference(sdata->vif.chanctx_conf);\n\n\tif (!chanctx_conf)\n\t\tgoto out;\n\n\tif (sdata->vif.type == NL80211_IFTYPE_AP) {\n\t\tstruct beacon_data *beacon =\n\t\t\t\trcu_dereference(sdata->u.ap.beacon);\n\n\t\tif (!beacon || !beacon->head)\n\t\t\tgoto out;\n\n\t\tps = &sdata->u.ap.ps;\n\t} else if (ieee80211_vif_is_mesh(&sdata->vif)) {\n\t\tps = &sdata->u.mesh.ps;\n\t} else {\n\t\tgoto out;\n\t}\n\n\tif (ps->dtim_count != 0 || !ps->dtim_bc_mc)\n\t\tgoto out; /* send buffered bc/mc only after DTIM beacon */\n\n\twhile (1) {\n\t\tskb = skb_dequeue(&ps->bc_buf);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tlocal->total_ps_buffered--;\n\n\t\tif (!skb_queue_empty(&ps->bc_buf) && skb->len >= 2) {\n\t\t\tstruct ieee80211_hdr *hdr =\n\t\t\t\t(struct ieee80211_hdr *) skb->data;\n\t\t\t/* more buffered multicast/broadcast frames ==> set\n\t\t\t * MoreData flag in IEEE 802.11 header to inform PS\n\t\t\t * STAs */\n\t\t\thdr->frame_control |=\n\t\t\t\tcpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t\t}\n\n\t\tif (sdata->vif.type == NL80211_IFTYPE_AP)\n\t\t\tsdata = IEEE80211_DEV_TO_SUB_IF(skb->dev);\n\t\tif (!ieee80211_tx_prepare(sdata, &tx, NULL, skb))\n\t\t\tbreak;\n\t\tieee80211_free_txskb(hw, skb);\n\t}\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\n\ttx.flags |= IEEE80211_TX_PS_BUFFERED;\n\tinfo->band = chanctx_conf->def.chan->band;\n\n\tif (invoke_tx_handlers(&tx))\n\t\tskb = NULL;\n out:\n\trcu_read_unlock();\n\n\treturn skb;\n}\nEXPORT_SYMBOL(ieee80211_get_buffered_bc);\n\nint ieee80211_reserve_tid(struct ieee80211_sta *pubsta, u8 tid)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\tstruct ieee80211_local *local = sdata->local;\n\tint ret;\n\tu32 queues;\n\n\tlockdep_assert_held(&local->sta_mtx);\n\n\t/* only some cases are supported right now */\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -EINVAL;\n\t}\n\n\tif (WARN_ON(tid >= IEEE80211_NUM_UPS))\n\t\treturn -EINVAL;\n\n\tif (sta->reserved_tid == tid) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (sta->reserved_tid != IEEE80211_TID_UNRESERVED) {\n\t\tsdata_err(sdata, \"TID reservation already active\\n\");\n\t\tret = -EALREADY;\n\t\tgoto out;\n\t}\n\n\tieee80211_stop_vif_queues(sdata->local, sdata,\n\t\t\t\t  IEEE80211_QUEUE_STOP_REASON_RESERVE_TID);\n\n\tsynchronize_net();\n\n\t/* Tear down BA sessions so we stop aggregating on this TID */\n\tif (ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION)) {\n\t\tset_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\t\t__ieee80211_stop_tx_ba_session(sta, tid,\n\t\t\t\t\t       AGG_STOP_LOCAL_REQUEST);\n\t}\n\n\tqueues = BIT(sdata->vif.hw_queue[ieee802_1d_to_ac[tid]]);\n\t__ieee80211_flush_queues(local, sdata, queues, false);\n\n\tsta->reserved_tid = tid;\n\n\tieee80211_wake_vif_queues(local, sdata,\n\t\t\t\t  IEEE80211_QUEUE_STOP_REASON_RESERVE_TID);\n\n\tif (ieee80211_hw_check(&local->hw, AMPDU_AGGREGATION))\n\t\tclear_sta_flag(sta, WLAN_STA_BLOCK_BA);\n\n\tret = 0;\n out:\n\treturn ret;\n}\nEXPORT_SYMBOL(ieee80211_reserve_tid);\n\nvoid ieee80211_unreserve_tid(struct ieee80211_sta *pubsta, u8 tid)\n{\n\tstruct sta_info *sta = container_of(pubsta, struct sta_info, sta);\n\tstruct ieee80211_sub_if_data *sdata = sta->sdata;\n\n\tlockdep_assert_held(&sdata->local->sta_mtx);\n\n\t/* only some cases are supported right now */\n\tswitch (sdata->vif.type) {\n\tcase NL80211_IFTYPE_STATION:\n\tcase NL80211_IFTYPE_AP:\n\tcase NL80211_IFTYPE_AP_VLAN:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (tid != sta->reserved_tid) {\n\t\tsdata_err(sdata, \"TID to unreserve (%d) isn't reserved\\n\", tid);\n\t\treturn;\n\t}\n\n\tsta->reserved_tid = IEEE80211_TID_UNRESERVED;\n}\nEXPORT_SYMBOL(ieee80211_unreserve_tid);\n\nvoid __ieee80211_tx_skb_tid_band(struct ieee80211_sub_if_data *sdata,\n\t\t\t\t struct sk_buff *skb, int tid,\n\t\t\t\t enum nl80211_band band)\n{\n\tint ac = ieee80211_ac_from_tid(tid);\n\n\tskb_reset_mac_header(skb);\n\tskb_set_queue_mapping(skb, ac);\n\tskb->priority = tid;\n\n\tskb->dev = sdata->dev;\n\n\t/*\n\t * The other path calling ieee80211_xmit is from the tasklet,\n\t * and while we can handle concurrent transmissions locking\n\t * requirements are that we do not come into tx with bhs on.\n\t */\n\tlocal_bh_disable();\n\tIEEE80211_SKB_CB(skb)->band = band;\n\tieee80211_xmit(sdata, NULL, skb);\n\tlocal_bh_enable();\n}\n\nint ieee80211_tx_control_port(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      const u8 *buf, size_t len,\n\t\t\t      const u8 *dest, __be16 proto, bool unencrypted,\n\t\t\t      u64 *cookie)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff *skb;\n\tstruct ethhdr *ehdr;\n\tu32 ctrl_flags = 0;\n\tu32 flags = 0;\n\n\t/* Only accept CONTROL_PORT_PROTOCOL configured in CONNECT/ASSOCIATE\n\t * or Pre-Authentication\n\t */\n\tif (proto != sdata->control_port_protocol &&\n\t    proto != cpu_to_be16(ETH_P_PREAUTH))\n\t\treturn -EINVAL;\n\n\tif (proto == sdata->control_port_protocol)\n\t\tctrl_flags |= IEEE80211_TX_CTRL_PORT_CTRL_PROTO |\n\t\t\t      IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP;\n\n\tif (unencrypted)\n\t\tflags |= IEEE80211_TX_INTFL_DONT_ENCRYPT;\n\n\tif (cookie)\n\t\tctrl_flags |= IEEE80211_TX_CTL_REQ_TX_STATUS;\n\n\tflags |= IEEE80211_TX_INTFL_NL80211_FRAME_TX |\n\t\t IEEE80211_TX_CTL_INJECTED;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom +\n\t\t\t    sizeof(struct ethhdr) + len);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom + sizeof(struct ethhdr));\n\n\tskb_put_data(skb, buf, len);\n\n\tehdr = skb_push(skb, sizeof(struct ethhdr));\n\tmemcpy(ehdr->h_dest, dest, ETH_ALEN);\n\tmemcpy(ehdr->h_source, sdata->vif.addr, ETH_ALEN);\n\tehdr->h_proto = proto;\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_802_3);\n\tskb_reset_network_header(skb);\n\tskb_reset_mac_header(skb);\n\n\t/* mutex lock is only needed for incrementing the cookie counter */\n\tmutex_lock(&local->mtx);\n\n\tlocal_bh_disable();\n\t__ieee80211_subif_start_xmit(skb, skb->dev, flags, ctrl_flags, cookie);\n\tlocal_bh_enable();\n\n\tmutex_unlock(&local->mtx);\n\n\treturn 0;\n}\n\nint ieee80211_probe_mesh_link(struct wiphy *wiphy, struct net_device *dev,\n\t\t\t      const u8 *buf, size_t len)\n{\n\tstruct ieee80211_sub_if_data *sdata = IEEE80211_DEV_TO_SUB_IF(dev);\n\tstruct ieee80211_local *local = sdata->local;\n\tstruct sk_buff *skb;\n\n\tskb = dev_alloc_skb(local->hw.extra_tx_headroom + len +\n\t\t\t    30 + /* header size */\n\t\t\t    18); /* 11s header size */\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb_reserve(skb, local->hw.extra_tx_headroom);\n\tskb_put_data(skb, buf, len);\n\n\tskb->dev = dev;\n\tskb->protocol = htons(ETH_P_802_3);\n\tskb_reset_network_header(skb);\n\tskb_reset_mac_header(skb);\n\n\tlocal_bh_disable();\n\t__ieee80211_subif_start_xmit(skb, skb->dev, 0,\n\t\t\t\t     IEEE80211_TX_CTRL_SKIP_MPATH_LOOKUP,\n\t\t\t\t     NULL);\n\tlocal_bh_enable();\n\n\treturn 0;\n}\n"}, "1": {"id": 1, "path": "/src/include/linux/skbuff.h", "content": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\tDefinitions for the 'struct sk_buff' memory handlers.\n *\n *\tAuthors:\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tFlorian La Roche, <rzsfl@rz.uni-sb.de>\n */\n\n#ifndef _LINUX_SKBUFF_H\n#define _LINUX_SKBUFF_H\n\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/time.h>\n#include <linux/bug.h>\n#include <linux/bvec.h>\n#include <linux/cache.h>\n#include <linux/rbtree.h>\n#include <linux/socket.h>\n#include <linux/refcount.h>\n\n#include <linux/atomic.h>\n#include <asm/types.h>\n#include <linux/spinlock.h>\n#include <linux/net.h>\n#include <linux/textsearch.h>\n#include <net/checksum.h>\n#include <linux/rcupdate.h>\n#include <linux/hrtimer.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdev_features.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <net/flow_dissector.h>\n#include <linux/splice.h>\n#include <linux/in6.h>\n#include <linux/if_packet.h>\n#include <net/flow.h>\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <linux/netfilter/nf_conntrack_common.h>\n#endif\n\n/* The interface for checksum offload between the stack and networking drivers\n * is as follows...\n *\n * A. IP checksum related features\n *\n * Drivers advertise checksum offload capabilities in the features of a device.\n * From the stack's point of view these are capabilities offered by the driver.\n * A driver typically only advertises features that it is capable of offloading\n * to its device.\n *\n * The checksum related features are:\n *\n *\tNETIF_F_HW_CSUM\t- The driver (or its device) is able to compute one\n *\t\t\t  IP (one's complement) checksum for any combination\n *\t\t\t  of protocols or protocol layering. The checksum is\n *\t\t\t  computed and set in a packet per the CHECKSUM_PARTIAL\n *\t\t\t  interface (see below).\n *\n *\tNETIF_F_IP_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv4. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv4|TCP or\n *\t\t\t  IPv4|UDP where the Protocol field in the IPv4 header\n *\t\t\t  is TCP or UDP. The IPv4 header may contain IP options.\n *\t\t\t  This feature cannot be set in features for a device\n *\t\t\t  with NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_IPV6_CSUM - Driver (device) is only able to checksum plain\n *\t\t\t  TCP or UDP packets over IPv6. These are specifically\n *\t\t\t  unencapsulated packets of the form IPv6|TCP or\n *\t\t\t  IPv6|UDP where the Next Header field in the IPv6\n *\t\t\t  header is either TCP or UDP. IPv6 extension headers\n *\t\t\t  are not supported with this feature. This feature\n *\t\t\t  cannot be set in features for a device with\n *\t\t\t  NETIF_F_HW_CSUM also set. This feature is being\n *\t\t\t  DEPRECATED (see below).\n *\n *\tNETIF_F_RXCSUM - Driver (device) performs receive checksum offload.\n *\t\t\t This flag is only used to disable the RX checksum\n *\t\t\t feature for a device. The stack will accept receive\n *\t\t\t checksum indication in packets received on a device\n *\t\t\t regardless of whether NETIF_F_RXCSUM is set.\n *\n * B. Checksumming of received packets by device. Indication of checksum\n *    verification is set in skb->ip_summed. Possible values are:\n *\n * CHECKSUM_NONE:\n *\n *   Device did not checksum this packet e.g. due to lack of capabilities.\n *   The packet contains full (though not verified) checksum in packet but\n *   not in skb->csum. Thus, skb->csum is undefined in this case.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   The hardware you're dealing with doesn't calculate the full checksum\n *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums\n *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY\n *   if their checksums are okay. skb->csum is still undefined in this case\n *   though. A driver or device must never modify the checksum field in the\n *   packet even if checksum is verified.\n *\n *   CHECKSUM_UNNECESSARY is applicable to following protocols:\n *     TCP: IPv6 and IPv4.\n *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a\n *       zero UDP checksum for either IPv4 or IPv6, the networking stack\n *       may perform further validation in this case.\n *     GRE: only if the checksum is present in the header.\n *     SCTP: indicates the CRC in SCTP header has been validated.\n *     FCOE: indicates the CRC in FC frame has been validated.\n *\n *   skb->csum_level indicates the number of consecutive checksums found in\n *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.\n *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet\n *   and a device is able to verify the checksums for UDP (possibly zero),\n *   GRE (checksum flag is set) and TCP, skb->csum_level would be set to\n *   two. If the device were only able to verify the UDP checksum and not\n *   GRE, either because it doesn't support GRE checksum or because GRE\n *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is\n *   not considered in this case).\n *\n * CHECKSUM_COMPLETE:\n *\n *   This is the most generic way. The device supplied checksum of the _whole_\n *   packet as seen by netif_rx() and fills in skb->csum. This means the\n *   hardware doesn't need to parse L3/L4 headers to implement this.\n *\n *   Notes:\n *   - Even if device supports only some protocols, but is able to produce\n *     skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.\n *   - CHECKSUM_COMPLETE is not applicable to SCTP and FCoE protocols.\n *\n * CHECKSUM_PARTIAL:\n *\n *   A checksum is set up to be offloaded to a device as described in the\n *   output description for CHECKSUM_PARTIAL. This may occur on a packet\n *   received directly from another Linux OS, e.g., a virtualized Linux kernel\n *   on the same host, or it may be set in the input path in GRO or remote\n *   checksum offload. For the purposes of checksum verification, the checksum\n *   referred to by skb->csum_start + skb->csum_offset and any preceding\n *   checksums in the packet are considered verified. Any checksums in the\n *   packet that are after the checksum being offloaded are not considered to\n *   be verified.\n *\n * C. Checksumming on transmit for non-GSO. The stack requests checksum offload\n *    in the skb->ip_summed for a packet. Values are:\n *\n * CHECKSUM_PARTIAL:\n *\n *   The driver is required to checksum the packet as seen by hard_start_xmit()\n *   from skb->csum_start up to the end, and to record/write the checksum at\n *   offset skb->csum_start + skb->csum_offset. A driver may verify that the\n *   csum_start and csum_offset values are valid values given the length and\n *   offset of the packet, but it should not attempt to validate that the\n *   checksum refers to a legitimate transport layer checksum -- it is the\n *   purview of the stack to validate that csum_start and csum_offset are set\n *   correctly.\n *\n *   When the stack requests checksum offload for a packet, the driver MUST\n *   ensure that the checksum is set correctly. A driver can either offload the\n *   checksum calculation to the device, or call skb_checksum_help (in the case\n *   that the device does not support offload for a particular checksum).\n *\n *   NETIF_F_IP_CSUM and NETIF_F_IPV6_CSUM are being deprecated in favor of\n *   NETIF_F_HW_CSUM. New devices should use NETIF_F_HW_CSUM to indicate\n *   checksum offload capability.\n *   skb_csum_hwoffload_help() can be called to resolve CHECKSUM_PARTIAL based\n *   on network device checksumming capabilities: if a packet does not match\n *   them, skb_checksum_help or skb_crc32c_help (depending on the value of\n *   csum_not_inet, see item D.) is called to resolve the checksum.\n *\n * CHECKSUM_NONE:\n *\n *   The skb was already checksummed by the protocol, or a checksum is not\n *   required.\n *\n * CHECKSUM_UNNECESSARY:\n *\n *   This has the same meaning as CHECKSUM_NONE for checksum offload on\n *   output.\n *\n * CHECKSUM_COMPLETE:\n *   Not used in checksum output. If a driver observes a packet with this value\n *   set in skbuff, it should treat the packet as if CHECKSUM_NONE were set.\n *\n * D. Non-IP checksum (CRC) offloads\n *\n *   NETIF_F_SCTP_CRC - This feature indicates that a device is capable of\n *     offloading the SCTP CRC in a packet. To perform this offload the stack\n *     will set csum_start and csum_offset accordingly, set ip_summed to\n *     CHECKSUM_PARTIAL and set csum_not_inet to 1, to provide an indication in\n *     the skbuff that the CHECKSUM_PARTIAL refers to CRC32c.\n *     A driver that supports both IP checksum offload and SCTP CRC32c offload\n *     must verify which offload is configured for a packet by testing the\n *     value of skb->csum_not_inet; skb_crc32c_csum_help is provided to resolve\n *     CHECKSUM_PARTIAL on skbs where csum_not_inet is set to 1.\n *\n *   NETIF_F_FCOE_CRC - This feature indicates that a device is capable of\n *     offloading the FCOE CRC in a packet. To perform this offload the stack\n *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset\n *     accordingly. Note that there is no indication in the skbuff that the\n *     CHECKSUM_PARTIAL refers to an FCOE checksum, so a driver that supports\n *     both IP checksum offload and FCOE CRC offload must verify which offload\n *     is configured for a packet, presumably by inspecting packet headers.\n *\n * E. Checksumming on output with GSO.\n *\n * In the case of a GSO packet (skb_is_gso(skb) is true), checksum offload\n * is implied by the SKB_GSO_* flags in gso_type. Most obviously, if the\n * gso_type is SKB_GSO_TCPV4 or SKB_GSO_TCPV6, TCP checksum offload as\n * part of the GSO operation is implied. If a checksum is being offloaded\n * with GSO then ip_summed is CHECKSUM_PARTIAL, and both csum_start and\n * csum_offset are set to refer to the outermost checksum being offloaded\n * (two offloaded checksums are possible with UDP encapsulation).\n */\n\n/* Don't change this without changing skb_csum_unnecessary! */\n#define CHECKSUM_NONE\t\t0\n#define CHECKSUM_UNNECESSARY\t1\n#define CHECKSUM_COMPLETE\t2\n#define CHECKSUM_PARTIAL\t3\n\n/* Maximum value in skb->csum_level */\n#define SKB_MAX_CSUM_LEVEL\t3\n\n#define SKB_DATA_ALIGN(X)\tALIGN(X, SMP_CACHE_BYTES)\n#define SKB_WITH_OVERHEAD(X)\t\\\n\t((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n#define SKB_MAX_ORDER(X, ORDER) \\\n\tSKB_WITH_OVERHEAD((PAGE_SIZE << (ORDER)) - (X))\n#define SKB_MAX_HEAD(X)\t\t(SKB_MAX_ORDER((X), 0))\n#define SKB_MAX_ALLOC\t\t(SKB_MAX_ORDER(0, 2))\n\n/* return minimum truesize of one skb containing X bytes of data */\n#define SKB_TRUESIZE(X) ((X) +\t\t\t\t\t\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct sk_buff)) +\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\nstruct ahash_request;\nstruct net_device;\nstruct scatterlist;\nstruct pipe_inode_info;\nstruct iov_iter;\nstruct napi_struct;\nstruct bpf_prog;\nunion bpf_attr;\nstruct skb_ext;\n\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\nstruct nf_bridge_info {\n\tenum {\n\t\tBRNF_PROTO_UNCHANGED,\n\t\tBRNF_PROTO_8021Q,\n\t\tBRNF_PROTO_PPPOE\n\t} orig_proto:8;\n\tu8\t\t\tpkt_otherhost:1;\n\tu8\t\t\tin_prerouting:1;\n\tu8\t\t\tbridged_dnat:1;\n\t__u16\t\t\tfrag_max_size;\n\tstruct net_device\t*physindev;\n\n\t/* always valid & non-NULL from FORWARD on, for physdev match */\n\tstruct net_device\t*physoutdev;\n\tunion {\n\t\t/* prerouting: detect dnat in orig/reply direction */\n\t\t__be32          ipv4_daddr;\n\t\tstruct in6_addr ipv6_daddr;\n\n\t\t/* after prerouting + nat detected: store original source\n\t\t * mac since neigh resolution overwrites it, only used while\n\t\t * skb is out in neigh layer.\n\t\t */\n\t\tchar neigh_header[8];\n\t};\n};\n#endif\n\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n/* Chain in tc_skb_ext will be used to share the tc chain with\n * ovs recirc_id. It will be set to the current chain by tc\n * and read by ovs to recirc_id.\n */\nstruct tc_skb_ext {\n\t__u32 chain;\n\t__u16 mru;\n};\n#endif\n\nstruct sk_buff_head {\n\t/* These two members must be first. */\n\tstruct sk_buff\t*next;\n\tstruct sk_buff\t*prev;\n\n\t__u32\t\tqlen;\n\tspinlock_t\tlock;\n};\n\nstruct sk_buff;\n\n/* To allow 64K frame to be packed as single skb without frag_list we\n * require 64K/PAGE_SIZE pages plus 1 additional page to allow for\n * buffers which do not start on a page boundary.\n *\n * Since GRO uses frags we allocate at least 16 regardless of page\n * size.\n */\n#if (65536/PAGE_SIZE + 1) < 16\n#define MAX_SKB_FRAGS 16UL\n#else\n#define MAX_SKB_FRAGS (65536/PAGE_SIZE + 1)\n#endif\nextern int sysctl_max_skb_frags;\n\n/* Set skb_shinfo(skb)->gso_size to this in case you want skb_segment to\n * segment using its current segmentation instead.\n */\n#define GSO_BY_FRAGS\t0xFFFF\n\ntypedef struct bio_vec skb_frag_t;\n\n/**\n * skb_frag_size() - Returns the size of a skb fragment\n * @frag: skb fragment\n */\nstatic inline unsigned int skb_frag_size(const skb_frag_t *frag)\n{\n\treturn frag->bv_len;\n}\n\n/**\n * skb_frag_size_set() - Sets the size of a skb fragment\n * @frag: skb fragment\n * @size: size of fragment\n */\nstatic inline void skb_frag_size_set(skb_frag_t *frag, unsigned int size)\n{\n\tfrag->bv_len = size;\n}\n\n/**\n * skb_frag_size_add() - Increments the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_size_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len += delta;\n}\n\n/**\n * skb_frag_size_sub() - Decrements the size of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to subtract\n */\nstatic inline void skb_frag_size_sub(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len -= delta;\n}\n\n/**\n * skb_frag_must_loop - Test if %p is a high memory page\n * @p: fragment's page\n */\nstatic inline bool skb_frag_must_loop(struct page *p)\n{\n#if defined(CONFIG_HIGHMEM)\n\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) || PageHighMem(p))\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n/**\n *\tskb_frag_foreach_page - loop over pages in a fragment\n *\n *\t@f:\t\tskb frag to operate on\n *\t@f_off:\t\toffset from start of f->bv_page\n *\t@f_len:\t\tlength from f_off to loop over\n *\t@p:\t\t(temp var) current page\n *\t@p_off:\t\t(temp var) offset from start of current page,\n *\t                           non-zero only on first page.\n *\t@p_len:\t\t(temp var) length in current page,\n *\t\t\t\t   < PAGE_SIZE only on first and last page.\n *\t@copied:\t(temp var) length so far, excluding current p_len.\n *\n *\tA fragment can hold a compound page, in which case per-page\n *\toperations, notably kmap_atomic, must be called for each\n *\tregular page.\n */\n#define skb_frag_foreach_page(f, f_off, f_len, p, p_off, p_len, copied)\t\\\n\tfor (p = skb_frag_page(f) + ((f_off) >> PAGE_SHIFT),\t\t\\\n\t     p_off = (f_off) & (PAGE_SIZE - 1),\t\t\t\t\\\n\t     p_len = skb_frag_must_loop(p) ?\t\t\t\t\\\n\t     min_t(u32, f_len, PAGE_SIZE - p_off) : f_len,\t\t\\\n\t     copied = 0;\t\t\t\t\t\t\\\n\t     copied < f_len;\t\t\t\t\t\t\\\n\t     copied += p_len, p++, p_off = 0,\t\t\t\t\\\n\t     p_len = min_t(u32, f_len - copied, PAGE_SIZE))\t\t\\\n\n#define HAVE_HW_TIME_STAMP\n\n/**\n * struct skb_shared_hwtstamps - hardware time stamps\n * @hwtstamp:\thardware time stamp transformed into duration\n *\t\tsince arbitrary point in time\n *\n * Software time stamps generated by ktime_get_real() are stored in\n * skb->tstamp.\n *\n * hwtstamps can only be compared against other hwtstamps from\n * the same device.\n *\n * This structure is attached to packets as part of the\n * &skb_shared_info. Use skb_hwtstamps() to get a pointer.\n */\nstruct skb_shared_hwtstamps {\n\tktime_t\thwtstamp;\n};\n\n/* Definitions for tx_flags in struct skb_shared_info */\nenum {\n\t/* generate hardware time stamp */\n\tSKBTX_HW_TSTAMP = 1 << 0,\n\n\t/* generate software time stamp when queueing packet to NIC */\n\tSKBTX_SW_TSTAMP = 1 << 1,\n\n\t/* device driver is going to provide hardware time stamp */\n\tSKBTX_IN_PROGRESS = 1 << 2,\n\n\t/* generate wifi status information (where possible) */\n\tSKBTX_WIFI_STATUS = 1 << 4,\n\n\t/* generate software time stamp when entering packet scheduling */\n\tSKBTX_SCHED_TSTAMP = 1 << 6,\n};\n\n#define SKBTX_ANY_SW_TSTAMP\t(SKBTX_SW_TSTAMP    | \\\n\t\t\t\t SKBTX_SCHED_TSTAMP)\n#define SKBTX_ANY_TSTAMP\t(SKBTX_HW_TSTAMP | SKBTX_ANY_SW_TSTAMP)\n\n/* Definitions for flags in struct skb_shared_info */\nenum {\n\t/* use zcopy routines */\n\tSKBFL_ZEROCOPY_ENABLE = BIT(0),\n\n\t/* This indicates at least one fragment might be overwritten\n\t * (as in vmsplice(), sendfile() ...)\n\t * If we need to compute a TX checksum, we'll need to copy\n\t * all frags to avoid possible bad checksum\n\t */\n\tSKBFL_SHARED_FRAG = BIT(1),\n};\n\n#define SKBFL_ZEROCOPY_FRAG\t(SKBFL_ZEROCOPY_ENABLE | SKBFL_SHARED_FRAG)\n\n/*\n * The callback notifies userspace to release buffers when skb DMA is done in\n * lower device, the skb last reference should be 0 when calling this.\n * The zerocopy_success argument is true if zero copy transmit occurred,\n * false on data copy or out of memory error caused by data copy attempt.\n * The ctx field is used to track device context.\n * The desc field is used to track userspace buffer index.\n */\nstruct ubuf_info {\n\tvoid (*callback)(struct sk_buff *, struct ubuf_info *,\n\t\t\t bool zerocopy_success);\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long desc;\n\t\t\tvoid *ctx;\n\t\t};\n\t\tstruct {\n\t\t\tu32 id;\n\t\t\tu16 len;\n\t\t\tu16 zerocopy:1;\n\t\t\tu32 bytelen;\n\t\t};\n\t};\n\trefcount_t refcnt;\n\tu8 flags;\n\n\tstruct mmpin {\n\t\tstruct user_struct *user;\n\t\tunsigned int num_pg;\n\t} mmp;\n};\n\n#define skb_uarg(SKB)\t((struct ubuf_info *)(skb_shinfo(SKB)->destructor_arg))\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size);\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp);\n\nstruct ubuf_info *msg_zerocopy_alloc(struct sock *sk, size_t size);\nstruct ubuf_info *msg_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t       struct ubuf_info *uarg);\n\nvoid msg_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref);\n\nvoid msg_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t   bool success);\n\nint skb_zerocopy_iter_dgram(struct sk_buff *skb, struct msghdr *msg, int len);\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg);\n\n/* This data is invariant across clones and lives at\n * the end of the header data, ie. at skb->end.\n */\nstruct skb_shared_info {\n\t__u8\t\tflags;\n\t__u8\t\tmeta_len;\n\t__u8\t\tnr_frags;\n\t__u8\t\ttx_flags;\n\tunsigned short\tgso_size;\n\t/* Warning: this field is not always filled in (UFO)! */\n\tunsigned short\tgso_segs;\n\tstruct sk_buff\t*frag_list;\n\tstruct skb_shared_hwtstamps hwtstamps;\n\tunsigned int\tgso_type;\n\tu32\t\ttskey;\n\n\t/*\n\t * Warning : all fields before dataref are cleared in __alloc_skb()\n\t */\n\tatomic_t\tdataref;\n\n\t/* Intermediate layers must ensure that destructor_arg\n\t * remains valid until skb destructor */\n\tvoid *\t\tdestructor_arg;\n\n\t/* must be last field, see pskb_expand_head() */\n\tskb_frag_t\tfrags[MAX_SKB_FRAGS];\n};\n\n/* We divide dataref into two halves.  The higher 16 bits hold references\n * to the payload part of skb->data.  The lower 16 bits hold references to\n * the entire skb->data.  A clone of a headerless skb holds the length of\n * the header in skb->hdr_len.\n *\n * All users must obey the rule that the skb->data reference count must be\n * greater than or equal to the payload reference count.\n *\n * Holding a reference to the payload part means that the user does not\n * care about modifications to the header part of skb->data.\n */\n#define SKB_DATAREF_SHIFT 16\n#define SKB_DATAREF_MASK ((1 << SKB_DATAREF_SHIFT) - 1)\n\n\nenum {\n\tSKB_FCLONE_UNAVAILABLE,\t/* skb has no fclone (from head_cache) */\n\tSKB_FCLONE_ORIG,\t/* orig skb (from fclone_cache) */\n\tSKB_FCLONE_CLONE,\t/* companion fclone skb (from fclone_cache) */\n};\n\nenum {\n\tSKB_GSO_TCPV4 = 1 << 0,\n\n\t/* This indicates the skb is from an untrusted source. */\n\tSKB_GSO_DODGY = 1 << 1,\n\n\t/* This indicates the tcp segment has CWR set. */\n\tSKB_GSO_TCP_ECN = 1 << 2,\n\n\tSKB_GSO_TCP_FIXEDID = 1 << 3,\n\n\tSKB_GSO_TCPV6 = 1 << 4,\n\n\tSKB_GSO_FCOE = 1 << 5,\n\n\tSKB_GSO_GRE = 1 << 6,\n\n\tSKB_GSO_GRE_CSUM = 1 << 7,\n\n\tSKB_GSO_IPXIP4 = 1 << 8,\n\n\tSKB_GSO_IPXIP6 = 1 << 9,\n\n\tSKB_GSO_UDP_TUNNEL = 1 << 10,\n\n\tSKB_GSO_UDP_TUNNEL_CSUM = 1 << 11,\n\n\tSKB_GSO_PARTIAL = 1 << 12,\n\n\tSKB_GSO_TUNNEL_REMCSUM = 1 << 13,\n\n\tSKB_GSO_SCTP = 1 << 14,\n\n\tSKB_GSO_ESP = 1 << 15,\n\n\tSKB_GSO_UDP = 1 << 16,\n\n\tSKB_GSO_UDP_L4 = 1 << 17,\n\n\tSKB_GSO_FRAGLIST = 1 << 18,\n};\n\n#if BITS_PER_LONG > 32\n#define NET_SKBUFF_DATA_USES_OFFSET 1\n#endif\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\ntypedef unsigned int sk_buff_data_t;\n#else\ntypedef unsigned char *sk_buff_data_t;\n#endif\n\n/**\n *\tstruct sk_buff - socket buffer\n *\t@next: Next buffer in list\n *\t@prev: Previous buffer in list\n *\t@tstamp: Time we arrived/left\n *\t@skb_mstamp_ns: (aka @tstamp) earliest departure time; start point\n *\t\tfor retransmit timer\n *\t@rbnode: RB tree node, alternative to next/prev for netem/tcp\n *\t@list: queue head\n *\t@sk: Socket we are owned by\n *\t@ip_defrag_offset: (aka @sk) alternate use of @sk, used in\n *\t\tfragmentation management\n *\t@dev: Device we arrived on/are leaving by\n *\t@dev_scratch: (aka @dev) alternate use of @dev when @dev would be %NULL\n *\t@cb: Control buffer. Free for use by every layer. Put private vars here\n *\t@_skb_refdst: destination entry (with norefcount bit)\n *\t@sp: the security path, used for xfrm\n *\t@len: Length of actual data\n *\t@data_len: Data length\n *\t@mac_len: Length of link layer header\n *\t@hdr_len: writable header length of cloned skb\n *\t@csum: Checksum (must include start/offset pair)\n *\t@csum_start: Offset from skb->head where checksumming should start\n *\t@csum_offset: Offset from csum_start where checksum should be stored\n *\t@priority: Packet queueing priority\n *\t@ignore_df: allow local fragmentation\n *\t@cloned: Head may be cloned (check refcnt to be sure)\n *\t@ip_summed: Driver fed us an IP checksum\n *\t@nohdr: Payload reference only, must not modify header\n *\t@pkt_type: Packet class\n *\t@fclone: skbuff clone status\n *\t@ipvs_property: skbuff is owned by ipvs\n *\t@inner_protocol_type: whether the inner protocol is\n *\t\tENCAP_TYPE_ETHER or ENCAP_TYPE_IPPROTO\n *\t@remcsum_offload: remote checksum offload is enabled\n *\t@offload_fwd_mark: Packet was L2-forwarded in hardware\n *\t@offload_l3_fwd_mark: Packet was L3-forwarded in hardware\n *\t@tc_skip_classify: do not classify packet. set by IFB device\n *\t@tc_at_ingress: used within tc_classify to distinguish in/egress\n *\t@redirected: packet was redirected by packet classifier\n *\t@from_ingress: packet was redirected from the ingress path\n *\t@peeked: this packet has been seen already, so stats have been\n *\t\tdone for it, don't do them again\n *\t@nf_trace: netfilter packet trace flag\n *\t@protocol: Packet protocol from driver\n *\t@destructor: Destruct function\n *\t@tcp_tsorted_anchor: list structure for TCP (tp->tsorted_sent_queue)\n *\t@_nfct: Associated connection, if any (with nfctinfo bits)\n *\t@nf_bridge: Saved data about a bridged frame - see br_netfilter.c\n *\t@skb_iif: ifindex of device we arrived on\n *\t@tc_index: Traffic control index\n *\t@hash: the packet hash\n *\t@queue_mapping: Queue mapping for multiqueue devices\n *\t@head_frag: skb was allocated from page fragments,\n *\t\tnot allocated by kmalloc() or vmalloc().\n *\t@pfmemalloc: skbuff was allocated from PFMEMALLOC reserves\n *\t@active_extensions: active extensions (skb_ext_id types)\n *\t@ndisc_nodetype: router type (from link layer)\n *\t@ooo_okay: allow the mapping of a socket to a queue to be changed\n *\t@l4_hash: indicate hash is a canonical 4-tuple hash over transport\n *\t\tports.\n *\t@sw_hash: indicates hash was computed in software stack\n *\t@wifi_acked_valid: wifi_acked was set\n *\t@wifi_acked: whether frame was acked on wifi or not\n *\t@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS\n *\t@encapsulation: indicates the inner headers in the skbuff are valid\n *\t@encap_hdr_csum: software checksum is needed\n *\t@csum_valid: checksum is already valid\n *\t@csum_not_inet: use CRC32c to resolve CHECKSUM_PARTIAL\n *\t@csum_complete_sw: checksum was completed by software\n *\t@csum_level: indicates the number of consecutive checksums found in\n *\t\tthe packet minus one that have been verified as\n *\t\tCHECKSUM_UNNECESSARY (max 3)\n *\t@dst_pending_confirm: need to confirm neighbour\n *\t@decrypted: Decrypted SKB\n *\t@napi_id: id of the NAPI struct this skb came from\n *\t@sender_cpu: (aka @napi_id) source CPU in XPS\n *\t@secmark: security marking\n *\t@mark: Generic packet mark\n *\t@reserved_tailroom: (aka @mark) number of bytes of free space available\n *\t\tat the tail of an sk_buff\n *\t@vlan_present: VLAN tag is present\n *\t@vlan_proto: vlan encapsulation protocol\n *\t@vlan_tci: vlan tag control information\n *\t@inner_protocol: Protocol (encapsulation)\n *\t@inner_ipproto: (aka @inner_protocol) stores ipproto when\n *\t\tskb->inner_protocol_type == ENCAP_TYPE_IPPROTO;\n *\t@inner_transport_header: Inner transport layer header (encapsulation)\n *\t@inner_network_header: Network layer header (encapsulation)\n *\t@inner_mac_header: Link layer header (encapsulation)\n *\t@transport_header: Transport layer header\n *\t@network_header: Network layer header\n *\t@mac_header: Link layer header\n *\t@kcov_handle: KCOV remote handle for remote coverage collection\n *\t@tail: Tail pointer\n *\t@end: End pointer\n *\t@head: Head of buffer\n *\t@data: Data head pointer\n *\t@truesize: Buffer size\n *\t@users: User count - see {datagram,tcp}.c\n *\t@extensions: allocated extensions, valid if active_extensions is nonzero\n */\n\nstruct sk_buff {\n\tunion {\n\t\tstruct {\n\t\t\t/* These two members must be first. */\n\t\t\tstruct sk_buff\t\t*next;\n\t\t\tstruct sk_buff\t\t*prev;\n\n\t\t\tunion {\n\t\t\t\tstruct net_device\t*dev;\n\t\t\t\t/* Some protocols might use this space to store information,\n\t\t\t\t * while device pointer would be NULL.\n\t\t\t\t * UDP receive path is one user.\n\t\t\t\t */\n\t\t\t\tunsigned long\t\tdev_scratch;\n\t\t\t};\n\t\t};\n\t\tstruct rb_node\t\trbnode; /* used in netem, ip4 defrag, and tcp stack */\n\t\tstruct list_head\tlist;\n\t};\n\n\tunion {\n\t\tstruct sock\t\t*sk;\n\t\tint\t\t\tip_defrag_offset;\n\t};\n\n\tunion {\n\t\tktime_t\t\ttstamp;\n\t\tu64\t\tskb_mstamp_ns; /* earliest departure time */\n\t};\n\t/*\n\t * This is the control buffer. It is free to use for every\n\t * layer. Please put your private variables there. If you\n\t * want to keep them across layers you have to do a skb_clone()\n\t * first. This is owned by whoever has the skb queued ATM.\n\t */\n\tchar\t\t\tcb[48] __aligned(8);\n\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long\t_skb_refdst;\n\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb);\n\t\t};\n\t\tstruct list_head\ttcp_tsorted_anchor;\n\t};\n\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tunsigned long\t\t _nfct;\n#endif\n\tunsigned int\t\tlen,\n\t\t\t\tdata_len;\n\t__u16\t\t\tmac_len,\n\t\t\t\thdr_len;\n\n\t/* Following fields are _not_ copied in __copy_skb_header()\n\t * Note that queue_mapping is here mostly to fill a hole.\n\t */\n\t__u16\t\t\tqueue_mapping;\n\n/* if you move cloned around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK\t(1 << 7)\n#else\n#define CLONED_MASK\t1\n#endif\n#define CLONED_OFFSET()\t\toffsetof(struct sk_buff, __cloned_offset)\n\n\t/* private: */\n\t__u8\t\t\t__cloned_offset[0];\n\t/* public: */\n\t__u8\t\t\tcloned:1,\n\t\t\t\tnohdr:1,\n\t\t\t\tfclone:2,\n\t\t\t\tpeeked:1,\n\t\t\t\thead_frag:1,\n\t\t\t\tpfmemalloc:1;\n#ifdef CONFIG_SKB_EXTENSIONS\n\t__u8\t\t\tactive_extensions;\n#endif\n\t/* fields enclosed in headers_start/headers_end are copied\n\t * using a single memcpy() in __copy_skb_header()\n\t */\n\t/* private: */\n\t__u32\t\t\theaders_start[0];\n\t/* public: */\n\n/* if you move pkt_type around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_TYPE_MAX\t(7 << 5)\n#else\n#define PKT_TYPE_MAX\t7\n#endif\n#define PKT_TYPE_OFFSET()\toffsetof(struct sk_buff, __pkt_type_offset)\n\n\t/* private: */\n\t__u8\t\t\t__pkt_type_offset[0];\n\t/* public: */\n\t__u8\t\t\tpkt_type:3;\n\t__u8\t\t\tignore_df:1;\n\t__u8\t\t\tnf_trace:1;\n\t__u8\t\t\tip_summed:2;\n\t__u8\t\t\tooo_okay:1;\n\n\t__u8\t\t\tl4_hash:1;\n\t__u8\t\t\tsw_hash:1;\n\t__u8\t\t\twifi_acked_valid:1;\n\t__u8\t\t\twifi_acked:1;\n\t__u8\t\t\tno_fcs:1;\n\t/* Indicates the inner headers are valid in the skbuff. */\n\t__u8\t\t\tencapsulation:1;\n\t__u8\t\t\tencap_hdr_csum:1;\n\t__u8\t\t\tcsum_valid:1;\n\n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_VLAN_PRESENT_BIT\t7\n#else\n#define PKT_VLAN_PRESENT_BIT\t0\n#endif\n#define PKT_VLAN_PRESENT_OFFSET()\toffsetof(struct sk_buff, __pkt_vlan_present_offset)\n\t/* private: */\n\t__u8\t\t\t__pkt_vlan_present_offset[0];\n\t/* public: */\n\t__u8\t\t\tvlan_present:1;\n\t__u8\t\t\tcsum_complete_sw:1;\n\t__u8\t\t\tcsum_level:2;\n\t__u8\t\t\tcsum_not_inet:1;\n\t__u8\t\t\tdst_pending_confirm:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n\t__u8\t\t\tndisc_nodetype:2;\n#endif\n\n\t__u8\t\t\tipvs_property:1;\n\t__u8\t\t\tinner_protocol_type:1;\n\t__u8\t\t\tremcsum_offload:1;\n#ifdef CONFIG_NET_SWITCHDEV\n\t__u8\t\t\toffload_fwd_mark:1;\n\t__u8\t\t\toffload_l3_fwd_mark:1;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\t__u8\t\t\ttc_skip_classify:1;\n\t__u8\t\t\ttc_at_ingress:1;\n#endif\n#ifdef CONFIG_NET_REDIRECT\n\t__u8\t\t\tredirected:1;\n\t__u8\t\t\tfrom_ingress:1;\n#endif\n#ifdef CONFIG_TLS_DEVICE\n\t__u8\t\t\tdecrypted:1;\n#endif\n\n#ifdef CONFIG_NET_SCHED\n\t__u16\t\t\ttc_index;\t/* traffic control index */\n#endif\n\n\tunion {\n\t\t__wsum\t\tcsum;\n\t\tstruct {\n\t\t\t__u16\tcsum_start;\n\t\t\t__u16\tcsum_offset;\n\t\t};\n\t};\n\t__u32\t\t\tpriority;\n\tint\t\t\tskb_iif;\n\t__u32\t\t\thash;\n\t__be16\t\t\tvlan_proto;\n\t__u16\t\t\tvlan_tci;\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\n\tunion {\n\t\tunsigned int\tnapi_id;\n\t\tunsigned int\tsender_cpu;\n\t};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n\t__u32\t\tsecmark;\n#endif\n\n\tunion {\n\t\t__u32\t\tmark;\n\t\t__u32\t\treserved_tailroom;\n\t};\n\n\tunion {\n\t\t__be16\t\tinner_protocol;\n\t\t__u8\t\tinner_ipproto;\n\t};\n\n\t__u16\t\t\tinner_transport_header;\n\t__u16\t\t\tinner_network_header;\n\t__u16\t\t\tinner_mac_header;\n\n\t__be16\t\t\tprotocol;\n\t__u16\t\t\ttransport_header;\n\t__u16\t\t\tnetwork_header;\n\t__u16\t\t\tmac_header;\n\n#ifdef CONFIG_KCOV\n\tu64\t\t\tkcov_handle;\n#endif\n\n\t/* private: */\n\t__u32\t\t\theaders_end[0];\n\t/* public: */\n\n\t/* These elements must be at the end, see alloc_skb() for details.  */\n\tsk_buff_data_t\t\ttail;\n\tsk_buff_data_t\t\tend;\n\tunsigned char\t\t*head,\n\t\t\t\t*data;\n\tunsigned int\t\ttruesize;\n\trefcount_t\t\tusers;\n\n#ifdef CONFIG_SKB_EXTENSIONS\n\t/* only useable after checking ->active_extensions != 0 */\n\tstruct skb_ext\t\t*extensions;\n#endif\n};\n\n#ifdef __KERNEL__\n/*\n *\tHandling routines are only of interest to the kernel\n */\n\n#define SKB_ALLOC_FCLONE\t0x01\n#define SKB_ALLOC_RX\t\t0x02\n#define SKB_ALLOC_NAPI\t\t0x04\n\n/**\n * skb_pfmemalloc - Test if the skb was allocated from PFMEMALLOC reserves\n * @skb: buffer\n */\nstatic inline bool skb_pfmemalloc(const struct sk_buff *skb)\n{\n\treturn unlikely(skb->pfmemalloc);\n}\n\n/*\n * skb might have a dst pointer attached, refcounted or not.\n * _skb_refdst low order bit is set if refcount was _not_ taken\n */\n#define SKB_DST_NOREF\t1UL\n#define SKB_DST_PTRMASK\t~(SKB_DST_NOREF)\n\n/**\n * skb_dst - returns skb dst_entry\n * @skb: buffer\n *\n * Returns skb dst_entry, regardless of reference taken or not.\n */\nstatic inline struct dst_entry *skb_dst(const struct sk_buff *skb)\n{\n\t/* If refdst was not refcounted, check we still are in a\n\t * rcu_read_lock section\n\t */\n\tWARN_ON((skb->_skb_refdst & SKB_DST_NOREF) &&\n\t\t!rcu_read_lock_held() &&\n\t\t!rcu_read_lock_bh_held());\n\treturn (struct dst_entry *)(skb->_skb_refdst & SKB_DST_PTRMASK);\n}\n\n/**\n * skb_dst_set - sets skb dst\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was taken on dst and should\n * be released by skb_dst_drop()\n */\nstatic inline void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tskb->_skb_refdst = (unsigned long)dst;\n}\n\n/**\n * skb_dst_set_noref - sets skb dst, hopefully, without taking reference\n * @skb: buffer\n * @dst: dst entry\n *\n * Sets skb dst, assuming a reference was not taken on dst.\n * If dst entry is cached, we do not take reference and dst_release\n * will be avoided by refdst_drop. If dst entry is not cached, we take\n * reference, so that last dst_release can destroy the dst immediately.\n */\nstatic inline void skb_dst_set_noref(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tWARN_ON(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\tskb->_skb_refdst = (unsigned long)dst | SKB_DST_NOREF;\n}\n\n/**\n * skb_dst_is_noref - Test if skb dst isn't refcounted\n * @skb: buffer\n */\nstatic inline bool skb_dst_is_noref(const struct sk_buff *skb)\n{\n\treturn (skb->_skb_refdst & SKB_DST_NOREF) && skb_dst(skb);\n}\n\n/**\n * skb_rtable - Returns the skb &rtable\n * @skb: buffer\n */\nstatic inline struct rtable *skb_rtable(const struct sk_buff *skb)\n{\n\treturn (struct rtable *)skb_dst(skb);\n}\n\n/* For mangling skb->pkt_type from user space side from applications\n * such as nft, tc, etc, we only allow a conservative subset of\n * possible pkt_types to be set.\n*/\nstatic inline bool skb_pkt_type_ok(u32 ptype)\n{\n\treturn ptype <= PACKET_OTHERHOST;\n}\n\n/**\n * skb_napi_id - Returns the skb's NAPI id\n * @skb: buffer\n */\nstatic inline unsigned int skb_napi_id(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\treturn skb->napi_id;\n#else\n\treturn 0;\n#endif\n}\n\n/**\n * skb_unref - decrement the skb's reference count\n * @skb: buffer\n *\n * Returns true if we can free the skb.\n */\nstatic inline bool skb_unref(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn false;\n\tif (likely(refcount_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!refcount_dec_and_test(&skb->users)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid skb_release_head_state(struct sk_buff *skb);\nvoid kfree_skb(struct sk_buff *skb);\nvoid kfree_skb_list(struct sk_buff *segs);\nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt);\nvoid skb_tx_error(struct sk_buff *skb);\n\n#ifdef CONFIG_TRACEPOINTS\nvoid consume_skb(struct sk_buff *skb);\n#else\nstatic inline void consume_skb(struct sk_buff *skb)\n{\n\treturn kfree_skb(skb);\n}\n#endif\n\nvoid __consume_stateless_skb(struct sk_buff *skb);\nvoid  __kfree_skb(struct sk_buff *skb);\nextern struct kmem_cache *skbuff_head_cache;\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen);\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize);\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t priority, int flags,\n\t\t\t    int node);\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size);\n\n/**\n * alloc_skb - allocate a network buffer\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb(unsigned int size,\n\t\t\t\t\tgfp_t priority)\n{\n\treturn __alloc_skb(size, priority, 0, NUMA_NO_NODE);\n}\n\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask);\nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first);\n\n/* Layout of fast clones : [skb1][skb2][fclone_ref] */\nstruct sk_buff_fclones {\n\tstruct sk_buff\tskb1;\n\n\tstruct sk_buff\tskb2;\n\n\trefcount_t\tfclone_ref;\n};\n\n/**\n *\tskb_fclone_busy - check if fclone is busy\n *\t@sk: socket\n *\t@skb: buffer\n *\n * Returns true if skb is a fast clone, and its clone is not freed.\n * Some drivers call skb_orphan() in their ndo_start_xmit(),\n * so we also check that this didnt happen.\n */\nstatic inline bool skb_fclone_busy(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct sk_buff_fclones *fclones;\n\n\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\treturn skb->fclone == SKB_FCLONE_ORIG &&\n\t       refcount_read(&fclones->fclone_ref) > 1 &&\n\t       fclones->skb2.sk == sk;\n}\n\n/**\n * alloc_skb_fclone - allocate a network buffer from fclone cache\n * @size: size to allocate\n * @priority: allocation mask\n *\n * This function is a convenient wrapper around __alloc_skb().\n */\nstatic inline struct sk_buff *alloc_skb_fclone(unsigned int size,\n\t\t\t\t\t       gfp_t priority)\n{\n\treturn __alloc_skb(size, priority, SKB_ALLOC_FCLONE, NUMA_NO_NODE);\n}\n\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src);\nvoid skb_headers_offset_update(struct sk_buff *skb, int off);\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask);\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t priority);\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old);\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t priority);\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone);\nstatic inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\n\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);\n}\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail, gfp_t gfp_mask);\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb,\n\t\t\t\t     unsigned int headroom);\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb, int newheadroom,\n\t\t\t\tint newtailroom, gfp_t priority);\nint __must_check skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t\t     int offset, int len);\nint __must_check skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t      int offset, int len);\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer);\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error);\n\n/**\n *\tskb_pad\t\t\t-\tzero pad the tail of an skb\n *\t@skb: buffer to pad\n *\t@pad: space to pad\n *\n *\tEnsure that a buffer is followed by a padding area that is zero\n *\tfilled. Used by network drivers which may DMA or transfer data\n *\tbeyond the buffer end onto the wire.\n *\n *\tMay return error in out of memory cases. The skb is freed on error.\n */\nstatic inline int skb_pad(struct sk_buff *skb, int pad)\n{\n\treturn __skb_pad(skb, pad, true);\n}\n#define dev_kfree_skb(a)\tconsume_skb(a)\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size);\n\nstruct skb_seq_state {\n\t__u32\t\tlower_offset;\n\t__u32\t\tupper_offset;\n\t__u32\t\tfrag_idx;\n\t__u32\t\tstepped_offset;\n\tstruct sk_buff\t*root_skb;\n\tstruct sk_buff\t*cur_skb;\n\t__u8\t\t*frag_data;\n\t__u32\t\tfrag_off;\n};\n\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st);\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st);\nvoid skb_abort_seq_read(struct skb_seq_state *st);\n\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config);\n\n/*\n * Packet hash types specify the type of hash in skb_set_hash.\n *\n * Hash types refer to the protocol layer addresses which are used to\n * construct a packet's hash. The hashes are used to differentiate or identify\n * flows of the protocol layer for the hash type. Hash types are either\n * layer-2 (L2), layer-3 (L3), or layer-4 (L4).\n *\n * Properties of hashes:\n *\n * 1) Two packets in different flows have different hash values\n * 2) Two packets in the same flow should have the same hash value\n *\n * A hash at a higher layer is considered to be more specific. A driver should\n * set the most specific hash possible.\n *\n * A driver cannot indicate a more specific hash than the layer at which a hash\n * was computed. For instance an L3 hash cannot be set as an L4 hash.\n *\n * A driver may indicate a hash level which is less specific than the\n * actual layer the hash was computed on. For instance, a hash computed\n * at L4 may be considered an L3 hash. This should only be done if the\n * driver can't unambiguously determine that the HW computed the hash at\n * the higher layer. Note that the \"should\" in the second property above\n * permits this.\n */\nenum pkt_hash_types {\n\tPKT_HASH_TYPE_NONE,\t/* Undefined type */\n\tPKT_HASH_TYPE_L2,\t/* Input: src_MAC, dest_MAC */\n\tPKT_HASH_TYPE_L3,\t/* Input: src_IP, dst_IP */\n\tPKT_HASH_TYPE_L4,\t/* Input: src_IP, dst_IP, src_port, dst_port */\n};\n\nstatic inline void skb_clear_hash(struct sk_buff *skb)\n{\n\tskb->hash = 0;\n\tskb->sw_hash = 0;\n\tskb->l4_hash = 0;\n}\n\nstatic inline void skb_clear_hash_if_not_l4(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash)\n\t\tskb_clear_hash(skb);\n}\n\nstatic inline void\n__skb_set_hash(struct sk_buff *skb, __u32 hash, bool is_sw, bool is_l4)\n{\n\tskb->l4_hash = is_l4;\n\tskb->sw_hash = is_sw;\n\tskb->hash = hash;\n}\n\nstatic inline void\nskb_set_hash(struct sk_buff *skb, __u32 hash, enum pkt_hash_types type)\n{\n\t/* Used by drivers to set hash from HW */\n\t__skb_set_hash(skb, hash, false, type == PKT_HASH_TYPE_L4);\n}\n\nstatic inline void\n__skb_set_sw_hash(struct sk_buff *skb, __u32 hash, bool is_l4)\n{\n\t__skb_set_hash(skb, hash, true, is_l4);\n}\n\nvoid __skb_get_hash(struct sk_buff *skb);\nu32 __skb_get_hash_symmetric(const struct sk_buff *skb);\nu32 skb_get_poff(const struct sk_buff *skb);\nu32 __skb_get_poff(const struct sk_buff *skb, void *data,\n\t\t   const struct flow_keys_basic *keys, int hlen);\n__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,\n\t\t\t    void *data, int hlen_proto);\n\nstatic inline __be32 skb_flow_get_ports(const struct sk_buff *skb,\n\t\t\t\t\tint thoff, u8 ip_proto)\n{\n\treturn __skb_flow_get_ports(skb, thoff, ip_proto, NULL, 0);\n}\n\nvoid skb_flow_dissector_init(struct flow_dissector *flow_dissector,\n\t\t\t     const struct flow_dissector_key *key,\n\t\t\t     unsigned int key_count);\n\nstruct bpf_flow_dissector;\nbool bpf_flow_dissect(struct bpf_prog *prog, struct bpf_flow_dissector *ctx,\n\t\t      __be16 proto, int nhoff, int hlen, unsigned int flags);\n\nbool __skb_flow_dissect(const struct net *net,\n\t\t\tconst struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container,\n\t\t\tvoid *data, __be16 proto, int nhoff, int hlen,\n\t\t\tunsigned int flags);\n\nstatic inline bool skb_flow_dissect(const struct sk_buff *skb,\n\t\t\t\t    struct flow_dissector *flow_dissector,\n\t\t\t\t    void *target_container, unsigned int flags)\n{\n\treturn __skb_flow_dissect(NULL, skb, flow_dissector,\n\t\t\t\t  target_container, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool skb_flow_dissect_flow_keys(const struct sk_buff *skb,\n\t\t\t\t\t      struct flow_keys *flow,\n\t\t\t\t\t      unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(NULL, skb, &flow_keys_dissector,\n\t\t\t\t  flow, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool\nskb_flow_dissect_flow_keys_basic(const struct net *net,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t struct flow_keys_basic *flow, void *data,\n\t\t\t\t __be16 proto, int nhoff, int hlen,\n\t\t\t\t unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(net, skb, &flow_keys_basic_dissector, flow,\n\t\t\t\t  data, proto, nhoff, hlen, flags);\n}\n\nvoid skb_flow_dissect_meta(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\n/* Gets a skb connection tracking info, ctinfo map should be a\n * map of mapsize to translate enum ip_conntrack_info states\n * to user states.\n */\nvoid\nskb_flow_dissect_ct(const struct sk_buff *skb,\n\t\t    struct flow_dissector *flow_dissector,\n\t\t    void *target_container,\n\t\t    u16 *ctinfo_map,\n\t\t    size_t mapsize);\nvoid\nskb_flow_dissect_tunnel_info(const struct sk_buff *skb,\n\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t     void *target_container);\n\nvoid skb_flow_dissect_hash(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\nstatic inline __u32 skb_get_hash(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash && !skb->sw_hash)\n\t\t__skb_get_hash(skb);\n\n\treturn skb->hash;\n}\n\nstatic inline __u32 skb_get_hash_flowi6(struct sk_buff *skb, const struct flowi6 *fl6)\n{\n\tif (!skb->l4_hash && !skb->sw_hash) {\n\t\tstruct flow_keys keys;\n\t\t__u32 hash = __get_hash_from_flowi6(fl6, &keys);\n\n\t\t__skb_set_sw_hash(skb, hash, flow_keys_have_l4(&keys));\n\t}\n\n\treturn skb->hash;\n}\n\n__u32 skb_get_hash_perturb(const struct sk_buff *skb,\n\t\t\t   const siphash_key_t *perturb);\n\nstatic inline __u32 skb_get_hash_raw(const struct sk_buff *skb)\n{\n\treturn skb->hash;\n}\n\nstatic inline void skb_copy_hash(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->hash = from->hash;\n\tto->sw_hash = from->sw_hash;\n\tto->l4_hash = from->l4_hash;\n};\n\nstatic inline void skb_copy_decrypted(struct sk_buff *to,\n\t\t\t\t      const struct sk_buff *from)\n{\n#ifdef CONFIG_TLS_DEVICE\n\tto->decrypted = from->decrypted;\n#endif\n}\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n#else\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end - skb->head;\n}\n#endif\n\n/* Internal */\n#define skb_shinfo(SKB)\t((struct skb_shared_info *)(skb_end_pointer(SKB)))\n\nstatic inline struct skb_shared_hwtstamps *skb_hwtstamps(struct sk_buff *skb)\n{\n\treturn &skb_shinfo(skb)->hwtstamps;\n}\n\nstatic inline struct ubuf_info *skb_zcopy(struct sk_buff *skb)\n{\n\tbool is_zcopy = skb && skb_shinfo(skb)->flags & SKBFL_ZEROCOPY_ENABLE;\n\n\treturn is_zcopy ? skb_uarg(skb) : NULL;\n}\n\nstatic inline void net_zcopy_get(struct ubuf_info *uarg)\n{\n\trefcount_inc(&uarg->refcnt);\n}\n\nstatic inline void skb_zcopy_init(struct sk_buff *skb, struct ubuf_info *uarg)\n{\n\tskb_shinfo(skb)->destructor_arg = uarg;\n\tskb_shinfo(skb)->flags |= uarg->flags;\n}\n\nstatic inline void skb_zcopy_set(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t\t bool *have_ref)\n{\n\tif (skb && uarg && !skb_zcopy(skb)) {\n\t\tif (unlikely(have_ref && *have_ref))\n\t\t\t*have_ref = false;\n\t\telse\n\t\t\tnet_zcopy_get(uarg);\n\t\tskb_zcopy_init(skb, uarg);\n\t}\n}\n\nstatic inline void skb_zcopy_set_nouarg(struct sk_buff *skb, void *val)\n{\n\tskb_shinfo(skb)->destructor_arg = (void *)((uintptr_t) val | 0x1UL);\n\tskb_shinfo(skb)->flags |= SKBFL_ZEROCOPY_FRAG;\n}\n\nstatic inline bool skb_zcopy_is_nouarg(struct sk_buff *skb)\n{\n\treturn (uintptr_t) skb_shinfo(skb)->destructor_arg & 0x1UL;\n}\n\nstatic inline void *skb_zcopy_get_nouarg(struct sk_buff *skb)\n{\n\treturn (void *)((uintptr_t) skb_shinfo(skb)->destructor_arg & ~0x1UL);\n}\n\nstatic inline void net_zcopy_put(struct ubuf_info *uarg)\n{\n\tif (uarg)\n\t\tuarg->callback(NULL, uarg, true);\n}\n\nstatic inline void net_zcopy_put_abort(struct ubuf_info *uarg, bool have_uref)\n{\n\tif (uarg) {\n\t\tif (uarg->callback == msg_zerocopy_callback)\n\t\t\tmsg_zerocopy_put_abort(uarg, have_uref);\n\t\telse if (have_uref)\n\t\t\tnet_zcopy_put(uarg);\n\t}\n}\n\n/* Release a reference on a zerocopy structure */\nstatic inline void skb_zcopy_clear(struct sk_buff *skb, bool zerocopy_success)\n{\n\tstruct ubuf_info *uarg = skb_zcopy(skb);\n\n\tif (uarg) {\n\t\tif (!skb_zcopy_is_nouarg(skb))\n\t\t\tuarg->callback(skb, uarg, zerocopy_success);\n\n\t\tskb_shinfo(skb)->flags &= ~SKBFL_ZEROCOPY_FRAG;\n\t}\n}\n\nstatic inline void skb_mark_not_on_list(struct sk_buff *skb)\n{\n\tskb->next = NULL;\n}\n\n/* Iterate through singly-linked GSO fragments of an skb. */\n#define skb_list_walk_safe(first, skb, next_skb)                               \\\n\tfor ((skb) = (first), (next_skb) = (skb) ? (skb)->next : NULL; (skb);  \\\n\t     (skb) = (next_skb), (next_skb) = (skb) ? (skb)->next : NULL)\n\nstatic inline void skb_list_del_init(struct sk_buff *skb)\n{\n\t__list_del_entry(&skb->list);\n\tskb_mark_not_on_list(skb);\n}\n\n/**\n *\tskb_queue_empty - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n */\nstatic inline int skb_queue_empty(const struct sk_buff_head *list)\n{\n\treturn list->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_empty_lockless - check if a queue is empty\n *\t@list: queue head\n *\n *\tReturns true if the queue is empty, false otherwise.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline bool skb_queue_empty_lockless(const struct sk_buff_head *list)\n{\n\treturn READ_ONCE(list->next) == (const struct sk_buff *) list;\n}\n\n\n/**\n *\tskb_queue_is_last - check if skb is the last entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the last buffer on the list.\n */\nstatic inline bool skb_queue_is_last(const struct sk_buff_head *list,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn skb->next == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_is_first - check if skb is the first entry in the queue\n *\t@list: queue head\n *\t@skb: buffer\n *\n *\tReturns true if @skb is the first buffer on the list.\n */\nstatic inline bool skb_queue_is_first(const struct sk_buff_head *list,\n\t\t\t\t      const struct sk_buff *skb)\n{\n\treturn skb->prev == (const struct sk_buff *) list;\n}\n\n/**\n *\tskb_queue_next - return the next packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the next packet in @list after @skb.  It is only valid to\n *\tcall this if skb_queue_is_last() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_next(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_last(list, skb));\n\treturn skb->next;\n}\n\n/**\n *\tskb_queue_prev - return the prev packet in the queue\n *\t@list: queue head\n *\t@skb: current buffer\n *\n *\tReturn the prev packet in @list before @skb.  It is only valid to\n *\tcall this if skb_queue_is_first() evaluates to false.\n */\nstatic inline struct sk_buff *skb_queue_prev(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t/* This BUG_ON may seem severe, but if we just return then we\n\t * are going to dereference garbage.\n\t */\n\tBUG_ON(skb_queue_is_first(list, skb));\n\treturn skb->prev;\n}\n\n/**\n *\tskb_get - reference buffer\n *\t@skb: buffer to reference\n *\n *\tMakes another reference to a socket buffer and returns a pointer\n *\tto the buffer.\n */\nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)\n{\n\trefcount_inc(&skb->users);\n\treturn skb;\n}\n\n/*\n * If users == 1, we are the only owner and can avoid redundant atomic changes.\n */\n\n/**\n *\tskb_cloned - is the buffer a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if the buffer was generated with skb_clone() and is\n *\tone of multiple shared copies of the buffer. Cloned buffers are\n *\tshared data so must not be written to under normal circumstances.\n */\nstatic inline int skb_cloned(const struct sk_buff *skb)\n{\n\treturn skb->cloned &&\n\t       (atomic_read(&skb_shinfo(skb)->dataref) & SKB_DATAREF_MASK) != 1;\n}\n\nstatic inline int skb_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\tskb_header_cloned - is the header a clone\n *\t@skb: buffer to check\n *\n *\tReturns true if modifying the header part of the buffer requires\n *\tthe data to be copied.\n */\nstatic inline int skb_header_cloned(const struct sk_buff *skb)\n{\n\tint dataref;\n\n\tif (!skb->cloned)\n\t\treturn 0;\n\n\tdataref = atomic_read(&skb_shinfo(skb)->dataref);\n\tdataref = (dataref & SKB_DATAREF_MASK) - (dataref >> SKB_DATAREF_SHIFT);\n\treturn dataref != 1;\n}\n\nstatic inline int skb_header_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_header_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n/**\n *\t__skb_header_release - release reference to header\n *\t@skb: buffer to operate on\n */\nstatic inline void __skb_header_release(struct sk_buff *skb)\n{\n\tskb->nohdr = 1;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1 + (1 << SKB_DATAREF_SHIFT));\n}\n\n\n/**\n *\tskb_shared - is the buffer shared\n *\t@skb: buffer to check\n *\n *\tReturns true if more than one person has a reference to this\n *\tbuffer.\n */\nstatic inline int skb_shared(const struct sk_buff *skb)\n{\n\treturn refcount_read(&skb->users) != 1;\n}\n\n/**\n *\tskb_share_check - check if buffer is shared and if so clone it\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the buffer is shared the buffer is cloned and the old copy\n *\tdrops a reference. A new clone with a single reference is returned.\n *\tIf the buffer is not shared the original buffer is returned. When\n *\tbeing called from interrupt status or with spinlocks held pri must\n *\tbe GFP_ATOMIC.\n *\n *\tNULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\n\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/*\n *\tCopy shared buffers into a new sk_buff. We effectively do COW on\n *\tpackets to handle cases where we have a local reader and forward\n *\tand a couple of other messy ones. The normal one is tcpdumping\n *\ta packet thats being forwarded.\n */\n\n/**\n *\tskb_unshare - make a copy of a shared buffer\n *\t@skb: buffer to check\n *\t@pri: priority for memory allocation\n *\n *\tIf the socket buffer is a clone then this function creates a new\n *\tcopy of the data, drops a reference count on the old copy and returns\n *\tthe new copy with the reference count at 1. If the buffer is not a clone\n *\tthe original buffer is returned. When called with a spinlock held or\n *\tfrom interrupt state @pri must be %GFP_ATOMIC\n *\n *\t%NULL is returned on a memory allocation failure.\n */\nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\n\t\t\t\t\t  gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_cloned(skb)) {\n\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\n\n\t\t/* Free our shared copy */\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n/**\n *\tskb_peek - peek at the head of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the head element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = list_->next;\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n}\n\n/**\n *\t__skb_peek - peek at the head of a non-empty &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tLike skb_peek(), but the caller knows that the list is not empty.\n */\nstatic inline struct sk_buff *__skb_peek(const struct sk_buff_head *list_)\n{\n\treturn list_->next;\n}\n\n/**\n *\tskb_peek_next - peek skb following the given one from a queue\n *\t@skb: skb to start from\n *\t@list_: list to peek at\n *\n *\tReturns %NULL when the end of the list is met or a pointer to the\n *\tnext element. The reference count is not incremented and the\n *\treference is therefore volatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_next(struct sk_buff *skb,\n\t\tconst struct sk_buff_head *list_)\n{\n\tstruct sk_buff *next = skb->next;\n\n\tif (next == (struct sk_buff *)list_)\n\t\tnext = NULL;\n\treturn next;\n}\n\n/**\n *\tskb_peek_tail - peek at the tail of an &sk_buff_head\n *\t@list_: list to peek at\n *\n *\tPeek an &sk_buff. Unlike most other operations you _MUST_\n *\tbe careful with this one. A peek leaves the buffer on the\n *\tlist and someone else may run off with it. You must hold\n *\tthe appropriate locks or have a private queue to do this.\n *\n *\tReturns %NULL for an empty list or a pointer to the tail element.\n *\tThe reference count is not incremented and the reference is therefore\n *\tvolatile. Use with caution.\n */\nstatic inline struct sk_buff *skb_peek_tail(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = READ_ONCE(list_->prev);\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n\n}\n\n/**\n *\tskb_queue_len\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n */\nstatic inline __u32 skb_queue_len(const struct sk_buff_head *list_)\n{\n\treturn list_->qlen;\n}\n\n/**\n *\tskb_queue_len_lockless\t- get queue length\n *\t@list_: list to measure\n *\n *\tReturn the length of an &sk_buff queue.\n *\tThis variant can be used in lockless contexts.\n */\nstatic inline __u32 skb_queue_len_lockless(const struct sk_buff_head *list_)\n{\n\treturn READ_ONCE(list_->qlen);\n}\n\n/**\n *\t__skb_queue_head_init - initialize non-spinlock portions of sk_buff_head\n *\t@list: queue to initialize\n *\n *\tThis initializes only the list and queue length aspects of\n *\tan sk_buff_head object.  This allows to initialize the list\n *\taspects of an sk_buff_head without reinitializing things like\n *\tthe spinlock.  It can also be used for on-stack sk_buff_head\n *\tobjects where the spinlock is known to not be used.\n */\nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)\n{\n\tlist->prev = list->next = (struct sk_buff *)list;\n\tlist->qlen = 0;\n}\n\n/*\n * This function creates a split out lock class for each invocation;\n * this is needed for now since a whole lot of users of the skb-queue\n * infrastructure in drivers have different locking usage (in hardirq)\n * than the networking core (in softirq only). In the long run either the\n * network layer or drivers should need annotation to consolidate the\n * main types of usage into 3 classes.\n */\nstatic inline void skb_queue_head_init(struct sk_buff_head *list)\n{\n\tspin_lock_init(&list->lock);\n\t__skb_queue_head_init(list);\n}\n\nstatic inline void skb_queue_head_init_class(struct sk_buff_head *list,\n\t\tstruct lock_class_key *class)\n{\n\tskb_queue_head_init(list);\n\tlockdep_set_class(&list->lock, class);\n}\n\n/*\n *\tInsert an sk_buff on a list.\n *\n *\tThe \"__skb_xxxx()\" functions are the non-atomic ones that\n *\tcan only be called with interrupts disabled.\n */\nstatic inline void __skb_insert(struct sk_buff *newsk,\n\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\n\t\t\t\tstruct sk_buff_head *list)\n{\n\t/* See skb_queue_empty_lockless() and skb_peek_tail()\n\t * for the opposite READ_ONCE()\n\t */\n\tWRITE_ONCE(newsk->next, next);\n\tWRITE_ONCE(newsk->prev, prev);\n\tWRITE_ONCE(next->prev, newsk);\n\tWRITE_ONCE(prev->next, newsk);\n\tlist->qlen++;\n}\n\nstatic inline void __skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *prev,\n\t\t\t\t      struct sk_buff *next)\n{\n\tstruct sk_buff *first = list->next;\n\tstruct sk_buff *last = list->prev;\n\n\tWRITE_ONCE(first->prev, prev);\n\tWRITE_ONCE(prev->next, first);\n\n\tWRITE_ONCE(last->next, next);\n\tWRITE_ONCE(next->prev, last);\n}\n\n/**\n *\tskb_queue_splice - join two skb lists, this is designed for stacks\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_init(struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail - join two skb lists, each list being a queue\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n */\nstatic inline void skb_queue_splice_tail(const struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n/**\n *\tskb_queue_splice_tail_init - join two skb lists and reinitialise the emptied list\n *\t@list: the new list to add\n *\t@head: the place to add it in the first list\n *\n *\tEach of the lists is a queue.\n *\tThe list at @list is reinitialised\n */\nstatic inline void skb_queue_splice_tail_init(struct sk_buff_head *list,\n\t\t\t\t\t      struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n/**\n *\t__skb_queue_after - queue a buffer at the list head\n *\t@list: list to use\n *\t@prev: place after this buffer\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer int the middle of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_after(struct sk_buff_head *list,\n\t\t\t\t     struct sk_buff *prev,\n\t\t\t\t     struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, prev, prev->next, list);\n}\n\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk,\n\t\tstruct sk_buff_head *list);\n\nstatic inline void __skb_queue_before(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *next,\n\t\t\t\t      struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, next->prev, next, list);\n}\n\n/**\n *\t__skb_queue_head - queue a buffer at the list head\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the start of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_head(struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff *newsk)\n{\n\t__skb_queue_after(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/**\n *\t__skb_queue_tail - queue a buffer at the list tail\n *\t@list: list to use\n *\t@newsk: buffer to queue\n *\n *\tQueue a buffer at the end of a list. This function takes no locks\n *\tand you must therefore hold required locks before calling it.\n *\n *\tA buffer cannot be placed on two lists at the same time.\n */\nstatic inline void __skb_queue_tail(struct sk_buff_head *list,\n\t\t\t\t   struct sk_buff *newsk)\n{\n\t__skb_queue_before(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk);\n\n/*\n * remove sk_buff from list. _Must_ be called atomically, and with\n * the list known..\n */\nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list);\nstatic inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tstruct sk_buff *next, *prev;\n\n\tWRITE_ONCE(list->qlen, list->qlen - 1);\n\tnext\t   = skb->next;\n\tprev\t   = skb->prev;\n\tskb->next  = skb->prev = NULL;\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n}\n\n/**\n *\t__skb_dequeue - remove from the head of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the head of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The head item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list);\n\n/**\n *\t__skb_dequeue_tail - remove from the tail of the queue\n *\t@list: list to dequeue from\n *\n *\tRemove the tail of the list. This function does not take any locks\n *\tso must be used with appropriate locks held only. The tail item is\n *\treturned or %NULL if the list is empty.\n */\nstatic inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek_tail(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list);\n\n\nstatic inline bool skb_is_nonlinear(const struct sk_buff *skb)\n{\n\treturn skb->data_len;\n}\n\nstatic inline unsigned int skb_headlen(const struct sk_buff *skb)\n{\n\treturn skb->len - skb->data_len;\n}\n\nstatic inline unsigned int __skb_pagelen(const struct sk_buff *skb)\n{\n\tunsigned int i, len = 0;\n\n\tfor (i = skb_shinfo(skb)->nr_frags - 1; (int)i >= 0; i--)\n\t\tlen += skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\treturn len;\n}\n\nstatic inline unsigned int skb_pagelen(const struct sk_buff *skb)\n{\n\treturn skb_headlen(skb) + __skb_pagelen(skb);\n}\n\n/**\n * __skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * Initialises the @i'th fragment of @skb to point to &size bytes at\n * offset @off within @page.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void __skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t\tstruct page *page, int off, int size)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t/*\n\t * Propagate page pfmemalloc to the skb if we can. The problem is\n\t * that not all callers have unique ownership of the page but rely\n\t * on page_is_pfmemalloc doing the right thing(tm).\n\t */\n\tfrag->bv_page\t\t  = page;\n\tfrag->bv_offset\t\t  = off;\n\tskb_frag_size_set(frag, size);\n\n\tpage = compound_head(page);\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc\t= true;\n}\n\n/**\n * skb_fill_page_desc - initialise a paged fragment in an skb\n * @skb: buffer containing fragment to be initialised\n * @i: paged fragment index to initialise\n * @page: the page to use for this fragment\n * @off: the offset to the data with @page\n * @size: the length of the data\n *\n * As per __skb_fill_page_desc() -- initialises the @i'th fragment of\n * @skb to point to @size bytes at offset @off within @page. In\n * addition updates @skb such that @i is the last fragment.\n *\n * Does not take any additional reference on the fragment.\n */\nstatic inline void skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t      struct page *page, int off, int size)\n{\n\t__skb_fill_page_desc(skb, i, page, off, size);\n\tskb_shinfo(skb)->nr_frags = i + 1;\n}\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize);\n\n#define SKB_LINEAR_ASSERT(skb)  BUG_ON(skb_is_nonlinear(skb))\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data - skb->head;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_tail_pointer(skb);\n\tskb->tail += offset;\n}\n\n#else /* NET_SKBUFF_DATA_USES_OFFSET */\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb->tail = skb->data + offset;\n}\n\n#endif /* NET_SKBUFF_DATA_USES_OFFSET */\n\n/*\n *\tAdd data to an sk_buff\n */\nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len);\nvoid *skb_put(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t   unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\treturn tmp;\n}\n\nstatic inline void __skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)__skb_put(skb, 1) = val;\n}\n\nstatic inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\n\treturn tmp;\n}\n\nstatic inline void *skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\n\treturn tmp;\n}\n\nstatic inline void skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)skb_put(skb, 1) = val;\n}\n\nvoid *skb_push(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\treturn skb->data;\n}\n\nvoid *skb_pull(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tskb->len -= len;\n\tBUG_ON(skb->len < skb->data_len);\n\treturn skb->data += len;\n}\n\nstatic inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __skb_pull(skb, len);\n}\n\nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta);\n\nstatic inline void *__pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (len > skb_headlen(skb) &&\n\t    !__pskb_pull_tail(skb, len - skb_headlen(skb)))\n\t\treturn NULL;\n\tskb->len -= len;\n\treturn skb->data += len;\n}\n\nstatic inline void *pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __pskb_pull(skb, len);\n}\n\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len <= skb_headlen(skb)))\n\t\treturn true;\n\tif (unlikely(len > skb->len))\n\t\treturn false;\n\treturn __pskb_pull_tail(skb, len - skb_headlen(skb)) != NULL;\n}\n\nvoid skb_condense(struct sk_buff *skb);\n\n/**\n *\tskb_headroom - bytes at buffer head\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the head of an &sk_buff.\n */\nstatic inline unsigned int skb_headroom(const struct sk_buff *skb)\n{\n\treturn skb->data - skb->head;\n}\n\n/**\n *\tskb_tailroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n */\nstatic inline int skb_tailroom(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? 0 : skb->end - skb->tail;\n}\n\n/**\n *\tskb_availroom - bytes at buffer end\n *\t@skb: buffer to check\n *\n *\tReturn the number of bytes of free space at the tail of an sk_buff\n *\tallocated by sk_stream_alloc()\n */\nstatic inline int skb_availroom(const struct sk_buff *skb)\n{\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\treturn skb->end - skb->tail - skb->reserved_tailroom;\n}\n\n/**\n *\tskb_reserve - adjust headroom\n *\t@skb: buffer to alter\n *\t@len: bytes to move\n *\n *\tIncrease the headroom of an empty &sk_buff by reducing the tail\n *\troom. This is only allowed for an empty buffer.\n */\nstatic inline void skb_reserve(struct sk_buff *skb, int len)\n{\n\tskb->data += len;\n\tskb->tail += len;\n}\n\n/**\n *\tskb_tailroom_reserve - adjust reserved_tailroom\n *\t@skb: buffer to alter\n *\t@mtu: maximum amount of headlen permitted\n *\t@needed_tailroom: minimum amount of reserved_tailroom\n *\n *\tSet reserved_tailroom so that headlen can be as large as possible but\n *\tnot larger than mtu and tailroom cannot be smaller than\n *\tneeded_tailroom.\n *\tThe required headroom should already have been reserved before using\n *\tthis function.\n */\nstatic inline void skb_tailroom_reserve(struct sk_buff *skb, unsigned int mtu,\n\t\t\t\t\tunsigned int needed_tailroom)\n{\n\tSKB_LINEAR_ASSERT(skb);\n\tif (mtu < skb_tailroom(skb) - needed_tailroom)\n\t\t/* use at most mtu */\n\t\tskb->reserved_tailroom = skb_tailroom(skb) - mtu;\n\telse\n\t\t/* use up to all available space */\n\t\tskb->reserved_tailroom = needed_tailroom;\n}\n\n#define ENCAP_TYPE_ETHER\t0\n#define ENCAP_TYPE_IPPROTO\t1\n\nstatic inline void skb_set_inner_protocol(struct sk_buff *skb,\n\t\t\t\t\t  __be16 protocol)\n{\n\tskb->inner_protocol = protocol;\n\tskb->inner_protocol_type = ENCAP_TYPE_ETHER;\n}\n\nstatic inline void skb_set_inner_ipproto(struct sk_buff *skb,\n\t\t\t\t\t __u8 ipproto)\n{\n\tskb->inner_ipproto = ipproto;\n\tskb->inner_protocol_type = ENCAP_TYPE_IPPROTO;\n}\n\nstatic inline void skb_reset_inner_headers(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->mac_header;\n\tskb->inner_network_header = skb->network_header;\n\tskb->inner_transport_header = skb->transport_header;\n}\n\nstatic inline void skb_reset_mac_len(struct sk_buff *skb)\n{\n\tskb->mac_len = skb->network_header - skb->mac_header;\n}\n\nstatic inline unsigned char *skb_inner_transport_header(const struct sk_buff\n\t\t\t\t\t\t\t*skb)\n{\n\treturn skb->head + skb->inner_transport_header;\n}\n\nstatic inline int skb_inner_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_transport_header(skb) - skb->data;\n}\n\nstatic inline void skb_reset_inner_transport_header(struct sk_buff *skb)\n{\n\tskb->inner_transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_transport_header(struct sk_buff *skb,\n\t\t\t\t\t\t   const int offset)\n{\n\tskb_reset_inner_transport_header(skb);\n\tskb->inner_transport_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_network_header;\n}\n\nstatic inline void skb_reset_inner_network_header(struct sk_buff *skb)\n{\n\tskb->inner_network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_network_header(struct sk_buff *skb,\n\t\t\t\t\t\tconst int offset)\n{\n\tskb_reset_inner_network_header(skb);\n\tskb->inner_network_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_mac_header;\n}\n\nstatic inline void skb_reset_inner_mac_header(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_mac_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_inner_mac_header(skb);\n\tskb->inner_mac_header += offset;\n}\nstatic inline bool skb_transport_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->transport_header != (typeof(skb->transport_header))~0U;\n}\n\nstatic inline unsigned char *skb_transport_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->transport_header;\n}\n\nstatic inline void skb_reset_transport_header(struct sk_buff *skb)\n{\n\tskb->transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_transport_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_transport_header(skb);\n\tskb->transport_header += offset;\n}\n\nstatic inline unsigned char *skb_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->network_header;\n}\n\nstatic inline void skb_reset_network_header(struct sk_buff *skb)\n{\n\tskb->network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_network_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_network_header(skb);\n\tskb->network_header += offset;\n}\n\nstatic inline unsigned char *skb_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->mac_header;\n}\n\nstatic inline int skb_mac_offset(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_mac_header_len(const struct sk_buff *skb)\n{\n\treturn skb->network_header - skb->mac_header;\n}\n\nstatic inline int skb_mac_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->mac_header != (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_unset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_reset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_mac_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_mac_header(skb);\n\tskb->mac_header += offset;\n}\n\nstatic inline void skb_pop_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->network_header;\n}\n\nstatic inline void skb_probe_transport_header(struct sk_buff *skb)\n{\n\tstruct flow_keys_basic keys;\n\n\tif (skb_transport_header_was_set(skb))\n\t\treturn;\n\n\tif (skb_flow_dissect_flow_keys_basic(NULL, skb, &keys,\n\t\t\t\t\t     NULL, 0, 0, 0, 0))\n\t\tskb_set_transport_header(skb, keys.control.thoff);\n}\n\nstatic inline void skb_mac_header_rebuild(struct sk_buff *skb)\n{\n\tif (skb_mac_header_was_set(skb)) {\n\t\tconst unsigned char *old_mac = skb_mac_header(skb);\n\n\t\tskb_set_mac_header(skb, -skb->mac_len);\n\t\tmemmove(skb_mac_header(skb), old_mac, skb->mac_len);\n\t}\n}\n\nstatic inline int skb_checksum_start_offset(const struct sk_buff *skb)\n{\n\treturn skb->csum_start - skb_headroom(skb);\n}\n\nstatic inline unsigned char *skb_checksum_start(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->csum_start;\n}\n\nstatic inline int skb_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_transport_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->transport_header - skb->network_header;\n}\n\nstatic inline u32 skb_inner_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->inner_transport_header - skb->inner_network_header;\n}\n\nstatic inline int skb_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_network_header(skb) - skb->data;\n}\n\nstatic inline int skb_inner_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_network_header(skb) - skb->data;\n}\n\nstatic inline int pskb_network_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn pskb_may_pull(skb, skb_network_offset(skb) + len);\n}\n\n/*\n * CPUs often take a performance hit when accessing unaligned memory\n * locations. The actual performance hit varies, it can be small if the\n * hardware handles it or large if we have to take an exception and fix it\n * in software.\n *\n * Since an ethernet header is 14 bytes network drivers often end up with\n * the IP header at an unaligned offset. The IP header can be aligned by\n * shifting the start of the packet by 2 bytes. Drivers should do this\n * with:\n *\n * skb_reserve(skb, NET_IP_ALIGN);\n *\n * The downside to this alignment of the IP header is that the DMA is now\n * unaligned. On some architectures the cost of an unaligned DMA is high\n * and this cost outweighs the gains made by aligning the IP header.\n *\n * Since this trade off varies between architectures, we allow NET_IP_ALIGN\n * to be overridden.\n */\n#ifndef NET_IP_ALIGN\n#define NET_IP_ALIGN\t2\n#endif\n\n/*\n * The networking layer reserves some headroom in skb data (via\n * dev_alloc_skb). This is used to avoid having to reallocate skb data when\n * the header has to grow. In the default case, if the header has to grow\n * 32 bytes or less we avoid the reallocation.\n *\n * Unfortunately this headroom changes the DMA alignment of the resulting\n * network packet. As for NET_IP_ALIGN, this unaligned DMA is expensive\n * on some architectures. An architecture can override this value,\n * perhaps setting it to a cacheline in size (since that will maintain\n * cacheline alignment of the DMA). It must be a power of 2.\n *\n * Various parts of the networking layer expect at least 32 bytes of\n * headroom, you should not reduce this.\n *\n * Using max(32, L1_CACHE_BYTES) makes sense (especially with RPS)\n * to reduce average number of cache lines per packet.\n * get_rps_cpu() for example only access one 64 bytes aligned block :\n * NET_IP_ALIGN(2) + ethernet_header(14) + IP_header(20/40) + ports(8)\n */\n#ifndef NET_SKB_PAD\n#define NET_SKB_PAD\tmax(32, L1_CACHE_BYTES)\n#endif\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline void __skb_set_length(struct sk_buff *skb, unsigned int len)\n{\n\tif (WARN_ON(skb_is_nonlinear(skb)))\n\t\treturn;\n\tskb->len = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void __skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\t__skb_set_length(skb, len);\n}\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline int __pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->data_len)\n\t\treturn ___pskb_trim(skb, len);\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\treturn (len < skb->len) ? __pskb_trim(skb, len) : 0;\n}\n\n/**\n *\tpskb_trim_unique - remove end from a paged unique (not cloned) buffer\n *\t@skb: buffer to alter\n *\t@len: new length\n *\n *\tThis is identical to pskb_trim except that the caller knows that\n *\tthe skb is not cloned so we should never get an error due to out-\n *\tof-memory.\n */\nstatic inline void pskb_trim_unique(struct sk_buff *skb, unsigned int len)\n{\n\tint err = pskb_trim(skb, len);\n\tBUG_ON(err);\n}\n\nstatic inline int __skb_grow(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int diff = len - skb->len;\n\n\tif (skb_tailroom(skb) < diff) {\n\t\tint ret = pskb_expand_head(skb, 0, diff - skb_tailroom(skb),\n\t\t\t\t\t   GFP_ATOMIC);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\t__skb_set_length(skb, len);\n\treturn 0;\n}\n\n/**\n *\tskb_orphan - orphan a buffer\n *\t@skb: buffer to orphan\n *\n *\tIf a buffer currently has an owner then we call the owner's\n *\tdestructor function and make the @skb unowned. The buffer continues\n *\tto exist but is no longer charged to its former owner.\n */\nstatic inline void skb_orphan(struct sk_buff *skb)\n{\n\tif (skb->destructor) {\n\t\tskb->destructor(skb);\n\t\tskb->destructor = NULL;\n\t\tskb->sk\t\t= NULL;\n\t} else {\n\t\tBUG_ON(skb->sk);\n\t}\n}\n\n/**\n *\tskb_orphan_frags - orphan the frags contained in a buffer\n *\t@skb: buffer to orphan frags from\n *\t@gfp_mask: allocation mask for replacement pages\n *\n *\tFor each frag in the SKB which needs a destructor (i.e. has an\n *\towner) create a copy of that frag and release the original\n *\tpage by calling the destructor.\n */\nstatic inline int skb_orphan_frags(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\tif (!skb_zcopy_is_nouarg(skb) &&\n\t    skb_uarg(skb)->callback == msg_zerocopy_callback)\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/* Frags must be orphaned, even if refcounted, if skb might loop to rx path */\nstatic inline int skb_orphan_frags_rx(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n/**\n *\t__skb_queue_purge - empty a list\n *\t@list: list to empty\n *\n *\tDelete all buffers on an &sk_buff list. Each buffer is removed from\n *\tthe list and one reference dropped. This function does not take the\n *\tlist lock and the caller must hold the relevant locks to use it.\n */\nstatic inline void __skb_queue_purge(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\twhile ((skb = __skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}\nvoid skb_queue_purge(struct sk_buff_head *list);\n\nunsigned int skb_rbtree_purge(struct rb_root *root);\n\nvoid *netdev_alloc_frag(unsigned int fragsz);\n\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,\n\t\t\t\t   gfp_t gfp_mask);\n\n/**\n *\tnetdev_alloc_skb - allocate an skbuff for rx on a specific device\n *\t@dev: network device to receive on\n *\t@length: length to allocate\n *\n *\tAllocate a new &sk_buff and assign it a usage count of one. The\n *\tbuffer has unspecified headroom built in. Users should allocate\n *\tthe headroom they think they need without accounting for the\n *\tbuilt in space. The built in space is used for optimisations.\n *\n *\t%NULL is returned if there is no free memory. Although this function\n *\tallocates memory it can be called from an interrupt.\n */\nstatic inline struct sk_buff *netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t\t       unsigned int length)\n{\n\treturn __netdev_alloc_skb(dev, length, GFP_ATOMIC);\n}\n\n/* legacy helper around __netdev_alloc_skb() */\nstatic inline struct sk_buff *__dev_alloc_skb(unsigned int length,\n\t\t\t\t\t      gfp_t gfp_mask)\n{\n\treturn __netdev_alloc_skb(NULL, length, gfp_mask);\n}\n\n/* legacy helper around netdev_alloc_skb() */\nstatic inline struct sk_buff *dev_alloc_skb(unsigned int length)\n{\n\treturn netdev_alloc_skb(NULL, length);\n}\n\n\nstatic inline struct sk_buff *__netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length, gfp_t gfp)\n{\n\tstruct sk_buff *skb = __netdev_alloc_skb(dev, length + NET_IP_ALIGN, gfp);\n\n\tif (NET_IP_ALIGN && skb)\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\treturn skb;\n}\n\nstatic inline struct sk_buff *netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length)\n{\n\treturn __netdev_alloc_skb_ip_align(dev, length, GFP_ATOMIC);\n}\n\nstatic inline void skb_free_frag(void *addr)\n{\n\tpage_frag_free(addr);\n}\n\nvoid *napi_alloc_frag(unsigned int fragsz);\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t unsigned int length, gfp_t gfp_mask);\nstatic inline struct sk_buff *napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t\t     unsigned int length)\n{\n\treturn __napi_alloc_skb(napi, length, GFP_ATOMIC);\n}\nvoid napi_consume_skb(struct sk_buff *skb, int budget);\n\nvoid __kfree_skb_flush(void);\nvoid __kfree_skb_defer(struct sk_buff *skb);\n\n/**\n * __dev_alloc_pages - allocate page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n * @order: size of the allocation\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n*/\nstatic inline struct page *__dev_alloc_pages(gfp_t gfp_mask,\n\t\t\t\t\t     unsigned int order)\n{\n\t/* This piece of code contains several assumptions.\n\t * 1.  This is for device Rx, therefor a cold page is preferred.\n\t * 2.  The expectation is the user wants a compound page.\n\t * 3.  If requesting a order 0 page it will not be compound\n\t *     due to the check to see if order has a value in prep_new_page\n\t * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to\n\t *     code in gfp_to_alloc_flags that should be enforcing this.\n\t */\n\tgfp_mask |= __GFP_COMP | __GFP_MEMALLOC;\n\n\treturn alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);\n}\n\nstatic inline struct page *dev_alloc_pages(unsigned int order)\n{\n\treturn __dev_alloc_pages(GFP_ATOMIC | __GFP_NOWARN, order);\n}\n\n/**\n * __dev_alloc_page - allocate a page for network Rx\n * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx\n *\n * Allocate a new page.\n *\n * %NULL is returned if there is no free memory.\n */\nstatic inline struct page *__dev_alloc_page(gfp_t gfp_mask)\n{\n\treturn __dev_alloc_pages(gfp_mask, 0);\n}\n\nstatic inline struct page *dev_alloc_page(void)\n{\n\treturn dev_alloc_pages(0);\n}\n\n/**\n *\tskb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page\n *\t@page: The page that was allocated from skb_alloc_page\n *\t@skb: The skb that may need pfmemalloc set\n */\nstatic inline void skb_propagate_pfmemalloc(struct page *page,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc = true;\n}\n\n/**\n * skb_frag_off() - Returns the offset of a skb fragment\n * @frag: the paged fragment\n */\nstatic inline unsigned int skb_frag_off(const skb_frag_t *frag)\n{\n\treturn frag->bv_offset;\n}\n\n/**\n * skb_frag_off_add() - Increments the offset of a skb fragment by @delta\n * @frag: skb fragment\n * @delta: value to add\n */\nstatic inline void skb_frag_off_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_offset += delta;\n}\n\n/**\n * skb_frag_off_set() - Sets the offset of a skb fragment\n * @frag: skb fragment\n * @offset: offset of fragment\n */\nstatic inline void skb_frag_off_set(skb_frag_t *frag, unsigned int offset)\n{\n\tfrag->bv_offset = offset;\n}\n\n/**\n * skb_frag_off_copy() - Sets the offset of a skb fragment from another fragment\n * @fragto: skb fragment where offset is set\n * @fragfrom: skb fragment offset is copied from\n */\nstatic inline void skb_frag_off_copy(skb_frag_t *fragto,\n\t\t\t\t     const skb_frag_t *fragfrom)\n{\n\tfragto->bv_offset = fragfrom->bv_offset;\n}\n\n/**\n * skb_frag_page - retrieve the page referred to by a paged fragment\n * @frag: the paged fragment\n *\n * Returns the &struct page associated with @frag.\n */\nstatic inline struct page *skb_frag_page(const skb_frag_t *frag)\n{\n\treturn frag->bv_page;\n}\n\n/**\n * __skb_frag_ref - take an addition reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Takes an additional reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_ref(skb_frag_t *frag)\n{\n\tget_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_ref - take an addition reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset.\n *\n * Takes an additional reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_ref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_ref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * __skb_frag_unref - release a reference on a paged fragment.\n * @frag: the paged fragment\n *\n * Releases a reference on the paged fragment @frag.\n */\nstatic inline void __skb_frag_unref(skb_frag_t *frag)\n{\n\tput_page(skb_frag_page(frag));\n}\n\n/**\n * skb_frag_unref - release a reference on a paged fragment of an skb.\n * @skb: the buffer\n * @f: the fragment offset\n *\n * Releases a reference on the @f'th paged fragment of @skb.\n */\nstatic inline void skb_frag_unref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_unref(&skb_shinfo(skb)->frags[f]);\n}\n\n/**\n * skb_frag_address - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. The page must already\n * be mapped.\n */\nstatic inline void *skb_frag_address(const skb_frag_t *frag)\n{\n\treturn page_address(skb_frag_page(frag)) + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_address_safe - gets the address of the data contained in a paged fragment\n * @frag: the paged fragment buffer\n *\n * Returns the address of the data within @frag. Checks that the page\n * is mapped and returns %NULL otherwise.\n */\nstatic inline void *skb_frag_address_safe(const skb_frag_t *frag)\n{\n\tvoid *ptr = page_address(skb_frag_page(frag));\n\tif (unlikely(!ptr))\n\t\treturn NULL;\n\n\treturn ptr + skb_frag_off(frag);\n}\n\n/**\n * skb_frag_page_copy() - sets the page in a fragment from another fragment\n * @fragto: skb fragment where page is set\n * @fragfrom: skb fragment page is copied from\n */\nstatic inline void skb_frag_page_copy(skb_frag_t *fragto,\n\t\t\t\t      const skb_frag_t *fragfrom)\n{\n\tfragto->bv_page = fragfrom->bv_page;\n}\n\n/**\n * __skb_frag_set_page - sets the page contained in a paged fragment\n * @frag: the paged fragment\n * @page: the page to set\n *\n * Sets the fragment @frag to contain @page.\n */\nstatic inline void __skb_frag_set_page(skb_frag_t *frag, struct page *page)\n{\n\tfrag->bv_page = page;\n}\n\n/**\n * skb_frag_set_page - sets the page contained in a paged fragment of an skb\n * @skb: the buffer\n * @f: the fragment offset\n * @page: the page to set\n *\n * Sets the @f'th fragment of @skb to contain @page.\n */\nstatic inline void skb_frag_set_page(struct sk_buff *skb, int f,\n\t\t\t\t     struct page *page)\n{\n\t__skb_frag_set_page(&skb_shinfo(skb)->frags[f], page);\n}\n\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t prio);\n\n/**\n * skb_frag_dma_map - maps a paged fragment via the DMA API\n * @dev: the device to map the fragment to\n * @frag: the paged fragment to map\n * @offset: the offset within the fragment (starting at the\n *          fragment's own offset)\n * @size: the number of bytes to map\n * @dir: the direction of the mapping (``PCI_DMA_*``)\n *\n * Maps the page associated with @frag to @device.\n */\nstatic inline dma_addr_t skb_frag_dma_map(struct device *dev,\n\t\t\t\t\t  const skb_frag_t *frag,\n\t\t\t\t\t  size_t offset, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\treturn dma_map_page(dev, skb_frag_page(frag),\n\t\t\t    skb_frag_off(frag) + offset, size, dir);\n}\n\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\n\t\t\t\t\tgfp_t gfp_mask)\n{\n\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);\n}\n\n\nstatic inline struct sk_buff *pskb_copy_for_clone(struct sk_buff *skb,\n\t\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, skb_headroom(skb), gfp_mask, true);\n}\n\n\n/**\n *\tskb_clone_writable - is the header of a clone writable\n *\t@skb: buffer to check\n *\t@len: length up to which to write\n *\n *\tReturns true if modifying the header part of the cloned buffer\n *\tdoes not requires the data to be copied.\n */\nstatic inline int skb_clone_writable(const struct sk_buff *skb, unsigned int len)\n{\n\treturn !skb_header_cloned(skb) &&\n\t       skb_headroom(skb) + len <= skb->hdr_len;\n}\n\nstatic inline int skb_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\treturn skb_cloned(skb) && !skb_clone_writable(skb, write_len) &&\n\t       pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\n\t\t\t    int cloned)\n{\n\tint delta = 0;\n\n\tif (headroom > skb_headroom(skb))\n\t\tdelta = headroom - skb_headroom(skb);\n\n\tif (delta || cloned)\n\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\n\t\t\t\t\tGFP_ATOMIC);\n\treturn 0;\n}\n\n/**\n *\tskb_cow - copy header of skb when it is required\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tIf the skb passed lacks sufficient headroom or its data part\n *\tis shared, data is reallocated. If reallocation fails, an error\n *\tis returned and original skb is not changed.\n *\n *\tThe result is skb with writable area skb->head...skb->tail\n *\tand at least @headroom of space at head.\n */\nstatic inline int skb_cow(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_cloned(skb));\n}\n\n/**\n *\tskb_cow_head - skb_cow but only making the head writable\n *\t@skb: buffer to cow\n *\t@headroom: needed headroom\n *\n *\tThis function is identical to skb_cow except that we replace the\n *\tskb_cloned check by skb_header_cloned.  It should be used when\n *\tyou only need to push on some header and do not need to modify\n *\tthe data.\n */\nstatic inline int skb_cow_head(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_header_cloned(skb));\n}\n\n/**\n *\tskb_padto\t- pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int skb_padto(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int size = skb->len;\n\tif (likely(size >= len))\n\t\treturn 0;\n\treturn skb_pad(skb, len - size);\n}\n\n/**\n *\t__skb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\t@free_on_error: free buffer on error\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error if @free_on_error is true.\n */\nstatic inline int __must_check __skb_put_padto(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int len,\n\t\t\t\t\t       bool free_on_error)\n{\n\tunsigned int size = skb->len;\n\n\tif (unlikely(size < len)) {\n\t\tlen -= size;\n\t\tif (__skb_pad(skb, len, free_on_error))\n\t\t\treturn -ENOMEM;\n\t\t__skb_put(skb, len);\n\t}\n\treturn 0;\n}\n\n/**\n *\tskb_put_padto - increase size and pad an skbuff up to a minimal size\n *\t@skb: buffer to pad\n *\t@len: minimal length\n *\n *\tPads up a buffer to ensure the trailing bytes exist and are\n *\tblanked. If the buffer already contains sufficient data it\n *\tis untouched. Otherwise it is extended. Returns zero on\n *\tsuccess. The skb is freed on error.\n */\nstatic inline int __must_check skb_put_padto(struct sk_buff *skb, unsigned int len)\n{\n\treturn __skb_put_padto(skb, len, true);\n}\n\nstatic inline int skb_add_data(struct sk_buff *skb,\n\t\t\t       struct iov_iter *from, int copy)\n{\n\tconst int off = skb->len;\n\n\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t__wsum csum = 0;\n\t\tif (csum_and_copy_from_iter_full(skb_put(skb, copy), copy,\n\t\t\t\t\t         &csum, from)) {\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, off);\n\t\t\treturn 0;\n\t\t}\n\t} else if (copy_from_iter_full(skb_put(skb, copy), copy, from))\n\t\treturn 0;\n\n\t__skb_trim(skb, off);\n\treturn -EFAULT;\n}\n\nstatic inline bool skb_can_coalesce(struct sk_buff *skb, int i,\n\t\t\t\t    const struct page *page, int off)\n{\n\tif (skb_zcopy(skb))\n\t\treturn false;\n\tif (i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];\n\n\t\treturn page == skb_frag_page(frag) &&\n\t\t       off == skb_frag_off(frag) + skb_frag_size(frag);\n\t}\n\treturn false;\n}\n\nstatic inline int __skb_linearize(struct sk_buff *skb)\n{\n\treturn __pskb_pull_tail(skb, skb->data_len) ? 0 : -ENOMEM;\n}\n\n/**\n *\tskb_linearize - convert paged skb to linear one\n *\t@skb: buffer to linarize\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? __skb_linearize(skb) : 0;\n}\n\n/**\n * skb_has_shared_frag - can any frag be overwritten\n * @skb: buffer to test\n *\n * Return true if the skb has at least one frag that might be modified\n * by an external entity (as in vmsplice()/sendfile())\n */\nstatic inline bool skb_has_shared_frag(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       skb_shinfo(skb)->flags & SKBFL_SHARED_FRAG;\n}\n\n/**\n *\tskb_linearize_cow - make sure skb is linear and writable\n *\t@skb: buffer to process\n *\n *\tIf there is no free memory -ENOMEM is returned, otherwise zero\n *\tis returned and the old skb data released.\n */\nstatic inline int skb_linearize_cow(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) || skb_cloned(skb) ?\n\t       __skb_linearize(skb) : 0;\n}\n\nstatic __always_inline void\n__skb_postpull_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) < 0)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n *\tskb_postpull_rcsum - update checksum for received skb after pull\n *\t@skb: buffer to update\n *\t@start: start of data before pull\n *\t@len: length of data pulled\n *\n *\tAfter doing a pull on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum, or set ip_summed to\n *\tCHECKSUM_NONE so that it can be recomputed from scratch.\n */\nstatic inline void skb_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpull_rcsum(skb, start, len, 0);\n}\n\nstatic __always_inline void\n__skb_postpush_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_add(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n}\n\n/**\n *\tskb_postpush_rcsum - update checksum for received skb after push\n *\t@skb: buffer to update\n *\t@start: start of data after push\n *\t@len: length of data pushed\n *\n *\tAfter doing a push on a received packet, you need to call this to\n *\tupdate the CHECKSUM_COMPLETE checksum.\n */\nstatic inline void skb_postpush_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpush_rcsum(skb, start, len, 0);\n}\n\nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len);\n\n/**\n *\tskb_push_rcsum - push skb and update receive checksum\n *\t@skb: buffer to update\n *\t@len: length of data pulled\n *\n *\tThis function performs an skb_push on the packet and updates\n *\tthe CHECKSUM_COMPLETE checksum.  It should be used on\n *\treceive path processing instead of skb_push unless you know\n *\tthat the checksum difference is zero (e.g., a valid IP header)\n *\tor you are setting ip_summed to CHECKSUM_NONE.\n */\nstatic inline void *skb_push_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tskb_push(skb, len);\n\tskb_postpush_rcsum(skb, skb->data, len);\n\treturn skb->data;\n}\n\nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len);\n/**\n *\tpskb_trim_rcsum - trim received skb and update checksum\n *\t@skb: buffer to trim\n *\t@len: new length\n *\n *\tThis is exactly the same as pskb_trim except that it ensures the\n *\tchecksum of received packets are still valid after the operation.\n *\tIt can change skb pointers.\n */\n\nstatic inline int pskb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len >= skb->len))\n\t\treturn 0;\n\treturn pskb_trim_rcsum_slow(skb, len);\n}\n\nstatic inline int __skb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int __skb_grow_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\treturn __skb_grow(skb, len);\n}\n\n#define rb_to_skb(rb) rb_entry_safe(rb, struct sk_buff, rbnode)\n#define skb_rb_first(root) rb_to_skb(rb_first(root))\n#define skb_rb_last(root)  rb_to_skb(rb_last(root))\n#define skb_rb_next(skb)   rb_to_skb(rb_next(&(skb)->rbnode))\n#define skb_rb_prev(skb)   rb_to_skb(rb_prev(&(skb)->rbnode))\n\n#define skb_queue_walk(queue, skb) \\\n\t\tfor (skb = (queue)->next;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_queue_walk_safe(queue, skb, tmp)\t\t\t\t\t\\\n\t\tfor (skb = (queue)->next, tmp = skb->next;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_walk_from(queue, skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != (struct sk_buff *)(queue);\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_rbtree_walk(skb, root)\t\t\t\t\t\t\\\n\t\tfor (skb = skb_rb_first(root); skb != NULL;\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from(skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != NULL;\t\t\t\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from_safe(skb, tmp)\t\t\t\t\t\\\n\t\tfor (; tmp = skb ? skb_rb_next(skb) : NULL, (skb != NULL);\t\\\n\t\t     skb = tmp)\n\n#define skb_queue_walk_from_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (tmp = skb->next;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_reverse_walk(queue, skb) \\\n\t\tfor (skb = (queue)->prev;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->prev)\n\n#define skb_queue_reverse_walk_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (skb = (queue)->prev, tmp = skb->prev;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\n#define skb_queue_reverse_walk_from_safe(queue, skb, tmp)\t\t\t\\\n\t\tfor (tmp = skb->prev;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\nstatic inline bool skb_has_frag_list(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->frag_list != NULL;\n}\n\nstatic inline void skb_frag_list_init(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->frag_list = NULL;\n}\n\n#define skb_walk_frags(skb, iter)\t\\\n\tfor (iter = skb_shinfo(skb)->frag_list; iter; iter = iter->next)\n\n\nint __skb_wait_for_more_packets(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t\tint *err, long *timeo_p,\n\t\t\t\tconst struct sk_buff *skb);\nstruct sk_buff *__skb_try_recv_from_queue(struct sock *sk,\n\t\t\t\t\t  struct sk_buff_head *queue,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  int *off, int *err,\n\t\t\t\t\t  struct sk_buff **last);\nstruct sk_buff *__skb_try_recv_datagram(struct sock *sk,\n\t\t\t\t\tstruct sk_buff_head *queue,\n\t\t\t\t\tunsigned int flags, int *off, int *err,\n\t\t\t\t\tstruct sk_buff **last);\nstruct sk_buff *__skb_recv_datagram(struct sock *sk,\n\t\t\t\t    struct sk_buff_head *sk_queue,\n\t\t\t\t    unsigned int flags, int *off, int *err);\nstruct sk_buff *skb_recv_datagram(struct sock *sk, unsigned flags, int noblock,\n\t\t\t\t  int *err);\n__poll_t datagram_poll(struct file *file, struct socket *sock,\n\t\t\t   struct poll_table_struct *wait);\nint skb_copy_datagram_iter(const struct sk_buff *from, int offset,\n\t\t\t   struct iov_iter *to, int size);\nstatic inline int skb_copy_datagram_msg(const struct sk_buff *from, int offset,\n\t\t\t\t\tstruct msghdr *msg, int size)\n{\n\treturn skb_copy_datagram_iter(from, offset, &msg->msg_iter, size);\n}\nint skb_copy_and_csum_datagram_msg(struct sk_buff *skb, int hlen,\n\t\t\t\t   struct msghdr *msg);\nint skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len,\n\t\t\t   struct ahash_request *hash);\nint skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,\n\t\t\t\t struct iov_iter *from, int len);\nint zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *frm);\nvoid skb_free_datagram(struct sock *sk, struct sk_buff *skb);\nvoid __skb_free_datagram_locked(struct sock *sk, struct sk_buff *skb, int len);\nstatic inline void skb_free_datagram_locked(struct sock *sk,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\t__skb_free_datagram_locked(sk, skb, 0);\n}\nint skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags);\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len);\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len);\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset, u8 *to,\n\t\t\t      int len);\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int len,\n\t\t    unsigned int flags);\nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len);\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to);\nunsigned int skb_zerocopy_headlen(const struct sk_buff *from);\nint skb_zerocopy(struct sk_buff *to, struct sk_buff *from,\n\t\t int len, int hlen);\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len);\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen);\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet);\nbool skb_gso_validate_network_len(const struct sk_buff *skb, unsigned int mtu);\nbool skb_gso_validate_mac_len(const struct sk_buff *skb, unsigned int len);\nstruct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);\nstruct sk_buff *skb_segment_list(struct sk_buff *skb, netdev_features_t features,\n\t\t\t\t unsigned int offset);\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb);\nint skb_ensure_writable(struct sk_buff *skb, int write_len);\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci);\nint skb_vlan_pop(struct sk_buff *skb);\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci);\nint skb_eth_pop(struct sk_buff *skb);\nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src);\nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet);\nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet);\nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse);\nint skb_mpls_dec_ttl(struct sk_buff *skb);\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off, int to_copy,\n\t\t\t     gfp_t gfp);\n\nstatic inline int memcpy_from_msg(void *data, struct msghdr *msg, int len)\n{\n\treturn copy_from_iter_full(data, len, &msg->msg_iter) ? 0 : -EFAULT;\n}\n\nstatic inline int memcpy_to_msg(struct msghdr *msg, void *data, int len)\n{\n\treturn copy_to_iter(data, len, &msg->msg_iter) == len ? 0 : -EFAULT;\n}\n\nstruct skb_checksum_ops {\n\t__wsum (*update)(const void *mem, int len, __wsum wsum);\n\t__wsum (*combine)(__wsum csum, __wsum csum2, int offset, int len);\n};\n\nextern const struct skb_checksum_ops *crc32c_csum_stub __read_mostly;\n\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops);\n__wsum skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t    __wsum csum);\n\nstatic inline void * __must_check\n__skb_header_pointer(const struct sk_buff *skb, int offset,\n\t\t     int len, void *data, int hlen, void *buffer)\n{\n\tif (hlen - offset >= len)\n\t\treturn data + offset;\n\n\tif (!skb ||\n\t    skb_copy_bits(skb, offset, buffer, len) < 0)\n\t\treturn NULL;\n\n\treturn buffer;\n}\n\nstatic inline void * __must_check\nskb_header_pointer(const struct sk_buff *skb, int offset, int len, void *buffer)\n{\n\treturn __skb_header_pointer(skb, offset, len, skb->data,\n\t\t\t\t    skb_headlen(skb), buffer);\n}\n\n/**\n *\tskb_needs_linearize - check if we need to linearize a given skb\n *\t\t\t      depending on the given device features.\n *\t@skb: socket buffer to check\n *\t@features: net device features\n *\n *\tReturns true if either:\n *\t1. skb has frag_list and the device doesn't support FRAGLIST, or\n *\t2. skb is fragmented and the device does not support SG.\n */\nstatic inline bool skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       ((skb_has_frag_list(skb) && !(features & NETIF_F_FRAGLIST)) ||\n\t\t(skb_shinfo(skb)->nr_frags && !(features & NETIF_F_SG)));\n}\n\nstatic inline void skb_copy_from_linear_data(const struct sk_buff *skb,\n\t\t\t\t\t     void *to,\n\t\t\t\t\t     const unsigned int len)\n{\n\tmemcpy(to, skb->data, len);\n}\n\nstatic inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,\n\t\t\t\t\t\t    const int offset, void *to,\n\t\t\t\t\t\t    const unsigned int len)\n{\n\tmemcpy(to, skb->data + offset, len);\n}\n\nstatic inline void skb_copy_to_linear_data(struct sk_buff *skb,\n\t\t\t\t\t   const void *from,\n\t\t\t\t\t   const unsigned int len)\n{\n\tmemcpy(skb->data, from, len);\n}\n\nstatic inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,\n\t\t\t\t\t\t  const int offset,\n\t\t\t\t\t\t  const void *from,\n\t\t\t\t\t\t  const unsigned int len)\n{\n\tmemcpy(skb->data + offset, from, len);\n}\n\nvoid skb_init(void);\n\nstatic inline ktime_t skb_get_ktime(const struct sk_buff *skb)\n{\n\treturn skb->tstamp;\n}\n\n/**\n *\tskb_get_timestamp - get timestamp from a skb\n *\t@skb: skb to get stamp from\n *\t@stamp: pointer to struct __kernel_old_timeval to store stamp in\n *\n *\tTimestamps are stored in the skb as offsets to a base timestamp.\n *\tThis function converts the offset back to a struct timeval and stores\n *\tit in stamp.\n */\nstatic inline void skb_get_timestamp(const struct sk_buff *skb,\n\t\t\t\t     struct __kernel_old_timeval *stamp)\n{\n\t*stamp = ns_to_kernel_old_timeval(skb->tstamp);\n}\n\nstatic inline void skb_get_new_timestamp(const struct sk_buff *skb,\n\t\t\t\t\t struct __kernel_sock_timeval *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_usec = ts.tv_nsec / 1000;\n}\n\nstatic inline void skb_get_timestampns(const struct sk_buff *skb,\n\t\t\t\t       struct __kernel_old_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void skb_get_new_timestampns(const struct sk_buff *skb,\n\t\t\t\t\t   struct __kernel_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void __net_timestamp(struct sk_buff *skb)\n{\n\tskb->tstamp = ktime_get_real();\n}\n\nstatic inline ktime_t net_timedelta(ktime_t t)\n{\n\treturn ktime_sub(ktime_get_real(), t);\n}\n\nstatic inline ktime_t net_invalid_timestamp(void)\n{\n\treturn 0;\n}\n\nstatic inline u8 skb_metadata_len(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->meta_len;\n}\n\nstatic inline void *skb_metadata_end(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb);\n}\n\nstatic inline bool __skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\t  const struct sk_buff *skb_b,\n\t\t\t\t\t  u8 meta_len)\n{\n\tconst void *a = skb_metadata_end(skb_a);\n\tconst void *b = skb_metadata_end(skb_b);\n\t/* Using more efficient varaiant than plain call to memcmp(). */\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tu64 diffs = 0;\n\n\tswitch (meta_len) {\n#define __it(x, op) (x -= sizeof(u##op))\n#define __it_diff(a, b, op) (*(u##op *)__it(a, op)) ^ (*(u##op *)__it(b, op))\n\tcase 32: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 24: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 16: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  8: diffs |= __it_diff(a, b, 64);\n\t\tbreak;\n\tcase 28: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 20: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 12: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  4: diffs |= __it_diff(a, b, 32);\n\t\tbreak;\n\t}\n\treturn diffs;\n#else\n\treturn memcmp(a - meta_len, b - meta_len, meta_len);\n#endif\n}\n\nstatic inline bool skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\tconst struct sk_buff *skb_b)\n{\n\tu8 len_a = skb_metadata_len(skb_a);\n\tu8 len_b = skb_metadata_len(skb_b);\n\n\tif (!(len_a | len_b))\n\t\treturn false;\n\n\treturn len_a != len_b ?\n\t       true : __skb_metadata_differs(skb_a, skb_b, len_a);\n}\n\nstatic inline void skb_metadata_set(struct sk_buff *skb, u8 meta_len)\n{\n\tskb_shinfo(skb)->meta_len = meta_len;\n}\n\nstatic inline void skb_metadata_clear(struct sk_buff *skb)\n{\n\tskb_metadata_set(skb, 0);\n}\n\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb);\n\n#ifdef CONFIG_NETWORK_PHY_TIMESTAMPING\n\nvoid skb_clone_tx_timestamp(struct sk_buff *skb);\nbool skb_defer_rx_timestamp(struct sk_buff *skb);\n\n#else /* CONFIG_NETWORK_PHY_TIMESTAMPING */\n\nstatic inline void skb_clone_tx_timestamp(struct sk_buff *skb)\n{\n}\n\nstatic inline bool skb_defer_rx_timestamp(struct sk_buff *skb)\n{\n\treturn false;\n}\n\n#endif /* !CONFIG_NETWORK_PHY_TIMESTAMPING */\n\n/**\n * skb_complete_tx_timestamp() - deliver cloned skb with tx timestamps\n *\n * PHY drivers may accept clones of transmitted packets for\n * timestamping via their phy_driver.txtstamp method. These drivers\n * must call this function to return the skb back to the stack with a\n * timestamp.\n *\n * @skb: clone of the original outgoing packet\n * @hwtstamps: hardware time stamps\n *\n */\nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype);\n\n/**\n * skb_tstamp_tx - queue clone of skb with send time stamps\n * @orig_skb:\tthe original outgoing packet\n * @hwtstamps:\thardware time stamps, may be NULL if not available\n *\n * If the skb has a socket associated, then this function clones the\n * skb (thus sharing the actual data and optional structures), stores\n * the optional hardware time stamping information (if non NULL) or\n * generates a software time stamp (otherwise), then queues the clone\n * to the error queue of the socket.  Errors are silently ignored.\n */\nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps);\n\n/**\n * skb_tx_timestamp() - Driver hook for transmit timestamping\n *\n * Ethernet MAC Drivers should call this function in their hard_xmit()\n * function immediately before giving the sk_buff to the MAC hardware.\n *\n * Specifically, one should make absolutely sure that this function is\n * called before TX completion of this packet can trigger.  Otherwise\n * the packet could potentially already be freed.\n *\n * @skb: A socket buffer.\n */\nstatic inline void skb_tx_timestamp(struct sk_buff *skb)\n{\n\tskb_clone_tx_timestamp(skb);\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_SW_TSTAMP)\n\t\tskb_tstamp_tx(skb, NULL);\n}\n\n/**\n * skb_complete_wifi_ack - deliver skb with wifi status\n *\n * @skb: the original outgoing packet\n * @acked: ack status\n *\n */\nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len);\n__sum16 __skb_checksum_complete(struct sk_buff *skb);\n\nstatic inline int skb_csum_unnecessary(const struct sk_buff *skb)\n{\n\treturn ((skb->ip_summed == CHECKSUM_UNNECESSARY) ||\n\t\tskb->csum_valid ||\n\t\t(skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) >= 0));\n}\n\n/**\n *\tskb_checksum_complete - Calculate checksum of an entire packet\n *\t@skb: packet to process\n *\n *\tThis function calculates the checksum over the entire packet plus\n *\tthe value of skb->csum.  The latter can be used to supply the\n *\tchecksum of a pseudo header as used by TCP/UDP.  It returns the\n *\tchecksum.\n *\n *\tFor protocols that contain complete checksums such as ICMP/TCP/UDP,\n *\tthis function can be used to verify that checksum on received\n *\tpackets.  In that case the function should return zero if the\n *\tchecksum is correct.  In particular, this function will return zero\n *\tif skb->ip_summed is CHECKSUM_UNNECESSARY which indicates that the\n *\thardware has already verified the correctness of the checksum.\n */\nstatic inline __sum16 skb_checksum_complete(struct sk_buff *skb)\n{\n\treturn skb_csum_unnecessary(skb) ?\n\t       0 : __skb_checksum_complete(skb);\n}\n\nstatic inline void __skb_decr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level == 0)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\telse\n\t\t\tskb->csum_level--;\n\t}\n}\n\nstatic inline void __skb_incr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level < SKB_MAX_CSUM_LEVEL)\n\t\t\tskb->csum_level++;\n\t} else if (skb->ip_summed == CHECKSUM_NONE) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = 0;\n\t}\n}\n\nstatic inline void __skb_reset_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tskb->csum_level = 0;\n\t}\n}\n\n/* Check if we need to perform checksum complete validation.\n *\n * Returns true if checksum complete is needed, false otherwise\n * (either checksum is unnecessary or zero checksum is allowed).\n */\nstatic inline bool __skb_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t  bool zero_okay,\n\t\t\t\t\t\t  __sum16 check)\n{\n\tif (skb_csum_unnecessary(skb) || (zero_okay && !check)) {\n\t\tskb->csum_valid = 1;\n\t\t__skb_decr_checksum_unnecessary(skb);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n/* For small packets <= CHECKSUM_BREAK perform checksum complete directly\n * in checksum_init.\n */\n#define CHECKSUM_BREAK 76\n\n/* Unset checksum-complete\n *\n * Unset checksum complete can be done when packet is being modified\n * (uncompressed for instance) and checksum-complete value is\n * invalidated.\n */\nstatic inline void skb_checksum_complete_unset(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/* Validate (init) checksum based on checksum complete.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete. In the latter\n *\tcase the ip_summed will not be CHECKSUM_UNNECESSARY and the pseudo\n *\tchecksum is stored in skb->csum for use in __skb_checksum_complete\n *   non-zero: value of invalid checksum\n *\n */\nstatic inline __sum16 __skb_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t       bool complete,\n\t\t\t\t\t\t       __wsum psum)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_fold(csum_add(psum, skb->csum))) {\n\t\t\tskb->csum_valid = 1;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = psum;\n\n\tif (complete || skb->len <= CHECKSUM_BREAK) {\n\t\t__sum16 csum;\n\n\t\tcsum = __skb_checksum_complete(skb);\n\t\tskb->csum_valid = !csum;\n\t\treturn csum;\n\t}\n\n\treturn 0;\n}\n\nstatic inline __wsum null_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn 0;\n}\n\n/* Perform checksum validate (init). Note that this is a macro since we only\n * want to calculate the pseudo header which is an input function if necessary.\n * First we try to validate without any computation (checksum unnecessary) and\n * then calculate based on checksum complete calling the function to compute\n * pseudo header.\n *\n * Return values:\n *   0: checksum is validated or try to in skb_checksum_complete\n *   non-zero: value of invalid checksum\n */\n#define __skb_checksum_validate(skb, proto, complete,\t\t\t\\\n\t\t\t\tzero_okay, check, compute_pseudo)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tskb->csum_valid = 0;\t\t\t\t\t\t\\\n\tif (__skb_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_checksum_validate_complete(skb,\t\t\\\n\t\t\t\tcomplete, compute_pseudo(skb, proto));\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_checksum_init(skb, proto, compute_pseudo)\t\t\t\\\n\t__skb_checksum_validate(skb, proto, false, false, 0, compute_pseudo)\n\n#define skb_checksum_init_zero_check(skb, proto, check, compute_pseudo)\t\\\n\t__skb_checksum_validate(skb, proto, false, true, check, compute_pseudo)\n\n#define skb_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, false, 0, compute_pseudo)\n\n#define skb_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, true, check, compute_pseudo)\n\n#define skb_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_checksum_validate(skb, 0, true, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (skb->ip_summed == CHECKSUM_NONE && skb->csum_valid);\n}\n\nstatic inline void __skb_checksum_convert(struct sk_buff *skb, __wsum pseudo)\n{\n\tskb->csum = ~pseudo;\n\tskb->ip_summed = CHECKSUM_COMPLETE;\n}\n\n#define skb_checksum_try_convert(skb, proto, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_checksum_convert_check(skb))\t\t\t\t\\\n\t\t__skb_checksum_convert(skb, compute_pseudo(skb, proto)); \\\n} while (0)\n\nstatic inline void skb_remcsum_adjust_partial(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t      u16 start, u16 offset)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = ((unsigned char *)ptr + start) - skb->head;\n\tskb->csum_offset = offset - start;\n}\n\n/* Update skbuf and packet to reflect the remote checksum offload operation.\n * When called, ptr indicates the starting point for skb->csum when\n * ip_summed is CHECKSUM_COMPLETE. If we need create checksum complete\n * here, skb_postpull_rcsum is done so skb->csum start is ptr.\n */\nstatic inline void skb_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t       int start, int offset, bool nopartial)\n{\n\t__wsum delta;\n\n\tif (!nopartial) {\n\t\tskb_remcsum_adjust_partial(skb, ptr, start, offset);\n\t\treturn;\n\t}\n\n\t if (unlikely(skb->ip_summed != CHECKSUM_COMPLETE)) {\n\t\t__skb_checksum_complete(skb);\n\t\tskb_postpull_rcsum(skb, skb->data, ptr - (void *)skb->data);\n\t}\n\n\tdelta = remcsum_adjust(ptr, skb->csum, start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tskb->csum = csum_add(skb->csum, delta);\n}\n\nstatic inline struct nf_conntrack *skb_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn (void *)(skb->_nfct & NFCT_PTRMASK);\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline unsigned long skb_get_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn skb->_nfct;\n#else\n\treturn 0UL;\n#endif\n}\n\nstatic inline void skb_set_nfct(struct sk_buff *skb, unsigned long nfct)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tskb->_nfct = nfct;\n#endif\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\nenum skb_ext_id {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tSKB_EXT_BRIDGE_NF,\n#endif\n#ifdef CONFIG_XFRM\n\tSKB_EXT_SEC_PATH,\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\tTC_SKB_EXT,\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\tSKB_EXT_MPTCP,\n#endif\n\tSKB_EXT_NUM, /* must be last */\n};\n\n/**\n *\tstruct skb_ext - sk_buff extensions\n *\t@refcnt: 1 on allocation, deallocated on 0\n *\t@offset: offset to add to @data to obtain extension address\n *\t@chunks: size currently allocated, stored in SKB_EXT_ALIGN_SHIFT units\n *\t@data: start of extension data, variable sized\n *\n *\tNote: offsets/lengths are stored in chunks of 8 bytes, this allows\n *\tto use 'u8' types while allowing up to 2kb worth of extension data.\n */\nstruct skb_ext {\n\trefcount_t refcnt;\n\tu8 offset[SKB_EXT_NUM]; /* in chunks of 8 bytes */\n\tu8 chunks;\t\t/* same */\n\tchar data[] __aligned(8);\n};\n\nstruct skb_ext *__skb_ext_alloc(gfp_t flags);\nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext);\nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_put(struct skb_ext *ext);\n\nstatic inline void skb_ext_put(struct sk_buff *skb)\n{\n\tif (skb->active_extensions)\n\t\t__skb_ext_put(skb->extensions);\n}\n\nstatic inline void __skb_ext_copy(struct sk_buff *dst,\n\t\t\t\t  const struct sk_buff *src)\n{\n\tdst->active_extensions = src->active_extensions;\n\n\tif (src->active_extensions) {\n\t\tstruct skb_ext *ext = src->extensions;\n\n\t\trefcount_inc(&ext->refcnt);\n\t\tdst->extensions = ext;\n\t}\n}\n\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n\tskb_ext_put(dst);\n\t__skb_ext_copy(dst, src);\n}\n\nstatic inline bool __skb_ext_exist(const struct skb_ext *ext, enum skb_ext_id i)\n{\n\treturn !!ext->offset[i];\n}\n\nstatic inline bool skb_ext_exist(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\treturn skb->active_extensions & (1 << id);\n}\n\nstatic inline void skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id))\n\t\t__skb_ext_del(skb, id);\n}\n\nstatic inline void *skb_ext_find(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id)) {\n\t\tstruct skb_ext *ext = skb->extensions;\n\n\t\treturn (void *)ext + (ext->offset[id] << 3);\n\t}\n\n\treturn NULL;\n}\n\nstatic inline void skb_ext_reset(struct sk_buff *skb)\n{\n\tif (unlikely(skb->active_extensions)) {\n\t\t__skb_ext_put(skb->extensions);\n\t\tskb->active_extensions = 0;\n\t}\n}\n\nstatic inline bool skb_has_extensions(struct sk_buff *skb)\n{\n\treturn unlikely(skb->active_extensions);\n}\n#else\nstatic inline void skb_ext_put(struct sk_buff *skb) {}\nstatic inline void skb_ext_reset(struct sk_buff *skb) {}\nstatic inline void skb_ext_del(struct sk_buff *skb, int unused) {}\nstatic inline void __skb_ext_copy(struct sk_buff *d, const struct sk_buff *s) {}\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *s) {}\nstatic inline bool skb_has_extensions(struct sk_buff *skb) { return false; }\n#endif /* CONFIG_SKB_EXTENSIONS */\n\nstatic inline void nf_reset_ct(struct sk_buff *skb)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(skb));\n\tskb->_nfct = 0;\n#endif\n}\n\nstatic inline void nf_reset_trace(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tskb->nf_trace = 0;\n#endif\n}\n\nstatic inline void ipvs_reset(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IP_VS)\n\tskb->ipvs_property = 0;\n#endif\n}\n\n/* Note: This doesn't put any conntrack info in dst. */\nstatic inline void __nf_copy(struct sk_buff *dst, const struct sk_buff *src,\n\t\t\t     bool copy)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tdst->_nfct = src->_nfct;\n\tnf_conntrack_get(skb_nfct(src));\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || defined(CONFIG_NF_TABLES)\n\tif (copy)\n\t\tdst->nf_trace = src->nf_trace;\n#endif\n}\n\nstatic inline void nf_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(dst));\n#endif\n\t__nf_copy(dst, src, true);\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->secmark = from->secmark;\n}\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{\n\tskb->secmark = 0;\n}\n#else\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{ }\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{ }\n#endif\n\nstatic inline int secpath_exists(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_exist(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool skb_irq_freeable(const struct sk_buff *skb)\n{\n\treturn !skb->destructor &&\n\t\t!secpath_exists(skb) &&\n\t\t!skb_nfct(skb) &&\n\t\t!skb->_skb_refdst &&\n\t\t!skb_has_frag_list(skb);\n}\n\nstatic inline void skb_set_queue_mapping(struct sk_buff *skb, u16 queue_mapping)\n{\n\tskb->queue_mapping = queue_mapping;\n}\n\nstatic inline u16 skb_get_queue_mapping(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping;\n}\n\nstatic inline void skb_copy_queue_mapping(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->queue_mapping = from->queue_mapping;\n}\n\nstatic inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)\n{\n\tskb->queue_mapping = rx_queue + 1;\n}\n\nstatic inline u16 skb_get_rx_queue(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping - 1;\n}\n\nstatic inline bool skb_rx_queue_recorded(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping != 0;\n}\n\nstatic inline void skb_set_dst_pending_confirm(struct sk_buff *skb, u32 val)\n{\n\tskb->dst_pending_confirm = val;\n}\n\nstatic inline bool skb_get_dst_pending_confirm(const struct sk_buff *skb)\n{\n\treturn skb->dst_pending_confirm != 0;\n}\n\nstatic inline struct sec_path *skb_sec_path(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_find(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn NULL;\n#endif\n}\n\n/* Keeps track of mac header offset relative to skb->head.\n * It is useful for TSO of Tunneling protocol. e.g. GRE.\n * For non-tunnel skb it points to skb_mac_header() and for\n * tunnel skb it points to outer mac header.\n * Keeps track of level of encapsulation of network headers.\n */\nstruct skb_gso_cb {\n\tunion {\n\t\tint\tmac_offset;\n\t\tint\tdata_offset;\n\t};\n\tint\tencap_level;\n\t__wsum\tcsum;\n\t__u16\tcsum_start;\n};\n#define SKB_GSO_CB_OFFSET\t32\n#define SKB_GSO_CB(skb) ((struct skb_gso_cb *)((skb)->cb + SKB_GSO_CB_OFFSET))\n\nstatic inline int skb_tnl_header_len(const struct sk_buff *inner_skb)\n{\n\treturn (skb_mac_header(inner_skb) - inner_skb->head) -\n\t\tSKB_GSO_CB(inner_skb)->mac_offset;\n}\n\nstatic inline int gso_pskb_expand_head(struct sk_buff *skb, int extra)\n{\n\tint new_headroom, headroom;\n\tint ret;\n\n\theadroom = skb_headroom(skb);\n\tret = pskb_expand_head(skb, extra, 0, GFP_ATOMIC);\n\tif (ret)\n\t\treturn ret;\n\n\tnew_headroom = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->mac_offset += (new_headroom - headroom);\n\treturn 0;\n}\n\nstatic inline void gso_reset_checksum(struct sk_buff *skb, __wsum res)\n{\n\t/* Do not update partial checksums if remote checksum is enabled. */\n\tif (skb->remcsum_offload)\n\t\treturn;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = skb_checksum_start(skb) - skb->head;\n}\n\n/* Compute the checksum for a gso segment. First compute the checksum value\n * from the start of transport header to SKB_GSO_CB(skb)->csum_start, and\n * then add in skb->csum (checksum from csum_start to end of packet).\n * skb->csum and csum_start are then updated to reflect the checksum of the\n * resultant packet starting from the transport header-- the resultant checksum\n * is in the res argument (i.e. normally zero or ~ of checksum of a pseudo\n * header.\n */\nstatic inline __sum16 gso_make_checksum(struct sk_buff *skb, __wsum res)\n{\n\tunsigned char *csum_start = skb_transport_header(skb);\n\tint plen = (skb->head + SKB_GSO_CB(skb)->csum_start) - csum_start;\n\t__wsum partial = SKB_GSO_CB(skb)->csum;\n\n\tSKB_GSO_CB(skb)->csum = res;\n\tSKB_GSO_CB(skb)->csum_start = csum_start - skb->head;\n\n\treturn csum_fold(csum_partial(csum_start, plen, partial));\n}\n\nstatic inline bool skb_is_gso(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_size;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_v6(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_sctp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_SCTP;\n}\n\n/* Note: Should be called only if skb_is_gso(skb) is true */\nstatic inline bool skb_is_gso_tcp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6);\n}\n\nstatic inline void skb_gso_reset(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->gso_size = 0;\n\tskb_shinfo(skb)->gso_segs = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n}\n\nstatic inline void skb_increase_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 increment)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size += increment;\n}\n\nstatic inline void skb_decrease_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 decrement)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size -= decrement;\n}\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb);\n\nstatic inline bool skb_warn_if_lro(const struct sk_buff *skb)\n{\n\t/* LRO sets gso_size but not gso_type, whereas if GSO is really\n\t * wanted then gso_type will be set. */\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (skb_is_nonlinear(skb) && shinfo->gso_size != 0 &&\n\t    unlikely(shinfo->gso_type == 0)) {\n\t\t__skb_warn_lro_forwarding(skb);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void skb_forward_csum(struct sk_buff *skb)\n{\n\t/* Unfortunately we don't support this one.  Any brave souls? */\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n/**\n * skb_checksum_none_assert - make sure skb ip_summed is CHECKSUM_NONE\n * @skb: skb to check\n *\n * fresh skbs have their ip_summed set to CHECKSUM_NONE.\n * Instead of forcing ip_summed to CHECKSUM_NONE, we can\n * use this helper, to document places where we make this assertion.\n */\nstatic inline void skb_checksum_none_assert(const struct sk_buff *skb)\n{\n#ifdef DEBUG\n\tBUG_ON(skb->ip_summed != CHECKSUM_NONE);\n#endif\n}\n\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);\n\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate);\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb));\n\n/**\n * skb_head_is_locked - Determine if the skb->head is locked down\n * @skb: skb to check\n *\n * The head on skbs build around a head frag can be removed if they are\n * not cloned.  This function returns true if the skb head is locked down\n * due to either being allocated via kmalloc, or by being a clone with\n * multiple references to the head.\n */\nstatic inline bool skb_head_is_locked(const struct sk_buff *skb)\n{\n\treturn !skb->head_frag || skb_cloned(skb);\n}\n\n/* Local Checksum Offload.\n * Compute outer checksum based on the assumption that the\n * inner checksum will be offloaded later.\n * See Documentation/networking/checksum-offloads.rst for\n * explanation of how this works.\n * Fill in outer checksum adjustment (e.g. with sum of outer\n * pseudo-header) before calling.\n * Also ensure that inner checksum is in linear data area.\n */\nstatic inline __wsum lco_csum(struct sk_buff *skb)\n{\n\tunsigned char *csum_start = skb_checksum_start(skb);\n\tunsigned char *l4_hdr = skb_transport_header(skb);\n\t__wsum partial;\n\n\t/* Start with complement of inner checksum adjustment */\n\tpartial = ~csum_unfold(*(__force __sum16 *)(csum_start +\n\t\t\t\t\t\t    skb->csum_offset));\n\n\t/* Add in checksum of our headers (incl. outer checksum\n\t * adjustment filled in by caller) and return result.\n\t */\n\treturn csum_partial(l4_hdr, csum_start - l4_hdr, partial);\n}\n\nstatic inline bool skb_is_redirected(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\treturn skb->redirected;\n#else\n\treturn false;\n#endif\n}\n\nstatic inline void skb_set_redirected(struct sk_buff *skb, bool from_ingress)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 1;\n\tskb->from_ingress = from_ingress;\n\tif (skb->from_ingress)\n\t\tskb->tstamp = 0;\n#endif\n}\n\nstatic inline void skb_reset_redirect(struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_REDIRECT\n\tskb->redirected = 0;\n#endif\n}\n\nstatic inline void skb_set_kcov_handle(struct sk_buff *skb,\n\t\t\t\t       const u64 kcov_handle)\n{\n#ifdef CONFIG_KCOV\n\tskb->kcov_handle = kcov_handle;\n#endif\n}\n\nstatic inline u64 skb_get_kcov_handle(struct sk_buff *skb)\n{\n#ifdef CONFIG_KCOV\n\treturn skb->kcov_handle;\n#else\n\treturn 0;\n#endif\n}\n\n#endif\t/* __KERNEL__ */\n#endif\t/* _LINUX_SKBUFF_H */\n"}, "2": {"id": 2, "path": "/src/include/linux/byteorder/generic.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BYTEORDER_GENERIC_H\n#define _LINUX_BYTEORDER_GENERIC_H\n\n/*\n * linux/byteorder/generic.h\n * Generic Byte-reordering support\n *\n * The \"... p\" macros, like le64_to_cpup, can be used with pointers\n * to unaligned data, but there will be a performance penalty on \n * some architectures.  Use get_unaligned for unaligned data.\n *\n * Francois-Rene Rideau <fare@tunes.org> 19970707\n *    gathered all the good ideas from all asm-foo/byteorder.h into one file,\n *    cleaned them up.\n *    I hope it is compliant with non-GCC compilers.\n *    I decided to put __BYTEORDER_HAS_U64__ in byteorder.h,\n *    because I wasn't sure it would be ok to put it in types.h\n *    Upgraded it to 2.1.43\n * Francois-Rene Rideau <fare@tunes.org> 19971012\n *    Upgraded it to 2.1.57\n *    to please Linus T., replaced huge #ifdef's between little/big endian\n *    by nestedly #include'd files.\n * Francois-Rene Rideau <fare@tunes.org> 19971205\n *    Made it to 2.1.71; now a facelift:\n *    Put files under include/linux/byteorder/\n *    Split swab from generic support.\n *\n * TODO:\n *   = Regular kernel maintainers could also replace all these manual\n *    byteswap macros that remain, disseminated among drivers,\n *    after some grep or the sources...\n *   = Linus might want to rename all these macros and files to fit his taste,\n *    to fit his personal naming scheme.\n *   = it seems that a few drivers would also appreciate\n *    nybble swapping support...\n *   = every architecture could add their byteswap macro in asm/byteorder.h\n *    see how some architectures already do (i386, alpha, ppc, etc)\n *   = cpu_to_beXX and beXX_to_cpu might some day need to be well\n *    distinguished throughout the kernel. This is not the case currently,\n *    since little endian, big endian, and pdp endian machines needn't it.\n *    But this might be the case for, say, a port of Linux to 20/21 bit\n *    architectures (and F21 Linux addict around?).\n */\n\n/*\n * The following macros are to be defined by <asm/byteorder.h>:\n *\n * Conversion of long and short int between network and host format\n *\tntohl(__u32 x)\n *\tntohs(__u16 x)\n *\thtonl(__u32 x)\n *\thtons(__u16 x)\n * It seems that some programs (which? where? or perhaps a standard? POSIX?)\n * might like the above to be functions, not macros (why?).\n * if that's true, then detect them, and take measures.\n * Anyway, the measure is: define only ___ntohl as a macro instead,\n * and in a separate file, have\n * unsigned long inline ntohl(x){return ___ntohl(x);}\n *\n * The same for constant arguments\n *\t__constant_ntohl(__u32 x)\n *\t__constant_ntohs(__u16 x)\n *\t__constant_htonl(__u32 x)\n *\t__constant_htons(__u16 x)\n *\n * Conversion of XX-bit integers (16- 32- or 64-)\n * between native CPU format and little/big endian format\n * 64-bit stuff only defined for proper architectures\n *\tcpu_to_[bl]eXX(__uXX x)\n *\t[bl]eXX_to_cpu(__uXX x)\n *\n * The same, but takes a pointer to the value to convert\n *\tcpu_to_[bl]eXXp(__uXX x)\n *\t[bl]eXX_to_cpup(__uXX x)\n *\n * The same, but change in situ\n *\tcpu_to_[bl]eXXs(__uXX x)\n *\t[bl]eXX_to_cpus(__uXX x)\n *\n * See asm-foo/byteorder.h for examples of how to provide\n * architecture-optimized versions\n *\n */\n\n#define cpu_to_le64 __cpu_to_le64\n#define le64_to_cpu __le64_to_cpu\n#define cpu_to_le32 __cpu_to_le32\n#define le32_to_cpu __le32_to_cpu\n#define cpu_to_le16 __cpu_to_le16\n#define le16_to_cpu __le16_to_cpu\n#define cpu_to_be64 __cpu_to_be64\n#define be64_to_cpu __be64_to_cpu\n#define cpu_to_be32 __cpu_to_be32\n#define be32_to_cpu __be32_to_cpu\n#define cpu_to_be16 __cpu_to_be16\n#define be16_to_cpu __be16_to_cpu\n#define cpu_to_le64p __cpu_to_le64p\n#define le64_to_cpup __le64_to_cpup\n#define cpu_to_le32p __cpu_to_le32p\n#define le32_to_cpup __le32_to_cpup\n#define cpu_to_le16p __cpu_to_le16p\n#define le16_to_cpup __le16_to_cpup\n#define cpu_to_be64p __cpu_to_be64p\n#define be64_to_cpup __be64_to_cpup\n#define cpu_to_be32p __cpu_to_be32p\n#define be32_to_cpup __be32_to_cpup\n#define cpu_to_be16p __cpu_to_be16p\n#define be16_to_cpup __be16_to_cpup\n#define cpu_to_le64s __cpu_to_le64s\n#define le64_to_cpus __le64_to_cpus\n#define cpu_to_le32s __cpu_to_le32s\n#define le32_to_cpus __le32_to_cpus\n#define cpu_to_le16s __cpu_to_le16s\n#define le16_to_cpus __le16_to_cpus\n#define cpu_to_be64s __cpu_to_be64s\n#define be64_to_cpus __be64_to_cpus\n#define cpu_to_be32s __cpu_to_be32s\n#define be32_to_cpus __be32_to_cpus\n#define cpu_to_be16s __cpu_to_be16s\n#define be16_to_cpus __be16_to_cpus\n\n/*\n * They have to be macros in order to do the constant folding\n * correctly - if the argument passed into a inline function\n * it is no longer constant according to gcc..\n */\n\n#undef ntohl\n#undef ntohs\n#undef htonl\n#undef htons\n\n#define ___htonl(x) __cpu_to_be32(x)\n#define ___htons(x) __cpu_to_be16(x)\n#define ___ntohl(x) __be32_to_cpu(x)\n#define ___ntohs(x) __be16_to_cpu(x)\n\n#define htonl(x) ___htonl(x)\n#define ntohl(x) ___ntohl(x)\n#define htons(x) ___htons(x)\n#define ntohs(x) ___ntohs(x)\n\nstatic inline void le16_add_cpu(__le16 *var, u16 val)\n{\n\t*var = cpu_to_le16(le16_to_cpu(*var) + val);\n}\n\nstatic inline void le32_add_cpu(__le32 *var, u32 val)\n{\n\t*var = cpu_to_le32(le32_to_cpu(*var) + val);\n}\n\nstatic inline void le64_add_cpu(__le64 *var, u64 val)\n{\n\t*var = cpu_to_le64(le64_to_cpu(*var) + val);\n}\n\n/* XXX: this stuff can be optimized */\nstatic inline void le32_to_cpu_array(u32 *buf, unsigned int words)\n{\n\twhile (words--) {\n\t\t__le32_to_cpus(buf);\n\t\tbuf++;\n\t}\n}\n\nstatic inline void cpu_to_le32_array(u32 *buf, unsigned int words)\n{\n\twhile (words--) {\n\t\t__cpu_to_le32s(buf);\n\t\tbuf++;\n\t}\n}\n\nstatic inline void be16_add_cpu(__be16 *var, u16 val)\n{\n\t*var = cpu_to_be16(be16_to_cpu(*var) + val);\n}\n\nstatic inline void be32_add_cpu(__be32 *var, u32 val)\n{\n\t*var = cpu_to_be32(be32_to_cpu(*var) + val);\n}\n\nstatic inline void be64_add_cpu(__be64 *var, u64 val)\n{\n\t*var = cpu_to_be64(be64_to_cpu(*var) + val);\n}\n\nstatic inline void cpu_to_be32_array(__be32 *dst, const u32 *src, size_t len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++)\n\t\tdst[i] = cpu_to_be32(src[i]);\n}\n\nstatic inline void be32_to_cpu_array(u32 *dst, const __be32 *src, size_t len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++)\n\t\tdst[i] = be32_to_cpu(src[i]);\n}\n\n#endif /* _LINUX_BYTEORDER_GENERIC_H */\n"}, "3": {"id": 3, "path": "/src/include/uapi/linux/byteorder/little_endian.h", "content": "/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */\n#ifndef _UAPI_LINUX_BYTEORDER_LITTLE_ENDIAN_H\n#define _UAPI_LINUX_BYTEORDER_LITTLE_ENDIAN_H\n\n#ifndef __LITTLE_ENDIAN\n#define __LITTLE_ENDIAN 1234\n#endif\n#ifndef __LITTLE_ENDIAN_BITFIELD\n#define __LITTLE_ENDIAN_BITFIELD\n#endif\n\n#include <linux/types.h>\n#include <linux/swab.h>\n\n#define __constant_htonl(x) ((__force __be32)___constant_swab32((x)))\n#define __constant_ntohl(x) ___constant_swab32((__force __be32)(x))\n#define __constant_htons(x) ((__force __be16)___constant_swab16((x)))\n#define __constant_ntohs(x) ___constant_swab16((__force __be16)(x))\n#define __constant_cpu_to_le64(x) ((__force __le64)(__u64)(x))\n#define __constant_le64_to_cpu(x) ((__force __u64)(__le64)(x))\n#define __constant_cpu_to_le32(x) ((__force __le32)(__u32)(x))\n#define __constant_le32_to_cpu(x) ((__force __u32)(__le32)(x))\n#define __constant_cpu_to_le16(x) ((__force __le16)(__u16)(x))\n#define __constant_le16_to_cpu(x) ((__force __u16)(__le16)(x))\n#define __constant_cpu_to_be64(x) ((__force __be64)___constant_swab64((x)))\n#define __constant_be64_to_cpu(x) ___constant_swab64((__force __u64)(__be64)(x))\n#define __constant_cpu_to_be32(x) ((__force __be32)___constant_swab32((x)))\n#define __constant_be32_to_cpu(x) ___constant_swab32((__force __u32)(__be32)(x))\n#define __constant_cpu_to_be16(x) ((__force __be16)___constant_swab16((x)))\n#define __constant_be16_to_cpu(x) ___constant_swab16((__force __u16)(__be16)(x))\n#define __cpu_to_le64(x) ((__force __le64)(__u64)(x))\n#define __le64_to_cpu(x) ((__force __u64)(__le64)(x))\n#define __cpu_to_le32(x) ((__force __le32)(__u32)(x))\n#define __le32_to_cpu(x) ((__force __u32)(__le32)(x))\n#define __cpu_to_le16(x) ((__force __le16)(__u16)(x))\n#define __le16_to_cpu(x) ((__force __u16)(__le16)(x))\n#define __cpu_to_be64(x) ((__force __be64)__swab64((x)))\n#define __be64_to_cpu(x) __swab64((__force __u64)(__be64)(x))\n#define __cpu_to_be32(x) ((__force __be32)__swab32((x)))\n#define __be32_to_cpu(x) __swab32((__force __u32)(__be32)(x))\n#define __cpu_to_be16(x) ((__force __be16)__swab16((x)))\n#define __be16_to_cpu(x) __swab16((__force __u16)(__be16)(x))\n\nstatic __always_inline __le64 __cpu_to_le64p(const __u64 *p)\n{\n\treturn (__force __le64)*p;\n}\nstatic __always_inline __u64 __le64_to_cpup(const __le64 *p)\n{\n\treturn (__force __u64)*p;\n}\nstatic __always_inline __le32 __cpu_to_le32p(const __u32 *p)\n{\n\treturn (__force __le32)*p;\n}\nstatic __always_inline __u32 __le32_to_cpup(const __le32 *p)\n{\n\treturn (__force __u32)*p;\n}\nstatic __always_inline __le16 __cpu_to_le16p(const __u16 *p)\n{\n\treturn (__force __le16)*p;\n}\nstatic __always_inline __u16 __le16_to_cpup(const __le16 *p)\n{\n\treturn (__force __u16)*p;\n}\nstatic __always_inline __be64 __cpu_to_be64p(const __u64 *p)\n{\n\treturn (__force __be64)__swab64p(p);\n}\nstatic __always_inline __u64 __be64_to_cpup(const __be64 *p)\n{\n\treturn __swab64p((__u64 *)p);\n}\nstatic __always_inline __be32 __cpu_to_be32p(const __u32 *p)\n{\n\treturn (__force __be32)__swab32p(p);\n}\nstatic __always_inline __u32 __be32_to_cpup(const __be32 *p)\n{\n\treturn __swab32p((__u32 *)p);\n}\nstatic __always_inline __be16 __cpu_to_be16p(const __u16 *p)\n{\n\treturn (__force __be16)__swab16p(p);\n}\nstatic __always_inline __u16 __be16_to_cpup(const __be16 *p)\n{\n\treturn __swab16p((__u16 *)p);\n}\n#define __cpu_to_le64s(x) do { (void)(x); } while (0)\n#define __le64_to_cpus(x) do { (void)(x); } while (0)\n#define __cpu_to_le32s(x) do { (void)(x); } while (0)\n#define __le32_to_cpus(x) do { (void)(x); } while (0)\n#define __cpu_to_le16s(x) do { (void)(x); } while (0)\n#define __le16_to_cpus(x) do { (void)(x); } while (0)\n#define __cpu_to_be64s(x) __swab64s((x))\n#define __be64_to_cpus(x) __swab64s((x))\n#define __cpu_to_be32s(x) __swab32s((x))\n#define __be32_to_cpus(x) __swab32s((x))\n#define __cpu_to_be16s(x) __swab16s((x))\n#define __be16_to_cpus(x) __swab16s((x))\n\n\n#endif /* _UAPI_LINUX_BYTEORDER_LITTLE_ENDIAN_H */\n"}, "4": {"id": 4, "path": "/src/include/uapi/linux/swab.h", "content": "/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */\n#ifndef _UAPI_LINUX_SWAB_H\n#define _UAPI_LINUX_SWAB_H\n\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <asm/bitsperlong.h>\n#include <asm/swab.h>\n\n/*\n * casts are necessary for constants, because we never know how for sure\n * how U/UL/ULL map to __u16, __u32, __u64. At least not in a portable way.\n */\n#define ___constant_swab16(x) ((__u16)(\t\t\t\t\\\n\t(((__u16)(x) & (__u16)0x00ffU) << 8) |\t\t\t\\\n\t(((__u16)(x) & (__u16)0xff00U) >> 8)))\n\n#define ___constant_swab32(x) ((__u32)(\t\t\t\t\\\n\t(((__u32)(x) & (__u32)0x000000ffUL) << 24) |\t\t\\\n\t(((__u32)(x) & (__u32)0x0000ff00UL) <<  8) |\t\t\\\n\t(((__u32)(x) & (__u32)0x00ff0000UL) >>  8) |\t\t\\\n\t(((__u32)(x) & (__u32)0xff000000UL) >> 24)))\n\n#define ___constant_swab64(x) ((__u64)(\t\t\t\t\\\n\t(((__u64)(x) & (__u64)0x00000000000000ffULL) << 56) |\t\\\n\t(((__u64)(x) & (__u64)0x000000000000ff00ULL) << 40) |\t\\\n\t(((__u64)(x) & (__u64)0x0000000000ff0000ULL) << 24) |\t\\\n\t(((__u64)(x) & (__u64)0x00000000ff000000ULL) <<  8) |\t\\\n\t(((__u64)(x) & (__u64)0x000000ff00000000ULL) >>  8) |\t\\\n\t(((__u64)(x) & (__u64)0x0000ff0000000000ULL) >> 24) |\t\\\n\t(((__u64)(x) & (__u64)0x00ff000000000000ULL) >> 40) |\t\\\n\t(((__u64)(x) & (__u64)0xff00000000000000ULL) >> 56)))\n\n#define ___constant_swahw32(x) ((__u32)(\t\t\t\\\n\t(((__u32)(x) & (__u32)0x0000ffffUL) << 16) |\t\t\\\n\t(((__u32)(x) & (__u32)0xffff0000UL) >> 16)))\n\n#define ___constant_swahb32(x) ((__u32)(\t\t\t\\\n\t(((__u32)(x) & (__u32)0x00ff00ffUL) << 8) |\t\t\\\n\t(((__u32)(x) & (__u32)0xff00ff00UL) >> 8)))\n\n/*\n * Implement the following as inlines, but define the interface using\n * macros to allow constant folding when possible:\n * ___swab16, ___swab32, ___swab64, ___swahw32, ___swahb32\n */\n\nstatic inline __attribute_const__ __u16 __fswab16(__u16 val)\n{\n#if defined (__arch_swab16)\n\treturn __arch_swab16(val);\n#else\n\treturn ___constant_swab16(val);\n#endif\n}\n\nstatic inline __attribute_const__ __u32 __fswab32(__u32 val)\n{\n#if defined(__arch_swab32)\n\treturn __arch_swab32(val);\n#else\n\treturn ___constant_swab32(val);\n#endif\n}\n\nstatic inline __attribute_const__ __u64 __fswab64(__u64 val)\n{\n#if defined (__arch_swab64)\n\treturn __arch_swab64(val);\n#elif defined(__SWAB_64_THRU_32__)\n\t__u32 h = val >> 32;\n\t__u32 l = val & ((1ULL << 32) - 1);\n\treturn (((__u64)__fswab32(l)) << 32) | ((__u64)(__fswab32(h)));\n#else\n\treturn ___constant_swab64(val);\n#endif\n}\n\nstatic inline __attribute_const__ __u32 __fswahw32(__u32 val)\n{\n#ifdef __arch_swahw32\n\treturn __arch_swahw32(val);\n#else\n\treturn ___constant_swahw32(val);\n#endif\n}\n\nstatic inline __attribute_const__ __u32 __fswahb32(__u32 val)\n{\n#ifdef __arch_swahb32\n\treturn __arch_swahb32(val);\n#else\n\treturn ___constant_swahb32(val);\n#endif\n}\n\n/**\n * __swab16 - return a byteswapped 16-bit value\n * @x: value to byteswap\n */\n#ifdef __HAVE_BUILTIN_BSWAP16__\n#define __swab16(x) (__u16)__builtin_bswap16((__u16)(x))\n#else\n#define __swab16(x)\t\t\t\t\\\n\t(__builtin_constant_p((__u16)(x)) ?\t\\\n\t___constant_swab16(x) :\t\t\t\\\n\t__fswab16(x))\n#endif\n\n/**\n * __swab32 - return a byteswapped 32-bit value\n * @x: value to byteswap\n */\n#ifdef __HAVE_BUILTIN_BSWAP32__\n#define __swab32(x) (__u32)__builtin_bswap32((__u32)(x))\n#else\n#define __swab32(x)\t\t\t\t\\\n\t(__builtin_constant_p((__u32)(x)) ?\t\\\n\t___constant_swab32(x) :\t\t\t\\\n\t__fswab32(x))\n#endif\n\n/**\n * __swab64 - return a byteswapped 64-bit value\n * @x: value to byteswap\n */\n#ifdef __HAVE_BUILTIN_BSWAP64__\n#define __swab64(x) (__u64)__builtin_bswap64((__u64)(x))\n#else\n#define __swab64(x)\t\t\t\t\\\n\t(__builtin_constant_p((__u64)(x)) ?\t\\\n\t___constant_swab64(x) :\t\t\t\\\n\t__fswab64(x))\n#endif\n\nstatic __always_inline unsigned long __swab(const unsigned long y)\n{\n#if __BITS_PER_LONG == 64\n\treturn __swab64(y);\n#else /* __BITS_PER_LONG == 32 */\n\treturn __swab32(y);\n#endif\n}\n\n/**\n * __swahw32 - return a word-swapped 32-bit value\n * @x: value to wordswap\n *\n * __swahw32(0x12340000) is 0x00001234\n */\n#define __swahw32(x)\t\t\t\t\\\n\t(__builtin_constant_p((__u32)(x)) ?\t\\\n\t___constant_swahw32(x) :\t\t\\\n\t__fswahw32(x))\n\n/**\n * __swahb32 - return a high and low byte-swapped 32-bit value\n * @x: value to byteswap\n *\n * __swahb32(0x12345678) is 0x34127856\n */\n#define __swahb32(x)\t\t\t\t\\\n\t(__builtin_constant_p((__u32)(x)) ?\t\\\n\t___constant_swahb32(x) :\t\t\\\n\t__fswahb32(x))\n\n/**\n * __swab16p - return a byteswapped 16-bit value from a pointer\n * @p: pointer to a naturally-aligned 16-bit value\n */\nstatic __always_inline __u16 __swab16p(const __u16 *p)\n{\n#ifdef __arch_swab16p\n\treturn __arch_swab16p(p);\n#else\n\treturn __swab16(*p);\n#endif\n}\n\n/**\n * __swab32p - return a byteswapped 32-bit value from a pointer\n * @p: pointer to a naturally-aligned 32-bit value\n */\nstatic __always_inline __u32 __swab32p(const __u32 *p)\n{\n#ifdef __arch_swab32p\n\treturn __arch_swab32p(p);\n#else\n\treturn __swab32(*p);\n#endif\n}\n\n/**\n * __swab64p - return a byteswapped 64-bit value from a pointer\n * @p: pointer to a naturally-aligned 64-bit value\n */\nstatic __always_inline __u64 __swab64p(const __u64 *p)\n{\n#ifdef __arch_swab64p\n\treturn __arch_swab64p(p);\n#else\n\treturn __swab64(*p);\n#endif\n}\n\n/**\n * __swahw32p - return a wordswapped 32-bit value from a pointer\n * @p: pointer to a naturally-aligned 32-bit value\n *\n * See __swahw32() for details of wordswapping.\n */\nstatic inline __u32 __swahw32p(const __u32 *p)\n{\n#ifdef __arch_swahw32p\n\treturn __arch_swahw32p(p);\n#else\n\treturn __swahw32(*p);\n#endif\n}\n\n/**\n * __swahb32p - return a high and low byteswapped 32-bit value from a pointer\n * @p: pointer to a naturally-aligned 32-bit value\n *\n * See __swahb32() for details of high/low byteswapping.\n */\nstatic inline __u32 __swahb32p(const __u32 *p)\n{\n#ifdef __arch_swahb32p\n\treturn __arch_swahb32p(p);\n#else\n\treturn __swahb32(*p);\n#endif\n}\n\n/**\n * __swab16s - byteswap a 16-bit value in-place\n * @p: pointer to a naturally-aligned 16-bit value\n */\nstatic inline void __swab16s(__u16 *p)\n{\n#ifdef __arch_swab16s\n\t__arch_swab16s(p);\n#else\n\t*p = __swab16p(p);\n#endif\n}\n/**\n * __swab32s - byteswap a 32-bit value in-place\n * @p: pointer to a naturally-aligned 32-bit value\n */\nstatic __always_inline void __swab32s(__u32 *p)\n{\n#ifdef __arch_swab32s\n\t__arch_swab32s(p);\n#else\n\t*p = __swab32p(p);\n#endif\n}\n\n/**\n * __swab64s - byteswap a 64-bit value in-place\n * @p: pointer to a naturally-aligned 64-bit value\n */\nstatic __always_inline void __swab64s(__u64 *p)\n{\n#ifdef __arch_swab64s\n\t__arch_swab64s(p);\n#else\n\t*p = __swab64p(p);\n#endif\n}\n\n/**\n * __swahw32s - wordswap a 32-bit value in-place\n * @p: pointer to a naturally-aligned 32-bit value\n *\n * See __swahw32() for details of wordswapping\n */\nstatic inline void __swahw32s(__u32 *p)\n{\n#ifdef __arch_swahw32s\n\t__arch_swahw32s(p);\n#else\n\t*p = __swahw32p(p);\n#endif\n}\n\n/**\n * __swahb32s - high and low byteswap a 32-bit value in-place\n * @p: pointer to a naturally-aligned 32-bit value\n *\n * See __swahb32() for details of high and low byte swapping\n */\nstatic inline void __swahb32s(__u32 *p)\n{\n#ifdef __arch_swahb32s\n\t__arch_swahb32s(p);\n#else\n\t*p = __swahb32p(p);\n#endif\n}\n\n\n#endif /* _UAPI_LINUX_SWAB_H */\n"}, "5": {"id": 5, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "6": {"id": 6, "path": "/src/include/linux/rcupdate.h", "content": "/* SPDX-License-Identifier: GPL-2.0+ */\n/*\n * Read-Copy Update mechanism for mutual exclusion\n *\n * Copyright IBM Corporation, 2001\n *\n * Author: Dipankar Sarma <dipankar@in.ibm.com>\n *\n * Based on the original work by Paul McKenney <paulmck@vnet.ibm.com>\n * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.\n * Papers:\n * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf\n * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)\n *\n * For detailed explanation of Read-Copy Update mechanism see -\n *\t\thttp://lse.sourceforge.net/locking/rcupdate.html\n *\n */\n\n#ifndef __LINUX_RCUPDATE_H\n#define __LINUX_RCUPDATE_H\n\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/atomic.h>\n#include <linux/irqflags.h>\n#include <linux/preempt.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <asm/processor.h>\n#include <linux/cpumask.h>\n\n#define ULONG_CMP_GE(a, b)\t(ULONG_MAX / 2 >= (a) - (b))\n#define ULONG_CMP_LT(a, b)\t(ULONG_MAX / 2 < (a) - (b))\n#define ulong2long(a)\t\t(*(long *)(&(a)))\n#define USHORT_CMP_GE(a, b)\t(USHRT_MAX / 2 >= (unsigned short)((a) - (b)))\n#define USHORT_CMP_LT(a, b)\t(USHRT_MAX / 2 < (unsigned short)((a) - (b)))\n\n/* Exported common interfaces */\nvoid call_rcu(struct rcu_head *head, rcu_callback_t func);\nvoid rcu_barrier_tasks(void);\nvoid rcu_barrier_tasks_rude(void);\nvoid synchronize_rcu(void);\n\n#ifdef CONFIG_PREEMPT_RCU\n\nvoid __rcu_read_lock(void);\nvoid __rcu_read_unlock(void);\n\n/*\n * Defined as a macro as it is a very low level header included from\n * areas that don't even know about current.  This gives the rcu_read_lock()\n * nesting depth, but makes sense only if CONFIG_PREEMPT_RCU -- in other\n * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.\n */\n#define rcu_preempt_depth() (current->rcu_read_lock_nesting)\n\n#else /* #ifdef CONFIG_PREEMPT_RCU */\n\n#ifdef CONFIG_TINY_RCU\n#define rcu_read_unlock_strict() do { } while (0)\n#else\nvoid rcu_read_unlock_strict(void);\n#endif\n\nstatic inline void __rcu_read_lock(void)\n{\n\tpreempt_disable();\n}\n\nstatic inline void __rcu_read_unlock(void)\n{\n\tpreempt_enable();\n\trcu_read_unlock_strict();\n}\n\nstatic inline int rcu_preempt_depth(void)\n{\n\treturn 0;\n}\n\n#endif /* #else #ifdef CONFIG_PREEMPT_RCU */\n\n/* Internal to kernel */\nvoid rcu_init(void);\nextern int rcu_scheduler_active __read_mostly;\nvoid rcu_sched_clock_irq(int user);\nvoid rcu_report_dead(unsigned int cpu);\nvoid rcutree_migrate_callbacks(int cpu);\n\n#ifdef CONFIG_TASKS_RCU_GENERIC\nvoid rcu_init_tasks_generic(void);\n#else\nstatic inline void rcu_init_tasks_generic(void) { }\n#endif\n\n#ifdef CONFIG_RCU_STALL_COMMON\nvoid rcu_sysrq_start(void);\nvoid rcu_sysrq_end(void);\n#else /* #ifdef CONFIG_RCU_STALL_COMMON */\nstatic inline void rcu_sysrq_start(void) { }\nstatic inline void rcu_sysrq_end(void) { }\n#endif /* #else #ifdef CONFIG_RCU_STALL_COMMON */\n\n#ifdef CONFIG_NO_HZ_FULL\nvoid rcu_user_enter(void);\nvoid rcu_user_exit(void);\n#else\nstatic inline void rcu_user_enter(void) { }\nstatic inline void rcu_user_exit(void) { }\n#endif /* CONFIG_NO_HZ_FULL */\n\n#ifdef CONFIG_RCU_NOCB_CPU\nvoid rcu_init_nohz(void);\nint rcu_nocb_cpu_offload(int cpu);\nint rcu_nocb_cpu_deoffload(int cpu);\nvoid rcu_nocb_flush_deferred_wakeup(void);\n#else /* #ifdef CONFIG_RCU_NOCB_CPU */\nstatic inline void rcu_init_nohz(void) { }\nstatic inline int rcu_nocb_cpu_offload(int cpu) { return -EINVAL; }\nstatic inline int rcu_nocb_cpu_deoffload(int cpu) { return 0; }\nstatic inline void rcu_nocb_flush_deferred_wakeup(void) { }\n#endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */\n\n/**\n * RCU_NONIDLE - Indicate idle-loop code that needs RCU readers\n * @a: Code that RCU needs to pay attention to.\n *\n * RCU read-side critical sections are forbidden in the inner idle loop,\n * that is, between the rcu_idle_enter() and the rcu_idle_exit() -- RCU\n * will happily ignore any such read-side critical sections.  However,\n * things like powertop need tracepoints in the inner idle loop.\n *\n * This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())\n * will tell RCU that it needs to pay attention, invoke its argument\n * (in this example, calling the do_something_with_RCU() function),\n * and then tell RCU to go back to ignoring this CPU.  It is permissible\n * to nest RCU_NONIDLE() wrappers, but not indefinitely (but the limit is\n * on the order of a million or so, even on 32-bit systems).  It is\n * not legal to block within RCU_NONIDLE(), nor is it permissible to\n * transfer control either into or out of RCU_NONIDLE()'s statement.\n */\n#define RCU_NONIDLE(a) \\\n\tdo { \\\n\t\trcu_irq_enter_irqson(); \\\n\t\tdo { a; } while (0); \\\n\t\trcu_irq_exit_irqson(); \\\n\t} while (0)\n\n/*\n * Note a quasi-voluntary context switch for RCU-tasks's benefit.\n * This is a macro rather than an inline function to avoid #include hell.\n */\n#ifdef CONFIG_TASKS_RCU_GENERIC\n\n# ifdef CONFIG_TASKS_RCU\n# define rcu_tasks_classic_qs(t, preempt)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (!(preempt) && READ_ONCE((t)->rcu_tasks_holdout))\t\\\n\t\t\tWRITE_ONCE((t)->rcu_tasks_holdout, false);\t\\\n\t} while (0)\nvoid call_rcu_tasks(struct rcu_head *head, rcu_callback_t func);\nvoid synchronize_rcu_tasks(void);\n# else\n# define rcu_tasks_classic_qs(t, preempt) do { } while (0)\n# define call_rcu_tasks call_rcu\n# define synchronize_rcu_tasks synchronize_rcu\n# endif\n\n# ifdef CONFIG_TASKS_RCU_TRACE\n# define rcu_tasks_trace_qs(t)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (!likely(READ_ONCE((t)->trc_reader_checked)) &&\t\\\n\t\t    !unlikely(READ_ONCE((t)->trc_reader_nesting))) {\t\\\n\t\t\tsmp_store_release(&(t)->trc_reader_checked, true); \\\n\t\t\tsmp_mb(); /* Readers partitioned by store. */\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n# else\n# define rcu_tasks_trace_qs(t) do { } while (0)\n# endif\n\n#define rcu_tasks_qs(t, preempt)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\trcu_tasks_classic_qs((t), (preempt));\t\t\t\t\\\n\trcu_tasks_trace_qs((t));\t\t\t\t\t\\\n} while (0)\n\n# ifdef CONFIG_TASKS_RUDE_RCU\nvoid call_rcu_tasks_rude(struct rcu_head *head, rcu_callback_t func);\nvoid synchronize_rcu_tasks_rude(void);\n# endif\n\n#define rcu_note_voluntary_context_switch(t) rcu_tasks_qs(t, false)\nvoid exit_tasks_rcu_start(void);\nvoid exit_tasks_rcu_finish(void);\n#else /* #ifdef CONFIG_TASKS_RCU_GENERIC */\n#define rcu_tasks_qs(t, preempt) do { } while (0)\n#define rcu_note_voluntary_context_switch(t) do { } while (0)\n#define call_rcu_tasks call_rcu\n#define synchronize_rcu_tasks synchronize_rcu\nstatic inline void exit_tasks_rcu_start(void) { }\nstatic inline void exit_tasks_rcu_finish(void) { }\n#endif /* #else #ifdef CONFIG_TASKS_RCU_GENERIC */\n\n/**\n * cond_resched_tasks_rcu_qs - Report potential quiescent states to RCU\n *\n * This macro resembles cond_resched(), except that it is defined to\n * report potential quiescent states to RCU-tasks even if the cond_resched()\n * machinery were to be shut off, as some advocate for PREEMPTION kernels.\n */\n#define cond_resched_tasks_rcu_qs() \\\ndo { \\\n\trcu_tasks_qs(current, false); \\\n\tcond_resched(); \\\n} while (0)\n\n/*\n * Infrastructure to implement the synchronize_() primitives in\n * TREE_RCU and rcu_barrier_() primitives in TINY_RCU.\n */\n\n#if defined(CONFIG_TREE_RCU)\n#include <linux/rcutree.h>\n#elif defined(CONFIG_TINY_RCU)\n#include <linux/rcutiny.h>\n#else\n#error \"Unknown RCU implementation specified to kernel configuration\"\n#endif\n\n/*\n * The init_rcu_head_on_stack() and destroy_rcu_head_on_stack() calls\n * are needed for dynamic initialization and destruction of rcu_head\n * on the stack, and init_rcu_head()/destroy_rcu_head() are needed for\n * dynamic initialization and destruction of statically allocated rcu_head\n * structures.  However, rcu_head structures allocated dynamically in the\n * heap don't need any initialization.\n */\n#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD\nvoid init_rcu_head(struct rcu_head *head);\nvoid destroy_rcu_head(struct rcu_head *head);\nvoid init_rcu_head_on_stack(struct rcu_head *head);\nvoid destroy_rcu_head_on_stack(struct rcu_head *head);\n#else /* !CONFIG_DEBUG_OBJECTS_RCU_HEAD */\nstatic inline void init_rcu_head(struct rcu_head *head) { }\nstatic inline void destroy_rcu_head(struct rcu_head *head) { }\nstatic inline void init_rcu_head_on_stack(struct rcu_head *head) { }\nstatic inline void destroy_rcu_head_on_stack(struct rcu_head *head) { }\n#endif\t/* #else !CONFIG_DEBUG_OBJECTS_RCU_HEAD */\n\n#if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU)\nbool rcu_lockdep_current_cpu_online(void);\n#else /* #if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU) */\nstatic inline bool rcu_lockdep_current_cpu_online(void) { return true; }\n#endif /* #else #if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU) */\n\nextern struct lockdep_map rcu_lock_map;\nextern struct lockdep_map rcu_bh_lock_map;\nextern struct lockdep_map rcu_sched_lock_map;\nextern struct lockdep_map rcu_callback_map;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\nstatic inline void rcu_lock_acquire(struct lockdep_map *map)\n{\n\tlock_acquire(map, 0, 0, 2, 0, NULL, _THIS_IP_);\n}\n\nstatic inline void rcu_lock_release(struct lockdep_map *map)\n{\n\tlock_release(map, _THIS_IP_);\n}\n\nint debug_lockdep_rcu_enabled(void);\nint rcu_read_lock_held(void);\nint rcu_read_lock_bh_held(void);\nint rcu_read_lock_sched_held(void);\nint rcu_read_lock_any_held(void);\n\n#else /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */\n\n# define rcu_lock_acquire(a)\t\tdo { } while (0)\n# define rcu_lock_release(a)\t\tdo { } while (0)\n\nstatic inline int rcu_read_lock_held(void)\n{\n\treturn 1;\n}\n\nstatic inline int rcu_read_lock_bh_held(void)\n{\n\treturn 1;\n}\n\nstatic inline int rcu_read_lock_sched_held(void)\n{\n\treturn !preemptible();\n}\n\nstatic inline int rcu_read_lock_any_held(void)\n{\n\treturn !preemptible();\n}\n\n#endif /* #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */\n\n#ifdef CONFIG_PROVE_RCU\n\n/**\n * RCU_LOCKDEP_WARN - emit lockdep splat if specified condition is met\n * @c: condition to check\n * @s: informative message\n */\n#define RCU_LOCKDEP_WARN(c, s)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstatic bool __section(\".data.unlikely\") __warned;\t\\\n\t\tif (debug_lockdep_rcu_enabled() && !__warned && (c)) {\t\\\n\t\t\t__warned = true;\t\t\t\t\\\n\t\t\tlockdep_rcu_suspicious(__FILE__, __LINE__, s);\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#if defined(CONFIG_PROVE_RCU) && !defined(CONFIG_PREEMPT_RCU)\nstatic inline void rcu_preempt_sleep_check(void)\n{\n\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_lock_map),\n\t\t\t \"Illegal context switch in RCU read-side critical section\");\n}\n#else /* #ifdef CONFIG_PROVE_RCU */\nstatic inline void rcu_preempt_sleep_check(void) { }\n#endif /* #else #ifdef CONFIG_PROVE_RCU */\n\n#define rcu_sleep_check()\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\trcu_preempt_sleep_check();\t\t\t\t\\\n\t\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),\t\\\n\t\t\t\t \"Illegal context switch in RCU-bh read-side critical section\"); \\\n\t\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_sched_lock_map),\t\\\n\t\t\t\t \"Illegal context switch in RCU-sched read-side critical section\"); \\\n\t} while (0)\n\n#else /* #ifdef CONFIG_PROVE_RCU */\n\n#define RCU_LOCKDEP_WARN(c, s) do { } while (0 && (c))\n#define rcu_sleep_check() do { } while (0)\n\n#endif /* #else #ifdef CONFIG_PROVE_RCU */\n\n/*\n * Helper functions for rcu_dereference_check(), rcu_dereference_protected()\n * and rcu_assign_pointer().  Some of these could be folded into their\n * callers, but they are left separate in order to ease introduction of\n * multiple pointers markings to match different RCU implementations\n * (e.g., __srcu), should this make sense in the future.\n */\n\n#ifdef __CHECKER__\n#define rcu_check_sparse(p, space) \\\n\t((void)(((typeof(*p) space *)p) == p))\n#else /* #ifdef __CHECKER__ */\n#define rcu_check_sparse(p, space)\n#endif /* #else #ifdef __CHECKER__ */\n\n#define __rcu_access_pointer(p, space) \\\n({ \\\n\ttypeof(*p) *_________p1 = (typeof(*p) *__force)READ_ONCE(p); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(_________p1)); \\\n})\n#define __rcu_dereference_check(p, c, space) \\\n({ \\\n\t/* Dependency order vs. p above. */ \\\n\ttypeof(*p) *________p1 = (typeof(*p) *__force)READ_ONCE(p); \\\n\tRCU_LOCKDEP_WARN(!(c), \"suspicious rcu_dereference_check() usage\"); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(________p1)); \\\n})\n#define __rcu_dereference_protected(p, c, space) \\\n({ \\\n\tRCU_LOCKDEP_WARN(!(c), \"suspicious rcu_dereference_protected() usage\"); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(p)); \\\n})\n#define rcu_dereference_raw(p) \\\n({ \\\n\t/* Dependency order vs. p above. */ \\\n\ttypeof(p) ________p1 = READ_ONCE(p); \\\n\t((typeof(*p) __force __kernel *)(________p1)); \\\n})\n\n/**\n * RCU_INITIALIZER() - statically initialize an RCU-protected global variable\n * @v: The value to statically initialize with.\n */\n#define RCU_INITIALIZER(v) (typeof(*(v)) __force __rcu *)(v)\n\n/**\n * rcu_assign_pointer() - assign to RCU-protected pointer\n * @p: pointer to assign to\n * @v: value to assign (publish)\n *\n * Assigns the specified value to the specified RCU-protected\n * pointer, ensuring that any concurrent RCU readers will see\n * any prior initialization.\n *\n * Inserts memory barriers on architectures that require them\n * (which is most of them), and also prevents the compiler from\n * reordering the code that initializes the structure after the pointer\n * assignment.  More importantly, this call documents which pointers\n * will be dereferenced by RCU read-side code.\n *\n * In some special cases, you may use RCU_INIT_POINTER() instead\n * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due\n * to the fact that it does not constrain either the CPU or the compiler.\n * That said, using RCU_INIT_POINTER() when you should have used\n * rcu_assign_pointer() is a very bad thing that results in\n * impossible-to-diagnose memory corruption.  So please be careful.\n * See the RCU_INIT_POINTER() comment header for details.\n *\n * Note that rcu_assign_pointer() evaluates each of its arguments only\n * once, appearances notwithstanding.  One of the \"extra\" evaluations\n * is in typeof() and the other visible only to sparse (__CHECKER__),\n * neither of which actually execute the argument.  As with most cpp\n * macros, this execute-arguments-only-once property is important, so\n * please be careful when making changes to rcu_assign_pointer() and the\n * other macros that it invokes.\n */\n#define rcu_assign_pointer(p, v)\t\t\t\t\t      \\\ndo {\t\t\t\t\t\t\t\t\t      \\\n\tuintptr_t _r_a_p__v = (uintptr_t)(v);\t\t\t\t      \\\n\trcu_check_sparse(p, __rcu);\t\t\t\t\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\tif (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)\t      \\\n\t\tWRITE_ONCE((p), (typeof(p))(_r_a_p__v));\t\t      \\\n\telse\t\t\t\t\t\t\t\t      \\\n\t\tsmp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \\\n} while (0)\n\n/**\n * rcu_replace_pointer() - replace an RCU pointer, returning its old value\n * @rcu_ptr: RCU pointer, whose old value is returned\n * @ptr: regular pointer\n * @c: the lockdep conditions under which the dereference will take place\n *\n * Perform a replacement, where @rcu_ptr is an RCU-annotated\n * pointer and @c is the lockdep argument that is passed to the\n * rcu_dereference_protected() call used to read that pointer.  The old\n * value of @rcu_ptr is returned, and @rcu_ptr is set to @ptr.\n */\n#define rcu_replace_pointer(rcu_ptr, ptr, c)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(ptr) __tmp = rcu_dereference_protected((rcu_ptr), (c));\t\\\n\trcu_assign_pointer((rcu_ptr), (ptr));\t\t\t\t\\\n\t__tmp;\t\t\t\t\t\t\t\t\\\n})\n\n/**\n * rcu_access_pointer() - fetch RCU pointer with no dereferencing\n * @p: The pointer to read\n *\n * Return the value of the specified RCU-protected pointer, but omit the\n * lockdep checks for being in an RCU read-side critical section.  This is\n * useful when the value of this pointer is accessed, but the pointer is\n * not dereferenced, for example, when testing an RCU-protected pointer\n * against NULL.  Although rcu_access_pointer() may also be used in cases\n * where update-side locks prevent the value of the pointer from changing,\n * you should instead use rcu_dereference_protected() for this use case.\n *\n * It is also permissible to use rcu_access_pointer() when read-side\n * access to the pointer was removed at least one grace period ago, as\n * is the case in the context of the RCU callback that is freeing up\n * the data, or after a synchronize_rcu() returns.  This can be useful\n * when tearing down multi-linked structures after a grace period\n * has elapsed.\n */\n#define rcu_access_pointer(p) __rcu_access_pointer((p), __rcu)\n\n/**\n * rcu_dereference_check() - rcu_dereference with debug checking\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * Do an rcu_dereference(), but check that the conditions under which the\n * dereference will take place are correct.  Typically the conditions\n * indicate the various locking conditions that should be held at that\n * point.  The check should return true if the conditions are satisfied.\n * An implicit check for being in an RCU read-side critical section\n * (rcu_read_lock()) is included.\n *\n * For example:\n *\n *\tbar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock));\n *\n * could be used to indicate to lockdep that foo->bar may only be dereferenced\n * if either rcu_read_lock() is held, or that the lock required to replace\n * the bar struct at foo->bar is held.\n *\n * Note that the list of conditions may also include indications of when a lock\n * need not be held, for example during initialisation or destruction of the\n * target struct:\n *\n *\tbar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock) ||\n *\t\t\t\t\t      atomic_read(&foo->usage) == 0);\n *\n * Inserts memory barriers on architectures that require them\n * (currently only the Alpha), prevents the compiler from refetching\n * (and from merging fetches), and, more importantly, documents exactly\n * which pointers are protected by RCU and checks that the pointer is\n * annotated as __rcu.\n */\n#define rcu_dereference_check(p, c) \\\n\t__rcu_dereference_check((p), (c) || rcu_read_lock_held(), __rcu)\n\n/**\n * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * This is the RCU-bh counterpart to rcu_dereference_check().\n */\n#define rcu_dereference_bh_check(p, c) \\\n\t__rcu_dereference_check((p), (c) || rcu_read_lock_bh_held(), __rcu)\n\n/**\n * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * This is the RCU-sched counterpart to rcu_dereference_check().\n */\n#define rcu_dereference_sched_check(p, c) \\\n\t__rcu_dereference_check((p), (c) || rcu_read_lock_sched_held(), \\\n\t\t\t\t__rcu)\n\n/*\n * The tracing infrastructure traces RCU (we want that), but unfortunately\n * some of the RCU checks causes tracing to lock up the system.\n *\n * The no-tracing version of rcu_dereference_raw() must not call\n * rcu_read_lock_held().\n */\n#define rcu_dereference_raw_check(p) __rcu_dereference_check((p), 1, __rcu)\n\n/**\n * rcu_dereference_protected() - fetch RCU pointer when updates prevented\n * @p: The pointer to read, prior to dereferencing\n * @c: The conditions under which the dereference will take place\n *\n * Return the value of the specified RCU-protected pointer, but omit\n * the READ_ONCE().  This is useful in cases where update-side locks\n * prevent the value of the pointer from changing.  Please note that this\n * primitive does *not* prevent the compiler from repeating this reference\n * or combining it with other references, so it should not be used without\n * protection of appropriate locks.\n *\n * This function is only for update-side use.  Using this function\n * when protected only by rcu_read_lock() will result in infrequent\n * but very ugly failures.\n */\n#define rcu_dereference_protected(p, c) \\\n\t__rcu_dereference_protected((p), (c), __rcu)\n\n\n/**\n * rcu_dereference() - fetch RCU-protected pointer for dereferencing\n * @p: The pointer to read, prior to dereferencing\n *\n * This is a simple wrapper around rcu_dereference_check().\n */\n#define rcu_dereference(p) rcu_dereference_check(p, 0)\n\n/**\n * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing\n * @p: The pointer to read, prior to dereferencing\n *\n * Makes rcu_dereference_check() do the dirty work.\n */\n#define rcu_dereference_bh(p) rcu_dereference_bh_check(p, 0)\n\n/**\n * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing\n * @p: The pointer to read, prior to dereferencing\n *\n * Makes rcu_dereference_check() do the dirty work.\n */\n#define rcu_dereference_sched(p) rcu_dereference_sched_check(p, 0)\n\n/**\n * rcu_pointer_handoff() - Hand off a pointer from RCU to other mechanism\n * @p: The pointer to hand off\n *\n * This is simply an identity function, but it documents where a pointer\n * is handed off from RCU to some other synchronization mechanism, for\n * example, reference counting or locking.  In C11, it would map to\n * kill_dependency().  It could be used as follows::\n *\n *\trcu_read_lock();\n *\tp = rcu_dereference(gp);\n *\tlong_lived = is_long_lived(p);\n *\tif (long_lived) {\n *\t\tif (!atomic_inc_not_zero(p->refcnt))\n *\t\t\tlong_lived = false;\n *\t\telse\n *\t\t\tp = rcu_pointer_handoff(p);\n *\t}\n *\trcu_read_unlock();\n */\n#define rcu_pointer_handoff(p) (p)\n\n/**\n * rcu_read_lock() - mark the beginning of an RCU read-side critical section\n *\n * When synchronize_rcu() is invoked on one CPU while other CPUs\n * are within RCU read-side critical sections, then the\n * synchronize_rcu() is guaranteed to block until after all the other\n * CPUs exit their critical sections.  Similarly, if call_rcu() is invoked\n * on one CPU while other CPUs are within RCU read-side critical\n * sections, invocation of the corresponding RCU callback is deferred\n * until after the all the other CPUs exit their critical sections.\n *\n * Note, however, that RCU callbacks are permitted to run concurrently\n * with new RCU read-side critical sections.  One way that this can happen\n * is via the following sequence of events: (1) CPU 0 enters an RCU\n * read-side critical section, (2) CPU 1 invokes call_rcu() to register\n * an RCU callback, (3) CPU 0 exits the RCU read-side critical section,\n * (4) CPU 2 enters a RCU read-side critical section, (5) the RCU\n * callback is invoked.  This is legal, because the RCU read-side critical\n * section that was running concurrently with the call_rcu() (and which\n * therefore might be referencing something that the corresponding RCU\n * callback would free up) has completed before the corresponding\n * RCU callback is invoked.\n *\n * RCU read-side critical sections may be nested.  Any deferred actions\n * will be deferred until the outermost RCU read-side critical section\n * completes.\n *\n * You can avoid reading and understanding the next paragraph by\n * following this rule: don't put anything in an rcu_read_lock() RCU\n * read-side critical section that would block in a !PREEMPTION kernel.\n * But if you want the full story, read on!\n *\n * In non-preemptible RCU implementations (pure TREE_RCU and TINY_RCU),\n * it is illegal to block while in an RCU read-side critical section.\n * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPTION\n * kernel builds, RCU read-side critical sections may be preempted,\n * but explicit blocking is illegal.  Finally, in preemptible RCU\n * implementations in real-time (with -rt patchset) kernel builds, RCU\n * read-side critical sections may be preempted and they may also block, but\n * only when acquiring spinlocks that are subject to priority inheritance.\n */\nstatic __always_inline void rcu_read_lock(void)\n{\n\t__rcu_read_lock();\n\t__acquire(RCU);\n\trcu_lock_acquire(&rcu_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock() used illegally while idle\");\n}\n\n/*\n * So where is rcu_write_lock()?  It does not exist, as there is no\n * way for writers to lock out RCU readers.  This is a feature, not\n * a bug -- this property is what provides RCU's performance benefits.\n * Of course, writers must coordinate with each other.  The normal\n * spinlock primitives work well for this, but any other technique may be\n * used as well.  RCU does not care how the writers keep out of each\n * others' way, as long as they do so.\n */\n\n/**\n * rcu_read_unlock() - marks the end of an RCU read-side critical section.\n *\n * In most situations, rcu_read_unlock() is immune from deadlock.\n * However, in kernels built with CONFIG_RCU_BOOST, rcu_read_unlock()\n * is responsible for deboosting, which it does via rt_mutex_unlock().\n * Unfortunately, this function acquires the scheduler's runqueue and\n * priority-inheritance spinlocks.  This means that deadlock could result\n * if the caller of rcu_read_unlock() already holds one of these locks or\n * any lock that is ever acquired while holding them.\n *\n * That said, RCU readers are never priority boosted unless they were\n * preempted.  Therefore, one way to avoid deadlock is to make sure\n * that preemption never happens within any RCU read-side critical\n * section whose outermost rcu_read_unlock() is called with one of\n * rt_mutex_unlock()'s locks held.  Such preemption can be avoided in\n * a number of ways, for example, by invoking preempt_disable() before\n * critical section's outermost rcu_read_lock().\n *\n * Given that the set of locks acquired by rt_mutex_unlock() might change\n * at any time, a somewhat more future-proofed approach is to make sure\n * that that preemption never happens within any RCU read-side critical\n * section whose outermost rcu_read_unlock() is called with irqs disabled.\n * This approach relies on the fact that rt_mutex_unlock() currently only\n * acquires irq-disabled locks.\n *\n * The second of these two approaches is best in most situations,\n * however, the first approach can also be useful, at least to those\n * developers willing to keep abreast of the set of locks acquired by\n * rt_mutex_unlock().\n *\n * See rcu_read_lock() for more information.\n */\nstatic inline void rcu_read_unlock(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock() used illegally while idle\");\n\t__release(RCU);\n\t__rcu_read_unlock();\n\trcu_lock_release(&rcu_lock_map); /* Keep acq info for rls diags. */\n}\n\n/**\n * rcu_read_lock_bh() - mark the beginning of an RCU-bh critical section\n *\n * This is equivalent of rcu_read_lock(), but also disables softirqs.\n * Note that anything else that disables softirqs can also serve as\n * an RCU read-side critical section.\n *\n * Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()\n * must occur in the same context, for example, it is illegal to invoke\n * rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()\n * was invoked from some other task.\n */\nstatic inline void rcu_read_lock_bh(void)\n{\n\tlocal_bh_disable();\n\t__acquire(RCU_BH);\n\trcu_lock_acquire(&rcu_bh_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock_bh() used illegally while idle\");\n}\n\n/**\n * rcu_read_unlock_bh() - marks the end of a softirq-only RCU critical section\n *\n * See rcu_read_lock_bh() for more information.\n */\nstatic inline void rcu_read_unlock_bh(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock_bh() used illegally while idle\");\n\trcu_lock_release(&rcu_bh_lock_map);\n\t__release(RCU_BH);\n\tlocal_bh_enable();\n}\n\n/**\n * rcu_read_lock_sched() - mark the beginning of a RCU-sched critical section\n *\n * This is equivalent of rcu_read_lock(), but disables preemption.\n * Read-side critical sections can also be introduced by anything else\n * that disables preemption, including local_irq_disable() and friends.\n *\n * Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()\n * must occur in the same context, for example, it is illegal to invoke\n * rcu_read_unlock_sched() from process context if the matching\n * rcu_read_lock_sched() was invoked from an NMI handler.\n */\nstatic inline void rcu_read_lock_sched(void)\n{\n\tpreempt_disable();\n\t__acquire(RCU_SCHED);\n\trcu_lock_acquire(&rcu_sched_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock_sched() used illegally while idle\");\n}\n\n/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */\nstatic inline notrace void rcu_read_lock_sched_notrace(void)\n{\n\tpreempt_disable_notrace();\n\t__acquire(RCU_SCHED);\n}\n\n/**\n * rcu_read_unlock_sched() - marks the end of a RCU-classic critical section\n *\n * See rcu_read_lock_sched() for more information.\n */\nstatic inline void rcu_read_unlock_sched(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock_sched() used illegally while idle\");\n\trcu_lock_release(&rcu_sched_lock_map);\n\t__release(RCU_SCHED);\n\tpreempt_enable();\n}\n\n/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */\nstatic inline notrace void rcu_read_unlock_sched_notrace(void)\n{\n\t__release(RCU_SCHED);\n\tpreempt_enable_notrace();\n}\n\n/**\n * RCU_INIT_POINTER() - initialize an RCU protected pointer\n * @p: The pointer to be initialized.\n * @v: The value to initialized the pointer to.\n *\n * Initialize an RCU-protected pointer in special cases where readers\n * do not need ordering constraints on the CPU or the compiler.  These\n * special cases are:\n *\n * 1.\tThis use of RCU_INIT_POINTER() is NULLing out the pointer *or*\n * 2.\tThe caller has taken whatever steps are required to prevent\n *\tRCU readers from concurrently accessing this pointer *or*\n * 3.\tThe referenced data structure has already been exposed to\n *\treaders either at compile time or via rcu_assign_pointer() *and*\n *\n *\ta.\tYou have not made *any* reader-visible changes to\n *\t\tthis structure since then *or*\n *\tb.\tIt is OK for readers accessing this structure from its\n *\t\tnew location to see the old state of the structure.  (For\n *\t\texample, the changes were to statistical counters or to\n *\t\tother state where exact synchronization is not required.)\n *\n * Failure to follow these rules governing use of RCU_INIT_POINTER() will\n * result in impossible-to-diagnose memory corruption.  As in the structures\n * will look OK in crash dumps, but any concurrent RCU readers might\n * see pre-initialized values of the referenced data structure.  So\n * please be very careful how you use RCU_INIT_POINTER()!!!\n *\n * If you are creating an RCU-protected linked structure that is accessed\n * by a single external-to-structure RCU-protected pointer, then you may\n * use RCU_INIT_POINTER() to initialize the internal RCU-protected\n * pointers, but you must use rcu_assign_pointer() to initialize the\n * external-to-structure pointer *after* you have completely initialized\n * the reader-accessible portions of the linked structure.\n *\n * Note that unlike rcu_assign_pointer(), RCU_INIT_POINTER() provides no\n * ordering guarantees for either the CPU or the compiler.\n */\n#define RCU_INIT_POINTER(p, v) \\\n\tdo { \\\n\t\trcu_check_sparse(p, __rcu); \\\n\t\tWRITE_ONCE(p, RCU_INITIALIZER(v)); \\\n\t} while (0)\n\n/**\n * RCU_POINTER_INITIALIZER() - statically initialize an RCU protected pointer\n * @p: The pointer to be initialized.\n * @v: The value to initialized the pointer to.\n *\n * GCC-style initialization for an RCU-protected pointer in a structure field.\n */\n#define RCU_POINTER_INITIALIZER(p, v) \\\n\t\t.p = RCU_INITIALIZER(v)\n\n/*\n * Does the specified offset indicate that the corresponding rcu_head\n * structure can be handled by kvfree_rcu()?\n */\n#define __is_kvfree_rcu_offset(offset) ((offset) < 4096)\n\n/**\n * kfree_rcu() - kfree an object after a grace period.\n * @ptr: pointer to kfree for both single- and double-argument invocations.\n * @rhf: the name of the struct rcu_head within the type of @ptr,\n *       but only for double-argument invocations.\n *\n * Many rcu callbacks functions just call kfree() on the base structure.\n * These functions are trivial, but their size adds up, and furthermore\n * when they are used in a kernel module, that module must invoke the\n * high-latency rcu_barrier() function at module-unload time.\n *\n * The kfree_rcu() function handles this issue.  Rather than encoding a\n * function address in the embedded rcu_head structure, kfree_rcu() instead\n * encodes the offset of the rcu_head structure within the base structure.\n * Because the functions are not allowed in the low-order 4096 bytes of\n * kernel virtual memory, offsets up to 4095 bytes can be accommodated.\n * If the offset is larger than 4095 bytes, a compile-time error will\n * be generated in kvfree_rcu_arg_2(). If this error is triggered, you can\n * either fall back to use of call_rcu() or rearrange the structure to\n * position the rcu_head structure into the first 4096 bytes.\n *\n * Note that the allowable offset might decrease in the future, for example,\n * to allow something like kmem_cache_free_rcu().\n *\n * The BUILD_BUG_ON check must not involve any function calls, hence the\n * checks are done in macros here.\n */\n#define kfree_rcu kvfree_rcu\n\n/**\n * kvfree_rcu() - kvfree an object after a grace period.\n *\n * This macro consists of one or two arguments and it is\n * based on whether an object is head-less or not. If it\n * has a head then a semantic stays the same as it used\n * to be before:\n *\n *     kvfree_rcu(ptr, rhf);\n *\n * where @ptr is a pointer to kvfree(), @rhf is the name\n * of the rcu_head structure within the type of @ptr.\n *\n * When it comes to head-less variant, only one argument\n * is passed and that is just a pointer which has to be\n * freed after a grace period. Therefore the semantic is\n *\n *     kvfree_rcu(ptr);\n *\n * where @ptr is a pointer to kvfree().\n *\n * Please note, head-less way of freeing is permitted to\n * use from a context that has to follow might_sleep()\n * annotation. Otherwise, please switch and embed the\n * rcu_head structure within the type of @ptr.\n */\n#define kvfree_rcu(...) KVFREE_GET_MACRO(__VA_ARGS__,\t\t\\\n\tkvfree_rcu_arg_2, kvfree_rcu_arg_1)(__VA_ARGS__)\n\n#define KVFREE_GET_MACRO(_1, _2, NAME, ...) NAME\n#define kvfree_rcu_arg_2(ptr, rhf)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypeof (ptr) ___p = (ptr);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (___p) {\t\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(!__is_kvfree_rcu_offset(offsetof(typeof(*(ptr)), rhf)));\t\\\n\t\tkvfree_call_rcu(&((___p)->rhf), (rcu_callback_t)(unsigned long)\t\t\\\n\t\t\t(offsetof(typeof(*(ptr)), rhf)));\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define kvfree_rcu_arg_1(ptr)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\ttypeof(ptr) ___p = (ptr);\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (___p)\t\t\t\t\t\t\\\n\t\tkvfree_call_rcu(NULL, (rcu_callback_t) (___p));\t\\\n} while (0)\n\n/*\n * Place this after a lock-acquisition primitive to guarantee that\n * an UNLOCK+LOCK pair acts as a full barrier.  This guarantee applies\n * if the UNLOCK and LOCK are executed by the same CPU or if the\n * UNLOCK and LOCK operate on the same lock variable.\n */\n#ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE\n#define smp_mb__after_unlock_lock()\tsmp_mb()  /* Full ordering for lock. */\n#else /* #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE */\n#define smp_mb__after_unlock_lock()\tdo { } while (0)\n#endif /* #else #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE */\n\n\n/* Has the specified rcu_head structure been handed to call_rcu()? */\n\n/**\n * rcu_head_init - Initialize rcu_head for rcu_head_after_call_rcu()\n * @rhp: The rcu_head structure to initialize.\n *\n * If you intend to invoke rcu_head_after_call_rcu() to test whether a\n * given rcu_head structure has already been passed to call_rcu(), then\n * you must also invoke this rcu_head_init() function on it just after\n * allocating that structure.  Calls to this function must not race with\n * calls to call_rcu(), rcu_head_after_call_rcu(), or callback invocation.\n */\nstatic inline void rcu_head_init(struct rcu_head *rhp)\n{\n\trhp->func = (rcu_callback_t)~0L;\n}\n\n/**\n * rcu_head_after_call_rcu() - Has this rcu_head been passed to call_rcu()?\n * @rhp: The rcu_head structure to test.\n * @f: The function passed to call_rcu() along with @rhp.\n *\n * Returns @true if the @rhp has been passed to call_rcu() with @func,\n * and @false otherwise.  Emits a warning in any other case, including\n * the case where @rhp has already been invoked after a grace period.\n * Calls to this function must not race with callback invocation.  One way\n * to avoid such races is to enclose the call to rcu_head_after_call_rcu()\n * in an RCU read-side critical section that includes a read-side fetch\n * of the pointer to the structure containing @rhp.\n */\nstatic inline bool\nrcu_head_after_call_rcu(struct rcu_head *rhp, rcu_callback_t f)\n{\n\trcu_callback_t func = READ_ONCE(rhp->func);\n\n\tif (func == f)\n\t\treturn true;\n\tWARN_ON_ONCE(func != (rcu_callback_t)~0L);\n\treturn false;\n}\n\n/* kernel/ksysfs.c definitions */\nextern int rcu_expedited;\nextern int rcu_normal;\n\n#endif /* __LINUX_RCUPDATE_H */\n"}, "7": {"id": 7, "path": "/src/include/asm-generic/rwonce.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Prevent the compiler from merging or refetching reads or writes. The\n * compiler is also forbidden from reordering successive instances of\n * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some\n * particular ordering. One way to make the compiler aware of ordering is to\n * put the two invocations of READ_ONCE or WRITE_ONCE in different C\n * statements.\n *\n * These two macros will also work on aggregate data types like structs or\n * unions.\n *\n * Their two major use cases are: (1) Mediating communication between\n * process-level code and irq/NMI handlers, all running on the same CPU,\n * and (2) Ensuring that the compiler does not fold, spindle, or otherwise\n * mutilate accesses that either do not require ordering or that interact\n * with an explicit memory barrier or atomic instruction that provides the\n * required ordering.\n */\n#ifndef __ASM_GENERIC_RWONCE_H\n#define __ASM_GENERIC_RWONCE_H\n\n#ifndef __ASSEMBLY__\n\n#include <linux/compiler_types.h>\n#include <linux/kasan-checks.h>\n#include <linux/kcsan-checks.h>\n\n/*\n * Yes, this permits 64-bit accesses on 32-bit architectures. These will\n * actually be atomic in some cases (namely Armv7 + LPAE), but for others we\n * rely on the access being split into 2x32-bit accesses for a 32-bit quantity\n * (e.g. a virtual address) and a strong prevailing wind.\n */\n#define compiletime_assert_rwonce_type(t)\t\t\t\t\t\\\n\tcompiletime_assert(__native_word(t) || sizeof(t) == sizeof(long long),\t\\\n\t\t\"Unsupported access size for {READ,WRITE}_ONCE().\")\n\n/*\n * Use __READ_ONCE() instead of READ_ONCE() if you do not require any\n * atomicity. Note that this may result in tears!\n */\n#ifndef __READ_ONCE\n#define __READ_ONCE(x)\t(*(const volatile __unqual_scalar_typeof(x) *)&(x))\n#endif\n\n#define READ_ONCE(x)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__READ_ONCE(x);\t\t\t\t\t\t\t\\\n})\n\n#define __WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t*(volatile typeof(x) *)&(x) = (val);\t\t\t\t\\\n} while (0)\n\n#define WRITE_ONCE(x, val)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert_rwonce_type(x);\t\t\t\t\\\n\t__WRITE_ONCE(x, val);\t\t\t\t\t\t\\\n} while (0)\n\nstatic __no_sanitize_or_inline\nunsigned long __read_once_word_nocheck(const void *addr)\n{\n\treturn __READ_ONCE(*(unsigned long *)addr);\n}\n\n/*\n * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a\n * word from memory atomically but without telling KASAN/KCSAN. This is\n * usually used by unwinding code when walking the stack of a running process.\n */\n#define READ_ONCE_NOCHECK(x)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tcompiletime_assert(sizeof(x) == sizeof(unsigned long),\t\t\\\n\t\t\"Unsupported access size for READ_ONCE_NOCHECK().\");\t\\\n\t(typeof(x))__read_once_word_nocheck(&(x));\t\t\t\\\n})\n\nstatic __no_kasan_or_inline\nunsigned long read_word_at_a_time(const void *addr)\n{\n\tkasan_check_read(addr, 1);\n\treturn *(unsigned long *)addr;\n}\n\n#endif /* __ASSEMBLY__ */\n#endif\t/* __ASM_GENERIC_RWONCE_H */\n"}, "8": {"id": 8, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}}, "reports": [{"events": [{"location": {"col": 2, "file": 0, "line": 2518}, "message": "Taking true branch"}, {"location": {"col": 2, "file": 0, "line": 2531}, "message": "Control jumps to 'case NL80211_IFTYPE_AP_VLAN:'  at line 2532"}, {"location": {"col": 7, "file": 0, "line": 2533}, "message": "Assuming field 'use_4addr' is true"}, {"location": {"col": 3, "file": 0, "line": 2533}, "message": "Taking true branch"}, {"location": {"col": 4, "file": 0, "line": 2536}, "message": "Null pointer passed as 2nd argument to memory copy function"}, {"location": {"col": 4, "file": 0, "line": 2536}, "message": "Null pointer passed as 2nd argument to memory copy function"}], "macros": [], "notes": [], "path": "/src/net/mac80211/tx.c", "reportHash": "31bb9ed1a5b6a6d596b71c3ba13b87d6", "checkerName": "clang-analyzer-unix.cstring.NullArg", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 6, "file": 0, "line": 5480}, "message": "Assuming 'skb' is non-null"}, {"location": {"col": 2, "file": 0, "line": 5480}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 5483}, "message": "Calling 'skb_reserve'"}, {"location": {"col": 1, "file": 1, "line": 2398}, "message": "Returning without writing to 'skb->.sk', which participates in a condition later"}, {"location": {"col": 2, "file": 0, "line": 5483}, "message": "Returning from 'skb_reserve'"}, {"location": {"col": 18, "file": 0, "line": 5487}, "message": "'?' condition is true"}, {"location": {"col": 18, "file": 2, "line": 141}, "message": "expanded from macro 'htons'"}, {"location": {"col": 21, "file": 2, "line": 135}, "message": "expanded from macro '___htons'"}, {"location": {"col": 43, "file": 3, "line": 41}, "message": "expanded from macro '__cpu_to_be16'"}, {"location": {"col": 3, "file": 4, "line": 105}, "message": "expanded from macro '__swab16'"}, {"location": {"col": 2, "file": 0, "line": 5492}, "message": "Calling '__ieee80211_subif_start_xmit'"}, {"location": {"col": 6, "file": 0, "line": 3924}, "message": "Assuming the condition is false"}, {"location": {"col": 22, "file": 5, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 3924}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 3931}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 3934}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 3937}, "message": "Assuming field 'wake_tx_queue' is null"}, {"location": {"col": 2, "file": 0, "line": 3937}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 3943}, "message": "'sta' is non-null"}, {"location": {"col": 2, "file": 0, "line": 3943}, "message": "Taking true branch"}, {"location": {"col": 13, "file": 0, "line": 3948}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 6, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 6, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 6, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 8, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 13, "file": 0, "line": 3948}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 6, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 6, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 6, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 8, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 13, "file": 0, "line": 3948}, "message": "Left side of '||' is false"}, {"location": {"col": 28, "file": 6, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 6, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 6, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 21, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 3, "file": 8, "line": 282}, "message": "expanded from macro '__native_word'"}, {"location": {"col": 13, "file": 0, "line": 3948}, "message": "Left side of '||' is true"}, {"location": {"col": 28, "file": 6, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 6, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 6, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 2, "file": 7, "line": 49}, "message": "expanded from macro 'READ_ONCE'"}, {"location": {"col": 38, "file": 7, "line": 36}, "message": "expanded from macro 'compiletime_assert_rwonce_type'"}, {"location": {"col": 13, "file": 0, "line": 3948}, "message": "Taking false branch"}, {"location": {"col": 28, "file": 6, "line": 571}, "message": "expanded from macro 'rcu_dereference'"}, {"location": {"col": 2, "file": 6, "line": 513}, "message": "expanded from macro 'rcu_dereference_check'"}, {"location": {"col": 48, "file": 6, "line": 374}, "message": "expanded from macro '__rcu_dereference_check'"}, {"location": {"col": 11, "file": 0, "line": 3408}, "message": "Dereference of null pointer"}], "macros": [], "notes": [], "path": "/src/net/mac80211/tx.c", "reportHash": "cd56fd7d51222f0a4d50f96f7a85a556", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 5182}, "message": "Value stored to 'pos' is never read"}, {"location": {"col": 2, "file": 0, "line": 5182}, "message": "Value stored to 'pos' is never read"}], "macros": [], "notes": [], "path": "/src/net/mac80211/tx.c", "reportHash": "7d7d959418f84e1ffbc38909b1e0add4", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
