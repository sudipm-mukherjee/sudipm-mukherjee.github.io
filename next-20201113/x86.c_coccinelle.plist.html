<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/arch/x86/kvm/x86.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * derived from drivers/kvm/kvm_main.c\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright (C) 2008 Qumranet, Inc.\n * Copyright IBM Corporation, 2008\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *   Amit Shah    <amit.shah@qumranet.com>\n *   Ben-Ami Yassour <benami@il.ibm.com>\n */\n\n#include <linux/kvm_host.h>\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"mmu.h\"\n#include \"i8254.h\"\n#include \"tss.h\"\n#include \"kvm_cache_regs.h\"\n#include \"kvm_emulate.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"pmu.h\"\n#include \"hyperv.h\"\n#include \"lapic.h\"\n\n#include <linux/clocksource.h>\n#include <linux/interrupt.h>\n#include <linux/kvm.h>\n#include <linux/fs.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/moduleparam.h>\n#include <linux/mman.h>\n#include <linux/highmem.h>\n#include <linux/iommu.h>\n#include <linux/intel-iommu.h>\n#include <linux/cpufreq.h>\n#include <linux/user-return-notifier.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/perf_event.h>\n#include <linux/uaccess.h>\n#include <linux/hash.h>\n#include <linux/pci.h>\n#include <linux/timekeeper_internal.h>\n#include <linux/pvclock_gtod.h>\n#include <linux/kvm_irqfd.h>\n#include <linux/irqbypass.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/isolation.h>\n#include <linux/mem_encrypt.h>\n#include <linux/entry-kvm.h>\n\n#include <trace/events/kvm.h>\n\n#include <asm/debugreg.h>\n#include <asm/msr.h>\n#include <asm/desc.h>\n#include <asm/mce.h>\n#include <linux/kernel_stat.h>\n#include <asm/fpu/internal.h> /* Ugh! */\n#include <asm/pvclock.h>\n#include <asm/div64.h>\n#include <asm/irq_remapping.h>\n#include <asm/mshyperv.h>\n#include <asm/hypervisor.h>\n#include <asm/tlbflush.h>\n#include <asm/intel_pt.h>\n#include <asm/emulate_prefix.h>\n#include <clocksource/hyperv_timer.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\n#define MAX_IO_MSRS 256\n#define KVM_MAX_MCE_BANKS 32\nu64 __read_mostly kvm_mce_cap_supported = MCG_CTL_P | MCG_SER_P;\nEXPORT_SYMBOL_GPL(kvm_mce_cap_supported);\n\n#define emul_to_vcpu(ctxt) \\\n\t((struct kvm_vcpu *)(ctxt)->vcpu)\n\n/* EFER defaults:\n * - enable syscall per default because its emulated by KVM\n * - enable LME and LMA per default on 64 bit KVM\n */\n#ifdef CONFIG_X86_64\nstatic\nu64 __read_mostly efer_reserved_bits = ~((u64)(EFER_SCE | EFER_LME | EFER_LMA));\n#else\nstatic u64 __read_mostly efer_reserved_bits = ~((u64)EFER_SCE);\n#endif\n\nstatic u64 __read_mostly cr4_reserved_bits = CR4_RESERVED_BITS;\n\n#define KVM_X2APIC_API_VALID_FLAGS (KVM_X2APIC_API_USE_32BIT_IDS | \\\n                                    KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu);\nstatic void process_nmi(struct kvm_vcpu *vcpu);\nstatic void enter_smm(struct kvm_vcpu *vcpu);\nstatic void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);\nstatic void store_regs(struct kvm_vcpu *vcpu);\nstatic int sync_regs(struct kvm_vcpu *vcpu);\n\nstruct kvm_x86_ops kvm_x86_ops __read_mostly;\nEXPORT_SYMBOL_GPL(kvm_x86_ops);\n\nstatic bool __read_mostly ignore_msrs = 0;\nmodule_param(ignore_msrs, bool, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly report_ignored_msrs = true;\nmodule_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);\n\nunsigned int min_timer_period_us = 200;\nmodule_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly kvmclock_periodic_sync = true;\nmodule_param(kvmclock_periodic_sync, bool, S_IRUGO);\n\nbool __read_mostly kvm_has_tsc_control;\nEXPORT_SYMBOL_GPL(kvm_has_tsc_control);\nu32  __read_mostly kvm_max_guest_tsc_khz;\nEXPORT_SYMBOL_GPL(kvm_max_guest_tsc_khz);\nu8   __read_mostly kvm_tsc_scaling_ratio_frac_bits;\nEXPORT_SYMBOL_GPL(kvm_tsc_scaling_ratio_frac_bits);\nu64  __read_mostly kvm_max_tsc_scaling_ratio;\nEXPORT_SYMBOL_GPL(kvm_max_tsc_scaling_ratio);\nu64 __read_mostly kvm_default_tsc_scaling_ratio;\nEXPORT_SYMBOL_GPL(kvm_default_tsc_scaling_ratio);\n\n/* tsc tolerance in parts per million - default to 1/2 of the NTP threshold */\nstatic u32 __read_mostly tsc_tolerance_ppm = 250;\nmodule_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);\n\n/*\n * lapic timer advance (tscdeadline mode only) in nanoseconds.  '-1' enables\n * adaptive tuning starting from default advancment of 1000ns.  '0' disables\n * advancement entirely.  Any other value is used as-is and disables adaptive\n * tuning, i.e. allows priveleged userspace to set an exact advancement time.\n */\nstatic int __read_mostly lapic_timer_advance_ns = -1;\nmodule_param(lapic_timer_advance_ns, int, S_IRUGO | S_IWUSR);\n\nstatic bool __read_mostly vector_hashing = true;\nmodule_param(vector_hashing, bool, S_IRUGO);\n\nbool __read_mostly enable_vmware_backdoor = false;\nmodule_param(enable_vmware_backdoor, bool, S_IRUGO);\nEXPORT_SYMBOL_GPL(enable_vmware_backdoor);\n\nstatic bool __read_mostly force_emulation_prefix = false;\nmodule_param(force_emulation_prefix, bool, S_IRUGO);\n\nint __read_mostly pi_inject_timer = -1;\nmodule_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);\n\n/*\n * Restoring the host value for MSRs that are only consumed when running in\n * usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU\n * returns to userspace, i.e. the kernel can run with the guest's value.\n */\n#define KVM_MAX_NR_USER_RETURN_MSRS 16\n\nstruct kvm_user_return_msrs_global {\n\tint nr;\n\tu32 msrs[KVM_MAX_NR_USER_RETURN_MSRS];\n};\n\nstruct kvm_user_return_msrs {\n\tstruct user_return_notifier urn;\n\tbool registered;\n\tstruct kvm_user_return_msr_values {\n\t\tu64 host;\n\t\tu64 curr;\n\t} values[KVM_MAX_NR_USER_RETURN_MSRS];\n};\n\nstatic struct kvm_user_return_msrs_global __read_mostly user_return_msrs_global;\nstatic struct kvm_user_return_msrs __percpu *user_return_msrs;\n\n#define KVM_SUPPORTED_XCR0     (XFEATURE_MASK_FP | XFEATURE_MASK_SSE \\\n\t\t\t\t| XFEATURE_MASK_YMM | XFEATURE_MASK_BNDREGS \\\n\t\t\t\t| XFEATURE_MASK_BNDCSR | XFEATURE_MASK_AVX512 \\\n\t\t\t\t| XFEATURE_MASK_PKRU)\n\nu64 __read_mostly host_efer;\nEXPORT_SYMBOL_GPL(host_efer);\n\nbool __read_mostly allow_smaller_maxphyaddr = 0;\nEXPORT_SYMBOL_GPL(allow_smaller_maxphyaddr);\n\nstatic u64 __read_mostly host_xss;\nu64 __read_mostly supported_xss;\nEXPORT_SYMBOL_GPL(supported_xss);\n\nstruct kvm_stats_debugfs_item debugfs_entries[] = {\n\tVCPU_STAT(\"pf_fixed\", pf_fixed),\n\tVCPU_STAT(\"pf_guest\", pf_guest),\n\tVCPU_STAT(\"tlb_flush\", tlb_flush),\n\tVCPU_STAT(\"invlpg\", invlpg),\n\tVCPU_STAT(\"exits\", exits),\n\tVCPU_STAT(\"io_exits\", io_exits),\n\tVCPU_STAT(\"mmio_exits\", mmio_exits),\n\tVCPU_STAT(\"signal_exits\", signal_exits),\n\tVCPU_STAT(\"irq_window\", irq_window_exits),\n\tVCPU_STAT(\"nmi_window\", nmi_window_exits),\n\tVCPU_STAT(\"halt_exits\", halt_exits),\n\tVCPU_STAT(\"halt_successful_poll\", halt_successful_poll),\n\tVCPU_STAT(\"halt_attempted_poll\", halt_attempted_poll),\n\tVCPU_STAT(\"halt_poll_invalid\", halt_poll_invalid),\n\tVCPU_STAT(\"halt_wakeup\", halt_wakeup),\n\tVCPU_STAT(\"hypercalls\", hypercalls),\n\tVCPU_STAT(\"request_irq\", request_irq_exits),\n\tVCPU_STAT(\"irq_exits\", irq_exits),\n\tVCPU_STAT(\"host_state_reload\", host_state_reload),\n\tVCPU_STAT(\"fpu_reload\", fpu_reload),\n\tVCPU_STAT(\"insn_emulation\", insn_emulation),\n\tVCPU_STAT(\"insn_emulation_fail\", insn_emulation_fail),\n\tVCPU_STAT(\"irq_injections\", irq_injections),\n\tVCPU_STAT(\"nmi_injections\", nmi_injections),\n\tVCPU_STAT(\"req_event\", req_event),\n\tVCPU_STAT(\"l1d_flush\", l1d_flush),\n\tVCPU_STAT(\"halt_poll_success_ns\", halt_poll_success_ns),\n\tVCPU_STAT(\"halt_poll_fail_ns\", halt_poll_fail_ns),\n\tVM_STAT(\"mmu_shadow_zapped\", mmu_shadow_zapped),\n\tVM_STAT(\"mmu_pte_write\", mmu_pte_write),\n\tVM_STAT(\"mmu_pte_updated\", mmu_pte_updated),\n\tVM_STAT(\"mmu_pde_zapped\", mmu_pde_zapped),\n\tVM_STAT(\"mmu_flooded\", mmu_flooded),\n\tVM_STAT(\"mmu_recycled\", mmu_recycled),\n\tVM_STAT(\"mmu_cache_miss\", mmu_cache_miss),\n\tVM_STAT(\"mmu_unsync\", mmu_unsync),\n\tVM_STAT(\"remote_tlb_flush\", remote_tlb_flush),\n\tVM_STAT(\"largepages\", lpages, .mode = 0444),\n\tVM_STAT(\"nx_largepages_splitted\", nx_lpage_splits, .mode = 0444),\n\tVM_STAT(\"max_mmu_page_hash_collisions\", max_mmu_page_hash_collisions),\n\t{ NULL }\n};\n\nu64 __read_mostly host_xcr0;\nu64 __read_mostly supported_xcr0;\nEXPORT_SYMBOL_GPL(supported_xcr0);\n\nstatic struct kmem_cache *x86_fpu_cache;\n\nstatic struct kmem_cache *x86_emulator_cache;\n\n/*\n * When called, it means the previous get/set msr reached an invalid msr.\n * Return true if we want to ignore/silent this failed msr access.\n */\nstatic bool kvm_msr_ignored_check(struct kvm_vcpu *vcpu, u32 msr,\n\t\t\t\t  u64 data, bool write)\n{\n\tconst char *op = write ? \"wrmsr\" : \"rdmsr\";\n\n\tif (ignore_msrs) {\n\t\tif (report_ignored_msrs)\n\t\t\tkvm_pr_unimpl(\"ignored %s: 0x%x data 0x%llx\\n\",\n\t\t\t\t      op, msr, data);\n\t\t/* Mask the error */\n\t\treturn true;\n\t} else {\n\t\tkvm_debug_ratelimited(\"unhandled %s: 0x%x data 0x%llx\\n\",\n\t\t\t\t      op, msr, data);\n\t\treturn false;\n\t}\n}\n\nstatic struct kmem_cache *kvm_alloc_emulator_cache(void)\n{\n\tunsigned int useroffset = offsetof(struct x86_emulate_ctxt, src);\n\tunsigned int size = sizeof(struct x86_emulate_ctxt);\n\n\treturn kmem_cache_create_usercopy(\"x86_emulator\", size,\n\t\t\t\t\t  __alignof__(struct x86_emulate_ctxt),\n\t\t\t\t\t  SLAB_ACCOUNT, useroffset,\n\t\t\t\t\t  size - useroffset, NULL);\n}\n\nstatic int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt);\n\nstatic inline void kvm_async_pf_hash_reset(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\tfor (i = 0; i < ASYNC_PF_PER_VCPU; i++)\n\t\tvcpu->arch.apf.gfns[i] = ~0;\n}\n\nstatic void kvm_on_user_return(struct user_return_notifier *urn)\n{\n\tunsigned slot;\n\tstruct kvm_user_return_msrs *msrs\n\t\t= container_of(urn, struct kvm_user_return_msrs, urn);\n\tstruct kvm_user_return_msr_values *values;\n\tunsigned long flags;\n\n\t/*\n\t * Disabling irqs at this point since the following code could be\n\t * interrupted and executed through kvm_arch_hardware_disable()\n\t */\n\tlocal_irq_save(flags);\n\tif (msrs->registered) {\n\t\tmsrs->registered = false;\n\t\tuser_return_notifier_unregister(urn);\n\t}\n\tlocal_irq_restore(flags);\n\tfor (slot = 0; slot < user_return_msrs_global.nr; ++slot) {\n\t\tvalues = &msrs->values[slot];\n\t\tif (values->host != values->curr) {\n\t\t\twrmsrl(user_return_msrs_global.msrs[slot], values->host);\n\t\t\tvalues->curr = values->host;\n\t\t}\n\t}\n}\n\nvoid kvm_define_user_return_msr(unsigned slot, u32 msr)\n{\n\tBUG_ON(slot >= KVM_MAX_NR_USER_RETURN_MSRS);\n\tuser_return_msrs_global.msrs[slot] = msr;\n\tif (slot >= user_return_msrs_global.nr)\n\t\tuser_return_msrs_global.nr = slot + 1;\n}\nEXPORT_SYMBOL_GPL(kvm_define_user_return_msr);\n\nstatic void kvm_user_return_msr_cpu_online(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\tu64 value;\n\tint i;\n\n\tfor (i = 0; i < user_return_msrs_global.nr; ++i) {\n\t\trdmsrl_safe(user_return_msrs_global.msrs[i], &value);\n\t\tmsrs->values[i].host = value;\n\t\tmsrs->values[i].curr = value;\n\t}\n}\n\nint kvm_set_user_return_msr(unsigned slot, u64 value, u64 mask)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\tint err;\n\n\tvalue = (value & mask) | (msrs->values[slot].host & ~mask);\n\tif (value == msrs->values[slot].curr)\n\t\treturn 0;\n\terr = wrmsrl_safe(user_return_msrs_global.msrs[slot], value);\n\tif (err)\n\t\treturn 1;\n\n\tmsrs->values[slot].curr = value;\n\tif (!msrs->registered) {\n\t\tmsrs->urn.on_user_return = kvm_on_user_return;\n\t\tuser_return_notifier_register(&msrs->urn);\n\t\tmsrs->registered = true;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_user_return_msr);\n\nstatic void drop_user_return_notifiers(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\tstruct kvm_user_return_msrs *msrs = per_cpu_ptr(user_return_msrs, cpu);\n\n\tif (msrs->registered)\n\t\tkvm_on_user_return(&msrs->urn);\n}\n\nu64 kvm_get_apic_base(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.apic_base;\n}\nEXPORT_SYMBOL_GPL(kvm_get_apic_base);\n\nenum lapic_mode kvm_get_apic_mode(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_apic_mode(kvm_get_apic_base(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_get_apic_mode);\n\nint kvm_set_apic_base(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tenum lapic_mode old_mode = kvm_get_apic_mode(vcpu);\n\tenum lapic_mode new_mode = kvm_apic_mode(msr_info->data);\n\tu64 reserved_bits = ((~0ULL) << cpuid_maxphyaddr(vcpu)) | 0x2ff |\n\t\t(guest_cpuid_has(vcpu, X86_FEATURE_X2APIC) ? 0 : X2APIC_ENABLE);\n\n\tif ((msr_info->data & reserved_bits) != 0 || new_mode == LAPIC_MODE_INVALID)\n\t\treturn 1;\n\tif (!msr_info->host_initiated) {\n\t\tif (old_mode == LAPIC_MODE_X2APIC && new_mode == LAPIC_MODE_XAPIC)\n\t\t\treturn 1;\n\t\tif (old_mode == LAPIC_MODE_DISABLED && new_mode == LAPIC_MODE_X2APIC)\n\t\t\treturn 1;\n\t}\n\n\tkvm_lapic_set_base(vcpu, msr_info->data);\n\tkvm_recalculate_apic_map(vcpu->kvm);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_apic_base);\n\nasmlinkage __visible noinstr void kvm_spurious_fault(void)\n{\n\t/* Fault while not rebooting.  We want the trace. */\n\tBUG_ON(!kvm_rebooting);\n}\nEXPORT_SYMBOL_GPL(kvm_spurious_fault);\n\n#define EXCPT_BENIGN\t\t0\n#define EXCPT_CONTRIBUTORY\t1\n#define EXCPT_PF\t\t2\n\nstatic int exception_class(int vector)\n{\n\tswitch (vector) {\n\tcase PF_VECTOR:\n\t\treturn EXCPT_PF;\n\tcase DE_VECTOR:\n\tcase TS_VECTOR:\n\tcase NP_VECTOR:\n\tcase SS_VECTOR:\n\tcase GP_VECTOR:\n\t\treturn EXCPT_CONTRIBUTORY;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn EXCPT_BENIGN;\n}\n\n#define EXCPT_FAULT\t\t0\n#define EXCPT_TRAP\t\t1\n#define EXCPT_ABORT\t\t2\n#define EXCPT_INTERRUPT\t\t3\n\nstatic int exception_type(int vector)\n{\n\tunsigned int mask;\n\n\tif (WARN_ON(vector > 31 || vector == NMI_VECTOR))\n\t\treturn EXCPT_INTERRUPT;\n\n\tmask = 1 << vector;\n\n\t/* #DB is trap, as instruction watchpoints are handled elsewhere */\n\tif (mask & ((1 << DB_VECTOR) | (1 << BP_VECTOR) | (1 << OF_VECTOR)))\n\t\treturn EXCPT_TRAP;\n\n\tif (mask & ((1 << DF_VECTOR) | (1 << MC_VECTOR)))\n\t\treturn EXCPT_ABORT;\n\n\t/* Reserved exceptions will result in fault */\n\treturn EXCPT_FAULT;\n}\n\nvoid kvm_deliver_exception_payload(struct kvm_vcpu *vcpu)\n{\n\tunsigned nr = vcpu->arch.exception.nr;\n\tbool has_payload = vcpu->arch.exception.has_payload;\n\tunsigned long payload = vcpu->arch.exception.payload;\n\n\tif (!has_payload)\n\t\treturn;\n\n\tswitch (nr) {\n\tcase DB_VECTOR:\n\t\t/*\n\t\t * \"Certain debug exceptions may clear bit 0-3.  The\n\t\t * remaining contents of the DR6 register are never\n\t\t * cleared by the processor\".\n\t\t */\n\t\tvcpu->arch.dr6 &= ~DR_TRAP_BITS;\n\t\t/*\n\t\t * DR6.RTM is set by all #DB exceptions that don't clear it.\n\t\t */\n\t\tvcpu->arch.dr6 |= DR6_RTM;\n\t\tvcpu->arch.dr6 |= payload;\n\t\t/*\n\t\t * Bit 16 should be set in the payload whenever the #DB\n\t\t * exception should clear DR6.RTM. This makes the payload\n\t\t * compatible with the pending debug exceptions under VMX.\n\t\t * Though not currently documented in the SDM, this also\n\t\t * makes the payload compatible with the exit qualification\n\t\t * for #DB exceptions under VMX.\n\t\t */\n\t\tvcpu->arch.dr6 ^= payload & DR6_RTM;\n\n\t\t/*\n\t\t * The #DB payload is defined as compatible with the 'pending\n\t\t * debug exceptions' field under VMX, not DR6. While bit 12 is\n\t\t * defined in the 'pending debug exceptions' field (enabled\n\t\t * breakpoint), it is reserved and must be zero in DR6.\n\t\t */\n\t\tvcpu->arch.dr6 &= ~BIT(12);\n\t\tbreak;\n\tcase PF_VECTOR:\n\t\tvcpu->arch.cr2 = payload;\n\t\tbreak;\n\t}\n\n\tvcpu->arch.exception.has_payload = false;\n\tvcpu->arch.exception.payload = 0;\n}\nEXPORT_SYMBOL_GPL(kvm_deliver_exception_payload);\n\nstatic void kvm_multiple_exception(struct kvm_vcpu *vcpu,\n\t\tunsigned nr, bool has_error, u32 error_code,\n\t        bool has_payload, unsigned long payload, bool reinject)\n{\n\tu32 prev_nr;\n\tint class1, class2;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tif (!vcpu->arch.exception.pending && !vcpu->arch.exception.injected) {\n\tqueue:\n\t\tif (has_error && !is_protmode(vcpu))\n\t\t\thas_error = false;\n\t\tif (reinject) {\n\t\t\t/*\n\t\t\t * On vmentry, vcpu->arch.exception.pending is only\n\t\t\t * true if an event injection was blocked by\n\t\t\t * nested_run_pending.  In that case, however,\n\t\t\t * vcpu_enter_guest requests an immediate exit,\n\t\t\t * and the guest shouldn't proceed far enough to\n\t\t\t * need reinjection.\n\t\t\t */\n\t\t\tWARN_ON_ONCE(vcpu->arch.exception.pending);\n\t\t\tvcpu->arch.exception.injected = true;\n\t\t\tif (WARN_ON_ONCE(has_payload)) {\n\t\t\t\t/*\n\t\t\t\t * A reinjected event has already\n\t\t\t\t * delivered its payload.\n\t\t\t\t */\n\t\t\t\thas_payload = false;\n\t\t\t\tpayload = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tvcpu->arch.exception.pending = true;\n\t\t\tvcpu->arch.exception.injected = false;\n\t\t}\n\t\tvcpu->arch.exception.has_error_code = has_error;\n\t\tvcpu->arch.exception.nr = nr;\n\t\tvcpu->arch.exception.error_code = error_code;\n\t\tvcpu->arch.exception.has_payload = has_payload;\n\t\tvcpu->arch.exception.payload = payload;\n\t\tif (!is_guest_mode(vcpu))\n\t\t\tkvm_deliver_exception_payload(vcpu);\n\t\treturn;\n\t}\n\n\t/* to check exception */\n\tprev_nr = vcpu->arch.exception.nr;\n\tif (prev_nr == DF_VECTOR) {\n\t\t/* triple fault -> shutdown */\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\tclass1 = exception_class(prev_nr);\n\tclass2 = exception_class(nr);\n\tif ((class1 == EXCPT_CONTRIBUTORY && class2 == EXCPT_CONTRIBUTORY)\n\t\t|| (class1 == EXCPT_PF && class2 != EXCPT_BENIGN)) {\n\t\t/*\n\t\t * Generate double fault per SDM Table 5-5.  Set\n\t\t * exception.pending = true so that the double fault\n\t\t * can trigger a nested vmexit.\n\t\t */\n\t\tvcpu->arch.exception.pending = true;\n\t\tvcpu->arch.exception.injected = false;\n\t\tvcpu->arch.exception.has_error_code = true;\n\t\tvcpu->arch.exception.nr = DF_VECTOR;\n\t\tvcpu->arch.exception.error_code = 0;\n\t\tvcpu->arch.exception.has_payload = false;\n\t\tvcpu->arch.exception.payload = 0;\n\t} else\n\t\t/* replace previous exception with a new one in a hope\n\t\t   that instruction re-execution will regenerate lost\n\t\t   exception */\n\t\tgoto queue;\n}\n\nvoid kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, false, 0, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception);\n\nvoid kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, false, 0, true);\n}\nEXPORT_SYMBOL_GPL(kvm_requeue_exception);\n\nvoid kvm_queue_exception_p(struct kvm_vcpu *vcpu, unsigned nr,\n\t\t\t   unsigned long payload)\n{\n\tkvm_multiple_exception(vcpu, nr, false, 0, true, payload, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception_p);\n\nstatic void kvm_queue_exception_e_p(struct kvm_vcpu *vcpu, unsigned nr,\n\t\t\t\t    u32 error_code, unsigned long payload)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code,\n\t\t\t       true, payload, false);\n}\n\nint kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)\n{\n\tif (err)\n\t\tkvm_inject_gp(vcpu, 0);\n\telse\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_complete_insn_gp);\n\nvoid kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)\n{\n\t++vcpu->stat.pf_guest;\n\tvcpu->arch.exception.nested_apf =\n\t\tis_guest_mode(vcpu) && fault->async_page_fault;\n\tif (vcpu->arch.exception.nested_apf) {\n\t\tvcpu->arch.apf.nested_apf_token = fault->address;\n\t\tkvm_queue_exception_e(vcpu, PF_VECTOR, fault->error_code);\n\t} else {\n\t\tkvm_queue_exception_e_p(vcpu, PF_VECTOR, fault->error_code,\n\t\t\t\t\tfault->address);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_inject_page_fault);\n\nbool kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct x86_exception *fault)\n{\n\tstruct kvm_mmu *fault_mmu;\n\tWARN_ON_ONCE(fault->vector != PF_VECTOR);\n\n\tfault_mmu = fault->nested_page_fault ? vcpu->arch.mmu :\n\t\t\t\t\t       vcpu->arch.walk_mmu;\n\n\t/*\n\t * Invalidate the TLB entry for the faulting address, if it exists,\n\t * else the access will fault indefinitely (and to emulate hardware).\n\t */\n\tif ((fault->error_code & PFERR_PRESENT_MASK) &&\n\t    !(fault->error_code & PFERR_RSVD_MASK))\n\t\tkvm_mmu_invalidate_gva(vcpu, fault_mmu, fault->address,\n\t\t\t\t       fault_mmu->root_hpa);\n\n\tfault_mmu->inject_page_fault(vcpu, fault);\n\treturn fault->nested_page_fault;\n}\nEXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);\n\nvoid kvm_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tatomic_inc(&vcpu->arch.nmi_queued);\n\tkvm_make_request(KVM_REQ_NMI, vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_inject_nmi);\n\nvoid kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code, false, 0, false);\n}\nEXPORT_SYMBOL_GPL(kvm_queue_exception_e);\n\nvoid kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code)\n{\n\tkvm_multiple_exception(vcpu, nr, true, error_code, false, 0, true);\n}\nEXPORT_SYMBOL_GPL(kvm_requeue_exception_e);\n\n/*\n * Checks if cpl <= required_cpl; if true, return true.  Otherwise queue\n * a #GP and return false.\n */\nbool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl)\n{\n\tif (kvm_x86_ops.get_cpl(vcpu) <= required_cpl)\n\t\treturn true;\n\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_require_cpl);\n\nbool kvm_require_dr(struct kvm_vcpu *vcpu, int dr)\n{\n\tif ((dr != 4 && dr != 5) || !kvm_read_cr4_bits(vcpu, X86_CR4_DE))\n\t\treturn true;\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_require_dr);\n\n/*\n * This function will be used to read from the physical memory of the currently\n * running guest. The difference to kvm_vcpu_read_guest_page is that this function\n * can read from guest physical or from the guest's guest physical memory.\n */\nint kvm_read_guest_page_mmu(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t    gfn_t ngfn, void *data, int offset, int len,\n\t\t\t    u32 access)\n{\n\tstruct x86_exception exception;\n\tgfn_t real_gfn;\n\tgpa_t ngpa;\n\n\tngpa     = gfn_to_gpa(ngfn);\n\treal_gfn = mmu->translate_gpa(vcpu, ngpa, access, &exception);\n\tif (real_gfn == UNMAPPED_GVA)\n\t\treturn -EFAULT;\n\n\treal_gfn = gpa_to_gfn(real_gfn);\n\n\treturn kvm_vcpu_read_guest_page(vcpu, real_gfn, data, offset, len);\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_page_mmu);\n\nstatic int kvm_read_nested_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn,\n\t\t\t       void *data, int offset, int len, u32 access)\n{\n\treturn kvm_read_guest_page_mmu(vcpu, vcpu->arch.walk_mmu, gfn,\n\t\t\t\t       data, offset, len, access);\n}\n\nstatic inline u64 pdptr_rsvd_bits(struct kvm_vcpu *vcpu)\n{\n\treturn rsvd_bits(cpuid_maxphyaddr(vcpu), 63) | rsvd_bits(5, 8) |\n\t       rsvd_bits(1, 2);\n}\n\n/*\n * Load the pae pdptrs.  Return 1 if they are all valid, 0 otherwise.\n */\nint load_pdptrs(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu, unsigned long cr3)\n{\n\tgfn_t pdpt_gfn = cr3 >> PAGE_SHIFT;\n\tunsigned offset = ((cr3 & (PAGE_SIZE-1)) >> 5) << 2;\n\tint i;\n\tint ret;\n\tu64 pdpte[ARRAY_SIZE(mmu->pdptrs)];\n\n\tret = kvm_read_guest_page_mmu(vcpu, mmu, pdpt_gfn, pdpte,\n\t\t\t\t      offset * sizeof(u64), sizeof(pdpte),\n\t\t\t\t      PFERR_USER_MASK|PFERR_WRITE_MASK);\n\tif (ret < 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\tfor (i = 0; i < ARRAY_SIZE(pdpte); ++i) {\n\t\tif ((pdpte[i] & PT_PRESENT_MASK) &&\n\t\t    (pdpte[i] & pdptr_rsvd_bits(vcpu))) {\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = 1;\n\n\tmemcpy(mmu->pdptrs, pdpte, sizeof(mmu->pdptrs));\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_PDPTR);\n\nout:\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(load_pdptrs);\n\nbool pdptrs_changed(struct kvm_vcpu *vcpu)\n{\n\tu64 pdpte[ARRAY_SIZE(vcpu->arch.walk_mmu->pdptrs)];\n\tint offset;\n\tgfn_t gfn;\n\tint r;\n\n\tif (!is_pae_paging(vcpu))\n\t\treturn false;\n\n\tif (!kvm_register_is_available(vcpu, VCPU_EXREG_PDPTR))\n\t\treturn true;\n\n\tgfn = (kvm_read_cr3(vcpu) & 0xffffffe0ul) >> PAGE_SHIFT;\n\toffset = (kvm_read_cr3(vcpu) & 0xffffffe0ul) & (PAGE_SIZE - 1);\n\tr = kvm_read_nested_guest_page(vcpu, gfn, pdpte, offset, sizeof(pdpte),\n\t\t\t\t       PFERR_USER_MASK | PFERR_WRITE_MASK);\n\tif (r < 0)\n\t\treturn true;\n\n\treturn memcmp(pdpte, vcpu->arch.walk_mmu->pdptrs, sizeof(pdpte)) != 0;\n}\nEXPORT_SYMBOL_GPL(pdptrs_changed);\n\nint kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tunsigned long old_cr0 = kvm_read_cr0(vcpu);\n\tunsigned long pdptr_bits = X86_CR0_CD | X86_CR0_NW | X86_CR0_PG;\n\tunsigned long update_bits = X86_CR0_PG | X86_CR0_WP;\n\n\tcr0 |= X86_CR0_ET;\n\n#ifdef CONFIG_X86_64\n\tif (cr0 & 0xffffffff00000000UL)\n\t\treturn 1;\n#endif\n\n\tcr0 &= ~CR0_RESERVED_BITS;\n\n\tif ((cr0 & X86_CR0_NW) && !(cr0 & X86_CR0_CD))\n\t\treturn 1;\n\n\tif ((cr0 & X86_CR0_PG) && !(cr0 & X86_CR0_PE))\n\t\treturn 1;\n\n#ifdef CONFIG_X86_64\n\tif ((vcpu->arch.efer & EFER_LME) && !is_paging(vcpu) &&\n\t    (cr0 & X86_CR0_PG)) {\n\t\tint cs_db, cs_l;\n\n\t\tif (!is_pae(vcpu))\n\t\t\treturn 1;\n\t\tkvm_x86_ops.get_cs_db_l_bits(vcpu, &cs_db, &cs_l);\n\t\tif (cs_l)\n\t\t\treturn 1;\n\t}\n#endif\n\tif (!(vcpu->arch.efer & EFER_LME) && (cr0 & X86_CR0_PG) &&\n\t    is_pae(vcpu) && ((cr0 ^ old_cr0) & pdptr_bits) &&\n\t    !load_pdptrs(vcpu, vcpu->arch.walk_mmu, kvm_read_cr3(vcpu)))\n\t\treturn 1;\n\n\tif (!(cr0 & X86_CR0_PG) && kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE))\n\t\treturn 1;\n\n\tkvm_x86_ops.set_cr0(vcpu, cr0);\n\n\tif ((cr0 ^ old_cr0) & X86_CR0_PG) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_async_pf_hash_reset(vcpu);\n\t}\n\n\tif ((cr0 ^ old_cr0) & update_bits)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tif (((cr0 ^ old_cr0) & X86_CR0_CD) &&\n\t    kvm_arch_has_noncoherent_dma(vcpu->kvm) &&\n\t    !kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\tkvm_zap_gfn_range(vcpu->kvm, 0, ~0ULL);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr0);\n\nvoid kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw)\n{\n\t(void)kvm_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~0x0eul) | (msw & 0x0f));\n}\nEXPORT_SYMBOL_GPL(kvm_lmsw);\n\nvoid kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) {\n\n\t\tif (vcpu->arch.xcr0 != host_xcr0)\n\t\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, vcpu->arch.xcr0);\n\n\t\tif (vcpu->arch.xsaves_enabled &&\n\t\t    vcpu->arch.ia32_xss != host_xss)\n\t\t\twrmsrl(MSR_IA32_XSS, vcpu->arch.ia32_xss);\n\t}\n\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    (kvm_read_cr4_bits(vcpu, X86_CR4_PKE) ||\n\t     (vcpu->arch.xcr0 & XFEATURE_MASK_PKRU)) &&\n\t    vcpu->arch.pkru != vcpu->arch.host_pkru)\n\t\t__write_pkru(vcpu->arch.pkru);\n}\nEXPORT_SYMBOL_GPL(kvm_load_guest_xsave_state);\n\nvoid kvm_load_host_xsave_state(struct kvm_vcpu *vcpu)\n{\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    (kvm_read_cr4_bits(vcpu, X86_CR4_PKE) ||\n\t     (vcpu->arch.xcr0 & XFEATURE_MASK_PKRU))) {\n\t\tvcpu->arch.pkru = rdpkru();\n\t\tif (vcpu->arch.pkru != vcpu->arch.host_pkru)\n\t\t\t__write_pkru(vcpu->arch.host_pkru);\n\t}\n\n\tif (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE)) {\n\n\t\tif (vcpu->arch.xcr0 != host_xcr0)\n\t\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, host_xcr0);\n\n\t\tif (vcpu->arch.xsaves_enabled &&\n\t\t    vcpu->arch.ia32_xss != host_xss)\n\t\t\twrmsrl(MSR_IA32_XSS, host_xss);\n\t}\n\n}\nEXPORT_SYMBOL_GPL(kvm_load_host_xsave_state);\n\nstatic int __kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr)\n{\n\tu64 xcr0 = xcr;\n\tu64 old_xcr0 = vcpu->arch.xcr0;\n\tu64 valid_bits;\n\n\t/* Only support XCR_XFEATURE_ENABLED_MASK(xcr0) now  */\n\tif (index != XCR_XFEATURE_ENABLED_MASK)\n\t\treturn 1;\n\tif (!(xcr0 & XFEATURE_MASK_FP))\n\t\treturn 1;\n\tif ((xcr0 & XFEATURE_MASK_YMM) && !(xcr0 & XFEATURE_MASK_SSE))\n\t\treturn 1;\n\n\t/*\n\t * Do not allow the guest to set bits that we do not support\n\t * saving.  However, xcr0 bit 0 is always set, even if the\n\t * emulated CPU does not support XSAVE (see fx_init).\n\t */\n\tvalid_bits = vcpu->arch.guest_supported_xcr0 | XFEATURE_MASK_FP;\n\tif (xcr0 & ~valid_bits)\n\t\treturn 1;\n\n\tif ((!(xcr0 & XFEATURE_MASK_BNDREGS)) !=\n\t    (!(xcr0 & XFEATURE_MASK_BNDCSR)))\n\t\treturn 1;\n\n\tif (xcr0 & XFEATURE_MASK_AVX512) {\n\t\tif (!(xcr0 & XFEATURE_MASK_YMM))\n\t\t\treturn 1;\n\t\tif ((xcr0 & XFEATURE_MASK_AVX512) != XFEATURE_MASK_AVX512)\n\t\t\treturn 1;\n\t}\n\tvcpu->arch.xcr0 = xcr0;\n\n\tif ((xcr0 ^ old_xcr0) & XFEATURE_MASK_EXTEND)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\treturn 0;\n}\n\nint kvm_set_xcr(struct kvm_vcpu *vcpu, u32 index, u64 xcr)\n{\n\tif (kvm_x86_ops.get_cpl(vcpu) != 0 ||\n\t    __kvm_set_xcr(vcpu, index, xcr)) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_xcr);\n\nint kvm_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tif (cr4 & cr4_reserved_bits)\n\t\treturn -EINVAL;\n\n\tif (cr4 & vcpu->arch.cr4_guest_rsvd_bits)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_valid_cr4);\n\nint kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tunsigned long old_cr4 = kvm_read_cr4(vcpu);\n\tunsigned long pdptr_bits = X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PAE |\n\t\t\t\t   X86_CR4_SMEP;\n\tunsigned long mmu_role_bits = pdptr_bits | X86_CR4_SMAP | X86_CR4_PKE;\n\n\tif (kvm_valid_cr4(vcpu, cr4))\n\t\treturn 1;\n\n\tif (is_long_mode(vcpu)) {\n\t\tif (!(cr4 & X86_CR4_PAE))\n\t\t\treturn 1;\n\t\tif ((cr4 ^ old_cr4) & X86_CR4_LA57)\n\t\t\treturn 1;\n\t} else if (is_paging(vcpu) && (cr4 & X86_CR4_PAE)\n\t\t   && ((cr4 ^ old_cr4) & pdptr_bits)\n\t\t   && !load_pdptrs(vcpu, vcpu->arch.walk_mmu,\n\t\t\t\t   kvm_read_cr3(vcpu)))\n\t\treturn 1;\n\n\tif ((cr4 & X86_CR4_PCIDE) && !(old_cr4 & X86_CR4_PCIDE)) {\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_PCID))\n\t\t\treturn 1;\n\n\t\t/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */\n\t\tif ((kvm_read_cr3(vcpu) & X86_CR3_PCID_MASK) || !is_long_mode(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tif (kvm_x86_ops.set_cr4(vcpu, cr4))\n\t\treturn 1;\n\n\tif (((cr4 ^ old_cr4) & mmu_role_bits) ||\n\t    (!(cr4 & X86_CR4_PCIDE) && (old_cr4 & X86_CR4_PCIDE)))\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tif ((cr4 ^ old_cr4) & (X86_CR4_OSXSAVE | X86_CR4_PKE))\n\t\tkvm_update_cpuid_runtime(vcpu);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr4);\n\nint kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tbool skip_tlb_flush = false;\n#ifdef CONFIG_X86_64\n\tbool pcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);\n\n\tif (pcid_enabled) {\n\t\tskip_tlb_flush = cr3 & X86_CR3_PCID_NOFLUSH;\n\t\tcr3 &= ~X86_CR3_PCID_NOFLUSH;\n\t}\n#endif\n\n\tif (cr3 == kvm_read_cr3(vcpu) && !pdptrs_changed(vcpu)) {\n\t\tif (!skip_tlb_flush) {\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_long_mode(vcpu) &&\n\t    (cr3 & rsvd_bits(cpuid_maxphyaddr(vcpu), 63)))\n\t\treturn 1;\n\telse if (is_pae_paging(vcpu) &&\n\t\t !load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3))\n\t\treturn 1;\n\n\tkvm_mmu_new_pgd(vcpu, cr3, skip_tlb_flush, skip_tlb_flush);\n\tvcpu->arch.cr3 = cr3;\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR3);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr3);\n\nint kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8)\n{\n\tif (cr8 & CR8_RESERVED_BITS)\n\t\treturn 1;\n\tif (lapic_in_kernel(vcpu))\n\t\tkvm_lapic_set_tpr(vcpu, cr8);\n\telse\n\t\tvcpu->arch.cr8 = cr8;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_cr8);\n\nunsigned long kvm_get_cr8(struct kvm_vcpu *vcpu)\n{\n\tif (lapic_in_kernel(vcpu))\n\t\treturn kvm_lapic_get_cr8(vcpu);\n\telse\n\t\treturn vcpu->arch.cr8;\n}\nEXPORT_SYMBOL_GPL(kvm_get_cr8);\n\nstatic void kvm_update_dr0123(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\n\tif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)) {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; i++)\n\t\t\tvcpu->arch.eff_db[i] = vcpu->arch.db[i];\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_RELOAD;\n\t}\n}\n\nvoid kvm_update_dr7(struct kvm_vcpu *vcpu)\n{\n\tunsigned long dr7;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)\n\t\tdr7 = vcpu->arch.guest_debug_dr7;\n\telse\n\t\tdr7 = vcpu->arch.dr7;\n\tkvm_x86_ops.set_dr7(vcpu, dr7);\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_BP_ENABLED;\n\tif (dr7 & DR7_BP_EN_MASK)\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_BP_ENABLED;\n}\nEXPORT_SYMBOL_GPL(kvm_update_dr7);\n\nstatic u64 kvm_dr6_fixed(struct kvm_vcpu *vcpu)\n{\n\tu64 fixed = DR6_FIXED_1;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_RTM))\n\t\tfixed |= DR6_RTM;\n\treturn fixed;\n}\n\nstatic int __kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val)\n{\n\tsize_t size = ARRAY_SIZE(vcpu->arch.db);\n\n\tswitch (dr) {\n\tcase 0 ... 3:\n\t\tvcpu->arch.db[array_index_nospec(dr, size)] = val;\n\t\tif (!(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP))\n\t\t\tvcpu->arch.eff_db[dr] = val;\n\t\tbreak;\n\tcase 4:\n\tcase 6:\n\t\tif (!kvm_dr6_valid(val))\n\t\t\treturn -1; /* #GP */\n\t\tvcpu->arch.dr6 = (val & DR6_VOLATILE) | kvm_dr6_fixed(vcpu);\n\t\tbreak;\n\tcase 5:\n\tdefault: /* 7 */\n\t\tif (!kvm_dr7_valid(val))\n\t\t\treturn -1; /* #GP */\n\t\tvcpu->arch.dr7 = (val & DR7_VOLATILE) | DR7_FIXED_1;\n\t\tkvm_update_dr7(vcpu);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val)\n{\n\tif (__kvm_set_dr(vcpu, dr, val)) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_dr);\n\nint kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val)\n{\n\tsize_t size = ARRAY_SIZE(vcpu->arch.db);\n\n\tswitch (dr) {\n\tcase 0 ... 3:\n\t\t*val = vcpu->arch.db[array_index_nospec(dr, size)];\n\t\tbreak;\n\tcase 4:\n\tcase 6:\n\t\t*val = vcpu->arch.dr6;\n\t\tbreak;\n\tcase 5:\n\tdefault: /* 7 */\n\t\t*val = vcpu->arch.dr7;\n\t\tbreak;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_get_dr);\n\nbool kvm_rdpmc(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data;\n\tint err;\n\n\terr = kvm_pmu_rdpmc(vcpu, ecx, &data);\n\tif (err)\n\t\treturn err;\n\tkvm_rax_write(vcpu, (u32)data);\n\tkvm_rdx_write(vcpu, data >> 32);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(kvm_rdpmc);\n\n/*\n * List of msr numbers which we expose to userspace through KVM_GET_MSRS\n * and KVM_SET_MSRS, and KVM_GET_MSR_INDEX_LIST.\n *\n * The three MSR lists(msrs_to_save, emulated_msrs, msr_based_features)\n * extract the supported MSRs from the related const lists.\n * msrs_to_save is selected from the msrs_to_save_all to reflect the\n * capabilities of the host cpu. This capabilities test skips MSRs that are\n * kvm-specific. Those are put in emulated_msrs_all; filtering of emulated_msrs\n * may depend on host virtualization features rather than host cpu features.\n */\n\nstatic const u32 msrs_to_save_all[] = {\n\tMSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,\n\tMSR_STAR,\n#ifdef CONFIG_X86_64\n\tMSR_CSTAR, MSR_KERNEL_GS_BASE, MSR_SYSCALL_MASK, MSR_LSTAR,\n#endif\n\tMSR_IA32_TSC, MSR_IA32_CR_PAT, MSR_VM_HSAVE_PA,\n\tMSR_IA32_FEAT_CTL, MSR_IA32_BNDCFGS, MSR_TSC_AUX,\n\tMSR_IA32_SPEC_CTRL,\n\tMSR_IA32_RTIT_CTL, MSR_IA32_RTIT_STATUS, MSR_IA32_RTIT_CR3_MATCH,\n\tMSR_IA32_RTIT_OUTPUT_BASE, MSR_IA32_RTIT_OUTPUT_MASK,\n\tMSR_IA32_RTIT_ADDR0_A, MSR_IA32_RTIT_ADDR0_B,\n\tMSR_IA32_RTIT_ADDR1_A, MSR_IA32_RTIT_ADDR1_B,\n\tMSR_IA32_RTIT_ADDR2_A, MSR_IA32_RTIT_ADDR2_B,\n\tMSR_IA32_RTIT_ADDR3_A, MSR_IA32_RTIT_ADDR3_B,\n\tMSR_IA32_UMWAIT_CONTROL,\n\n\tMSR_ARCH_PERFMON_FIXED_CTR0, MSR_ARCH_PERFMON_FIXED_CTR1,\n\tMSR_ARCH_PERFMON_FIXED_CTR0 + 2, MSR_ARCH_PERFMON_FIXED_CTR0 + 3,\n\tMSR_CORE_PERF_FIXED_CTR_CTRL, MSR_CORE_PERF_GLOBAL_STATUS,\n\tMSR_CORE_PERF_GLOBAL_CTRL, MSR_CORE_PERF_GLOBAL_OVF_CTRL,\n\tMSR_ARCH_PERFMON_PERFCTR0, MSR_ARCH_PERFMON_PERFCTR1,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 2, MSR_ARCH_PERFMON_PERFCTR0 + 3,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 4, MSR_ARCH_PERFMON_PERFCTR0 + 5,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 6, MSR_ARCH_PERFMON_PERFCTR0 + 7,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 8, MSR_ARCH_PERFMON_PERFCTR0 + 9,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 10, MSR_ARCH_PERFMON_PERFCTR0 + 11,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 12, MSR_ARCH_PERFMON_PERFCTR0 + 13,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 14, MSR_ARCH_PERFMON_PERFCTR0 + 15,\n\tMSR_ARCH_PERFMON_PERFCTR0 + 16, MSR_ARCH_PERFMON_PERFCTR0 + 17,\n\tMSR_ARCH_PERFMON_EVENTSEL0, MSR_ARCH_PERFMON_EVENTSEL1,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 2, MSR_ARCH_PERFMON_EVENTSEL0 + 3,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 4, MSR_ARCH_PERFMON_EVENTSEL0 + 5,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 6, MSR_ARCH_PERFMON_EVENTSEL0 + 7,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 8, MSR_ARCH_PERFMON_EVENTSEL0 + 9,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 10, MSR_ARCH_PERFMON_EVENTSEL0 + 11,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 12, MSR_ARCH_PERFMON_EVENTSEL0 + 13,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 14, MSR_ARCH_PERFMON_EVENTSEL0 + 15,\n\tMSR_ARCH_PERFMON_EVENTSEL0 + 16, MSR_ARCH_PERFMON_EVENTSEL0 + 17,\n};\n\nstatic u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_all)];\nstatic unsigned num_msrs_to_save;\n\nstatic const u32 emulated_msrs_all[] = {\n\tMSR_KVM_SYSTEM_TIME, MSR_KVM_WALL_CLOCK,\n\tMSR_KVM_SYSTEM_TIME_NEW, MSR_KVM_WALL_CLOCK_NEW,\n\tHV_X64_MSR_GUEST_OS_ID, HV_X64_MSR_HYPERCALL,\n\tHV_X64_MSR_TIME_REF_COUNT, HV_X64_MSR_REFERENCE_TSC,\n\tHV_X64_MSR_TSC_FREQUENCY, HV_X64_MSR_APIC_FREQUENCY,\n\tHV_X64_MSR_CRASH_P0, HV_X64_MSR_CRASH_P1, HV_X64_MSR_CRASH_P2,\n\tHV_X64_MSR_CRASH_P3, HV_X64_MSR_CRASH_P4, HV_X64_MSR_CRASH_CTL,\n\tHV_X64_MSR_RESET,\n\tHV_X64_MSR_VP_INDEX,\n\tHV_X64_MSR_VP_RUNTIME,\n\tHV_X64_MSR_SCONTROL,\n\tHV_X64_MSR_STIMER0_CONFIG,\n\tHV_X64_MSR_VP_ASSIST_PAGE,\n\tHV_X64_MSR_REENLIGHTENMENT_CONTROL, HV_X64_MSR_TSC_EMULATION_CONTROL,\n\tHV_X64_MSR_TSC_EMULATION_STATUS,\n\tHV_X64_MSR_SYNDBG_OPTIONS,\n\tHV_X64_MSR_SYNDBG_CONTROL, HV_X64_MSR_SYNDBG_STATUS,\n\tHV_X64_MSR_SYNDBG_SEND_BUFFER, HV_X64_MSR_SYNDBG_RECV_BUFFER,\n\tHV_X64_MSR_SYNDBG_PENDING_BUFFER,\n\n\tMSR_KVM_ASYNC_PF_EN, MSR_KVM_STEAL_TIME,\n\tMSR_KVM_PV_EOI_EN, MSR_KVM_ASYNC_PF_INT, MSR_KVM_ASYNC_PF_ACK,\n\n\tMSR_IA32_TSC_ADJUST,\n\tMSR_IA32_TSCDEADLINE,\n\tMSR_IA32_ARCH_CAPABILITIES,\n\tMSR_IA32_PERF_CAPABILITIES,\n\tMSR_IA32_MISC_ENABLE,\n\tMSR_IA32_MCG_STATUS,\n\tMSR_IA32_MCG_CTL,\n\tMSR_IA32_MCG_EXT_CTL,\n\tMSR_IA32_SMBASE,\n\tMSR_SMI_COUNT,\n\tMSR_PLATFORM_INFO,\n\tMSR_MISC_FEATURES_ENABLES,\n\tMSR_AMD64_VIRT_SPEC_CTRL,\n\tMSR_IA32_POWER_CTL,\n\tMSR_IA32_UCODE_REV,\n\n\t/*\n\t * The following list leaves out MSRs whose values are determined\n\t * by arch/x86/kvm/vmx/nested.c based on CPUID or other MSRs.\n\t * We always support the \"true\" VMX control MSRs, even if the host\n\t * processor does not, so I am putting these registers here rather\n\t * than in msrs_to_save_all.\n\t */\n\tMSR_IA32_VMX_BASIC,\n\tMSR_IA32_VMX_TRUE_PINBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_PROCBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_EXIT_CTLS,\n\tMSR_IA32_VMX_TRUE_ENTRY_CTLS,\n\tMSR_IA32_VMX_MISC,\n\tMSR_IA32_VMX_CR0_FIXED0,\n\tMSR_IA32_VMX_CR4_FIXED0,\n\tMSR_IA32_VMX_VMCS_ENUM,\n\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\tMSR_IA32_VMX_EPT_VPID_CAP,\n\tMSR_IA32_VMX_VMFUNC,\n\n\tMSR_K7_HWCR,\n\tMSR_KVM_POLL_CONTROL,\n};\n\nstatic u32 emulated_msrs[ARRAY_SIZE(emulated_msrs_all)];\nstatic unsigned num_emulated_msrs;\n\n/*\n * List of msr numbers which are used to expose MSR-based features that\n * can be used by a hypervisor to validate requested CPU features.\n */\nstatic const u32 msr_based_features_all[] = {\n\tMSR_IA32_VMX_BASIC,\n\tMSR_IA32_VMX_TRUE_PINBASED_CTLS,\n\tMSR_IA32_VMX_PINBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_PROCBASED_CTLS,\n\tMSR_IA32_VMX_PROCBASED_CTLS,\n\tMSR_IA32_VMX_TRUE_EXIT_CTLS,\n\tMSR_IA32_VMX_EXIT_CTLS,\n\tMSR_IA32_VMX_TRUE_ENTRY_CTLS,\n\tMSR_IA32_VMX_ENTRY_CTLS,\n\tMSR_IA32_VMX_MISC,\n\tMSR_IA32_VMX_CR0_FIXED0,\n\tMSR_IA32_VMX_CR0_FIXED1,\n\tMSR_IA32_VMX_CR4_FIXED0,\n\tMSR_IA32_VMX_CR4_FIXED1,\n\tMSR_IA32_VMX_VMCS_ENUM,\n\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\tMSR_IA32_VMX_EPT_VPID_CAP,\n\tMSR_IA32_VMX_VMFUNC,\n\n\tMSR_F10H_DECFG,\n\tMSR_IA32_UCODE_REV,\n\tMSR_IA32_ARCH_CAPABILITIES,\n\tMSR_IA32_PERF_CAPABILITIES,\n};\n\nstatic u32 msr_based_features[ARRAY_SIZE(msr_based_features_all)];\nstatic unsigned int num_msr_based_features;\n\nstatic u64 kvm_get_arch_capabilities(void)\n{\n\tu64 data = 0;\n\n\tif (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))\n\t\trdmsrl(MSR_IA32_ARCH_CAPABILITIES, data);\n\n\t/*\n\t * If nx_huge_pages is enabled, KVM's shadow paging will ensure that\n\t * the nested hypervisor runs with NX huge pages.  If it is not,\n\t * L1 is anyway vulnerable to ITLB_MULTIHIT explots from other\n\t * L1 guests, so it need not worry about its own (L2) guests.\n\t */\n\tdata |= ARCH_CAP_PSCHANGE_MC_NO;\n\n\t/*\n\t * If we're doing cache flushes (either \"always\" or \"cond\")\n\t * we will do one whenever the guest does a vmlaunch/vmresume.\n\t * If an outer hypervisor is doing the cache flush for us\n\t * (VMENTER_L1D_FLUSH_NESTED_VM), we can safely pass that\n\t * capability to the guest too, and if EPT is disabled we're not\n\t * vulnerable.  Overall, only VMENTER_L1D_FLUSH_NEVER will\n\t * require a nested hypervisor to do a flush of its own.\n\t */\n\tif (l1tf_vmx_mitigation != VMENTER_L1D_FLUSH_NEVER)\n\t\tdata |= ARCH_CAP_SKIP_VMENTRY_L1DFLUSH;\n\n\tif (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))\n\t\tdata |= ARCH_CAP_RDCL_NO;\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tdata |= ARCH_CAP_SSB_NO;\n\tif (!boot_cpu_has_bug(X86_BUG_MDS))\n\t\tdata |= ARCH_CAP_MDS_NO;\n\n\t/*\n\t * On TAA affected systems:\n\t *      - nothing to do if TSX is disabled on the host.\n\t *      - we emulate TSX_CTRL if present on the host.\n\t *\t  This lets the guest use VERW to clear CPU buffers.\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_RTM))\n\t\tdata &= ~(ARCH_CAP_TAA_NO | ARCH_CAP_TSX_CTRL_MSR);\n\telse if (!boot_cpu_has_bug(X86_BUG_TAA))\n\t\tdata |= ARCH_CAP_TAA_NO;\n\n\treturn data;\n}\n\nstatic int kvm_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tswitch (msr->index) {\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tmsr->data = kvm_get_arch_capabilities();\n\t\tbreak;\n\tcase MSR_IA32_UCODE_REV:\n\t\trdmsrl_safe(msr->index, &msr->data);\n\t\tbreak;\n\tdefault:\n\t\treturn kvm_x86_ops.get_msr_feature(msr);\n\t}\n\treturn 0;\n}\n\nstatic int do_get_msr_feature(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\tstruct kvm_msr_entry msr;\n\tint r;\n\n\tmsr.index = index;\n\tr = kvm_get_msr_feature(&msr);\n\n\tif (r == KVM_MSR_RET_INVALID) {\n\t\t/* Unconditionally clear the output for simplicity */\n\t\t*data = 0;\n\t\tif (kvm_msr_ignored_check(vcpu, index, 0, false))\n\t\t\tr = 0;\n\t}\n\n\tif (r)\n\t\treturn r;\n\n\t*data = msr.data;\n\n\treturn 0;\n}\n\nstatic bool __kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tif (efer & EFER_FFXSR && !guest_cpuid_has(vcpu, X86_FEATURE_FXSR_OPT))\n\t\treturn false;\n\n\tif (efer & EFER_SVME && !guest_cpuid_has(vcpu, X86_FEATURE_SVM))\n\t\treturn false;\n\n\tif (efer & (EFER_LME | EFER_LMA) &&\n\t    !guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn false;\n\n\tif (efer & EFER_NX && !guest_cpuid_has(vcpu, X86_FEATURE_NX))\n\t\treturn false;\n\n\treturn true;\n\n}\nbool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tif (efer & efer_reserved_bits)\n\t\treturn false;\n\n\treturn __kvm_valid_efer(vcpu, efer);\n}\nEXPORT_SYMBOL_GPL(kvm_valid_efer);\n\nstatic int set_efer(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tu64 old_efer = vcpu->arch.efer;\n\tu64 efer = msr_info->data;\n\tint r;\n\n\tif (efer & efer_reserved_bits)\n\t\treturn 1;\n\n\tif (!msr_info->host_initiated) {\n\t\tif (!__kvm_valid_efer(vcpu, efer))\n\t\t\treturn 1;\n\n\t\tif (is_paging(vcpu) &&\n\t\t    (vcpu->arch.efer & EFER_LME) != (efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\tefer &= ~EFER_LMA;\n\tefer |= vcpu->arch.efer & EFER_LMA;\n\n\tr = kvm_x86_ops.set_efer(vcpu, efer);\n\tif (r) {\n\t\tWARN_ON(r > 0);\n\t\treturn r;\n\t}\n\n\t/* Update reserved bits */\n\tif ((efer ^ old_efer) & EFER_NX)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\treturn 0;\n}\n\nvoid kvm_enable_efer_bits(u64 mask)\n{\n       efer_reserved_bits &= ~mask;\n}\nEXPORT_SYMBOL_GPL(kvm_enable_efer_bits);\n\nbool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tstruct msr_bitmap_range *ranges = kvm->arch.msr_filter.ranges;\n\tu32 count = kvm->arch.msr_filter.count;\n\tu32 i;\n\tbool r = kvm->arch.msr_filter.default_allow;\n\tint idx;\n\n\t/* MSR filtering not set up or x2APIC enabled, allow everything */\n\tif (!count || (index >= 0x800 && index <= 0x8ff))\n\t\treturn true;\n\n\t/* Prevent collision with set_msr_filter */\n\tidx = srcu_read_lock(&kvm->srcu);\n\n\tfor (i = 0; i < count; i++) {\n\t\tu32 start = ranges[i].base;\n\t\tu32 end = start + ranges[i].nmsrs;\n\t\tu32 flags = ranges[i].flags;\n\t\tunsigned long *bitmap = ranges[i].bitmap;\n\n\t\tif ((index >= start) && (index < end) && (flags & type)) {\n\t\t\tr = !!test_bit(index - start, bitmap);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_msr_allowed);\n\n/*\n * Write @data into the MSR specified by @index.  Select MSR specific fault\n * checks are bypassed if @host_initiated is %true.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,\n\t\t\t bool host_initiated)\n{\n\tstruct msr_data msr;\n\n\tif (!host_initiated && !kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_WRITE))\n\t\treturn KVM_MSR_RET_FILTERED;\n\n\tswitch (index) {\n\tcase MSR_FS_BASE:\n\tcase MSR_GS_BASE:\n\tcase MSR_KERNEL_GS_BASE:\n\tcase MSR_CSTAR:\n\tcase MSR_LSTAR:\n\t\tif (is_noncanonical_address(data, vcpu))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\t/*\n\t\t * IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if\n\t\t * non-canonical address is written on Intel but not on\n\t\t * AMD (which ignores the top 32-bits, because it does\n\t\t * not implement 64-bit SYSENTER).\n\t\t *\n\t\t * 64-bit code should hence be able to write a non-canonical\n\t\t * value on AMD.  Making the address canonical ensures that\n\t\t * vmentry does not fail on Intel after writing a non-canonical\n\t\t * value, and that something deterministic happens if the guest\n\t\t * invokes 64-bit SYSENTER.\n\t\t */\n\t\tdata = get_canonical(data, vcpu_virt_addr_bits(vcpu));\n\t}\n\n\tmsr.data = data;\n\tmsr.index = index;\n\tmsr.host_initiated = host_initiated;\n\n\treturn kvm_x86_ops.set_msr(vcpu, &msr);\n}\n\nstatic int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     u32 index, u64 data, bool host_initiated)\n{\n\tint ret = __kvm_set_msr(vcpu, index, data, host_initiated);\n\n\tif (ret == KVM_MSR_RET_INVALID)\n\t\tif (kvm_msr_ignored_check(vcpu, index, data, true))\n\t\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * Read the MSR specified by @index into @data.  Select MSR specific fault\n * checks are bypassed if @host_initiated is %true.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nint __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,\n\t\t  bool host_initiated)\n{\n\tstruct msr_data msr;\n\tint ret;\n\n\tif (!host_initiated && !kvm_msr_allowed(vcpu, index, KVM_MSR_FILTER_READ))\n\t\treturn KVM_MSR_RET_FILTERED;\n\n\tmsr.index = index;\n\tmsr.host_initiated = host_initiated;\n\n\tret = kvm_x86_ops.get_msr(vcpu, &msr);\n\tif (!ret)\n\t\t*data = msr.data;\n\treturn ret;\n}\n\nstatic int kvm_get_msr_ignored_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     u32 index, u64 *data, bool host_initiated)\n{\n\tint ret = __kvm_get_msr(vcpu, index, data, host_initiated);\n\n\tif (ret == KVM_MSR_RET_INVALID) {\n\t\t/* Unconditionally clear *data for simplicity */\n\t\t*data = 0;\n\t\tif (kvm_msr_ignored_check(vcpu, index, 0, false))\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\nint kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data)\n{\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, false);\n}\nEXPORT_SYMBOL_GPL(kvm_get_msr);\n\nint kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data)\n{\n\treturn kvm_set_msr_ignored_check(vcpu, index, data, false);\n}\nEXPORT_SYMBOL_GPL(kvm_set_msr);\n\nstatic int complete_emulated_msr(struct kvm_vcpu *vcpu, bool is_read)\n{\n\tif (vcpu->run->msr.error) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t} else if (is_read) {\n\t\tkvm_rax_write(vcpu, (u32)vcpu->run->msr.data);\n\t\tkvm_rdx_write(vcpu, vcpu->run->msr.data >> 32);\n\t}\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int complete_emulated_rdmsr(struct kvm_vcpu *vcpu)\n{\n\treturn complete_emulated_msr(vcpu, true);\n}\n\nstatic int complete_emulated_wrmsr(struct kvm_vcpu *vcpu)\n{\n\treturn complete_emulated_msr(vcpu, false);\n}\n\nstatic u64 kvm_msr_reason(int r)\n{\n\tswitch (r) {\n\tcase KVM_MSR_RET_INVALID:\n\t\treturn KVM_MSR_EXIT_REASON_UNKNOWN;\n\tcase KVM_MSR_RET_FILTERED:\n\t\treturn KVM_MSR_EXIT_REASON_FILTER;\n\tdefault:\n\t\treturn KVM_MSR_EXIT_REASON_INVAL;\n\t}\n}\n\nstatic int kvm_msr_user_space(struct kvm_vcpu *vcpu, u32 index,\n\t\t\t      u32 exit_reason, u64 data,\n\t\t\t      int (*completion)(struct kvm_vcpu *vcpu),\n\t\t\t      int r)\n{\n\tu64 msr_reason = kvm_msr_reason(r);\n\n\t/* Check if the user wanted to know about this MSR fault */\n\tif (!(vcpu->kvm->arch.user_space_msr_mask & msr_reason))\n\t\treturn 0;\n\n\tvcpu->run->exit_reason = exit_reason;\n\tvcpu->run->msr.error = 0;\n\tmemset(vcpu->run->msr.pad, 0, sizeof(vcpu->run->msr.pad));\n\tvcpu->run->msr.reason = msr_reason;\n\tvcpu->run->msr.index = index;\n\tvcpu->run->msr.data = data;\n\tvcpu->arch.complete_userspace_io = completion;\n\n\treturn 1;\n}\n\nstatic int kvm_get_msr_user_space(struct kvm_vcpu *vcpu, u32 index, int r)\n{\n\treturn kvm_msr_user_space(vcpu, index, KVM_EXIT_X86_RDMSR, 0,\n\t\t\t\t   complete_emulated_rdmsr, r);\n}\n\nstatic int kvm_set_msr_user_space(struct kvm_vcpu *vcpu, u32 index, u64 data, int r)\n{\n\treturn kvm_msr_user_space(vcpu, index, KVM_EXIT_X86_WRMSR, data,\n\t\t\t\t   complete_emulated_wrmsr, r);\n}\n\nint kvm_emulate_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data;\n\tint r;\n\n\tr = kvm_get_msr(vcpu, ecx, &data);\n\n\t/* MSR read failed? See if we should ask user space */\n\tif (r && kvm_get_msr_user_space(vcpu, ecx, r)) {\n\t\t/* Bounce to user space */\n\t\treturn 0;\n\t}\n\n\t/* MSR read failed? Inject a #GP */\n\tif (r) {\n\t\ttrace_kvm_msr_read_ex(ecx);\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\ttrace_kvm_msr_read(ecx, data);\n\n\tkvm_rax_write(vcpu, data & -1u);\n\tkvm_rdx_write(vcpu, (data >> 32) & -1u);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_rdmsr);\n\nint kvm_emulate_wrmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = kvm_rcx_read(vcpu);\n\tu64 data = kvm_read_edx_eax(vcpu);\n\tint r;\n\n\tr = kvm_set_msr(vcpu, ecx, data);\n\n\t/* MSR write failed? See if we should ask user space */\n\tif (r && kvm_set_msr_user_space(vcpu, ecx, data, r))\n\t\t/* Bounce to user space */\n\t\treturn 0;\n\n\t/* Signal all other negative errors to userspace */\n\tif (r < 0)\n\t\treturn r;\n\n\t/* MSR write failed? Inject a #GP */\n\tif (r > 0) {\n\t\ttrace_kvm_msr_write_ex(ecx, data);\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\ttrace_kvm_msr_write(ecx, data);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_wrmsr);\n\nbool kvm_vcpu_exit_request(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->mode == EXITING_GUEST_MODE || kvm_request_pending(vcpu) ||\n\t\txfer_to_guest_mode_work_pending();\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_exit_request);\n\n/*\n * The fast path for frequent and performance sensitive wrmsr emulation,\n * i.e. the sending of IPI, sending IPI early in the VM-Exit flow reduces\n * the latency of virtual IPI by avoiding the expensive bits of transitioning\n * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the\n * other cases which must be called after interrupts are enabled on the host.\n */\nstatic int handle_fastpath_set_x2apic_icr_irqoff(struct kvm_vcpu *vcpu, u64 data)\n{\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(vcpu->arch.apic))\n\t\treturn 1;\n\n\tif (((data & APIC_SHORT_MASK) == APIC_DEST_NOSHORT) &&\n\t\t((data & APIC_DEST_MASK) == APIC_DEST_PHYSICAL) &&\n\t\t((data & APIC_MODE_MASK) == APIC_DM_FIXED) &&\n\t\t((u32)(data >> 32) != X2APIC_BROADCAST)) {\n\n\t\tdata &= ~(1 << 12);\n\t\tkvm_apic_send_ipi(vcpu->arch.apic, (u32)data, (u32)(data >> 32));\n\t\tkvm_lapic_set_reg(vcpu->arch.apic, APIC_ICR2, (u32)(data >> 32));\n\t\tkvm_lapic_set_reg(vcpu->arch.apic, APIC_ICR, (u32)data);\n\t\ttrace_kvm_apic_write(APIC_ICR, (u32)data);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int handle_fastpath_set_tscdeadline(struct kvm_vcpu *vcpu, u64 data)\n{\n\tif (!kvm_can_use_hv_timer(vcpu))\n\t\treturn 1;\n\n\tkvm_set_lapic_tscdeadline_msr(vcpu, data);\n\treturn 0;\n}\n\nfastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu)\n{\n\tu32 msr = kvm_rcx_read(vcpu);\n\tu64 data;\n\tfastpath_t ret = EXIT_FASTPATH_NONE;\n\n\tswitch (msr) {\n\tcase APIC_BASE_MSR + (APIC_ICR >> 4):\n\t\tdata = kvm_read_edx_eax(vcpu);\n\t\tif (!handle_fastpath_set_x2apic_icr_irqoff(vcpu, data)) {\n\t\t\tkvm_skip_emulated_instruction(vcpu);\n\t\t\tret = EXIT_FASTPATH_EXIT_HANDLED;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_TSCDEADLINE:\n\t\tdata = kvm_read_edx_eax(vcpu);\n\t\tif (!handle_fastpath_set_tscdeadline(vcpu, data)) {\n\t\t\tkvm_skip_emulated_instruction(vcpu);\n\t\t\tret = EXIT_FASTPATH_REENTER_GUEST;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (ret != EXIT_FASTPATH_NONE)\n\t\ttrace_kvm_msr_write(msr, data);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(handle_fastpath_set_msr_irqoff);\n\n/*\n * Adapt set_msr() to msr_io()'s calling convention\n */\nstatic int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\treturn kvm_get_msr_ignored_check(vcpu, index, data, true);\n}\n\nstatic int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)\n{\n\treturn kvm_set_msr_ignored_check(vcpu, index, *data, true);\n}\n\n#ifdef CONFIG_X86_64\nstruct pvclock_clock {\n\tint vclock_mode;\n\tu64 cycle_last;\n\tu64 mask;\n\tu32 mult;\n\tu32 shift;\n\tu64 base_cycles;\n\tu64 offset;\n};\n\nstruct pvclock_gtod_data {\n\tseqcount_t\tseq;\n\n\tstruct pvclock_clock clock; /* extract of a clocksource struct */\n\tstruct pvclock_clock raw_clock; /* extract of a clocksource struct */\n\n\tktime_t\t\toffs_boot;\n\tu64\t\twall_time_sec;\n};\n\nstatic struct pvclock_gtod_data pvclock_gtod_data;\n\nstatic void update_pvclock_gtod(struct timekeeper *tk)\n{\n\tstruct pvclock_gtod_data *vdata = &pvclock_gtod_data;\n\n\twrite_seqcount_begin(&vdata->seq);\n\n\t/* copy pvclock gtod data */\n\tvdata->clock.vclock_mode\t= tk->tkr_mono.clock->vdso_clock_mode;\n\tvdata->clock.cycle_last\t\t= tk->tkr_mono.cycle_last;\n\tvdata->clock.mask\t\t= tk->tkr_mono.mask;\n\tvdata->clock.mult\t\t= tk->tkr_mono.mult;\n\tvdata->clock.shift\t\t= tk->tkr_mono.shift;\n\tvdata->clock.base_cycles\t= tk->tkr_mono.xtime_nsec;\n\tvdata->clock.offset\t\t= tk->tkr_mono.base;\n\n\tvdata->raw_clock.vclock_mode\t= tk->tkr_raw.clock->vdso_clock_mode;\n\tvdata->raw_clock.cycle_last\t= tk->tkr_raw.cycle_last;\n\tvdata->raw_clock.mask\t\t= tk->tkr_raw.mask;\n\tvdata->raw_clock.mult\t\t= tk->tkr_raw.mult;\n\tvdata->raw_clock.shift\t\t= tk->tkr_raw.shift;\n\tvdata->raw_clock.base_cycles\t= tk->tkr_raw.xtime_nsec;\n\tvdata->raw_clock.offset\t\t= tk->tkr_raw.base;\n\n\tvdata->wall_time_sec            = tk->xtime_sec;\n\n\tvdata->offs_boot\t\t= tk->offs_boot;\n\n\twrite_seqcount_end(&vdata->seq);\n}\n\nstatic s64 get_kvmclock_base_ns(void)\n{\n\t/* Count up from boot time, but with the frequency of the raw clock.  */\n\treturn ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));\n}\n#else\nstatic s64 get_kvmclock_base_ns(void)\n{\n\t/* Master clock not used, so we can just use CLOCK_BOOTTIME.  */\n\treturn ktime_get_boottime_ns();\n}\n#endif\n\nstatic void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock)\n{\n\tint version;\n\tint r;\n\tstruct pvclock_wall_clock wc;\n\tu64 wall_nsec;\n\n\tkvm->arch.wall_clock = wall_clock;\n\n\tif (!wall_clock)\n\t\treturn;\n\n\tr = kvm_read_guest(kvm, wall_clock, &version, sizeof(version));\n\tif (r)\n\t\treturn;\n\n\tif (version & 1)\n\t\t++version;  /* first time write, random junk */\n\n\t++version;\n\n\tif (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))\n\t\treturn;\n\n\t/*\n\t * The guest calculates current wall clock time by adding\n\t * system time (updated by kvm_guest_time_update below) to the\n\t * wall clock specified here.  We do the reverse here.\n\t */\n\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\twc.nsec = do_div(wall_nsec, 1000000000);\n\twc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */\n\twc.version = version;\n\n\tkvm_write_guest(kvm, wall_clock, &wc, sizeof(wc));\n\n\tversion++;\n\tkvm_write_guest(kvm, wall_clock, &version, sizeof(version));\n}\n\nstatic void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,\n\t\t\t\t  bool old_msr, bool host_initiated)\n{\n\tstruct kvm_arch *ka = &vcpu->kvm->arch;\n\n\tif (vcpu->vcpu_id == 0 && !host_initiated) {\n\t\tif (ka->boot_vcpu_runs_old_kvmclock != old_msr)\n\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\n\t\tka->boot_vcpu_runs_old_kvmclock = old_msr;\n\t}\n\n\tvcpu->arch.time = system_time;\n\tkvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);\n\n\t/* we verify if the enable bit is set... */\n\tvcpu->arch.pv_time_enabled = false;\n\tif (!(system_time & 1))\n\t\treturn;\n\n\tif (!kvm_gfn_to_hva_cache_init(vcpu->kvm,\n\t\t\t\t       &vcpu->arch.pv_time, system_time & ~1ULL,\n\t\t\t\t       sizeof(struct pvclock_vcpu_time_info)))\n\t\tvcpu->arch.pv_time_enabled = true;\n\n\treturn;\n}\n\nstatic uint32_t div_frac(uint32_t dividend, uint32_t divisor)\n{\n\tdo_shl32_div32(dividend, divisor);\n\treturn dividend;\n}\n\nstatic void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,\n\t\t\t       s8 *pshift, u32 *pmultiplier)\n{\n\tuint64_t scaled64;\n\tint32_t  shift = 0;\n\tuint64_t tps64;\n\tuint32_t tps32;\n\n\ttps64 = base_hz;\n\tscaled64 = scaled_hz;\n\twhile (tps64 > scaled64*2 || tps64 & 0xffffffff00000000ULL) {\n\t\ttps64 >>= 1;\n\t\tshift--;\n\t}\n\n\ttps32 = (uint32_t)tps64;\n\twhile (tps32 <= scaled64 || scaled64 & 0xffffffff00000000ULL) {\n\t\tif (scaled64 & 0xffffffff00000000ULL || tps32 & 0x80000000)\n\t\t\tscaled64 >>= 1;\n\t\telse\n\t\t\ttps32 <<= 1;\n\t\tshift++;\n\t}\n\n\t*pshift = shift;\n\t*pmultiplier = div_frac(scaled64, tps32);\n}\n\n#ifdef CONFIG_X86_64\nstatic atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);\n#endif\n\nstatic DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);\nstatic unsigned long max_tsc_khz;\n\nstatic u32 adjust_tsc_khz(u32 khz, s32 ppm)\n{\n\tu64 v = (u64)khz * (1000000 + ppm);\n\tdo_div(v, 1000000);\n\treturn v;\n}\n\nstatic int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)\n{\n\tu64 ratio;\n\n\t/* Guest TSC same frequency as host TSC? */\n\tif (!scale) {\n\t\tvcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;\n\t\treturn 0;\n\t}\n\n\t/* TSC scaling supported? */\n\tif (!kvm_has_tsc_control) {\n\t\tif (user_tsc_khz > tsc_khz) {\n\t\t\tvcpu->arch.tsc_catchup = 1;\n\t\t\tvcpu->arch.tsc_always_catchup = 1;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tpr_warn_ratelimited(\"user requested TSC rate below hardware speed\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t/* TSC scaling required  - calculate ratio */\n\tratio = mul_u64_u32_div(1ULL << kvm_tsc_scaling_ratio_frac_bits,\n\t\t\t\tuser_tsc_khz, tsc_khz);\n\n\tif (ratio == 0 || ratio >= kvm_max_tsc_scaling_ratio) {\n\t\tpr_warn_ratelimited(\"Invalid TSC scaling ratio - virtual-tsc-khz=%u\\n\",\n\t\t\t            user_tsc_khz);\n\t\treturn -1;\n\t}\n\n\tvcpu->arch.tsc_scaling_ratio = ratio;\n\treturn 0;\n}\n\nstatic int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)\n{\n\tu32 thresh_lo, thresh_hi;\n\tint use_scaling = 0;\n\n\t/* tsc_khz can be zero if TSC calibration fails */\n\tif (user_tsc_khz == 0) {\n\t\t/* set tsc_scaling_ratio to a safe value */\n\t\tvcpu->arch.tsc_scaling_ratio = kvm_default_tsc_scaling_ratio;\n\t\treturn -1;\n\t}\n\n\t/* Compute a scale to convert nanoseconds in TSC cycles */\n\tkvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,\n\t\t\t   &vcpu->arch.virtual_tsc_shift,\n\t\t\t   &vcpu->arch.virtual_tsc_mult);\n\tvcpu->arch.virtual_tsc_khz = user_tsc_khz;\n\n\t/*\n\t * Compute the variation in TSC rate which is acceptable\n\t * within the range of tolerance and decide if the\n\t * rate being applied is within that bounds of the hardware\n\t * rate.  If so, no scaling or compensation need be done.\n\t */\n\tthresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);\n\tthresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);\n\tif (user_tsc_khz < thresh_lo || user_tsc_khz > thresh_hi) {\n\t\tpr_debug(\"kvm: requested TSC rate %u falls outside tolerance [%u,%u]\\n\", user_tsc_khz, thresh_lo, thresh_hi);\n\t\tuse_scaling = 1;\n\t}\n\treturn set_tsc_khz(vcpu, user_tsc_khz, use_scaling);\n}\n\nstatic u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)\n{\n\tu64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,\n\t\t\t\t      vcpu->arch.virtual_tsc_mult,\n\t\t\t\t      vcpu->arch.virtual_tsc_shift);\n\ttsc += vcpu->arch.this_tsc_write;\n\treturn tsc;\n}\n\nstatic inline int gtod_is_based_on_tsc(int mode)\n{\n\treturn mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;\n}\n\nstatic void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)\n{\n#ifdef CONFIG_X86_64\n\tbool vcpus_matched;\n\tstruct kvm_arch *ka = &vcpu->kvm->arch;\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\n\tvcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==\n\t\t\t atomic_read(&vcpu->kvm->online_vcpus));\n\n\t/*\n\t * Once the masterclock is enabled, always perform request in\n\t * order to update it.\n\t *\n\t * In order to enable masterclock, the host clocksource must be TSC\n\t * and the vcpus need to have matched TSCs.  When that happens,\n\t * perform request to enable masterclock.\n\t */\n\tif (ka->use_master_clock ||\n\t    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))\n\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\n\ttrace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,\n\t\t\t    atomic_read(&vcpu->kvm->online_vcpus),\n\t\t            ka->use_master_clock, gtod->clock.vclock_mode);\n#endif\n}\n\n/*\n * Multiply tsc by a fixed point number represented by ratio.\n *\n * The most significant 64-N bits (mult) of ratio represent the\n * integral part of the fixed point number; the remaining N bits\n * (frac) represent the fractional part, ie. ratio represents a fixed\n * point number (mult + frac * 2^(-N)).\n *\n * N equals to kvm_tsc_scaling_ratio_frac_bits.\n */\nstatic inline u64 __scale_tsc(u64 ratio, u64 tsc)\n{\n\treturn mul_u64_u64_shr(tsc, ratio, kvm_tsc_scaling_ratio_frac_bits);\n}\n\nu64 kvm_scale_tsc(struct kvm_vcpu *vcpu, u64 tsc)\n{\n\tu64 _tsc = tsc;\n\tu64 ratio = vcpu->arch.tsc_scaling_ratio;\n\n\tif (ratio != kvm_default_tsc_scaling_ratio)\n\t\t_tsc = __scale_tsc(ratio, tsc);\n\n\treturn _tsc;\n}\nEXPORT_SYMBOL_GPL(kvm_scale_tsc);\n\nstatic u64 kvm_compute_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)\n{\n\tu64 tsc;\n\n\ttsc = kvm_scale_tsc(vcpu, rdtsc());\n\n\treturn target_tsc - tsc;\n}\n\nu64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)\n{\n\treturn vcpu->arch.l1_tsc_offset + kvm_scale_tsc(vcpu, host_tsc);\n}\nEXPORT_SYMBOL_GPL(kvm_read_l1_tsc);\n\nstatic void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)\n{\n\tvcpu->arch.l1_tsc_offset = offset;\n\tvcpu->arch.tsc_offset = kvm_x86_ops.write_l1_tsc_offset(vcpu, offset);\n}\n\nstatic inline bool kvm_check_tsc_unstable(void)\n{\n#ifdef CONFIG_X86_64\n\t/*\n\t * TSC is marked unstable when we're running on Hyper-V,\n\t * 'TSC page' clocksource is good.\n\t */\n\tif (pvclock_gtod_data.clock.vclock_mode == VDSO_CLOCKMODE_HVCLOCK)\n\t\treturn false;\n#endif\n\treturn check_tsc_unstable();\n}\n\nstatic void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tu64 offset, ns, elapsed;\n\tunsigned long flags;\n\tbool matched;\n\tbool already_matched;\n\tbool synchronizing = false;\n\n\traw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);\n\toffset = kvm_compute_tsc_offset(vcpu, data);\n\tns = get_kvmclock_base_ns();\n\telapsed = ns - kvm->arch.last_tsc_nsec;\n\n\tif (vcpu->arch.virtual_tsc_khz) {\n\t\tif (data == 0) {\n\t\t\t/*\n\t\t\t * detection of vcpu initialization -- need to sync\n\t\t\t * with other vCPUs. This particularly helps to keep\n\t\t\t * kvm_clock stable after CPU hotplug\n\t\t\t */\n\t\t\tsynchronizing = true;\n\t\t} else {\n\t\t\tu64 tsc_exp = kvm->arch.last_tsc_write +\n\t\t\t\t\t\tnsec_to_cycles(vcpu, elapsed);\n\t\t\tu64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;\n\t\t\t/*\n\t\t\t * Special case: TSC write with a small delta (1 second)\n\t\t\t * of virtual cycle time against real time is\n\t\t\t * interpreted as an attempt to synchronize the CPU.\n\t\t\t */\n\t\t\tsynchronizing = data < tsc_exp + tsc_hz &&\n\t\t\t\t\tdata + tsc_hz > tsc_exp;\n\t\t}\n\t}\n\n\t/*\n\t * For a reliable TSC, we can match TSC offsets, and for an unstable\n\t * TSC, we add elapsed time in this computation.  We could let the\n\t * compensation code attempt to catch up if we fall behind, but\n\t * it's better to try to match offsets from the beginning.\n         */\n\tif (synchronizing &&\n\t    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {\n\t\tif (!kvm_check_tsc_unstable()) {\n\t\t\toffset = kvm->arch.cur_tsc_offset;\n\t\t} else {\n\t\t\tu64 delta = nsec_to_cycles(vcpu, elapsed);\n\t\t\tdata += delta;\n\t\t\toffset = kvm_compute_tsc_offset(vcpu, data);\n\t\t}\n\t\tmatched = true;\n\t\talready_matched = (vcpu->arch.this_tsc_generation == kvm->arch.cur_tsc_generation);\n\t} else {\n\t\t/*\n\t\t * We split periods of matched TSC writes into generations.\n\t\t * For each generation, we track the original measured\n\t\t * nanosecond time, offset, and write, so if TSCs are in\n\t\t * sync, we can match exact offset, and if not, we can match\n\t\t * exact software computation in compute_guest_tsc()\n\t\t *\n\t\t * These values are tracked in kvm->arch.cur_xxx variables.\n\t\t */\n\t\tkvm->arch.cur_tsc_generation++;\n\t\tkvm->arch.cur_tsc_nsec = ns;\n\t\tkvm->arch.cur_tsc_write = data;\n\t\tkvm->arch.cur_tsc_offset = offset;\n\t\tmatched = false;\n\t}\n\n\t/*\n\t * We also track th most recent recorded KHZ, write and time to\n\t * allow the matching interval to be extended at each write.\n\t */\n\tkvm->arch.last_tsc_nsec = ns;\n\tkvm->arch.last_tsc_write = data;\n\tkvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;\n\n\tvcpu->arch.last_guest_tsc = data;\n\n\t/* Keep track of which generation this VCPU has synchronized to */\n\tvcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;\n\tvcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;\n\tvcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;\n\n\tkvm_vcpu_write_tsc_offset(vcpu, offset);\n\traw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);\n\n\tspin_lock(&kvm->arch.pvclock_gtod_sync_lock);\n\tif (!matched) {\n\t\tkvm->arch.nr_vcpus_matched_tsc = 0;\n\t} else if (!already_matched) {\n\t\tkvm->arch.nr_vcpus_matched_tsc++;\n\t}\n\n\tkvm_track_tsc_matching(vcpu);\n\tspin_unlock(&kvm->arch.pvclock_gtod_sync_lock);\n}\n\nstatic inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   s64 adjustment)\n{\n\tu64 tsc_offset = vcpu->arch.l1_tsc_offset;\n\tkvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);\n}\n\nstatic inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)\n{\n\tif (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio)\n\t\tWARN_ON(adjustment < 0);\n\tadjustment = kvm_scale_tsc(vcpu, (u64) adjustment);\n\tadjust_tsc_offset_guest(vcpu, adjustment);\n}\n\n#ifdef CONFIG_X86_64\n\nstatic u64 read_tsc(void)\n{\n\tu64 ret = (u64)rdtsc_ordered();\n\tu64 last = pvclock_gtod_data.clock.cycle_last;\n\n\tif (likely(ret >= last))\n\t\treturn ret;\n\n\t/*\n\t * GCC likes to generate cmov here, but this branch is extremely\n\t * predictable (it's just a function of time and the likely is\n\t * very likely) and there's a data dependence, so force GCC\n\t * to generate a branch instead.  I don't barrier() because\n\t * we don't actually need a barrier, and if this function\n\t * ever gets inlined it will generate worse code.\n\t */\n\tasm volatile (\"\");\n\treturn last;\n}\n\nstatic inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,\n\t\t\t  int *mode)\n{\n\tlong v;\n\tu64 tsc_pg_val;\n\n\tswitch (clock->vclock_mode) {\n\tcase VDSO_CLOCKMODE_HVCLOCK:\n\t\ttsc_pg_val = hv_read_tsc_page_tsc(hv_get_tsc_page(),\n\t\t\t\t\t\t  tsc_timestamp);\n\t\tif (tsc_pg_val != U64_MAX) {\n\t\t\t/* TSC page valid */\n\t\t\t*mode = VDSO_CLOCKMODE_HVCLOCK;\n\t\t\tv = (tsc_pg_val - clock->cycle_last) &\n\t\t\t\tclock->mask;\n\t\t} else {\n\t\t\t/* TSC page invalid */\n\t\t\t*mode = VDSO_CLOCKMODE_NONE;\n\t\t}\n\t\tbreak;\n\tcase VDSO_CLOCKMODE_TSC:\n\t\t*mode = VDSO_CLOCKMODE_TSC;\n\t\t*tsc_timestamp = read_tsc();\n\t\tv = (*tsc_timestamp - clock->cycle_last) &\n\t\t\tclock->mask;\n\t\tbreak;\n\tdefault:\n\t\t*mode = VDSO_CLOCKMODE_NONE;\n\t}\n\n\tif (*mode == VDSO_CLOCKMODE_NONE)\n\t\t*tsc_timestamp = v = 0;\n\n\treturn v * clock->mult;\n}\n\nstatic int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tunsigned long seq;\n\tint mode;\n\tu64 ns;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&gtod->seq);\n\t\tns = gtod->raw_clock.base_cycles;\n\t\tns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);\n\t\tns >>= gtod->raw_clock.shift;\n\t\tns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));\n\t} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));\n\t*t = ns;\n\n\treturn mode;\n}\n\nstatic int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tunsigned long seq;\n\tint mode;\n\tu64 ns;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&gtod->seq);\n\t\tts->tv_sec = gtod->wall_time_sec;\n\t\tns = gtod->clock.base_cycles;\n\t\tns += vgettsc(&gtod->clock, tsc_timestamp, &mode);\n\t\tns >>= gtod->clock.shift;\n\t} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));\n\n\tts->tv_sec += __iter_div_u64_rem(ns, NSEC_PER_SEC, &ns);\n\tts->tv_nsec = ns;\n\n\treturn mode;\n}\n\n/* returns true if host is using TSC based clocksource */\nstatic bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)\n{\n\t/* checked again under seqlock below */\n\tif (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))\n\t\treturn false;\n\n\treturn gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,\n\t\t\t\t\t\t      tsc_timestamp));\n}\n\n/* returns true if host is using TSC based clocksource */\nstatic bool kvm_get_walltime_and_clockread(struct timespec64 *ts,\n\t\t\t\t\t   u64 *tsc_timestamp)\n{\n\t/* checked again under seqlock below */\n\tif (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))\n\t\treturn false;\n\n\treturn gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));\n}\n#endif\n\n/*\n *\n * Assuming a stable TSC across physical CPUS, and a stable TSC\n * across virtual CPUs, the following condition is possible.\n * Each numbered line represents an event visible to both\n * CPUs at the next numbered event.\n *\n * \"timespecX\" represents host monotonic time. \"tscX\" represents\n * RDTSC value.\n *\n * \t\tVCPU0 on CPU0\t\t|\tVCPU1 on CPU1\n *\n * 1.  read timespec0,tsc0\n * 2.\t\t\t\t\t| timespec1 = timespec0 + N\n * \t\t\t\t\t| tsc1 = tsc0 + M\n * 3. transition to guest\t\t| transition to guest\n * 4. ret0 = timespec0 + (rdtsc - tsc0) |\n * 5.\t\t\t\t        | ret1 = timespec1 + (rdtsc - tsc1)\n * \t\t\t\t        | ret1 = timespec0 + N + (rdtsc - (tsc0 + M))\n *\n * Since ret0 update is visible to VCPU1 at time 5, to obey monotonicity:\n *\n * \t- ret0 < ret1\n *\t- timespec0 + (rdtsc - tsc0) < timespec0 + N + (rdtsc - (tsc0 + M))\n *\t\t...\n *\t- 0 < N - M => M < N\n *\n * That is, when timespec0 != timespec1, M < N. Unfortunately that is not\n * always the case (the difference between two distinct xtime instances\n * might be smaller then the difference between corresponding TSC reads,\n * when updating guest vcpus pvclock areas).\n *\n * To avoid that problem, do not allow visibility of distinct\n * system_timestamp/tsc_timestamp values simultaneously: use a master\n * copy of host monotonic time values. Update that master copy\n * in lockstep.\n *\n * Rely on synchronization of host TSCs and guest TSCs for monotonicity.\n *\n */\n\nstatic void pvclock_update_vm_gtod_copy(struct kvm *kvm)\n{\n#ifdef CONFIG_X86_64\n\tstruct kvm_arch *ka = &kvm->arch;\n\tint vclock_mode;\n\tbool host_tsc_clocksource, vcpus_matched;\n\n\tvcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==\n\t\t\tatomic_read(&kvm->online_vcpus));\n\n\t/*\n\t * If the host uses TSC clock, then passthrough TSC as stable\n\t * to the guest.\n\t */\n\thost_tsc_clocksource = kvm_get_time_and_clockread(\n\t\t\t\t\t&ka->master_kernel_ns,\n\t\t\t\t\t&ka->master_cycle_now);\n\n\tka->use_master_clock = host_tsc_clocksource && vcpus_matched\n\t\t\t\t&& !ka->backwards_tsc_observed\n\t\t\t\t&& !ka->boot_vcpu_runs_old_kvmclock;\n\n\tif (ka->use_master_clock)\n\t\tatomic_set(&kvm_guest_has_master_clock, 1);\n\n\tvclock_mode = pvclock_gtod_data.clock.vclock_mode;\n\ttrace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,\n\t\t\t\t\tvcpus_matched);\n#endif\n}\n\nvoid kvm_make_mclock_inprogress_request(struct kvm *kvm)\n{\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);\n}\n\nstatic void kvm_gen_update_masterclock(struct kvm *kvm)\n{\n#ifdef CONFIG_X86_64\n\tint i;\n\tstruct kvm_vcpu *vcpu;\n\tstruct kvm_arch *ka = &kvm->arch;\n\n\tspin_lock(&ka->pvclock_gtod_sync_lock);\n\tkvm_make_mclock_inprogress_request(kvm);\n\t/* no guest entries from this point */\n\tpvclock_update_vm_gtod_copy(kvm);\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\t/* guest entries allowed */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);\n\n\tspin_unlock(&ka->pvclock_gtod_sync_lock);\n#endif\n}\n\nu64 get_kvmclock_ns(struct kvm *kvm)\n{\n\tstruct kvm_arch *ka = &kvm->arch;\n\tstruct pvclock_vcpu_time_info hv_clock;\n\tu64 ret;\n\n\tspin_lock(&ka->pvclock_gtod_sync_lock);\n\tif (!ka->use_master_clock) {\n\t\tspin_unlock(&ka->pvclock_gtod_sync_lock);\n\t\treturn get_kvmclock_base_ns() + ka->kvmclock_offset;\n\t}\n\n\thv_clock.tsc_timestamp = ka->master_cycle_now;\n\thv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;\n\tspin_unlock(&ka->pvclock_gtod_sync_lock);\n\n\t/* both __this_cpu_read() and rdtsc() should be on the same cpu */\n\tget_cpu();\n\n\tif (__this_cpu_read(cpu_tsc_khz)) {\n\t\tkvm_get_time_scale(NSEC_PER_SEC, __this_cpu_read(cpu_tsc_khz) * 1000LL,\n\t\t\t\t   &hv_clock.tsc_shift,\n\t\t\t\t   &hv_clock.tsc_to_system_mul);\n\t\tret = __pvclock_read_cycles(&hv_clock, rdtsc());\n\t} else\n\t\tret = get_kvmclock_base_ns() + ka->kvmclock_offset;\n\n\tput_cpu();\n\n\treturn ret;\n}\n\nstatic void kvm_setup_pvclock_page(struct kvm_vcpu *v)\n{\n\tstruct kvm_vcpu_arch *vcpu = &v->arch;\n\tstruct pvclock_vcpu_time_info guest_hv_clock;\n\n\tif (unlikely(kvm_read_guest_cached(v->kvm, &vcpu->pv_time,\n\t\t&guest_hv_clock, sizeof(guest_hv_clock))))\n\t\treturn;\n\n\t/* This VCPU is paused, but it's legal for a guest to read another\n\t * VCPU's kvmclock, so we really have to follow the specification where\n\t * it says that version is odd if data is being modified, and even after\n\t * it is consistent.\n\t *\n\t * Version field updates must be kept separate.  This is because\n\t * kvm_write_guest_cached might use a \"rep movs\" instruction, and\n\t * writes within a string instruction are weakly ordered.  So there\n\t * are three writes overall.\n\t *\n\t * As a small optimization, only write the version field in the first\n\t * and third write.  The vcpu->pv_time cache is still valid, because the\n\t * version field is the first in the struct.\n\t */\n\tBUILD_BUG_ON(offsetof(struct pvclock_vcpu_time_info, version) != 0);\n\n\tif (guest_hv_clock.version & 1)\n\t\t++guest_hv_clock.version;  /* first time write, random junk */\n\n\tvcpu->hv_clock.version = guest_hv_clock.version + 1;\n\tkvm_write_guest_cached(v->kvm, &vcpu->pv_time,\n\t\t\t\t&vcpu->hv_clock,\n\t\t\t\tsizeof(vcpu->hv_clock.version));\n\n\tsmp_wmb();\n\n\t/* retain PVCLOCK_GUEST_STOPPED if set in guest copy */\n\tvcpu->hv_clock.flags |= (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);\n\n\tif (vcpu->pvclock_set_guest_stopped_request) {\n\t\tvcpu->hv_clock.flags |= PVCLOCK_GUEST_STOPPED;\n\t\tvcpu->pvclock_set_guest_stopped_request = false;\n\t}\n\n\ttrace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);\n\n\tkvm_write_guest_cached(v->kvm, &vcpu->pv_time,\n\t\t\t\t&vcpu->hv_clock,\n\t\t\t\tsizeof(vcpu->hv_clock));\n\n\tsmp_wmb();\n\n\tvcpu->hv_clock.version++;\n\tkvm_write_guest_cached(v->kvm, &vcpu->pv_time,\n\t\t\t\t&vcpu->hv_clock,\n\t\t\t\tsizeof(vcpu->hv_clock.version));\n}\n\nstatic int kvm_guest_time_update(struct kvm_vcpu *v)\n{\n\tunsigned long flags, tgt_tsc_khz;\n\tstruct kvm_vcpu_arch *vcpu = &v->arch;\n\tstruct kvm_arch *ka = &v->kvm->arch;\n\ts64 kernel_ns;\n\tu64 tsc_timestamp, host_tsc;\n\tu8 pvclock_flags;\n\tbool use_master_clock;\n\n\tkernel_ns = 0;\n\thost_tsc = 0;\n\n\t/*\n\t * If the host uses TSC clock, then passthrough TSC as stable\n\t * to the guest.\n\t */\n\tspin_lock(&ka->pvclock_gtod_sync_lock);\n\tuse_master_clock = ka->use_master_clock;\n\tif (use_master_clock) {\n\t\thost_tsc = ka->master_cycle_now;\n\t\tkernel_ns = ka->master_kernel_ns;\n\t}\n\tspin_unlock(&ka->pvclock_gtod_sync_lock);\n\n\t/* Keep irq disabled to prevent changes to the clock */\n\tlocal_irq_save(flags);\n\ttgt_tsc_khz = __this_cpu_read(cpu_tsc_khz);\n\tif (unlikely(tgt_tsc_khz == 0)) {\n\t\tlocal_irq_restore(flags);\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, v);\n\t\treturn 1;\n\t}\n\tif (!use_master_clock) {\n\t\thost_tsc = rdtsc();\n\t\tkernel_ns = get_kvmclock_base_ns();\n\t}\n\n\ttsc_timestamp = kvm_read_l1_tsc(v, host_tsc);\n\n\t/*\n\t * We may have to catch up the TSC to match elapsed wall clock\n\t * time for two reasons, even if kvmclock is used.\n\t *   1) CPU could have been running below the maximum TSC rate\n\t *   2) Broken TSC compensation resets the base at each VCPU\n\t *      entry to avoid unknown leaps of TSC even when running\n\t *      again on the same CPU.  This may cause apparent elapsed\n\t *      time to disappear, and the guest to stand still or run\n\t *\tvery slowly.\n\t */\n\tif (vcpu->tsc_catchup) {\n\t\tu64 tsc = compute_guest_tsc(v, kernel_ns);\n\t\tif (tsc > tsc_timestamp) {\n\t\t\tadjust_tsc_offset_guest(v, tsc - tsc_timestamp);\n\t\t\ttsc_timestamp = tsc;\n\t\t}\n\t}\n\n\tlocal_irq_restore(flags);\n\n\t/* With all the info we got, fill in the values */\n\n\tif (kvm_has_tsc_control)\n\t\ttgt_tsc_khz = kvm_scale_tsc(v, tgt_tsc_khz);\n\n\tif (unlikely(vcpu->hw_tsc_khz != tgt_tsc_khz)) {\n\t\tkvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,\n\t\t\t\t   &vcpu->hv_clock.tsc_shift,\n\t\t\t\t   &vcpu->hv_clock.tsc_to_system_mul);\n\t\tvcpu->hw_tsc_khz = tgt_tsc_khz;\n\t}\n\n\tvcpu->hv_clock.tsc_timestamp = tsc_timestamp;\n\tvcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;\n\tvcpu->last_guest_tsc = tsc_timestamp;\n\n\t/* If the host uses TSC clocksource, then it is stable */\n\tpvclock_flags = 0;\n\tif (use_master_clock)\n\t\tpvclock_flags |= PVCLOCK_TSC_STABLE_BIT;\n\n\tvcpu->hv_clock.flags = pvclock_flags;\n\n\tif (vcpu->pv_time_enabled)\n\t\tkvm_setup_pvclock_page(v);\n\tif (v == kvm_get_vcpu(v->kvm, 0))\n\t\tkvm_hv_setup_tsc_page(v->kvm, &vcpu->hv_clock);\n\treturn 0;\n}\n\n/*\n * kvmclock updates which are isolated to a given vcpu, such as\n * vcpu->cpu migration, should not allow system_timestamp from\n * the rest of the vcpus to remain static. Otherwise ntp frequency\n * correction applies to one vcpu's system_timestamp but not\n * the others.\n *\n * So in those cases, request a kvmclock update for all vcpus.\n * We need to rate-limit these requests though, as they can\n * considerably slow guests that have a large number of vcpus.\n * The time for a remote vcpu to update its kvmclock is bound\n * by the delay we use to rate-limit the updates.\n */\n\n#define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)\n\nstatic void kvmclock_update_fn(struct work_struct *work)\n{\n\tint i;\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct kvm_arch *ka = container_of(dwork, struct kvm_arch,\n\t\t\t\t\t   kvmclock_update_work);\n\tstruct kvm *kvm = container_of(ka, struct kvm, arch);\n\tstruct kvm_vcpu *vcpu;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t}\n}\n\nstatic void kvm_gen_kvmclock_update(struct kvm_vcpu *v)\n{\n\tstruct kvm *kvm = v->kvm;\n\n\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, v);\n\tschedule_delayed_work(&kvm->arch.kvmclock_update_work,\n\t\t\t\t\tKVMCLOCK_UPDATE_DELAY);\n}\n\n#define KVMCLOCK_SYNC_PERIOD (300 * HZ)\n\nstatic void kvmclock_sync_fn(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct kvm_arch *ka = container_of(dwork, struct kvm_arch,\n\t\t\t\t\t   kvmclock_sync_work);\n\tstruct kvm *kvm = container_of(ka, struct kvm, arch);\n\n\tif (!kvmclock_periodic_sync)\n\t\treturn;\n\n\tschedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);\n\tschedule_delayed_work(&kvm->arch.kvmclock_sync_work,\n\t\t\t\t\tKVMCLOCK_SYNC_PERIOD);\n}\n\n/*\n * On AMD, HWCR[McStatusWrEn] controls whether setting MCi_STATUS results in #GP.\n */\nstatic bool can_set_mci_status(struct kvm_vcpu *vcpu)\n{\n\t/* McStatusWrEn enabled? */\n\tif (guest_cpuid_is_amd_or_hygon(vcpu))\n\t\treturn !!(vcpu->arch.msr_hwcr & BIT_ULL(18));\n\n\treturn false;\n}\n\nstatic int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tswitch (msr) {\n\tcase MSR_IA32_MCG_STATUS:\n\t\tvcpu->arch.mcg_status = data;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CTL:\n\t\tif (!(mcg_cap & MCG_CTL_P) &&\n\t\t    (data || !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tif (data != 0 && data != ~(u64)0)\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ctl = data;\n\t\tbreak;\n\tdefault:\n\t\tif (msr >= MSR_IA32_MC0_CTL &&\n\t\t    msr < MSR_IA32_MCx_CTL(bank_num)) {\n\t\t\tu32 offset = array_index_nospec(\n\t\t\t\tmsr - MSR_IA32_MC0_CTL,\n\t\t\t\tMSR_IA32_MCx_CTL(bank_num) - MSR_IA32_MC0_CTL);\n\n\t\t\t/* only 0 or all 1s can be written to IA32_MCi_CTL\n\t\t\t * some Linux kernels though clear bit 10 in bank 4 to\n\t\t\t * workaround a BIOS/GART TBL issue on AMD K8s, ignore\n\t\t\t * this to avoid an uncatched #GP in the guest\n\t\t\t */\n\t\t\tif ((offset & 0x3) == 0 &&\n\t\t\t    data != 0 && (data | (1 << 10)) != ~(u64)0)\n\t\t\t\treturn -1;\n\n\t\t\t/* MCi_STATUS */\n\t\t\tif (!msr_info->host_initiated &&\n\t\t\t    (offset & 0x3) == 1 && data != 0) {\n\t\t\t\tif (!can_set_mci_status(vcpu))\n\t\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\tvcpu->arch.mce_banks[offset] = data;\n\t\t\tbreak;\n\t\t}\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int xen_hvm_config(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tint lm = is_long_mode(vcpu);\n\tu8 *blob_addr = lm ? (u8 *)(long)kvm->arch.xen_hvm_config.blob_addr_64\n\t\t: (u8 *)(long)kvm->arch.xen_hvm_config.blob_addr_32;\n\tu8 blob_size = lm ? kvm->arch.xen_hvm_config.blob_size_64\n\t\t: kvm->arch.xen_hvm_config.blob_size_32;\n\tu32 page_num = data & ~PAGE_MASK;\n\tu64 page_addr = data & PAGE_MASK;\n\tu8 *page;\n\n\tif (page_num >= blob_size)\n\t\treturn 1;\n\n\tpage = memdup_user(blob_addr + (page_num * PAGE_SIZE), PAGE_SIZE);\n\tif (IS_ERR(page))\n\t\treturn PTR_ERR(page);\n\n\tif (kvm_vcpu_write_guest(vcpu, page_addr, page, PAGE_SIZE)) {\n\t\tkfree(page);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline bool kvm_pv_async_pf_enabled(struct kvm_vcpu *vcpu)\n{\n\tu64 mask = KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT;\n\n\treturn (vcpu->arch.apf.msr_en_val & mask) == mask;\n}\n\nstatic int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)\n{\n\tgpa_t gpa = data & ~0x3f;\n\n\t/* Bits 4:5 are reserved, Should be zero */\n\tif (data & 0x30)\n\t\treturn 1;\n\n\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_VMEXIT) &&\n\t    (data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT))\n\t\treturn 1;\n\n\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT) &&\n\t    (data & KVM_ASYNC_PF_DELIVERY_AS_INT))\n\t\treturn 1;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn data ? 1 : 0;\n\n\tvcpu->arch.apf.msr_en_val = data;\n\n\tif (!kvm_pv_async_pf_enabled(vcpu)) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_async_pf_hash_reset(vcpu);\n\t\treturn 0;\n\t}\n\n\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.apf.data, gpa,\n\t\t\t\t\tsizeof(u64)))\n\t\treturn 1;\n\n\tvcpu->arch.apf.send_user_only = !(data & KVM_ASYNC_PF_SEND_ALWAYS);\n\tvcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;\n\n\tkvm_async_pf_wakeup_all(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)\n{\n\t/* Bits 8-63 are reserved */\n\tif (data >> 8)\n\t\treturn 1;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\tvcpu->arch.apf.msr_int_val = data;\n\n\tvcpu->arch.apf.vec = data & KVM_ASYNC_PF_VEC_MASK;\n\n\treturn 0;\n}\n\nstatic void kvmclock_reset(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pv_time_enabled = false;\n\tvcpu->arch.time = 0;\n}\n\nstatic void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\tkvm_x86_ops.tlb_flush_all(vcpu);\n}\n\nstatic void kvm_vcpu_flush_tlb_guest(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.tlb_flush;\n\tkvm_x86_ops.tlb_flush_guest(vcpu);\n}\n\nstatic void record_steal_time(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_host_map map;\n\tstruct kvm_steal_time *st;\n\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\t/* -EAGAIN is returned in atomic context so we can just return. */\n\tif (kvm_map_gfn(vcpu, vcpu->arch.st.msr_val >> PAGE_SHIFT,\n\t\t\t&map, &vcpu->arch.st.cache, false))\n\t\treturn;\n\n\tst = map.hva +\n\t\toffset_in_page(vcpu->arch.st.msr_val & KVM_STEAL_VALID_BITS);\n\n\t/*\n\t * Doing a TLB flush here, on the guest's behalf, can avoid\n\t * expensive IPIs.\n\t */\n\tif (guest_pv_has(vcpu, KVM_FEATURE_PV_TLB_FLUSH)) {\n\t\ttrace_kvm_pv_tlb_flush(vcpu->vcpu_id,\n\t\t\t\t       st->preempted & KVM_VCPU_FLUSH_TLB);\n\t\tif (xchg(&st->preempted, 0) & KVM_VCPU_FLUSH_TLB)\n\t\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n\t}\n\n\tvcpu->arch.st.preempted = 0;\n\n\tif (st->version & 1)\n\t\tst->version += 1;  /* first time write, random junk */\n\n\tst->version += 1;\n\n\tsmp_wmb();\n\n\tst->steal += current->sched_info.run_delay -\n\t\tvcpu->arch.st.last_steal;\n\tvcpu->arch.st.last_steal = current->sched_info.run_delay;\n\n\tsmp_wmb();\n\n\tst->version += 1;\n\n\tkvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, false);\n}\n\nint kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tbool pr = false;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tswitch (msr) {\n\tcase MSR_AMD64_NB_CFG:\n\tcase MSR_IA32_UCODE_WRITE:\n\tcase MSR_VM_HSAVE_PA:\n\tcase MSR_AMD64_PATCH_LOADER:\n\tcase MSR_AMD64_BU_CFG2:\n\tcase MSR_AMD64_DC_CFG:\n\tcase MSR_F15H_EX_CFG:\n\t\tbreak;\n\n\tcase MSR_IA32_UCODE_REV:\n\t\tif (msr_info->host_initiated)\n\t\t\tvcpu->arch.microcode_version = data;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.arch_capabilities = data;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES: {\n\t\tstruct kvm_msr_entry msr_ent = {.index = msr, .data = 0};\n\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tif (guest_cpuid_has(vcpu, X86_FEATURE_PDCM) && kvm_get_msr_feature(&msr_ent))\n\t\t\treturn 1;\n\t\tif (data & ~msr_ent.data)\n\t\t\treturn 1;\n\n\t\tvcpu->arch.perf_capabilities = data;\n\n\t\treturn 0;\n\t\t}\n\tcase MSR_EFER:\n\t\treturn set_efer(vcpu, msr_info);\n\tcase MSR_K7_HWCR:\n\t\tdata &= ~(u64)0x40;\t/* ignore flush filter disable */\n\t\tdata &= ~(u64)0x100;\t/* ignore ignne emulation enable */\n\t\tdata &= ~(u64)0x8;\t/* ignore TLB cache disable */\n\n\t\t/* Handle McStatusWrEn */\n\t\tif (data == BIT_ULL(18)) {\n\t\t\tvcpu->arch.msr_hwcr = data;\n\t\t} else if (data != 0) {\n\t\t\tvcpu_unimpl(vcpu, \"unimplemented HWCR wrmsr: 0x%llx\\n\",\n\t\t\t\t    data);\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase MSR_FAM10H_MMIO_CONF_BASE:\n\t\tif (data != 0) {\n\t\t\tvcpu_unimpl(vcpu, \"unimplemented MMIO_CONF_BASE wrmsr: \"\n\t\t\t\t    \"0x%llx\\n\", data);\n\t\t\treturn 1;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_DEBUGCTLMSR:\n\t\tif (!data) {\n\t\t\t/* We support the non-activated case already */\n\t\t\tbreak;\n\t\t} else if (data & ~(DEBUGCTLMSR_LBR | DEBUGCTLMSR_BTF)) {\n\t\t\t/* Values other than LBR and BTF are vendor-specific,\n\t\t\t   thus reserved and should throw a #GP */\n\t\t\treturn 1;\n\t\t} else if (report_ignored_msrs)\n\t\t\tvcpu_unimpl(vcpu, \"%s: MSR_IA32_DEBUGCTLMSR 0x%llx, nop\\n\",\n\t\t\t\t    __func__, data);\n\t\tbreak;\n\tcase 0x200 ... 0x2ff:\n\t\treturn kvm_mtrr_set_msr(vcpu, msr, data);\n\tcase MSR_IA32_APICBASE:\n\t\treturn kvm_set_apic_base(vcpu, msr_info);\n\tcase APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:\n\t\treturn kvm_x2apic_msr_write(vcpu, msr, data);\n\tcase MSR_IA32_TSCDEADLINE:\n\t\tkvm_set_lapic_tscdeadline_msr(vcpu, data);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tif (guest_cpuid_has(vcpu, X86_FEATURE_TSC_ADJUST)) {\n\t\t\tif (!msr_info->host_initiated) {\n\t\t\t\ts64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;\n\t\t\t\tadjust_tsc_offset_guest(vcpu, adj);\n\t\t\t}\n\t\t\tvcpu->arch.ia32_tsc_adjust_msr = data;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_MISC_ENABLE:\n\t\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_MISC_ENABLE_NO_MWAIT) &&\n\t\t    ((vcpu->arch.ia32_misc_enable_msr ^ data) & MSR_IA32_MISC_ENABLE_MWAIT)) {\n\t\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_XMM3))\n\t\t\t\treturn 1;\n\t\t\tvcpu->arch.ia32_misc_enable_msr = data;\n\t\t\tkvm_update_cpuid_runtime(vcpu);\n\t\t} else {\n\t\t\tvcpu->arch.ia32_misc_enable_msr = data;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_SMBASE:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.smbase = data;\n\t\tbreak;\n\tcase MSR_IA32_POWER_CTL:\n\t\tvcpu->arch.msr_ia32_power_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_TSC:\n\t\tif (msr_info->host_initiated) {\n\t\t\tkvm_synchronize_tsc(vcpu, data);\n\t\t} else {\n\t\t\tu64 adj = kvm_compute_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;\n\t\t\tadjust_tsc_offset_guest(vcpu, adj);\n\t\t\tvcpu->arch.ia32_tsc_adjust_msr += adj;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_XSS:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XSAVES))\n\t\t\treturn 1;\n\t\t/*\n\t\t * KVM supports exposing PT to the guest, but does not support\n\t\t * IA32_XSS[bit 8]. Guests have to use RDMSR/WRMSR rather than\n\t\t * XSAVES/XRSTORS to save/restore PT MSRs.\n\t\t */\n\t\tif (data & ~supported_xss)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tbreak;\n\tcase MSR_SMI_COUNT:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvcpu->arch.smi_count = data;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tkvm_write_wall_clock(vcpu->kvm, data);\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tkvm_write_wall_clock(vcpu->kvm, data);\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tkvm_write_system_time(vcpu, data, false, msr_info->host_initiated);\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tkvm_write_system_time(vcpu, data, true,  msr_info->host_initiated);\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tif (kvm_pv_enable_async_pf(vcpu, data))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_INT:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tif (kvm_pv_enable_async_pf_int(vcpu, data))\n\t\t\treturn 1;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_ACK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\t\tif (data & 0x1) {\n\t\t\tvcpu->arch.apf.pageready_pending = false;\n\t\t\tkvm_check_async_pf_completion(vcpu);\n\t\t}\n\t\tbreak;\n\tcase MSR_KVM_STEAL_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))\n\t\t\treturn 1;\n\n\t\tif (unlikely(!sched_info_on()))\n\t\t\treturn 1;\n\n\t\tif (data & KVM_STEAL_RESERVED_MASK)\n\t\t\treturn 1;\n\n\t\tvcpu->arch.st.msr_val = data;\n\n\t\tif (!(data & KVM_MSR_ENABLED))\n\t\t\tbreak;\n\n\t\tkvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);\n\n\t\tbreak;\n\tcase MSR_KVM_PV_EOI_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))\n\t\t\treturn 1;\n\n\t\tif (kvm_lapic_enable_pv_eoi(vcpu, data, sizeof(u8)))\n\t\t\treturn 1;\n\t\tbreak;\n\n\tcase MSR_KVM_POLL_CONTROL:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_POLL_CONTROL))\n\t\t\treturn 1;\n\n\t\t/* only enable bit supported */\n\t\tif (data & (-1ULL << 1))\n\t\t\treturn 1;\n\n\t\tvcpu->arch.msr_kvm_poll_control = data;\n\t\tbreak;\n\n\tcase MSR_IA32_MCG_CTL:\n\tcase MSR_IA32_MCG_STATUS:\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\t\treturn set_msr_mce(vcpu, msr_info);\n\n\tcase MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3:\n\tcase MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1:\n\t\tpr = true;\n\t\tfallthrough;\n\tcase MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3:\n\tcase MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\n\t\tif (pr || data != 0)\n\t\t\tvcpu_unimpl(vcpu, \"disabled perfctr wrmsr: \"\n\t\t\t\t    \"0x%x data 0x%llx\\n\", msr, data);\n\t\tbreak;\n\tcase MSR_K7_CLK_CTL:\n\t\t/*\n\t\t * Ignore all writes to this no longer documented MSR.\n\t\t * Writes are only relevant for old K7 processors,\n\t\t * all pre-dating SVM, but a recommended workaround from\n\t\t * AMD for these chips. It is possible to specify the\n\t\t * affected processor models on the command line, hence\n\t\t * the need to ignore the workaround.\n\t\t */\n\t\tbreak;\n\tcase HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:\n\tcase HV_X64_MSR_SYNDBG_CONTROL ... HV_X64_MSR_SYNDBG_PENDING_BUFFER:\n\tcase HV_X64_MSR_SYNDBG_OPTIONS:\n\tcase HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:\n\tcase HV_X64_MSR_CRASH_CTL:\n\tcase HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT:\n\tcase HV_X64_MSR_REENLIGHTENMENT_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_STATUS:\n\t\treturn kvm_hv_set_msr_common(vcpu, msr, data,\n\t\t\t\t\t     msr_info->host_initiated);\n\tcase MSR_IA32_BBL_CR_CTL3:\n\t\t/* Drop writes to this legacy MSR -- see rdmsr\n\t\t * counterpart for further detail.\n\t\t */\n\t\tif (report_ignored_msrs)\n\t\t\tvcpu_unimpl(vcpu, \"ignored wrmsr: 0x%x data 0x%llx\\n\",\n\t\t\t\tmsr, data);\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_ID_LENGTH:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tvcpu->arch.osvw.length = data;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_STATUS:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tvcpu->arch.osvw.status = data;\n\t\tbreak;\n\tcase MSR_PLATFORM_INFO:\n\t\tif (!msr_info->host_initiated ||\n\t\t    (!(data & MSR_PLATFORM_INFO_CPUID_FAULT) &&\n\t\t     cpuid_fault_enabled(vcpu)))\n\t\t\treturn 1;\n\t\tvcpu->arch.msr_platform_info = data;\n\t\tbreak;\n\tcase MSR_MISC_FEATURES_ENABLES:\n\t\tif (data & ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT ||\n\t\t    (data & MSR_MISC_FEATURES_ENABLES_CPUID_FAULT &&\n\t\t     !supports_cpuid_fault(vcpu)))\n\t\t\treturn 1;\n\t\tvcpu->arch.msr_misc_features_enables = data;\n\t\tbreak;\n\tdefault:\n\t\tif (msr && (msr == vcpu->kvm->arch.xen_hvm_config.msr))\n\t\t\treturn xen_hvm_config(vcpu, data);\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr))\n\t\t\treturn kvm_pmu_set_msr(vcpu, msr_info);\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_set_msr_common);\n\nstatic int get_msr_mce(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata, bool host)\n{\n\tu64 data;\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\n\tswitch (msr) {\n\tcase MSR_IA32_P5_MC_ADDR:\n\tcase MSR_IA32_P5_MC_TYPE:\n\t\tdata = 0;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CAP:\n\t\tdata = vcpu->arch.mcg_cap;\n\t\tbreak;\n\tcase MSR_IA32_MCG_CTL:\n\t\tif (!(mcg_cap & MCG_CTL_P) && !host)\n\t\t\treturn 1;\n\t\tdata = vcpu->arch.mcg_ctl;\n\t\tbreak;\n\tcase MSR_IA32_MCG_STATUS:\n\t\tdata = vcpu->arch.mcg_status;\n\t\tbreak;\n\tdefault:\n\t\tif (msr >= MSR_IA32_MC0_CTL &&\n\t\t    msr < MSR_IA32_MCx_CTL(bank_num)) {\n\t\t\tu32 offset = array_index_nospec(\n\t\t\t\tmsr - MSR_IA32_MC0_CTL,\n\t\t\t\tMSR_IA32_MCx_CTL(bank_num) - MSR_IA32_MC0_CTL);\n\n\t\t\tdata = vcpu->arch.mce_banks[offset];\n\t\t\tbreak;\n\t\t}\n\t\treturn 1;\n\t}\n\t*pdata = data;\n\treturn 0;\n}\n\nint kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tswitch (msr_info->index) {\n\tcase MSR_IA32_PLATFORM_ID:\n\tcase MSR_IA32_EBL_CR_POWERON:\n\tcase MSR_IA32_DEBUGCTLMSR:\n\tcase MSR_IA32_LASTBRANCHFROMIP:\n\tcase MSR_IA32_LASTBRANCHTOIP:\n\tcase MSR_IA32_LASTINTFROMIP:\n\tcase MSR_IA32_LASTINTTOIP:\n\tcase MSR_K8_SYSCFG:\n\tcase MSR_K8_TSEG_ADDR:\n\tcase MSR_K8_TSEG_MASK:\n\tcase MSR_VM_HSAVE_PA:\n\tcase MSR_K8_INT_PENDING_MSG:\n\tcase MSR_AMD64_NB_CFG:\n\tcase MSR_FAM10H_MMIO_CONF_BASE:\n\tcase MSR_AMD64_BU_CFG2:\n\tcase MSR_IA32_PERF_CTL:\n\tcase MSR_AMD64_DC_CFG:\n\tcase MSR_F15H_EX_CFG:\n\t/*\n\t * Intel Sandy Bridge CPUs must support the RAPL (running average power\n\t * limit) MSRs. Just return 0, as we do not want to expose the host\n\t * data here. Do not conditionalize this on CPUID, as KVM does not do\n\t * so for existing CPU-specific MSRs.\n\t */\n\tcase MSR_RAPL_POWER_UNIT:\n\tcase MSR_PP0_ENERGY_STATUS:\t/* Power plane 0 (core) */\n\tcase MSR_PP1_ENERGY_STATUS:\t/* Power plane 1 (graphics uncore) */\n\tcase MSR_PKG_ENERGY_STATUS:\t/* Total package */\n\tcase MSR_DRAM_ENERGY_STATUS:\t/* DRAM controller */\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_F15H_PERF_CTL0 ... MSR_F15H_PERF_CTR5:\n\tcase MSR_K7_EVNTSEL0 ... MSR_K7_EVNTSEL3:\n\tcase MSR_K7_PERFCTR0 ... MSR_K7_PERFCTR3:\n\tcase MSR_P6_PERFCTR0 ... MSR_P6_PERFCTR1:\n\tcase MSR_P6_EVNTSEL0 ... MSR_P6_EVNTSEL1:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_IA32_UCODE_REV:\n\t\tmsr_info->data = vcpu->arch.microcode_version;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.arch_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_PDCM))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.perf_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_POWER_CTL:\n\t\tmsr_info->data = vcpu->arch.msr_ia32_power_ctl;\n\t\tbreak;\n\tcase MSR_IA32_TSC: {\n\t\t/*\n\t\t * Intel SDM states that MSR_IA32_TSC read adds the TSC offset\n\t\t * even when not intercepted. AMD manual doesn't explicitly\n\t\t * state this but appears to behave the same.\n\t\t *\n\t\t * On userspace reads and writes, however, we unconditionally\n\t\t * return L1's TSC value to ensure backwards-compatible\n\t\t * behavior for migration.\n\t\t */\n\t\tu64 tsc_offset = msr_info->host_initiated ? vcpu->arch.l1_tsc_offset :\n\t\t\t\t\t\t\t    vcpu->arch.tsc_offset;\n\n\t\tmsr_info->data = kvm_scale_tsc(vcpu, rdtsc()) + tsc_offset;\n\t\tbreak;\n\t}\n\tcase MSR_MTRRcap:\n\tcase 0x200 ... 0x2ff:\n\t\treturn kvm_mtrr_get_msr(vcpu, msr_info->index, &msr_info->data);\n\tcase 0xcd: /* fsb frequency */\n\t\tmsr_info->data = 3;\n\t\tbreak;\n\t\t/*\n\t\t * MSR_EBC_FREQUENCY_ID\n\t\t * Conservative value valid for even the basic CPU models.\n\t\t * Models 0,1: 000 in bits 23:21 indicating a bus speed of\n\t\t * 100MHz, model 2 000 in bits 18:16 indicating 100MHz,\n\t\t * and 266MHz for model 3, or 4. Set Core Clock\n\t\t * Frequency to System Bus Frequency Ratio to 1 (bits\n\t\t * 31:24) even though these are only valid for CPU\n\t\t * models > 2, however guests may end up dividing or\n\t\t * multiplying by zero otherwise.\n\t\t */\n\tcase MSR_EBC_FREQUENCY_ID:\n\t\tmsr_info->data = 1 << 24;\n\t\tbreak;\n\tcase MSR_IA32_APICBASE:\n\t\tmsr_info->data = kvm_get_apic_base(vcpu);\n\t\tbreak;\n\tcase APIC_BASE_MSR ... APIC_BASE_MSR + 0xff:\n\t\treturn kvm_x2apic_msr_read(vcpu, msr_info->index, &msr_info->data);\n\tcase MSR_IA32_TSCDEADLINE:\n\t\tmsr_info->data = kvm_get_lapic_tscdeadline_msr(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tmsr_info->data = (u64)vcpu->arch.ia32_tsc_adjust_msr;\n\t\tbreak;\n\tcase MSR_IA32_MISC_ENABLE:\n\t\tmsr_info->data = vcpu->arch.ia32_misc_enable_msr;\n\t\tbreak;\n\tcase MSR_IA32_SMBASE:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.smbase;\n\t\tbreak;\n\tcase MSR_SMI_COUNT:\n\t\tmsr_info->data = vcpu->arch.smi_count;\n\t\tbreak;\n\tcase MSR_IA32_PERF_STATUS:\n\t\t/* TSC increment by tick */\n\t\tmsr_info->data = 1000ULL;\n\t\t/* CPU multiplier */\n\t\tmsr_info->data |= (((uint64_t)4ULL) << 40);\n\t\tbreak;\n\tcase MSR_EFER:\n\t\tmsr_info->data = vcpu->arch.efer;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->kvm->arch.wall_clock;\n\t\tbreak;\n\tcase MSR_KVM_WALL_CLOCK_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->kvm->arch.wall_clock;\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.time;\n\t\tbreak;\n\tcase MSR_KVM_SYSTEM_TIME_NEW:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_CLOCKSOURCE2))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.time;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.apf.msr_en_val;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_INT:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.apf.msr_int_val;\n\t\tbreak;\n\tcase MSR_KVM_ASYNC_PF_ACK:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = 0;\n\t\tbreak;\n\tcase MSR_KVM_STEAL_TIME:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_STEAL_TIME))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.st.msr_val;\n\t\tbreak;\n\tcase MSR_KVM_PV_EOI_EN:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_EOI))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.pv_eoi.msr_val;\n\t\tbreak;\n\tcase MSR_KVM_POLL_CONTROL:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_POLL_CONTROL))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vcpu->arch.msr_kvm_poll_control;\n\t\tbreak;\n\tcase MSR_IA32_P5_MC_ADDR:\n\tcase MSR_IA32_P5_MC_TYPE:\n\tcase MSR_IA32_MCG_CAP:\n\tcase MSR_IA32_MCG_CTL:\n\tcase MSR_IA32_MCG_STATUS:\n\tcase MSR_IA32_MC0_CTL ... MSR_IA32_MCx_CTL(KVM_MAX_MCE_BANKS) - 1:\n\t\treturn get_msr_mce(vcpu, msr_info->index, &msr_info->data,\n\t\t\t\t   msr_info->host_initiated);\n\tcase MSR_IA32_XSS:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_XSAVES))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.ia32_xss;\n\t\tbreak;\n\tcase MSR_K7_CLK_CTL:\n\t\t/*\n\t\t * Provide expected ramp-up count for K7. All other\n\t\t * are set to zero, indicating minimum divisors for\n\t\t * every field.\n\t\t *\n\t\t * This prevents guest kernels on AMD host with CPU\n\t\t * type 6, model 8 and higher from exploding due to\n\t\t * the rdmsr failing.\n\t\t */\n\t\tmsr_info->data = 0x20000000;\n\t\tbreak;\n\tcase HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:\n\tcase HV_X64_MSR_SYNDBG_CONTROL ... HV_X64_MSR_SYNDBG_PENDING_BUFFER:\n\tcase HV_X64_MSR_SYNDBG_OPTIONS:\n\tcase HV_X64_MSR_CRASH_P0 ... HV_X64_MSR_CRASH_P4:\n\tcase HV_X64_MSR_CRASH_CTL:\n\tcase HV_X64_MSR_STIMER0_CONFIG ... HV_X64_MSR_STIMER3_COUNT:\n\tcase HV_X64_MSR_REENLIGHTENMENT_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_CONTROL:\n\tcase HV_X64_MSR_TSC_EMULATION_STATUS:\n\t\treturn kvm_hv_get_msr_common(vcpu,\n\t\t\t\t\t     msr_info->index, &msr_info->data,\n\t\t\t\t\t     msr_info->host_initiated);\n\tcase MSR_IA32_BBL_CR_CTL3:\n\t\t/* This legacy MSR exists but isn't fully documented in current\n\t\t * silicon.  It is however accessed by winxp in very narrow\n\t\t * scenarios where it sets bit #19, itself documented as\n\t\t * a \"reserved\" bit.  Best effort attempt to source coherent\n\t\t * read data here should the balance of the register be\n\t\t * interpreted by the guest:\n\t\t *\n\t\t * L2 cache control register 3: 64GB range, 256KB size,\n\t\t * enabled, latency 0x1, configured\n\t\t */\n\t\tmsr_info->data = 0xbe702111;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_ID_LENGTH:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.osvw.length;\n\t\tbreak;\n\tcase MSR_AMD64_OSVW_STATUS:\n\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_OSVW))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.osvw.status;\n\t\tbreak;\n\tcase MSR_PLATFORM_INFO:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !vcpu->kvm->arch.guest_can_read_msr_platform_info)\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.msr_platform_info;\n\t\tbreak;\n\tcase MSR_MISC_FEATURES_ENABLES:\n\t\tmsr_info->data = vcpu->arch.msr_misc_features_enables;\n\t\tbreak;\n\tcase MSR_K7_HWCR:\n\t\tmsr_info->data = vcpu->arch.msr_hwcr;\n\t\tbreak;\n\tdefault:\n\t\tif (kvm_pmu_is_valid_msr(vcpu, msr_info->index))\n\t\t\treturn kvm_pmu_get_msr(vcpu, msr_info);\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_get_msr_common);\n\n/*\n * Read or write a bunch of msrs. All parameters are kernel addresses.\n *\n * @return number of msrs set successfully.\n */\nstatic int __msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs *msrs,\n\t\t    struct kvm_msr_entry *entries,\n\t\t    int (*do_msr)(struct kvm_vcpu *vcpu,\n\t\t\t\t  unsigned index, u64 *data))\n{\n\tint i;\n\n\tfor (i = 0; i < msrs->nmsrs; ++i)\n\t\tif (do_msr(vcpu, entries[i].index, &entries[i].data))\n\t\t\tbreak;\n\n\treturn i;\n}\n\n/*\n * Read or write a bunch of msrs. Parameters are user addresses.\n *\n * @return number of msrs set successfully.\n */\nstatic int msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs __user *user_msrs,\n\t\t  int (*do_msr)(struct kvm_vcpu *vcpu,\n\t\t\t\tunsigned index, u64 *data),\n\t\t  int writeback)\n{\n\tstruct kvm_msrs msrs;\n\tstruct kvm_msr_entry *entries;\n\tint r, n;\n\tunsigned size;\n\n\tr = -EFAULT;\n\tif (copy_from_user(&msrs, user_msrs, sizeof(msrs)))\n\t\tgoto out;\n\n\tr = -E2BIG;\n\tif (msrs.nmsrs >= MAX_IO_MSRS)\n\t\tgoto out;\n\n\tsize = sizeof(struct kvm_msr_entry) * msrs.nmsrs;\n\tentries = memdup_user(user_msrs->entries, size);\n\tif (IS_ERR(entries)) {\n\t\tr = PTR_ERR(entries);\n\t\tgoto out;\n\t}\n\n\tr = n = __msr_io(vcpu, &msrs, entries, do_msr);\n\tif (r < 0)\n\t\tgoto out_free;\n\n\tr = -EFAULT;\n\tif (writeback && copy_to_user(user_msrs->entries, entries, size))\n\t\tgoto out_free;\n\n\tr = n;\n\nout_free:\n\tkfree(entries);\nout:\n\treturn r;\n}\n\nstatic inline bool kvm_can_mwait_in_guest(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_MWAIT) &&\n\t\t!boot_cpu_has_bug(X86_BUG_MONITOR) &&\n\t\tboot_cpu_has(X86_FEATURE_ARAT);\n}\n\nint kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)\n{\n\tint r = 0;\n\n\tswitch (ext) {\n\tcase KVM_CAP_IRQCHIP:\n\tcase KVM_CAP_HLT:\n\tcase KVM_CAP_MMU_SHADOW_CACHE_CONTROL:\n\tcase KVM_CAP_SET_TSS_ADDR:\n\tcase KVM_CAP_EXT_CPUID:\n\tcase KVM_CAP_EXT_EMUL_CPUID:\n\tcase KVM_CAP_CLOCKSOURCE:\n\tcase KVM_CAP_PIT:\n\tcase KVM_CAP_NOP_IO_DELAY:\n\tcase KVM_CAP_MP_STATE:\n\tcase KVM_CAP_SYNC_MMU:\n\tcase KVM_CAP_USER_NMI:\n\tcase KVM_CAP_REINJECT_CONTROL:\n\tcase KVM_CAP_IRQ_INJECT_STATUS:\n\tcase KVM_CAP_IOEVENTFD:\n\tcase KVM_CAP_IOEVENTFD_NO_LENGTH:\n\tcase KVM_CAP_PIT2:\n\tcase KVM_CAP_PIT_STATE2:\n\tcase KVM_CAP_SET_IDENTITY_MAP_ADDR:\n\tcase KVM_CAP_XEN_HVM:\n\tcase KVM_CAP_VCPU_EVENTS:\n\tcase KVM_CAP_HYPERV:\n\tcase KVM_CAP_HYPERV_VAPIC:\n\tcase KVM_CAP_HYPERV_SPIN:\n\tcase KVM_CAP_HYPERV_SYNIC:\n\tcase KVM_CAP_HYPERV_SYNIC2:\n\tcase KVM_CAP_HYPERV_VP_INDEX:\n\tcase KVM_CAP_HYPERV_EVENTFD:\n\tcase KVM_CAP_HYPERV_TLBFLUSH:\n\tcase KVM_CAP_HYPERV_SEND_IPI:\n\tcase KVM_CAP_HYPERV_CPUID:\n\tcase KVM_CAP_PCI_SEGMENT:\n\tcase KVM_CAP_DEBUGREGS:\n\tcase KVM_CAP_X86_ROBUST_SINGLESTEP:\n\tcase KVM_CAP_XSAVE:\n\tcase KVM_CAP_ASYNC_PF:\n\tcase KVM_CAP_ASYNC_PF_INT:\n\tcase KVM_CAP_GET_TSC_KHZ:\n\tcase KVM_CAP_KVMCLOCK_CTRL:\n\tcase KVM_CAP_READONLY_MEM:\n\tcase KVM_CAP_HYPERV_TIME:\n\tcase KVM_CAP_IOAPIC_POLARITY_IGNORED:\n\tcase KVM_CAP_TSC_DEADLINE_TIMER:\n\tcase KVM_CAP_DISABLE_QUIRKS:\n\tcase KVM_CAP_SET_BOOT_CPU_ID:\n \tcase KVM_CAP_SPLIT_IRQCHIP:\n\tcase KVM_CAP_IMMEDIATE_EXIT:\n\tcase KVM_CAP_PMU_EVENT_FILTER:\n\tcase KVM_CAP_GET_MSR_FEATURES:\n\tcase KVM_CAP_MSR_PLATFORM_INFO:\n\tcase KVM_CAP_EXCEPTION_PAYLOAD:\n\tcase KVM_CAP_SET_GUEST_DEBUG:\n\tcase KVM_CAP_LAST_CPU:\n\tcase KVM_CAP_X86_USER_SPACE_MSR:\n\tcase KVM_CAP_X86_MSR_FILTER:\n\tcase KVM_CAP_ENFORCE_PV_FEATURE_CPUID:\n\t\tr = 1;\n\t\tbreak;\n\tcase KVM_CAP_SYNC_REGS:\n\t\tr = KVM_SYNC_X86_VALID_FIELDS;\n\t\tbreak;\n\tcase KVM_CAP_ADJUST_CLOCK:\n\t\tr = KVM_CLOCK_TSC_STABLE;\n\t\tbreak;\n\tcase KVM_CAP_X86_DISABLE_EXITS:\n\t\tr |=  KVM_X86_DISABLE_EXITS_HLT | KVM_X86_DISABLE_EXITS_PAUSE |\n\t\t      KVM_X86_DISABLE_EXITS_CSTATE;\n\t\tif(kvm_can_mwait_in_guest())\n\t\t\tr |= KVM_X86_DISABLE_EXITS_MWAIT;\n\t\tbreak;\n\tcase KVM_CAP_X86_SMM:\n\t\t/* SMBASE is usually relocated above 1M on modern chipsets,\n\t\t * and SMM handlers might indeed rely on 4G segment limits,\n\t\t * so do not report SMM to be available if real mode is\n\t\t * emulated via vm86 mode.  Still, do not go to great lengths\n\t\t * to avoid userspace's usage of the feature, because it is a\n\t\t * fringe case that is not enabled except via specific settings\n\t\t * of the module parameters.\n\t\t */\n\t\tr = kvm_x86_ops.has_emulated_msr(MSR_IA32_SMBASE);\n\t\tbreak;\n\tcase KVM_CAP_VAPIC:\n\t\tr = !kvm_x86_ops.cpu_has_accelerated_tpr();\n\t\tbreak;\n\tcase KVM_CAP_NR_VCPUS:\n\t\tr = KVM_SOFT_MAX_VCPUS;\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPUS:\n\t\tr = KVM_MAX_VCPUS;\n\t\tbreak;\n\tcase KVM_CAP_MAX_VCPU_ID:\n\t\tr = KVM_MAX_VCPU_ID;\n\t\tbreak;\n\tcase KVM_CAP_PV_MMU:\t/* obsolete */\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_MCE:\n\t\tr = KVM_MAX_MCE_BANKS;\n\t\tbreak;\n\tcase KVM_CAP_XCRS:\n\t\tr = boot_cpu_has(X86_FEATURE_XSAVE);\n\t\tbreak;\n\tcase KVM_CAP_TSC_CONTROL:\n\t\tr = kvm_has_tsc_control;\n\t\tbreak;\n\tcase KVM_CAP_X2APIC_API:\n\t\tr = KVM_X2APIC_API_VALID_FLAGS;\n\t\tbreak;\n\tcase KVM_CAP_NESTED_STATE:\n\t\tr = kvm_x86_ops.nested_ops->get_state ?\n\t\t\tkvm_x86_ops.nested_ops->get_state(NULL, NULL, 0) : 0;\n\t\tbreak;\n\tcase KVM_CAP_HYPERV_DIRECT_TLBFLUSH:\n\t\tr = kvm_x86_ops.enable_direct_tlbflush != NULL;\n\t\tbreak;\n\tcase KVM_CAP_HYPERV_ENLIGHTENED_VMCS:\n\t\tr = kvm_x86_ops.nested_ops->enable_evmcs != NULL;\n\t\tbreak;\n\tcase KVM_CAP_SMALLER_MAXPHYADDR:\n\t\tr = (int) allow_smaller_maxphyaddr;\n\t\tbreak;\n\tcase KVM_CAP_STEAL_TIME:\n\t\tr = sched_info_on();\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn r;\n\n}\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n\t\t\tunsigned int ioctl, unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tlong r;\n\n\tswitch (ioctl) {\n\tcase KVM_GET_MSR_INDEX_LIST: {\n\t\tstruct kvm_msr_list __user *user_msr_list = argp;\n\t\tstruct kvm_msr_list msr_list;\n\t\tunsigned n;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tn = msr_list.nmsrs;\n\t\tmsr_list.nmsrs = num_msrs_to_save + num_emulated_msrs;\n\t\tif (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tr = -E2BIG;\n\t\tif (n < msr_list.nmsrs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(user_msr_list->indices, &msrs_to_save,\n\t\t\t\t num_msrs_to_save * sizeof(u32)))\n\t\t\tgoto out;\n\t\tif (copy_to_user(user_msr_list->indices + num_msrs_to_save,\n\t\t\t\t &emulated_msrs,\n\t\t\t\t num_emulated_msrs * sizeof(u32)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_SUPPORTED_CPUID:\n\tcase KVM_GET_EMULATED_CPUID: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\n\t\tr = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,\n\t\t\t\t\t    ioctl);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_X86_GET_MCE_CAP_SUPPORTED:\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &kvm_mce_cap_supported,\n\t\t\t\t sizeof(kvm_mce_cap_supported)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_GET_MSR_FEATURE_INDEX_LIST: {\n\t\tstruct kvm_msr_list __user *user_msr_list = argp;\n\t\tstruct kvm_msr_list msr_list;\n\t\tunsigned int n;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&msr_list, user_msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tn = msr_list.nmsrs;\n\t\tmsr_list.nmsrs = num_msr_based_features;\n\t\tif (copy_to_user(user_msr_list, &msr_list, sizeof(msr_list)))\n\t\t\tgoto out;\n\t\tr = -E2BIG;\n\t\tif (n < msr_list.nmsrs)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(user_msr_list->indices, &msr_based_features,\n\t\t\t\t num_msr_based_features * sizeof(u32)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MSRS:\n\t\tr = msr_io(NULL, argp, do_get_msr_feature, 1);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\nout:\n\treturn r;\n}\n\nstatic void wbinvd_ipi(void *garbage)\n{\n\twbinvd();\n}\n\nstatic bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_arch_has_noncoherent_dma(vcpu->kvm);\n}\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\t/* Address WBINVD may be executed by guest */\n\tif (need_emulate_wbinvd(vcpu)) {\n\t\tif (kvm_x86_ops.has_wbinvd_exit())\n\t\t\tcpumask_set_cpu(cpu, vcpu->arch.wbinvd_dirty_mask);\n\t\telse if (vcpu->cpu != -1 && vcpu->cpu != cpu)\n\t\t\tsmp_call_function_single(vcpu->cpu,\n\t\t\t\t\twbinvd_ipi, NULL, 1);\n\t}\n\n\tkvm_x86_ops.vcpu_load(vcpu, cpu);\n\n\t/* Save host pkru register if supported */\n\tvcpu->arch.host_pkru = read_pkru();\n\n\t/* Apply any externally detected TSC adjustments (due to suspend) */\n\tif (unlikely(vcpu->arch.tsc_offset_adjustment)) {\n\t\tadjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);\n\t\tvcpu->arch.tsc_offset_adjustment = 0;\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t}\n\n\tif (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {\n\t\ts64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :\n\t\t\t\trdtsc() - vcpu->arch.last_host_tsc;\n\t\tif (tsc_delta < 0)\n\t\t\tmark_tsc_unstable(\"KVM discovered backwards TSC\");\n\n\t\tif (kvm_check_tsc_unstable()) {\n\t\t\tu64 offset = kvm_compute_tsc_offset(vcpu,\n\t\t\t\t\t\tvcpu->arch.last_guest_tsc);\n\t\t\tkvm_vcpu_write_tsc_offset(vcpu, offset);\n\t\t\tvcpu->arch.tsc_catchup = 1;\n\t\t}\n\n\t\tif (kvm_lapic_hv_timer_in_use(vcpu))\n\t\t\tkvm_lapic_restart_hv_timer(vcpu);\n\n\t\t/*\n\t\t * On a host with synchronized TSC, there is no need to update\n\t\t * kvmclock on vcpu->cpu migration\n\t\t */\n\t\tif (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)\n\t\t\tkvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);\n\t\tif (vcpu->cpu != cpu)\n\t\t\tkvm_make_request(KVM_REQ_MIGRATE_TIMER, vcpu);\n\t\tvcpu->cpu = cpu;\n\t}\n\n\tkvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);\n}\n\nstatic void kvm_steal_time_set_preempted(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_host_map map;\n\tstruct kvm_steal_time *st;\n\n\tif (!(vcpu->arch.st.msr_val & KVM_MSR_ENABLED))\n\t\treturn;\n\n\tif (vcpu->arch.st.preempted)\n\t\treturn;\n\n\tif (kvm_map_gfn(vcpu, vcpu->arch.st.msr_val >> PAGE_SHIFT, &map,\n\t\t\t&vcpu->arch.st.cache, true))\n\t\treturn;\n\n\tst = map.hva +\n\t\toffset_in_page(vcpu->arch.st.msr_val & KVM_STEAL_VALID_BITS);\n\n\tst->preempted = vcpu->arch.st.preempted = KVM_VCPU_PREEMPTED;\n\n\tkvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, true);\n}\n\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tint idx;\n\n\tif (vcpu->preempted)\n\t\tvcpu->arch.preempted_in_kernel = !kvm_x86_ops.get_cpl(vcpu);\n\n\t/*\n\t * Disable page faults because we're in atomic context here.\n\t * kvm_write_guest_offset_cached() would call might_fault()\n\t * that relies on pagefault_disable() to tell if there's a\n\t * bug. NOTE: the write to guest memory may not go through if\n\t * during postcopy live migration or if there's heavy guest\n\t * paging.\n\t */\n\tpagefault_disable();\n\t/*\n\t * kvm_memslots() will be called by\n\t * kvm_write_guest_offset_cached() so take the srcu lock.\n\t */\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tkvm_steal_time_set_preempted(vcpu);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\tpagefault_enable();\n\tkvm_x86_ops.vcpu_put(vcpu);\n\tvcpu->arch.last_host_tsc = rdtsc();\n\t/*\n\t * If userspace has set any breakpoints or watchpoints, dr6 is restored\n\t * on every vmexit, but if not, we might have a stale dr6 from the\n\t * guest. do_debug expects dr6 to be cleared after it runs, do the same.\n\t */\n\tset_debugreg(0, 6);\n}\n\nstatic int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_lapic_state *s)\n{\n\tif (vcpu->arch.apicv_active)\n\t\tkvm_x86_ops.sync_pir_to_irr(vcpu);\n\n\treturn kvm_apic_get_state(vcpu, s);\n}\n\nstatic int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_lapic_state *s)\n{\n\tint r;\n\n\tr = kvm_apic_set_state(vcpu, s);\n\tif (r)\n\t\treturn r;\n\tupdate_cr8_intercept(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_cpu_accept_dm_intr(struct kvm_vcpu *vcpu)\n{\n\treturn (!lapic_in_kernel(vcpu) ||\n\t\tkvm_apic_accept_pic_intr(vcpu));\n}\n\n/*\n * if userspace requested an interrupt window, check that the\n * interrupt window is open.\n *\n * No need to exit to userspace if we already have an interrupt queued.\n */\nstatic int kvm_vcpu_ready_for_interrupt_injection(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_arch_interrupt_allowed(vcpu) &&\n\t\t!kvm_cpu_has_interrupt(vcpu) &&\n\t\t!kvm_event_needs_reinjection(vcpu) &&\n\t\tkvm_cpu_accept_dm_intr(vcpu);\n}\n\nstatic int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_interrupt *irq)\n{\n\tif (irq->irq >= KVM_NR_INTERRUPTS)\n\t\treturn -EINVAL;\n\n\tif (!irqchip_in_kernel(vcpu->kvm)) {\n\t\tkvm_queue_interrupt(vcpu, irq->irq, false);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * With in-kernel LAPIC, we only use this to inject EXTINT, so\n\t * fail for in-kernel 8259.\n\t */\n\tif (pic_in_kernel(vcpu->kvm))\n\t\treturn -ENXIO;\n\n\tif (vcpu->arch.pending_external_vector != -1)\n\t\treturn -EEXIST;\n\n\tvcpu->arch.pending_external_vector = irq->irq;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_nmi(struct kvm_vcpu *vcpu)\n{\n\tkvm_inject_nmi(vcpu);\n\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_smi(struct kvm_vcpu *vcpu)\n{\n\tkvm_make_request(KVM_REQ_SMI, vcpu);\n\n\treturn 0;\n}\n\nstatic int vcpu_ioctl_tpr_access_reporting(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   struct kvm_tpr_access_ctl *tac)\n{\n\tif (tac->flags)\n\t\treturn -EINVAL;\n\tvcpu->arch.tpr_access_reporting = !!tac->enabled;\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_setup_mce(struct kvm_vcpu *vcpu,\n\t\t\t\t\tu64 mcg_cap)\n{\n\tint r;\n\tunsigned bank_num = mcg_cap & 0xff, bank;\n\n\tr = -EINVAL;\n\tif (!bank_num || bank_num > KVM_MAX_MCE_BANKS)\n\t\tgoto out;\n\tif (mcg_cap & ~(kvm_mce_cap_supported | 0xff | 0xff0000))\n\t\tgoto out;\n\tr = 0;\n\tvcpu->arch.mcg_cap = mcg_cap;\n\t/* Init IA32_MCG_CTL to all 1s */\n\tif (mcg_cap & MCG_CTL_P)\n\t\tvcpu->arch.mcg_ctl = ~(u64)0;\n\t/* Init IA32_MCi_CTL to all 1s */\n\tfor (bank = 0; bank < bank_num; bank++)\n\t\tvcpu->arch.mce_banks[bank*4] = ~(u64)0;\n\n\tkvm_x86_ops.setup_mce(vcpu);\nout:\n\treturn r;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_mce(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct kvm_x86_mce *mce)\n{\n\tu64 mcg_cap = vcpu->arch.mcg_cap;\n\tunsigned bank_num = mcg_cap & 0xff;\n\tu64 *banks = vcpu->arch.mce_banks;\n\n\tif (mce->bank >= bank_num || !(mce->status & MCI_STATUS_VAL))\n\t\treturn -EINVAL;\n\t/*\n\t * if IA32_MCG_CTL is not all 1s, the uncorrected error\n\t * reporting is disabled\n\t */\n\tif ((mce->status & MCI_STATUS_UC) && (mcg_cap & MCG_CTL_P) &&\n\t    vcpu->arch.mcg_ctl != ~(u64)0)\n\t\treturn 0;\n\tbanks += 4 * mce->bank;\n\t/*\n\t * if IA32_MCi_CTL is not all 1s, the uncorrected error\n\t * reporting is disabled for the bank\n\t */\n\tif ((mce->status & MCI_STATUS_UC) && banks[0] != ~(u64)0)\n\t\treturn 0;\n\tif (mce->status & MCI_STATUS_UC) {\n\t\tif ((vcpu->arch.mcg_status & MCG_STATUS_MCIP) ||\n\t\t    !kvm_read_cr4_bits(vcpu, X86_CR4_MCE)) {\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\t\treturn 0;\n\t\t}\n\t\tif (banks[1] & MCI_STATUS_VAL)\n\t\t\tmce->status |= MCI_STATUS_OVER;\n\t\tbanks[2] = mce->addr;\n\t\tbanks[3] = mce->misc;\n\t\tvcpu->arch.mcg_status = mce->mcg_status;\n\t\tbanks[1] = mce->status;\n\t\tkvm_queue_exception(vcpu, MC_VECTOR);\n\t} else if (!(banks[1] & MCI_STATUS_VAL)\n\t\t   || !(banks[1] & MCI_STATUS_UC)) {\n\t\tif (banks[1] & MCI_STATUS_VAL)\n\t\t\tmce->status |= MCI_STATUS_OVER;\n\t\tbanks[2] = mce->addr;\n\t\tbanks[3] = mce->misc;\n\t\tbanks[1] = mce->status;\n\t} else\n\t\tbanks[1] |= MCI_STATUS_OVER;\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct kvm_vcpu_events *events)\n{\n\tprocess_nmi(vcpu);\n\n\t/*\n\t * In guest mode, payload delivery should be deferred,\n\t * so that the L1 hypervisor can intercept #PF before\n\t * CR2 is modified (or intercept #DB before DR6 is\n\t * modified under nVMX). Unless the per-VM capability,\n\t * KVM_CAP_EXCEPTION_PAYLOAD, is set, we may not defer the delivery of\n\t * an exception payload and handle after a KVM_GET_VCPU_EVENTS. Since we\n\t * opportunistically defer the exception payload, deliver it if the\n\t * capability hasn't been requested before processing a\n\t * KVM_GET_VCPU_EVENTS.\n\t */\n\tif (!vcpu->kvm->arch.exception_payload_enabled &&\n\t    vcpu->arch.exception.pending && vcpu->arch.exception.has_payload)\n\t\tkvm_deliver_exception_payload(vcpu);\n\n\t/*\n\t * The API doesn't provide the instruction length for software\n\t * exceptions, so don't report them. As long as the guest RIP\n\t * isn't advanced, we should expect to encounter the exception\n\t * again.\n\t */\n\tif (kvm_exception_is_soft(vcpu->arch.exception.nr)) {\n\t\tevents->exception.injected = 0;\n\t\tevents->exception.pending = 0;\n\t} else {\n\t\tevents->exception.injected = vcpu->arch.exception.injected;\n\t\tevents->exception.pending = vcpu->arch.exception.pending;\n\t\t/*\n\t\t * For ABI compatibility, deliberately conflate\n\t\t * pending and injected exceptions when\n\t\t * KVM_CAP_EXCEPTION_PAYLOAD isn't enabled.\n\t\t */\n\t\tif (!vcpu->kvm->arch.exception_payload_enabled)\n\t\t\tevents->exception.injected |=\n\t\t\t\tvcpu->arch.exception.pending;\n\t}\n\tevents->exception.nr = vcpu->arch.exception.nr;\n\tevents->exception.has_error_code = vcpu->arch.exception.has_error_code;\n\tevents->exception.error_code = vcpu->arch.exception.error_code;\n\tevents->exception_has_payload = vcpu->arch.exception.has_payload;\n\tevents->exception_payload = vcpu->arch.exception.payload;\n\n\tevents->interrupt.injected =\n\t\tvcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft;\n\tevents->interrupt.nr = vcpu->arch.interrupt.nr;\n\tevents->interrupt.soft = 0;\n\tevents->interrupt.shadow = kvm_x86_ops.get_interrupt_shadow(vcpu);\n\n\tevents->nmi.injected = vcpu->arch.nmi_injected;\n\tevents->nmi.pending = vcpu->arch.nmi_pending != 0;\n\tevents->nmi.masked = kvm_x86_ops.get_nmi_mask(vcpu);\n\tevents->nmi.pad = 0;\n\n\tevents->sipi_vector = 0; /* never valid when reporting to user space */\n\n\tevents->smi.smm = is_smm(vcpu);\n\tevents->smi.pending = vcpu->arch.smi_pending;\n\tevents->smi.smm_inside_nmi =\n\t\t!!(vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK);\n\tevents->smi.latched_init = kvm_lapic_latched_init(vcpu);\n\n\tevents->flags = (KVM_VCPUEVENT_VALID_NMI_PENDING\n\t\t\t | KVM_VCPUEVENT_VALID_SHADOW\n\t\t\t | KVM_VCPUEVENT_VALID_SMM);\n\tif (vcpu->kvm->arch.exception_payload_enabled)\n\t\tevents->flags |= KVM_VCPUEVENT_VALID_PAYLOAD;\n\n\tmemset(&events->reserved, 0, sizeof(events->reserved));\n}\n\nstatic void kvm_smm_changed(struct kvm_vcpu *vcpu);\n\nstatic int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      struct kvm_vcpu_events *events)\n{\n\tif (events->flags & ~(KVM_VCPUEVENT_VALID_NMI_PENDING\n\t\t\t      | KVM_VCPUEVENT_VALID_SIPI_VECTOR\n\t\t\t      | KVM_VCPUEVENT_VALID_SHADOW\n\t\t\t      | KVM_VCPUEVENT_VALID_SMM\n\t\t\t      | KVM_VCPUEVENT_VALID_PAYLOAD))\n\t\treturn -EINVAL;\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_PAYLOAD) {\n\t\tif (!vcpu->kvm->arch.exception_payload_enabled)\n\t\t\treturn -EINVAL;\n\t\tif (events->exception.pending)\n\t\t\tevents->exception.injected = 0;\n\t\telse\n\t\t\tevents->exception_has_payload = 0;\n\t} else {\n\t\tevents->exception.pending = 0;\n\t\tevents->exception_has_payload = 0;\n\t}\n\n\tif ((events->exception.injected || events->exception.pending) &&\n\t    (events->exception.nr > 31 || events->exception.nr == NMI_VECTOR))\n\t\treturn -EINVAL;\n\n\t/* INITs are latched while in SMM */\n\tif (events->flags & KVM_VCPUEVENT_VALID_SMM &&\n\t    (events->smi.smm || events->smi.pending) &&\n\t    vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED)\n\t\treturn -EINVAL;\n\n\tprocess_nmi(vcpu);\n\tvcpu->arch.exception.injected = events->exception.injected;\n\tvcpu->arch.exception.pending = events->exception.pending;\n\tvcpu->arch.exception.nr = events->exception.nr;\n\tvcpu->arch.exception.has_error_code = events->exception.has_error_code;\n\tvcpu->arch.exception.error_code = events->exception.error_code;\n\tvcpu->arch.exception.has_payload = events->exception_has_payload;\n\tvcpu->arch.exception.payload = events->exception_payload;\n\n\tvcpu->arch.interrupt.injected = events->interrupt.injected;\n\tvcpu->arch.interrupt.nr = events->interrupt.nr;\n\tvcpu->arch.interrupt.soft = events->interrupt.soft;\n\tif (events->flags & KVM_VCPUEVENT_VALID_SHADOW)\n\t\tkvm_x86_ops.set_interrupt_shadow(vcpu,\n\t\t\t\t\t\t  events->interrupt.shadow);\n\n\tvcpu->arch.nmi_injected = events->nmi.injected;\n\tif (events->flags & KVM_VCPUEVENT_VALID_NMI_PENDING)\n\t\tvcpu->arch.nmi_pending = events->nmi.pending;\n\tkvm_x86_ops.set_nmi_mask(vcpu, events->nmi.masked);\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_SIPI_VECTOR &&\n\t    lapic_in_kernel(vcpu))\n\t\tvcpu->arch.apic->sipi_vector = events->sipi_vector;\n\n\tif (events->flags & KVM_VCPUEVENT_VALID_SMM) {\n\t\tif (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {\n\t\t\tif (events->smi.smm)\n\t\t\t\tvcpu->arch.hflags |= HF_SMM_MASK;\n\t\t\telse\n\t\t\t\tvcpu->arch.hflags &= ~HF_SMM_MASK;\n\t\t\tkvm_smm_changed(vcpu);\n\t\t}\n\n\t\tvcpu->arch.smi_pending = events->smi.pending;\n\n\t\tif (events->smi.smm) {\n\t\t\tif (events->smi.smm_inside_nmi)\n\t\t\t\tvcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;\n\t\t\telse\n\t\t\t\tvcpu->arch.hflags &= ~HF_SMM_INSIDE_NMI_MASK;\n\t\t}\n\n\t\tif (lapic_in_kernel(vcpu)) {\n\t\t\tif (events->smi.latched_init)\n\t\t\t\tset_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);\n\t\t\telse\n\t\t\t\tclear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);\n\t\t}\n\t}\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_debugregs(struct kvm_vcpu *vcpu,\n\t\t\t\t\t     struct kvm_debugregs *dbgregs)\n{\n\tunsigned long val;\n\n\tmemcpy(dbgregs->db, vcpu->arch.db, sizeof(vcpu->arch.db));\n\tkvm_get_dr(vcpu, 6, &val);\n\tdbgregs->dr6 = val;\n\tdbgregs->dr7 = vcpu->arch.dr7;\n\tdbgregs->flags = 0;\n\tmemset(&dbgregs->reserved, 0, sizeof(dbgregs->reserved));\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_debugregs(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct kvm_debugregs *dbgregs)\n{\n\tif (dbgregs->flags)\n\t\treturn -EINVAL;\n\n\tif (dbgregs->dr6 & ~0xffffffffull)\n\t\treturn -EINVAL;\n\tif (dbgregs->dr7 & ~0xffffffffull)\n\t\treturn -EINVAL;\n\n\tmemcpy(vcpu->arch.db, dbgregs->db, sizeof(vcpu->arch.db));\n\tkvm_update_dr0123(vcpu);\n\tvcpu->arch.dr6 = dbgregs->dr6;\n\tvcpu->arch.dr7 = dbgregs->dr7;\n\tkvm_update_dr7(vcpu);\n\n\treturn 0;\n}\n\n#define XSTATE_COMPACTION_ENABLED (1ULL << 63)\n\nstatic void fill_xsave(u8 *dest, struct kvm_vcpu *vcpu)\n{\n\tstruct xregs_state *xsave = &vcpu->arch.guest_fpu->state.xsave;\n\tu64 xstate_bv = xsave->header.xfeatures;\n\tu64 valid;\n\n\t/*\n\t * Copy legacy XSAVE area, to avoid complications with CPUID\n\t * leaves 0 and 1 in the loop below.\n\t */\n\tmemcpy(dest, xsave, XSAVE_HDR_OFFSET);\n\n\t/* Set XSTATE_BV */\n\txstate_bv &= vcpu->arch.guest_supported_xcr0 | XFEATURE_MASK_FPSSE;\n\t*(u64 *)(dest + XSAVE_HDR_OFFSET) = xstate_bv;\n\n\t/*\n\t * Copy each region from the possibly compacted offset to the\n\t * non-compacted offset.\n\t */\n\tvalid = xstate_bv & ~XFEATURE_MASK_FPSSE;\n\twhile (valid) {\n\t\tu64 xfeature_mask = valid & -valid;\n\t\tint xfeature_nr = fls64(xfeature_mask) - 1;\n\t\tvoid *src = get_xsave_addr(xsave, xfeature_nr);\n\n\t\tif (src) {\n\t\t\tu32 size, offset, ecx, edx;\n\t\t\tcpuid_count(XSTATE_CPUID, xfeature_nr,\n\t\t\t\t    &size, &offset, &ecx, &edx);\n\t\t\tif (xfeature_nr == XFEATURE_PKRU)\n\t\t\t\tmemcpy(dest + offset, &vcpu->arch.pkru,\n\t\t\t\t       sizeof(vcpu->arch.pkru));\n\t\t\telse\n\t\t\t\tmemcpy(dest + offset, src, size);\n\n\t\t}\n\n\t\tvalid -= xfeature_mask;\n\t}\n}\n\nstatic void load_xsave(struct kvm_vcpu *vcpu, u8 *src)\n{\n\tstruct xregs_state *xsave = &vcpu->arch.guest_fpu->state.xsave;\n\tu64 xstate_bv = *(u64 *)(src + XSAVE_HDR_OFFSET);\n\tu64 valid;\n\n\t/*\n\t * Copy legacy XSAVE area, to avoid complications with CPUID\n\t * leaves 0 and 1 in the loop below.\n\t */\n\tmemcpy(xsave, src, XSAVE_HDR_OFFSET);\n\n\t/* Set XSTATE_BV and possibly XCOMP_BV.  */\n\txsave->header.xfeatures = xstate_bv;\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\txsave->header.xcomp_bv = host_xcr0 | XSTATE_COMPACTION_ENABLED;\n\n\t/*\n\t * Copy each region from the non-compacted offset to the\n\t * possibly compacted offset.\n\t */\n\tvalid = xstate_bv & ~XFEATURE_MASK_FPSSE;\n\twhile (valid) {\n\t\tu64 xfeature_mask = valid & -valid;\n\t\tint xfeature_nr = fls64(xfeature_mask) - 1;\n\t\tvoid *dest = get_xsave_addr(xsave, xfeature_nr);\n\n\t\tif (dest) {\n\t\t\tu32 size, offset, ecx, edx;\n\t\t\tcpuid_count(XSTATE_CPUID, xfeature_nr,\n\t\t\t\t    &size, &offset, &ecx, &edx);\n\t\t\tif (xfeature_nr == XFEATURE_PKRU)\n\t\t\t\tmemcpy(&vcpu->arch.pkru, src + offset,\n\t\t\t\t       sizeof(vcpu->arch.pkru));\n\t\t\telse\n\t\t\t\tmemcpy(dest, src + offset, size);\n\t\t}\n\n\t\tvalid -= xfeature_mask;\n\t}\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xsave(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct kvm_xsave *guest_xsave)\n{\n\tif (boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tmemset(guest_xsave, 0, sizeof(struct kvm_xsave));\n\t\tfill_xsave((u8 *) guest_xsave->region, vcpu);\n\t} else {\n\t\tmemcpy(guest_xsave->region,\n\t\t\t&vcpu->arch.guest_fpu->state.fxsave,\n\t\t\tsizeof(struct fxregs_state));\n\t\t*(u64 *)&guest_xsave->region[XSAVE_HDR_OFFSET / sizeof(u32)] =\n\t\t\tXFEATURE_MASK_FPSSE;\n\t}\n}\n\n#define XSAVE_MXCSR_OFFSET 24\n\nstatic int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_xsave *guest_xsave)\n{\n\tu64 xstate_bv =\n\t\t*(u64 *)&guest_xsave->region[XSAVE_HDR_OFFSET / sizeof(u32)];\n\tu32 mxcsr = *(u32 *)&guest_xsave->region[XSAVE_MXCSR_OFFSET / sizeof(u32)];\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\t/*\n\t\t * Here we allow setting states that are not present in\n\t\t * CPUID leaf 0xD, index 0, EDX:EAX.  This is for compatibility\n\t\t * with old userspace.\n\t\t */\n\t\tif (xstate_bv & ~supported_xcr0 || mxcsr & ~mxcsr_feature_mask)\n\t\t\treturn -EINVAL;\n\t\tload_xsave(vcpu, (u8 *)guest_xsave->region);\n\t} else {\n\t\tif (xstate_bv & ~XFEATURE_MASK_FPSSE ||\n\t\t\tmxcsr & ~mxcsr_feature_mask)\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&vcpu->arch.guest_fpu->state.fxsave,\n\t\t\tguest_xsave->region, sizeof(struct fxregs_state));\n\t}\n\treturn 0;\n}\n\nstatic void kvm_vcpu_ioctl_x86_get_xcrs(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_xcrs *guest_xcrs)\n{\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tguest_xcrs->nr_xcrs = 0;\n\t\treturn;\n\t}\n\n\tguest_xcrs->nr_xcrs = 1;\n\tguest_xcrs->flags = 0;\n\tguest_xcrs->xcrs[0].xcr = XCR_XFEATURE_ENABLED_MASK;\n\tguest_xcrs->xcrs[0].value = vcpu->arch.xcr0;\n}\n\nstatic int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct kvm_xcrs *guest_xcrs)\n{\n\tint i, r = 0;\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn -EINVAL;\n\n\tif (guest_xcrs->nr_xcrs > KVM_MAX_XCRS || guest_xcrs->flags)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < guest_xcrs->nr_xcrs; i++)\n\t\t/* Only support XCR0 currently */\n\t\tif (guest_xcrs->xcrs[i].xcr == XCR_XFEATURE_ENABLED_MASK) {\n\t\t\tr = __kvm_set_xcr(vcpu, XCR_XFEATURE_ENABLED_MASK,\n\t\t\t\tguest_xcrs->xcrs[i].value);\n\t\t\tbreak;\n\t\t}\n\tif (r)\n\t\tr = -EINVAL;\n\treturn r;\n}\n\n/*\n * kvm_set_guest_paused() indicates to the guest kernel that it has been\n * stopped by the hypervisor.  This function will be called from the host only.\n * EINVAL is returned when the host attempts to set the flag for a guest that\n * does not support pv clocks.\n */\nstatic int kvm_set_guest_paused(struct kvm_vcpu *vcpu)\n{\n\tif (!vcpu->arch.pv_time_enabled)\n\t\treturn -EINVAL;\n\tvcpu->arch.pvclock_set_guest_stopped_request = true;\n\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\treturn 0;\n}\n\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_enable_cap *cap)\n{\n\tint r;\n\tuint16_t vmcs_version;\n\tvoid __user *user_ptr;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_HYPERV_SYNIC2:\n\t\tif (cap->args[0])\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\n\tcase KVM_CAP_HYPERV_SYNIC:\n\t\tif (!irqchip_in_kernel(vcpu->kvm))\n\t\t\treturn -EINVAL;\n\t\treturn kvm_hv_activate_synic(vcpu, cap->cap ==\n\t\t\t\t\t     KVM_CAP_HYPERV_SYNIC2);\n\tcase KVM_CAP_HYPERV_ENLIGHTENED_VMCS:\n\t\tif (!kvm_x86_ops.nested_ops->enable_evmcs)\n\t\t\treturn -ENOTTY;\n\t\tr = kvm_x86_ops.nested_ops->enable_evmcs(vcpu, &vmcs_version);\n\t\tif (!r) {\n\t\t\tuser_ptr = (void __user *)(uintptr_t)cap->args[0];\n\t\t\tif (copy_to_user(user_ptr, &vmcs_version,\n\t\t\t\t\t sizeof(vmcs_version)))\n\t\t\t\tr = -EFAULT;\n\t\t}\n\t\treturn r;\n\tcase KVM_CAP_HYPERV_DIRECT_TLBFLUSH:\n\t\tif (!kvm_x86_ops.enable_direct_tlbflush)\n\t\t\treturn -ENOTTY;\n\n\t\treturn kvm_x86_ops.enable_direct_tlbflush(vcpu);\n\n\tcase KVM_CAP_ENFORCE_PV_FEATURE_CPUID:\n\t\tvcpu->arch.pv_cpuid.enforce = cap->args[0];\n\t\tif (vcpu->arch.pv_cpuid.enforce)\n\t\t\tkvm_update_pv_runtime(vcpu);\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n\t\t\t unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm_vcpu *vcpu = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r;\n\tunion {\n\t\tstruct kvm_lapic_state *lapic;\n\t\tstruct kvm_xsave *xsave;\n\t\tstruct kvm_xcrs *xcrs;\n\t\tvoid *buffer;\n\t} u;\n\n\tvcpu_load(vcpu);\n\n\tu.buffer = NULL;\n\tswitch (ioctl) {\n\tcase KVM_GET_LAPIC: {\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tu.lapic = kzalloc(sizeof(struct kvm_lapic_state),\n\t\t\t\tGFP_KERNEL_ACCOUNT);\n\n\t\tr = -ENOMEM;\n\t\tif (!u.lapic)\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_get_lapic(vcpu, u.lapic);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.lapic, sizeof(struct kvm_lapic_state)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_LAPIC: {\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tu.lapic = memdup_user(argp, sizeof(*u.lapic));\n\t\tif (IS_ERR(u.lapic)) {\n\t\t\tr = PTR_ERR(u.lapic);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_set_lapic(vcpu, u.lapic);\n\t\tbreak;\n\t}\n\tcase KVM_INTERRUPT: {\n\t\tstruct kvm_interrupt irq;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&irq, argp, sizeof(irq)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\n\t\tbreak;\n\t}\n\tcase KVM_NMI: {\n\t\tr = kvm_vcpu_ioctl_nmi(vcpu);\n\t\tbreak;\n\t}\n\tcase KVM_SMI: {\n\t\tr = kvm_vcpu_ioctl_smi(vcpu);\n\t\tbreak;\n\t}\n\tcase KVM_SET_CPUID: {\n\t\tstruct kvm_cpuid __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_set_cpuid(vcpu, &cpuid, cpuid_arg->entries);\n\t\tbreak;\n\t}\n\tcase KVM_SET_CPUID2: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_set_cpuid2(vcpu, &cpuid,\n\t\t\t\t\t      cpuid_arg->entries);\n\t\tbreak;\n\t}\n\tcase KVM_GET_CPUID2: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_get_cpuid2(vcpu, &cpuid,\n\t\t\t\t\t      cpuid_arg->entries);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_GET_MSRS: {\n\t\tint idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = msr_io(vcpu, argp, do_get_msr, 1);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_SET_MSRS: {\n\t\tint idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = msr_io(vcpu, argp, do_set_msr, 0);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_TPR_ACCESS_REPORTING: {\n\t\tstruct kvm_tpr_access_ctl tac;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&tac, argp, sizeof(tac)))\n\t\t\tgoto out;\n\t\tr = vcpu_ioctl_tpr_access_reporting(vcpu, &tac);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &tac, sizeof(tac)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t};\n\tcase KVM_SET_VAPIC_ADDR: {\n\t\tstruct kvm_vapic_addr va;\n\t\tint idx;\n\n\t\tr = -EINVAL;\n\t\tif (!lapic_in_kernel(vcpu))\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&va, argp, sizeof(va)))\n\t\t\tgoto out;\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_X86_SETUP_MCE: {\n\t\tu64 mcg_cap;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mcg_cap, argp, sizeof(mcg_cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_x86_setup_mce(vcpu, mcg_cap);\n\t\tbreak;\n\t}\n\tcase KVM_X86_SET_MCE: {\n\t\tstruct kvm_x86_mce mce;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&mce, argp, sizeof(mce)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_x86_set_mce(vcpu, &mce);\n\t\tbreak;\n\t}\n\tcase KVM_GET_VCPU_EVENTS: {\n\t\tstruct kvm_vcpu_events events;\n\n\t\tkvm_vcpu_ioctl_x86_get_vcpu_events(vcpu, &events);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &events, sizeof(struct kvm_vcpu_events)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_VCPU_EVENTS: {\n\t\tstruct kvm_vcpu_events events;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&events, argp, sizeof(struct kvm_vcpu_events)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_x86_set_vcpu_events(vcpu, &events);\n\t\tbreak;\n\t}\n\tcase KVM_GET_DEBUGREGS: {\n\t\tstruct kvm_debugregs dbgregs;\n\n\t\tkvm_vcpu_ioctl_x86_get_debugregs(vcpu, &dbgregs);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &dbgregs,\n\t\t\t\t sizeof(struct kvm_debugregs)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_DEBUGREGS: {\n\t\tstruct kvm_debugregs dbgregs;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&dbgregs, argp,\n\t\t\t\t   sizeof(struct kvm_debugregs)))\n\t\t\tbreak;\n\n\t\tr = kvm_vcpu_ioctl_x86_set_debugregs(vcpu, &dbgregs);\n\t\tbreak;\n\t}\n\tcase KVM_GET_XSAVE: {\n\t\tu.xsave = kzalloc(sizeof(struct kvm_xsave), GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xsave)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xsave(vcpu, u.xsave);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xsave, sizeof(struct kvm_xsave)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_XSAVE: {\n\t\tu.xsave = memdup_user(argp, sizeof(*u.xsave));\n\t\tif (IS_ERR(u.xsave)) {\n\t\t\tr = PTR_ERR(u.xsave);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_x86_set_xsave(vcpu, u.xsave);\n\t\tbreak;\n\t}\n\tcase KVM_GET_XCRS: {\n\t\tu.xcrs = kzalloc(sizeof(struct kvm_xcrs), GFP_KERNEL_ACCOUNT);\n\t\tr = -ENOMEM;\n\t\tif (!u.xcrs)\n\t\t\tbreak;\n\n\t\tkvm_vcpu_ioctl_x86_get_xcrs(vcpu, u.xcrs);\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, u.xcrs,\n\t\t\t\t sizeof(struct kvm_xcrs)))\n\t\t\tbreak;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_XCRS: {\n\t\tu.xcrs = memdup_user(argp, sizeof(*u.xcrs));\n\t\tif (IS_ERR(u.xcrs)) {\n\t\t\tr = PTR_ERR(u.xcrs);\n\t\t\tgoto out_nofree;\n\t\t}\n\n\t\tr = kvm_vcpu_ioctl_x86_set_xcrs(vcpu, u.xcrs);\n\t\tbreak;\n\t}\n\tcase KVM_SET_TSC_KHZ: {\n\t\tu32 user_tsc_khz;\n\n\t\tr = -EINVAL;\n\t\tuser_tsc_khz = (u32)arg;\n\n\t\tif (kvm_has_tsc_control &&\n\t\t    user_tsc_khz >= kvm_max_guest_tsc_khz)\n\t\t\tgoto out;\n\n\t\tif (user_tsc_khz == 0)\n\t\t\tuser_tsc_khz = tsc_khz;\n\n\t\tif (!kvm_set_tsc_khz(vcpu, user_tsc_khz))\n\t\t\tr = 0;\n\n\t\tgoto out;\n\t}\n\tcase KVM_GET_TSC_KHZ: {\n\t\tr = vcpu->arch.virtual_tsc_khz;\n\t\tgoto out;\n\t}\n\tcase KVM_KVMCLOCK_CTRL: {\n\t\tr = kvm_set_guest_paused(vcpu);\n\t\tgoto out;\n\t}\n\tcase KVM_ENABLE_CAP: {\n\t\tstruct kvm_enable_cap cap;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cap, argp, sizeof(cap)))\n\t\t\tgoto out;\n\t\tr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\n\t\tbreak;\n\t}\n\tcase KVM_GET_NESTED_STATE: {\n\t\tstruct kvm_nested_state __user *user_kvm_nested_state = argp;\n\t\tu32 user_data_size;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.nested_ops->get_state)\n\t\t\tbreak;\n\n\t\tBUILD_BUG_ON(sizeof(user_data_size) != sizeof(user_kvm_nested_state->size));\n\t\tr = -EFAULT;\n\t\tif (get_user(user_data_size, &user_kvm_nested_state->size))\n\t\t\tbreak;\n\n\t\tr = kvm_x86_ops.nested_ops->get_state(vcpu, user_kvm_nested_state,\n\t\t\t\t\t\t     user_data_size);\n\t\tif (r < 0)\n\t\t\tbreak;\n\n\t\tif (r > user_data_size) {\n\t\t\tif (put_user(r, &user_kvm_nested_state->size))\n\t\t\t\tr = -EFAULT;\n\t\t\telse\n\t\t\t\tr = -E2BIG;\n\t\t\tbreak;\n\t\t}\n\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_NESTED_STATE: {\n\t\tstruct kvm_nested_state __user *user_kvm_nested_state = argp;\n\t\tstruct kvm_nested_state kvm_state;\n\t\tint idx;\n\n\t\tr = -EINVAL;\n\t\tif (!kvm_x86_ops.nested_ops->set_state)\n\t\t\tbreak;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&kvm_state, user_kvm_nested_state, sizeof(kvm_state)))\n\t\t\tbreak;\n\n\t\tr = -EINVAL;\n\t\tif (kvm_state.size < sizeof(kvm_state))\n\t\t\tbreak;\n\n\t\tif (kvm_state.flags &\n\t\t    ~(KVM_STATE_NESTED_RUN_PENDING | KVM_STATE_NESTED_GUEST_MODE\n\t\t      | KVM_STATE_NESTED_EVMCS | KVM_STATE_NESTED_MTF_PENDING\n\t\t      | KVM_STATE_NESTED_GIF_SET))\n\t\t\tbreak;\n\n\t\t/* nested_run_pending implies guest_mode.  */\n\t\tif ((kvm_state.flags & KVM_STATE_NESTED_RUN_PENDING)\n\t\t    && !(kvm_state.flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\t\tbreak;\n\n\t\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = kvm_x86_ops.nested_ops->set_state(vcpu, user_kvm_nested_state, &kvm_state);\n\t\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\t\tbreak;\n\t}\n\tcase KVM_GET_SUPPORTED_HV_CPUID: {\n\t\tstruct kvm_cpuid2 __user *cpuid_arg = argp;\n\t\tstruct kvm_cpuid2 cpuid;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))\n\t\t\tgoto out;\n\n\t\tr = kvm_vcpu_ioctl_get_hv_cpuid(vcpu, &cpuid,\n\t\t\t\t\t\tcpuid_arg->entries);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(cpuid_arg, &cpuid, sizeof(cpuid)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tr = -EINVAL;\n\t}\nout:\n\tkfree(u.buffer);\nout_nofree:\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nvm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\n{\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic int kvm_vm_ioctl_set_tss_addr(struct kvm *kvm, unsigned long addr)\n{\n\tint ret;\n\n\tif (addr > (unsigned int)(-3 * PAGE_SIZE))\n\t\treturn -EINVAL;\n\tret = kvm_x86_ops.set_tss_addr(kvm, addr);\n\treturn ret;\n}\n\nstatic int kvm_vm_ioctl_set_identity_map_addr(struct kvm *kvm,\n\t\t\t\t\t      u64 ident_addr)\n{\n\treturn kvm_x86_ops.set_identity_map_addr(kvm, ident_addr);\n}\n\nstatic int kvm_vm_ioctl_set_nr_mmu_pages(struct kvm *kvm,\n\t\t\t\t\t unsigned long kvm_nr_mmu_pages)\n{\n\tif (kvm_nr_mmu_pages < KVM_MIN_ALLOC_MMU_PAGES)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tkvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);\n\tkvm->arch.n_requested_mmu_pages = kvm_nr_mmu_pages;\n\n\tmutex_unlock(&kvm->slots_lock);\n\treturn 0;\n}\n\nstatic unsigned long kvm_vm_ioctl_get_nr_mmu_pages(struct kvm *kvm)\n{\n\treturn kvm->arch.n_max_mmu_pages;\n}\n\nstatic int kvm_vm_ioctl_get_irqchip(struct kvm *kvm, struct kvm_irqchip *chip)\n{\n\tstruct kvm_pic *pic = kvm->arch.vpic;\n\tint r;\n\n\tr = 0;\n\tswitch (chip->chip_id) {\n\tcase KVM_IRQCHIP_PIC_MASTER:\n\t\tmemcpy(&chip->chip.pic, &pic->pics[0],\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tbreak;\n\tcase KVM_IRQCHIP_PIC_SLAVE:\n\t\tmemcpy(&chip->chip.pic, &pic->pics[1],\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tbreak;\n\tcase KVM_IRQCHIP_IOAPIC:\n\t\tkvm_get_ioapic(kvm, &chip->chip.ioapic);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_set_irqchip(struct kvm *kvm, struct kvm_irqchip *chip)\n{\n\tstruct kvm_pic *pic = kvm->arch.vpic;\n\tint r;\n\n\tr = 0;\n\tswitch (chip->chip_id) {\n\tcase KVM_IRQCHIP_PIC_MASTER:\n\t\tspin_lock(&pic->lock);\n\t\tmemcpy(&pic->pics[0], &chip->chip.pic,\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tspin_unlock(&pic->lock);\n\t\tbreak;\n\tcase KVM_IRQCHIP_PIC_SLAVE:\n\t\tspin_lock(&pic->lock);\n\t\tmemcpy(&pic->pics[1], &chip->chip.pic,\n\t\t\tsizeof(struct kvm_pic_state));\n\t\tspin_unlock(&pic->lock);\n\t\tbreak;\n\tcase KVM_IRQCHIP_IOAPIC:\n\t\tkvm_set_ioapic(kvm, &chip->chip.ioapic);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\tkvm_pic_update_irq(pic);\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_get_pit(struct kvm *kvm, struct kvm_pit_state *ps)\n{\n\tstruct kvm_kpit_state *kps = &kvm->arch.vpit->pit_state;\n\n\tBUILD_BUG_ON(sizeof(*ps) != sizeof(kps->channels));\n\n\tmutex_lock(&kps->lock);\n\tmemcpy(ps, &kps->channels, sizeof(*ps));\n\tmutex_unlock(&kps->lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_pit(struct kvm *kvm, struct kvm_pit_state *ps)\n{\n\tint i;\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\tmutex_lock(&pit->pit_state.lock);\n\tmemcpy(&pit->pit_state.channels, ps, sizeof(*ps));\n\tfor (i = 0; i < 3; i++)\n\t\tkvm_pit_load_count(pit, i, ps->channels[i].count, 0);\n\tmutex_unlock(&pit->pit_state.lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_get_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)\n{\n\tmutex_lock(&kvm->arch.vpit->pit_state.lock);\n\tmemcpy(ps->channels, &kvm->arch.vpit->pit_state.channels,\n\t\tsizeof(ps->channels));\n\tps->flags = kvm->arch.vpit->pit_state.flags;\n\tmutex_unlock(&kvm->arch.vpit->pit_state.lock);\n\tmemset(&ps->reserved, 0, sizeof(ps->reserved));\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)\n{\n\tint start = 0;\n\tint i;\n\tu32 prev_legacy, cur_legacy;\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\tmutex_lock(&pit->pit_state.lock);\n\tprev_legacy = pit->pit_state.flags & KVM_PIT_FLAGS_HPET_LEGACY;\n\tcur_legacy = ps->flags & KVM_PIT_FLAGS_HPET_LEGACY;\n\tif (!prev_legacy && cur_legacy)\n\t\tstart = 1;\n\tmemcpy(&pit->pit_state.channels, &ps->channels,\n\t       sizeof(pit->pit_state.channels));\n\tpit->pit_state.flags = ps->flags;\n\tfor (i = 0; i < 3; i++)\n\t\tkvm_pit_load_count(pit, i, pit->pit_state.channels[i].count,\n\t\t\t\t   start && i == 0);\n\tmutex_unlock(&pit->pit_state.lock);\n\treturn 0;\n}\n\nstatic int kvm_vm_ioctl_reinject(struct kvm *kvm,\n\t\t\t\t struct kvm_reinject_control *control)\n{\n\tstruct kvm_pit *pit = kvm->arch.vpit;\n\n\t/* pit->pit_state.lock was overloaded to prevent userspace from getting\n\t * an inconsistent state after running multiple KVM_REINJECT_CONTROL\n\t * ioctls in parallel.  Use a separate lock if that ioctl isn't rare.\n\t */\n\tmutex_lock(&pit->pit_state.lock);\n\tkvm_pit_set_reinject(pit, control->pit_reinject);\n\tmutex_unlock(&pit->pit_state.lock);\n\n\treturn 0;\n}\n\nvoid kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot)\n{\n\t/*\n\t * Flush potentially hardware-cached dirty pages to dirty_bitmap.\n\t */\n\tif (kvm_x86_ops.flush_log_dirty)\n\t\tkvm_x86_ops.flush_log_dirty(kvm);\n}\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,\n\t\t\tbool line_status)\n{\n\tif (!irqchip_in_kernel(kvm))\n\t\treturn -ENXIO;\n\n\tirq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,\n\t\t\t\t\tirq_event->irq, irq_event->level,\n\t\t\t\t\tline_status);\n\treturn 0;\n}\n\nint kvm_vm_ioctl_enable_cap(struct kvm *kvm,\n\t\t\t    struct kvm_enable_cap *cap)\n{\n\tint r;\n\n\tif (cap->flags)\n\t\treturn -EINVAL;\n\n\tswitch (cap->cap) {\n\tcase KVM_CAP_DISABLE_QUIRKS:\n\t\tkvm->arch.disabled_quirks = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_SPLIT_IRQCHIP: {\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] > MAX_NR_RESERVED_IOAPIC_PINS)\n\t\t\tgoto split_irqchip_unlock;\n\t\tr = -EEXIST;\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\tgoto split_irqchip_unlock;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto split_irqchip_unlock;\n\t\tr = kvm_setup_empty_irq_routing(kvm);\n\t\tif (r)\n\t\t\tgoto split_irqchip_unlock;\n\t\t/* Pairs with irqchip_in_kernel. */\n\t\tsmp_wmb();\n\t\tkvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;\n\t\tkvm->arch.nr_reserved_ioapic_pins = cap->args[0];\n\t\tr = 0;\nsplit_irqchip_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_CAP_X2APIC_API:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X2APIC_API_VALID_FLAGS)\n\t\t\tbreak;\n\n\t\tif (cap->args[0] & KVM_X2APIC_API_USE_32BIT_IDS)\n\t\t\tkvm->arch.x2apic_format = true;\n\t\tif (cap->args[0] & KVM_X2APIC_API_DISABLE_BROADCAST_QUIRK)\n\t\t\tkvm->arch.x2apic_broadcast_quirk_disabled = true;\n\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_DISABLE_EXITS:\n\t\tr = -EINVAL;\n\t\tif (cap->args[0] & ~KVM_X86_DISABLE_VALID_EXITS)\n\t\t\tbreak;\n\n\t\tif ((cap->args[0] & KVM_X86_DISABLE_EXITS_MWAIT) &&\n\t\t\tkvm_can_mwait_in_guest())\n\t\t\tkvm->arch.mwait_in_guest = true;\n\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_HLT)\n\t\t\tkvm->arch.hlt_in_guest = true;\n\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_PAUSE)\n\t\t\tkvm->arch.pause_in_guest = true;\n\t\tif (cap->args[0] & KVM_X86_DISABLE_EXITS_CSTATE)\n\t\t\tkvm->arch.cstate_in_guest = true;\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_MSR_PLATFORM_INFO:\n\t\tkvm->arch.guest_can_read_msr_platform_info = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_EXCEPTION_PAYLOAD:\n\t\tkvm->arch.exception_payload_enabled = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tcase KVM_CAP_X86_USER_SPACE_MSR:\n\t\tkvm->arch.user_space_msr_mask = cap->args[0];\n\t\tr = 0;\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic void kvm_clear_msr_filter(struct kvm *kvm)\n{\n\tu32 i;\n\tu32 count = kvm->arch.msr_filter.count;\n\tstruct msr_bitmap_range ranges[16];\n\n\tmutex_lock(&kvm->lock);\n\tkvm->arch.msr_filter.count = 0;\n\tmemcpy(ranges, kvm->arch.msr_filter.ranges, count * sizeof(ranges[0]));\n\tmutex_unlock(&kvm->lock);\n\tsynchronize_srcu(&kvm->srcu);\n\n\tfor (i = 0; i < count; i++)\n\t\tkfree(ranges[i].bitmap);\n}\n\nstatic int kvm_add_msr_filter(struct kvm *kvm, struct kvm_msr_filter_range *user_range)\n{\n\tstruct msr_bitmap_range *ranges = kvm->arch.msr_filter.ranges;\n\tstruct msr_bitmap_range range;\n\tunsigned long *bitmap = NULL;\n\tsize_t bitmap_size;\n\tint r;\n\n\tif (!user_range->nmsrs)\n\t\treturn 0;\n\n\tbitmap_size = BITS_TO_LONGS(user_range->nmsrs) * sizeof(long);\n\tif (!bitmap_size || bitmap_size > KVM_MSR_FILTER_MAX_BITMAP_SIZE)\n\t\treturn -EINVAL;\n\n\tbitmap = memdup_user((__user u8*)user_range->bitmap, bitmap_size);\n\tif (IS_ERR(bitmap))\n\t\treturn PTR_ERR(bitmap);\n\n\trange = (struct msr_bitmap_range) {\n\t\t.flags = user_range->flags,\n\t\t.base = user_range->base,\n\t\t.nmsrs = user_range->nmsrs,\n\t\t.bitmap = bitmap,\n\t};\n\n\tif (range.flags & ~(KVM_MSR_FILTER_READ | KVM_MSR_FILTER_WRITE)) {\n\t\tr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (!range.flags) {\n\t\tr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* Everything ok, add this range identifier to our global pool */\n\tranges[kvm->arch.msr_filter.count] = range;\n\t/* Make sure we filled the array before we tell anyone to walk it */\n\tsmp_wmb();\n\tkvm->arch.msr_filter.count++;\n\n\treturn 0;\nerr:\n\tkfree(bitmap);\n\treturn r;\n}\n\nstatic int kvm_vm_ioctl_set_msr_filter(struct kvm *kvm, void __user *argp)\n{\n\tstruct kvm_msr_filter __user *user_msr_filter = argp;\n\tstruct kvm_msr_filter filter;\n\tbool default_allow;\n\tint r = 0;\n\tbool empty = true;\n\tu32 i;\n\n\tif (copy_from_user(&filter, user_msr_filter, sizeof(filter)))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < ARRAY_SIZE(filter.ranges); i++)\n\t\tempty &= !filter.ranges[i].nmsrs;\n\n\tdefault_allow = !(filter.flags & KVM_MSR_FILTER_DEFAULT_DENY);\n\tif (empty && !default_allow)\n\t\treturn -EINVAL;\n\n\tkvm_clear_msr_filter(kvm);\n\n\tkvm->arch.msr_filter.default_allow = default_allow;\n\n\t/*\n\t * Protect from concurrent calls to this function that could trigger\n\t * a TOCTOU violation on kvm->arch.msr_filter.count.\n\t */\n\tmutex_lock(&kvm->lock);\n\tfor (i = 0; i < ARRAY_SIZE(filter.ranges); i++) {\n\t\tr = kvm_add_msr_filter(kvm, &filter.ranges[i]);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MSR_FILTER_CHANGED);\n\tmutex_unlock(&kvm->lock);\n\n\treturn r;\n}\n\nlong kvm_arch_vm_ioctl(struct file *filp,\n\t\t       unsigned int ioctl, unsigned long arg)\n{\n\tstruct kvm *kvm = filp->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint r = -ENOTTY;\n\t/*\n\t * This union makes it completely explicit to gcc-3.x\n\t * that these two variables' stack usage should be\n\t * combined, not added together.\n\t */\n\tunion {\n\t\tstruct kvm_pit_state ps;\n\t\tstruct kvm_pit_state2 ps2;\n\t\tstruct kvm_pit_config pit_config;\n\t} u;\n\n\tswitch (ioctl) {\n\tcase KVM_SET_TSS_ADDR:\n\t\tr = kvm_vm_ioctl_set_tss_addr(kvm, arg);\n\t\tbreak;\n\tcase KVM_SET_IDENTITY_MAP_ADDR: {\n\t\tu64 ident_addr;\n\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EINVAL;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto set_identity_unlock;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&ident_addr, argp, sizeof(ident_addr)))\n\t\t\tgoto set_identity_unlock;\n\t\tr = kvm_vm_ioctl_set_identity_map_addr(kvm, ident_addr);\nset_identity_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_SET_NR_MMU_PAGES:\n\t\tr = kvm_vm_ioctl_set_nr_mmu_pages(kvm, arg);\n\t\tbreak;\n\tcase KVM_GET_NR_MMU_PAGES:\n\t\tr = kvm_vm_ioctl_get_nr_mmu_pages(kvm);\n\t\tbreak;\n\tcase KVM_CREATE_IRQCHIP: {\n\t\tmutex_lock(&kvm->lock);\n\n\t\tr = -EEXIST;\n\t\tif (irqchip_in_kernel(kvm))\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = -EINVAL;\n\t\tif (kvm->created_vcpus)\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = kvm_pic_init(kvm);\n\t\tif (r)\n\t\t\tgoto create_irqchip_unlock;\n\n\t\tr = kvm_ioapic_init(kvm);\n\t\tif (r) {\n\t\t\tkvm_pic_destroy(kvm);\n\t\t\tgoto create_irqchip_unlock;\n\t\t}\n\n\t\tr = kvm_setup_default_irq_routing(kvm);\n\t\tif (r) {\n\t\t\tkvm_ioapic_destroy(kvm);\n\t\t\tkvm_pic_destroy(kvm);\n\t\t\tgoto create_irqchip_unlock;\n\t\t}\n\t\t/* Write kvm->irq_routing before enabling irqchip_in_kernel. */\n\t\tsmp_wmb();\n\t\tkvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;\n\tcreate_irqchip_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_CREATE_PIT:\n\t\tu.pit_config.flags = KVM_PIT_SPEAKER_DUMMY;\n\t\tgoto create_pit;\n\tcase KVM_CREATE_PIT2:\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.pit_config, argp,\n\t\t\t\t   sizeof(struct kvm_pit_config)))\n\t\t\tgoto out;\n\tcreate_pit:\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -EEXIST;\n\t\tif (kvm->arch.vpit)\n\t\t\tgoto create_pit_unlock;\n\t\tr = -ENOMEM;\n\t\tkvm->arch.vpit = kvm_create_pit(kvm, u.pit_config.flags);\n\t\tif (kvm->arch.vpit)\n\t\t\tr = 0;\n\tcreate_pit_unlock:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_GET_IRQCHIP: {\n\t\t/* 0: PIC master, 1: PIC slave, 2: IOAPIC */\n\t\tstruct kvm_irqchip *chip;\n\n\t\tchip = memdup_user(argp, sizeof(*chip));\n\t\tif (IS_ERR(chip)) {\n\t\t\tr = PTR_ERR(chip);\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = -ENXIO;\n\t\tif (!irqchip_kernel(kvm))\n\t\t\tgoto get_irqchip_out;\n\t\tr = kvm_vm_ioctl_get_irqchip(kvm, chip);\n\t\tif (r)\n\t\t\tgoto get_irqchip_out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, chip, sizeof(*chip)))\n\t\t\tgoto get_irqchip_out;\n\t\tr = 0;\n\tget_irqchip_out:\n\t\tkfree(chip);\n\t\tbreak;\n\t}\n\tcase KVM_SET_IRQCHIP: {\n\t\t/* 0: PIC master, 1: PIC slave, 2: IOAPIC */\n\t\tstruct kvm_irqchip *chip;\n\n\t\tchip = memdup_user(argp, sizeof(*chip));\n\t\tif (IS_ERR(chip)) {\n\t\t\tr = PTR_ERR(chip);\n\t\t\tgoto out;\n\t\t}\n\n\t\tr = -ENXIO;\n\t\tif (!irqchip_kernel(kvm))\n\t\t\tgoto set_irqchip_out;\n\t\tr = kvm_vm_ioctl_set_irqchip(kvm, chip);\n\tset_irqchip_out:\n\t\tkfree(chip);\n\t\tbreak;\n\t}\n\tcase KVM_GET_PIT: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps, argp, sizeof(struct kvm_pit_state)))\n\t\t\tgoto out;\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_pit(kvm, &u.ps);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &u.ps, sizeof(struct kvm_pit_state)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_PIT: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps, argp, sizeof(u.ps)))\n\t\t\tgoto out;\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto set_pit_out;\n\t\tr = kvm_vm_ioctl_set_pit(kvm, &u.ps);\nset_pit_out:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_GET_PIT2: {\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_get_pit2(kvm, &u.ps2);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &u.ps2, sizeof(u.ps2)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_PIT2: {\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&u.ps2, argp, sizeof(u.ps2)))\n\t\t\tgoto out;\n\t\tmutex_lock(&kvm->lock);\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto set_pit2_out;\n\t\tr = kvm_vm_ioctl_set_pit2(kvm, &u.ps2);\nset_pit2_out:\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\t}\n\tcase KVM_REINJECT_CONTROL: {\n\t\tstruct kvm_reinject_control control;\n\t\tr =  -EFAULT;\n\t\tif (copy_from_user(&control, argp, sizeof(control)))\n\t\t\tgoto out;\n\t\tr = -ENXIO;\n\t\tif (!kvm->arch.vpit)\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_reinject(kvm, &control);\n\t\tbreak;\n\t}\n\tcase KVM_SET_BOOT_CPU_ID:\n\t\tr = 0;\n\t\tmutex_lock(&kvm->lock);\n\t\tif (kvm->created_vcpus)\n\t\t\tr = -EBUSY;\n\t\telse\n\t\t\tkvm->arch.bsp_vcpu_id = arg;\n\t\tmutex_unlock(&kvm->lock);\n\t\tbreak;\n\tcase KVM_XEN_HVM_CONFIG: {\n\t\tstruct kvm_xen_hvm_config xhc;\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&xhc, argp, sizeof(xhc)))\n\t\t\tgoto out;\n\t\tr = -EINVAL;\n\t\tif (xhc.flags)\n\t\t\tgoto out;\n\t\tmemcpy(&kvm->arch.xen_hvm_config, &xhc, sizeof(xhc));\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_SET_CLOCK: {\n\t\tstruct kvm_clock_data user_ns;\n\t\tu64 now_ns;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&user_ns, argp, sizeof(user_ns)))\n\t\t\tgoto out;\n\n\t\tr = -EINVAL;\n\t\tif (user_ns.flags)\n\t\t\tgoto out;\n\n\t\tr = 0;\n\t\t/*\n\t\t * TODO: userspace has to take care of races with VCPU_RUN, so\n\t\t * kvm_gen_update_masterclock() can be cut down to locked\n\t\t * pvclock_update_vm_gtod_copy().\n\t\t */\n\t\tkvm_gen_update_masterclock(kvm);\n\t\tnow_ns = get_kvmclock_ns(kvm);\n\t\tkvm->arch.kvmclock_offset += user_ns.clock - now_ns;\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);\n\t\tbreak;\n\t}\n\tcase KVM_GET_CLOCK: {\n\t\tstruct kvm_clock_data user_ns;\n\t\tu64 now_ns;\n\n\t\tnow_ns = get_kvmclock_ns(kvm);\n\t\tuser_ns.clock = now_ns;\n\t\tuser_ns.flags = kvm->arch.use_master_clock ? KVM_CLOCK_TSC_STABLE : 0;\n\t\tmemset(&user_ns.pad, 0, sizeof(user_ns.pad));\n\n\t\tr = -EFAULT;\n\t\tif (copy_to_user(argp, &user_ns, sizeof(user_ns)))\n\t\t\tgoto out;\n\t\tr = 0;\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_OP: {\n\t\tr = -ENOTTY;\n\t\tif (kvm_x86_ops.mem_enc_op)\n\t\t\tr = kvm_x86_ops.mem_enc_op(kvm, argp);\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_REG_REGION: {\n\t\tstruct kvm_enc_region region;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&region, argp, sizeof(region)))\n\t\t\tgoto out;\n\n\t\tr = -ENOTTY;\n\t\tif (kvm_x86_ops.mem_enc_reg_region)\n\t\t\tr = kvm_x86_ops.mem_enc_reg_region(kvm, &region);\n\t\tbreak;\n\t}\n\tcase KVM_MEMORY_ENCRYPT_UNREG_REGION: {\n\t\tstruct kvm_enc_region region;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&region, argp, sizeof(region)))\n\t\t\tgoto out;\n\n\t\tr = -ENOTTY;\n\t\tif (kvm_x86_ops.mem_enc_unreg_region)\n\t\t\tr = kvm_x86_ops.mem_enc_unreg_region(kvm, &region);\n\t\tbreak;\n\t}\n\tcase KVM_HYPERV_EVENTFD: {\n\t\tstruct kvm_hyperv_eventfd hvevfd;\n\n\t\tr = -EFAULT;\n\t\tif (copy_from_user(&hvevfd, argp, sizeof(hvevfd)))\n\t\t\tgoto out;\n\t\tr = kvm_vm_ioctl_hv_eventfd(kvm, &hvevfd);\n\t\tbreak;\n\t}\n\tcase KVM_SET_PMU_EVENT_FILTER:\n\t\tr = kvm_vm_ioctl_set_pmu_event_filter(kvm, argp);\n\t\tbreak;\n\tcase KVM_X86_SET_MSR_FILTER:\n\t\tr = kvm_vm_ioctl_set_msr_filter(kvm, argp);\n\t\tbreak;\n\tdefault:\n\t\tr = -ENOTTY;\n\t}\nout:\n\treturn r;\n}\n\nstatic void kvm_init_msr_list(void)\n{\n\tstruct x86_pmu_capability x86_pmu;\n\tu32 dummy[2];\n\tunsigned i;\n\n\tBUILD_BUG_ON_MSG(INTEL_PMC_MAX_FIXED != 4,\n\t\t\t \"Please update the fixed PMCs in msrs_to_saved_all[]\");\n\n\tperf_get_x86_pmu_capability(&x86_pmu);\n\n\tnum_msrs_to_save = 0;\n\tnum_emulated_msrs = 0;\n\tnum_msr_based_features = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(msrs_to_save_all); i++) {\n\t\tif (rdmsr_safe(msrs_to_save_all[i], &dummy[0], &dummy[1]) < 0)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Even MSRs that are valid in the host may not be exposed\n\t\t * to the guests in some cases.\n\t\t */\n\t\tswitch (msrs_to_save_all[i]) {\n\t\tcase MSR_IA32_BNDCFGS:\n\t\t\tif (!kvm_mpx_supported())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_TSC_AUX:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_RDTSCP))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_UMWAIT_CONTROL:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_WAITPKG))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_CTL:\n\t\tcase MSR_IA32_RTIT_STATUS:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_CR3_MATCH:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t    !intel_pt_validate_hw_cap(PT_CAP_cr3_filtering))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_OUTPUT_BASE:\n\t\tcase MSR_IA32_RTIT_OUTPUT_MASK:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t\t(!intel_pt_validate_hw_cap(PT_CAP_topa_output) &&\n\t\t\t\t !intel_pt_validate_hw_cap(PT_CAP_single_range_output)))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT) ||\n\t\t\t\tmsrs_to_save_all[i] - MSR_IA32_RTIT_ADDR0_A >=\n\t\t\t\tintel_pt_validate_hw_cap(PT_CAP_num_address_ranges) * 2)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_ARCH_PERFMON_PERFCTR0 ... MSR_ARCH_PERFMON_PERFCTR0 + 17:\n\t\t\tif (msrs_to_save_all[i] - MSR_ARCH_PERFMON_PERFCTR0 >=\n\t\t\t    min(INTEL_PMC_MAX_GENERIC, x86_pmu.num_counters_gp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase MSR_ARCH_PERFMON_EVENTSEL0 ... MSR_ARCH_PERFMON_EVENTSEL0 + 17:\n\t\t\tif (msrs_to_save_all[i] - MSR_ARCH_PERFMON_EVENTSEL0 >=\n\t\t\t    min(INTEL_PMC_MAX_GENERIC, x86_pmu.num_counters_gp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tmsrs_to_save[num_msrs_to_save++] = msrs_to_save_all[i];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(emulated_msrs_all); i++) {\n\t\tif (!kvm_x86_ops.has_emulated_msr(emulated_msrs_all[i]))\n\t\t\tcontinue;\n\n\t\temulated_msrs[num_emulated_msrs++] = emulated_msrs_all[i];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(msr_based_features_all); i++) {\n\t\tstruct kvm_msr_entry msr;\n\n\t\tmsr.index = msr_based_features_all[i];\n\t\tif (kvm_get_msr_feature(&msr))\n\t\t\tcontinue;\n\n\t\tmsr_based_features[num_msr_based_features++] = msr_based_features_all[i];\n\t}\n}\n\nstatic int vcpu_mmio_write(struct kvm_vcpu *vcpu, gpa_t addr, int len,\n\t\t\t   const void *v)\n{\n\tint handled = 0;\n\tint n;\n\n\tdo {\n\t\tn = min(len, 8);\n\t\tif (!(lapic_in_kernel(vcpu) &&\n\t\t      !kvm_iodevice_write(vcpu, &vcpu->arch.apic->dev, addr, n, v))\n\t\t    && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))\n\t\t\tbreak;\n\t\thandled += n;\n\t\taddr += n;\n\t\tlen -= n;\n\t\tv += n;\n\t} while (len);\n\n\treturn handled;\n}\n\nstatic int vcpu_mmio_read(struct kvm_vcpu *vcpu, gpa_t addr, int len, void *v)\n{\n\tint handled = 0;\n\tint n;\n\n\tdo {\n\t\tn = min(len, 8);\n\t\tif (!(lapic_in_kernel(vcpu) &&\n\t\t      !kvm_iodevice_read(vcpu, &vcpu->arch.apic->dev,\n\t\t\t\t\t addr, n, v))\n\t\t    && kvm_io_bus_read(vcpu, KVM_MMIO_BUS, addr, n, v))\n\t\t\tbreak;\n\t\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ, n, addr, v);\n\t\thandled += n;\n\t\taddr += n;\n\t\tlen -= n;\n\t\tv += n;\n\t} while (len);\n\n\treturn handled;\n}\n\nstatic void kvm_set_segment(struct kvm_vcpu *vcpu,\n\t\t\tstruct kvm_segment *var, int seg)\n{\n\tkvm_x86_ops.set_segment(vcpu, var, seg);\n}\n\nvoid kvm_get_segment(struct kvm_vcpu *vcpu,\n\t\t     struct kvm_segment *var, int seg)\n{\n\tkvm_x86_ops.get_segment(vcpu, var, seg);\n}\n\ngpa_t translate_nested_gpa(struct kvm_vcpu *vcpu, gpa_t gpa, u32 access,\n\t\t\t   struct x86_exception *exception)\n{\n\tgpa_t t_gpa;\n\n\tBUG_ON(!mmu_is_nested(vcpu));\n\n\t/* NPT walks are always user-walks */\n\taccess |= PFERR_USER_MASK;\n\tt_gpa  = vcpu->arch.mmu->gva_to_gpa(vcpu, gpa, access, exception);\n\n\treturn t_gpa;\n}\n\ngpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t      struct x86_exception *exception)\n{\n\tu32 access = (kvm_x86_ops.get_cpl(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\treturn vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, access, exception);\n}\n\n gpa_t kvm_mmu_gva_to_gpa_fetch(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t\tstruct x86_exception *exception)\n{\n\tu32 access = (kvm_x86_ops.get_cpl(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\taccess |= PFERR_FETCH_MASK;\n\treturn vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, access, exception);\n}\n\ngpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t       struct x86_exception *exception)\n{\n\tu32 access = (kvm_x86_ops.get_cpl(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\taccess |= PFERR_WRITE_MASK;\n\treturn vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, access, exception);\n}\n\n/* uses this to access any guest's mapped memory without checking CPL */\ngpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t\tstruct x86_exception *exception)\n{\n\treturn vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, 0, exception);\n}\n\nstatic int kvm_read_guest_virt_helper(gva_t addr, void *val, unsigned int bytes,\n\t\t\t\t      struct kvm_vcpu *vcpu, u32 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tvoid *data = val;\n\tint r = X86EMUL_CONTINUE;\n\n\twhile (bytes) {\n\t\tgpa_t gpa = vcpu->arch.walk_mmu->gva_to_gpa(vcpu, addr, access,\n\t\t\t\t\t\t\t    exception);\n\t\tunsigned offset = addr & (PAGE_SIZE-1);\n\t\tunsigned toread = min(bytes, (unsigned)PAGE_SIZE - offset);\n\t\tint ret;\n\n\t\tif (gpa == UNMAPPED_GVA)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\tret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, data,\n\t\t\t\t\t       offset, toread);\n\t\tif (ret < 0) {\n\t\t\tr = X86EMUL_IO_NEEDED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= toread;\n\t\tdata += toread;\n\t\taddr += toread;\n\t}\nout:\n\treturn r;\n}\n\n/* used for instruction fetching */\nstatic int kvm_fetch_guest_virt(struct x86_emulate_ctxt *ctxt,\n\t\t\t\tgva_t addr, void *val, unsigned int bytes,\n\t\t\t\tstruct x86_exception *exception)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu32 access = (kvm_x86_ops.get_cpl(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\tunsigned offset;\n\tint ret;\n\n\t/* Inline kvm_read_guest_virt_helper for speed.  */\n\tgpa_t gpa = vcpu->arch.walk_mmu->gva_to_gpa(vcpu, addr, access|PFERR_FETCH_MASK,\n\t\t\t\t\t\t    exception);\n\tif (unlikely(gpa == UNMAPPED_GVA))\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\n\toffset = addr & (PAGE_SIZE-1);\n\tif (WARN_ON(offset + bytes > PAGE_SIZE))\n\t\tbytes = (unsigned)PAGE_SIZE - offset;\n\tret = kvm_vcpu_read_guest_page(vcpu, gpa >> PAGE_SHIFT, val,\n\t\t\t\t       offset, bytes);\n\tif (unlikely(ret < 0))\n\t\treturn X86EMUL_IO_NEEDED;\n\n\treturn X86EMUL_CONTINUE;\n}\n\nint kvm_read_guest_virt(struct kvm_vcpu *vcpu,\n\t\t\t       gva_t addr, void *val, unsigned int bytes,\n\t\t\t       struct x86_exception *exception)\n{\n\tu32 access = (kvm_x86_ops.get_cpl(vcpu) == 3) ? PFERR_USER_MASK : 0;\n\n\t/*\n\t * FIXME: this should call handle_emulation_failure if X86EMUL_IO_NEEDED\n\t * is returned, but our callers are not ready for that and they blindly\n\t * call kvm_inject_page_fault.  Ensure that they at least do not leak\n\t * uninitialized kernel stack memory into cr2 and error code.\n\t */\n\tmemset(exception, 0, sizeof(*exception));\n\treturn kvm_read_guest_virt_helper(addr, val, bytes, vcpu, access,\n\t\t\t\t\t  exception);\n}\nEXPORT_SYMBOL_GPL(kvm_read_guest_virt);\n\nstatic int emulator_read_std(struct x86_emulate_ctxt *ctxt,\n\t\t\t     gva_t addr, void *val, unsigned int bytes,\n\t\t\t     struct x86_exception *exception, bool system)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu32 access = 0;\n\n\tif (!system && kvm_x86_ops.get_cpl(vcpu) == 3)\n\t\taccess |= PFERR_USER_MASK;\n\n\treturn kvm_read_guest_virt_helper(addr, val, bytes, vcpu, access, exception);\n}\n\nstatic int kvm_read_guest_phys_system(struct x86_emulate_ctxt *ctxt,\n\t\tunsigned long addr, void *val, unsigned int bytes)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r = kvm_vcpu_read_guest(vcpu, addr, val, bytes);\n\n\treturn r < 0 ? X86EMUL_IO_NEEDED : X86EMUL_CONTINUE;\n}\n\nstatic int kvm_write_guest_virt_helper(gva_t addr, void *val, unsigned int bytes,\n\t\t\t\t      struct kvm_vcpu *vcpu, u32 access,\n\t\t\t\t      struct x86_exception *exception)\n{\n\tvoid *data = val;\n\tint r = X86EMUL_CONTINUE;\n\n\twhile (bytes) {\n\t\tgpa_t gpa =  vcpu->arch.walk_mmu->gva_to_gpa(vcpu, addr,\n\t\t\t\t\t\t\t     access,\n\t\t\t\t\t\t\t     exception);\n\t\tunsigned offset = addr & (PAGE_SIZE-1);\n\t\tunsigned towrite = min(bytes, (unsigned)PAGE_SIZE - offset);\n\t\tint ret;\n\n\t\tif (gpa == UNMAPPED_GVA)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\tret = kvm_vcpu_write_guest(vcpu, gpa, data, towrite);\n\t\tif (ret < 0) {\n\t\t\tr = X86EMUL_IO_NEEDED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes -= towrite;\n\t\tdata += towrite;\n\t\taddr += towrite;\n\t}\nout:\n\treturn r;\n}\n\nstatic int emulator_write_std(struct x86_emulate_ctxt *ctxt, gva_t addr, void *val,\n\t\t\t      unsigned int bytes, struct x86_exception *exception,\n\t\t\t      bool system)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu32 access = PFERR_WRITE_MASK;\n\n\tif (!system && kvm_x86_ops.get_cpl(vcpu) == 3)\n\t\taccess |= PFERR_USER_MASK;\n\n\treturn kvm_write_guest_virt_helper(addr, val, bytes, vcpu,\n\t\t\t\t\t   access, exception);\n}\n\nint kvm_write_guest_virt_system(struct kvm_vcpu *vcpu, gva_t addr, void *val,\n\t\t\t\tunsigned int bytes, struct x86_exception *exception)\n{\n\t/* kvm_write_guest_virt_system can pull in tons of pages. */\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\treturn kvm_write_guest_virt_helper(addr, val, bytes, vcpu,\n\t\t\t\t\t   PFERR_WRITE_MASK, exception);\n}\nEXPORT_SYMBOL_GPL(kvm_write_guest_virt_system);\n\nint handle_ud(struct kvm_vcpu *vcpu)\n{\n\tstatic const char kvm_emulate_prefix[] = { __KVM_EMULATE_PREFIX };\n\tint emul_type = EMULTYPE_TRAP_UD;\n\tchar sig[5]; /* ud2; .ascii \"kvm\" */\n\tstruct x86_exception e;\n\n\tif (unlikely(!kvm_x86_ops.can_emulate_instruction(vcpu, NULL, 0)))\n\t\treturn 1;\n\n\tif (force_emulation_prefix &&\n\t    kvm_read_guest_virt(vcpu, kvm_get_linear_rip(vcpu),\n\t\t\t\tsig, sizeof(sig), &e) == 0 &&\n\t    memcmp(sig, kvm_emulate_prefix, sizeof(sig)) == 0) {\n\t\tkvm_rip_write(vcpu, kvm_rip_read(vcpu) + sizeof(sig));\n\t\temul_type = EMULTYPE_TRAP_UD_FORCED;\n\t}\n\n\treturn kvm_emulate_instruction(vcpu, emul_type);\n}\nEXPORT_SYMBOL_GPL(handle_ud);\n\nstatic int vcpu_is_mmio_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\n\t\t\t    gpa_t gpa, bool write)\n{\n\t/* For APIC access vmexit */\n\tif ((gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)\n\t\treturn 1;\n\n\tif (vcpu_match_mmio_gpa(vcpu, gpa)) {\n\t\ttrace_vcpu_match_mmio(gva, gpa, write, true);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vcpu_mmio_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\n\t\t\t\tgpa_t *gpa, struct x86_exception *exception,\n\t\t\t\tbool write)\n{\n\tu32 access = ((kvm_x86_ops.get_cpl(vcpu) == 3) ? PFERR_USER_MASK : 0)\n\t\t| (write ? PFERR_WRITE_MASK : 0);\n\n\t/*\n\t * currently PKRU is only applied to ept enabled guest so\n\t * there is no pkey in EPT page table for L1 guest or EPT\n\t * shadow page table for L2 guest.\n\t */\n\tif (vcpu_match_mmio_gva(vcpu, gva)\n\t    && !permission_fault(vcpu, vcpu->arch.walk_mmu,\n\t\t\t\t vcpu->arch.mmio_access, 0, access)) {\n\t\t*gpa = vcpu->arch.mmio_gfn << PAGE_SHIFT |\n\t\t\t\t\t(gva & (PAGE_SIZE - 1));\n\t\ttrace_vcpu_match_mmio(gva, *gpa, write, false);\n\t\treturn 1;\n\t}\n\n\t*gpa = vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, access, exception);\n\n\tif (*gpa == UNMAPPED_GVA)\n\t\treturn -1;\n\n\treturn vcpu_is_mmio_gpa(vcpu, gva, *gpa, write);\n}\n\nint emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\tconst void *val, int bytes)\n{\n\tint ret;\n\n\tret = kvm_vcpu_write_guest(vcpu, gpa, val, bytes);\n\tif (ret < 0)\n\t\treturn 0;\n\tkvm_page_track_write(vcpu, gpa, val, bytes);\n\treturn 1;\n}\n\nstruct read_write_emulator_ops {\n\tint (*read_write_prepare)(struct kvm_vcpu *vcpu, void *val,\n\t\t\t\t  int bytes);\n\tint (*read_write_emulate)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t\t  void *val, int bytes);\n\tint (*read_write_mmio)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t       int bytes, void *val);\n\tint (*read_write_exit_mmio)(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t\t    void *val, int bytes);\n\tbool write;\n};\n\nstatic int read_prepare(struct kvm_vcpu *vcpu, void *val, int bytes)\n{\n\tif (vcpu->mmio_read_completed) {\n\t\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ, bytes,\n\t\t\t       vcpu->mmio_fragments[0].gpa, val);\n\t\tvcpu->mmio_read_completed = 0;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\tvoid *val, int bytes)\n{\n\treturn !kvm_vcpu_read_guest(vcpu, gpa, val, bytes);\n}\n\nstatic int write_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t void *val, int bytes)\n{\n\treturn emulator_write_phys(vcpu, gpa, val, bytes);\n}\n\nstatic int write_mmio(struct kvm_vcpu *vcpu, gpa_t gpa, int bytes, void *val)\n{\n\ttrace_kvm_mmio(KVM_TRACE_MMIO_WRITE, bytes, gpa, val);\n\treturn vcpu_mmio_write(vcpu, gpa, bytes, val);\n}\n\nstatic int read_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t  void *val, int bytes)\n{\n\ttrace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, bytes, gpa, NULL);\n\treturn X86EMUL_IO_NEEDED;\n}\n\nstatic int write_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t   void *val, int bytes)\n{\n\tstruct kvm_mmio_fragment *frag = &vcpu->mmio_fragments[0];\n\n\tmemcpy(vcpu->run->mmio.data, frag->data, min(8u, frag->len));\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic const struct read_write_emulator_ops read_emultor = {\n\t.read_write_prepare = read_prepare,\n\t.read_write_emulate = read_emulate,\n\t.read_write_mmio = vcpu_mmio_read,\n\t.read_write_exit_mmio = read_exit_mmio,\n};\n\nstatic const struct read_write_emulator_ops write_emultor = {\n\t.read_write_emulate = write_emulate,\n\t.read_write_mmio = write_mmio,\n\t.read_write_exit_mmio = write_exit_mmio,\n\t.write = true,\n};\n\nstatic int emulator_read_write_onepage(unsigned long addr, void *val,\n\t\t\t\t       unsigned int bytes,\n\t\t\t\t       struct x86_exception *exception,\n\t\t\t\t       struct kvm_vcpu *vcpu,\n\t\t\t\t       const struct read_write_emulator_ops *ops)\n{\n\tgpa_t gpa;\n\tint handled, ret;\n\tbool write = ops->write;\n\tstruct kvm_mmio_fragment *frag;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * If the exit was due to a NPF we may already have a GPA.\n\t * If the GPA is present, use it to avoid the GVA to GPA table walk.\n\t * Note, this cannot be used on string operations since string\n\t * operation using rep will only have the initial GPA from the NPF\n\t * occurred.\n\t */\n\tif (ctxt->gpa_available && emulator_can_use_gpa(ctxt) &&\n\t    (addr & ~PAGE_MASK) == (ctxt->gpa_val & ~PAGE_MASK)) {\n\t\tgpa = ctxt->gpa_val;\n\t\tret = vcpu_is_mmio_gpa(vcpu, addr, gpa, write);\n\t} else {\n\t\tret = vcpu_mmio_gva_to_gpa(vcpu, addr, &gpa, exception, write);\n\t\tif (ret < 0)\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\tif (!ret && ops->read_write_emulate(vcpu, gpa, val, bytes))\n\t\treturn X86EMUL_CONTINUE;\n\n\t/*\n\t * Is this MMIO handled locally?\n\t */\n\thandled = ops->read_write_mmio(vcpu, gpa, bytes, val);\n\tif (handled == bytes)\n\t\treturn X86EMUL_CONTINUE;\n\n\tgpa += handled;\n\tbytes -= handled;\n\tval += handled;\n\n\tWARN_ON(vcpu->mmio_nr_fragments >= KVM_MAX_MMIO_FRAGMENTS);\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_nr_fragments++];\n\tfrag->gpa = gpa;\n\tfrag->data = val;\n\tfrag->len = bytes;\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int emulator_read_write(struct x86_emulate_ctxt *ctxt,\n\t\t\tunsigned long addr,\n\t\t\tvoid *val, unsigned int bytes,\n\t\t\tstruct x86_exception *exception,\n\t\t\tconst struct read_write_emulator_ops *ops)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tgpa_t gpa;\n\tint rc;\n\n\tif (ops->read_write_prepare &&\n\t\t  ops->read_write_prepare(vcpu, val, bytes))\n\t\treturn X86EMUL_CONTINUE;\n\n\tvcpu->mmio_nr_fragments = 0;\n\n\t/* Crossing a page boundary? */\n\tif (((addr + bytes - 1) ^ addr) & PAGE_MASK) {\n\t\tint now;\n\n\t\tnow = -addr & ~PAGE_MASK;\n\t\trc = emulator_read_write_onepage(addr, val, now, exception,\n\t\t\t\t\t\t vcpu, ops);\n\n\t\tif (rc != X86EMUL_CONTINUE)\n\t\t\treturn rc;\n\t\taddr += now;\n\t\tif (ctxt->mode != X86EMUL_MODE_PROT64)\n\t\t\taddr = (u32)addr;\n\t\tval += now;\n\t\tbytes -= now;\n\t}\n\n\trc = emulator_read_write_onepage(addr, val, bytes, exception,\n\t\t\t\t\t vcpu, ops);\n\tif (rc != X86EMUL_CONTINUE)\n\t\treturn rc;\n\n\tif (!vcpu->mmio_nr_fragments)\n\t\treturn rc;\n\n\tgpa = vcpu->mmio_fragments[0].gpa;\n\n\tvcpu->mmio_needed = 1;\n\tvcpu->mmio_cur_fragment = 0;\n\n\tvcpu->run->mmio.len = min(8u, vcpu->mmio_fragments[0].len);\n\tvcpu->run->mmio.is_write = vcpu->mmio_is_write = ops->write;\n\tvcpu->run->exit_reason = KVM_EXIT_MMIO;\n\tvcpu->run->mmio.phys_addr = gpa;\n\n\treturn ops->read_write_exit_mmio(vcpu, gpa, val, bytes);\n}\n\nstatic int emulator_read_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  unsigned long addr,\n\t\t\t\t  void *val,\n\t\t\t\t  unsigned int bytes,\n\t\t\t\t  struct x86_exception *exception)\n{\n\treturn emulator_read_write(ctxt, addr, val, bytes,\n\t\t\t\t   exception, &read_emultor);\n}\n\nstatic int emulator_write_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t    unsigned long addr,\n\t\t\t    const void *val,\n\t\t\t    unsigned int bytes,\n\t\t\t    struct x86_exception *exception)\n{\n\treturn emulator_read_write(ctxt, addr, (void *)val, bytes,\n\t\t\t\t   exception, &write_emultor);\n}\n\n#define CMPXCHG_TYPE(t, ptr, old, new) \\\n\t(cmpxchg((t *)(ptr), *(t *)(old), *(t *)(new)) == *(t *)(old))\n\n#ifdef CONFIG_X86_64\n#  define CMPXCHG64(ptr, old, new) CMPXCHG_TYPE(u64, ptr, old, new)\n#else\n#  define CMPXCHG64(ptr, old, new) \\\n\t(cmpxchg64((u64 *)(ptr), *(u64 *)(old), *(u64 *)(new)) == *(u64 *)(old))\n#endif\n\nstatic int emulator_cmpxchg_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     unsigned long addr,\n\t\t\t\t     const void *old,\n\t\t\t\t     const void *new,\n\t\t\t\t     unsigned int bytes,\n\t\t\t\t     struct x86_exception *exception)\n{\n\tstruct kvm_host_map map;\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tu64 page_line_mask;\n\tgpa_t gpa;\n\tchar *kaddr;\n\tbool exchanged;\n\n\t/* guests cmpxchg8b have to be emulated atomically */\n\tif (bytes > 8 || (bytes & (bytes - 1)))\n\t\tgoto emul_write;\n\n\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, addr, NULL);\n\n\tif (gpa == UNMAPPED_GVA ||\n\t    (gpa & PAGE_MASK) == APIC_DEFAULT_PHYS_BASE)\n\t\tgoto emul_write;\n\n\t/*\n\t * Emulate the atomic as a straight write to avoid #AC if SLD is\n\t * enabled in the host and the access splits a cache line.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_SPLIT_LOCK_DETECT))\n\t\tpage_line_mask = ~(cache_line_size() - 1);\n\telse\n\t\tpage_line_mask = PAGE_MASK;\n\n\tif (((gpa + bytes - 1) & page_line_mask) != (gpa & page_line_mask))\n\t\tgoto emul_write;\n\n\tif (kvm_vcpu_map(vcpu, gpa_to_gfn(gpa), &map))\n\t\tgoto emul_write;\n\n\tkaddr = map.hva + offset_in_page(gpa);\n\n\tswitch (bytes) {\n\tcase 1:\n\t\texchanged = CMPXCHG_TYPE(u8, kaddr, old, new);\n\t\tbreak;\n\tcase 2:\n\t\texchanged = CMPXCHG_TYPE(u16, kaddr, old, new);\n\t\tbreak;\n\tcase 4:\n\t\texchanged = CMPXCHG_TYPE(u32, kaddr, old, new);\n\t\tbreak;\n\tcase 8:\n\t\texchanged = CMPXCHG64(kaddr, old, new);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\n\tif (!exchanged)\n\t\treturn X86EMUL_CMPXCHG_FAILED;\n\n\tkvm_page_track_write(vcpu, gpa, new, bytes);\n\n\treturn X86EMUL_CONTINUE;\n\nemul_write:\n\tprintk_once(KERN_WARNING \"kvm: emulating exchange as write\\n\");\n\n\treturn emulator_write_emulated(ctxt, addr, new, bytes, exception);\n}\n\nstatic int kernel_pio(struct kvm_vcpu *vcpu, void *pd)\n{\n\tint r = 0, i;\n\n\tfor (i = 0; i < vcpu->arch.pio.count; i++) {\n\t\tif (vcpu->arch.pio.in)\n\t\t\tr = kvm_io_bus_read(vcpu, KVM_PIO_BUS, vcpu->arch.pio.port,\n\t\t\t\t\t    vcpu->arch.pio.size, pd);\n\t\telse\n\t\t\tr = kvm_io_bus_write(vcpu, KVM_PIO_BUS,\n\t\t\t\t\t     vcpu->arch.pio.port, vcpu->arch.pio.size,\n\t\t\t\t\t     pd);\n\t\tif (r)\n\t\t\tbreak;\n\t\tpd += vcpu->arch.pio.size;\n\t}\n\treturn r;\n}\n\nstatic int emulator_pio_in_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t       unsigned short port, void *val,\n\t\t\t       unsigned int count, bool in)\n{\n\tvcpu->arch.pio.port = port;\n\tvcpu->arch.pio.in = in;\n\tvcpu->arch.pio.count  = count;\n\tvcpu->arch.pio.size = size;\n\n\tif (!kernel_pio(vcpu, vcpu->arch.pio_data)) {\n\t\tvcpu->arch.pio.count = 0;\n\t\treturn 1;\n\t}\n\n\tvcpu->run->exit_reason = KVM_EXIT_IO;\n\tvcpu->run->io.direction = in ? KVM_EXIT_IO_IN : KVM_EXIT_IO_OUT;\n\tvcpu->run->io.size = size;\n\tvcpu->run->io.data_offset = KVM_PIO_PAGE_OFFSET * PAGE_SIZE;\n\tvcpu->run->io.count = count;\n\tvcpu->run->io.port = port;\n\n\treturn 0;\n}\n\nstatic int emulator_pio_in(struct kvm_vcpu *vcpu, int size,\n\t\t\t   unsigned short port, void *val, unsigned int count)\n{\n\tint ret;\n\n\tif (vcpu->arch.pio.count)\n\t\tgoto data_avail;\n\n\tmemset(vcpu->arch.pio_data, 0, size * count);\n\n\tret = emulator_pio_in_out(vcpu, size, port, val, count, true);\n\tif (ret) {\ndata_avail:\n\t\tmemcpy(val, vcpu->arch.pio_data, size * count);\n\t\ttrace_kvm_pio(KVM_PIO_IN, port, size, count, vcpu->arch.pio_data);\n\t\tvcpu->arch.pio.count = 0;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t    int size, unsigned short port, void *val,\n\t\t\t\t    unsigned int count)\n{\n\treturn emulator_pio_in(emul_to_vcpu(ctxt), size, port, val, count);\n\n}\n\nstatic int emulator_pio_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t    unsigned short port, const void *val,\n\t\t\t    unsigned int count)\n{\n\tmemcpy(vcpu->arch.pio_data, val, size * count);\n\ttrace_kvm_pio(KVM_PIO_OUT, port, size, count, vcpu->arch.pio_data);\n\treturn emulator_pio_in_out(vcpu, size, port, (void *)val, count, false);\n}\n\nstatic int emulator_pio_out_emulated(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t     int size, unsigned short port,\n\t\t\t\t     const void *val, unsigned int count)\n{\n\treturn emulator_pio_out(emul_to_vcpu(ctxt), size, port, val, count);\n}\n\nstatic unsigned long get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\treturn kvm_x86_ops.get_segment_base(vcpu, seg);\n}\n\nstatic void emulator_invlpg(struct x86_emulate_ctxt *ctxt, ulong address)\n{\n\tkvm_mmu_invlpg(emul_to_vcpu(ctxt), address);\n}\n\nstatic int kvm_emulate_wbinvd_noskip(struct kvm_vcpu *vcpu)\n{\n\tif (!need_emulate_wbinvd(vcpu))\n\t\treturn X86EMUL_CONTINUE;\n\n\tif (kvm_x86_ops.has_wbinvd_exit()) {\n\t\tint cpu = get_cpu();\n\n\t\tcpumask_set_cpu(cpu, vcpu->arch.wbinvd_dirty_mask);\n\t\tsmp_call_function_many(vcpu->arch.wbinvd_dirty_mask,\n\t\t\t\twbinvd_ipi, NULL, 1);\n\t\tput_cpu();\n\t\tcpumask_clear(vcpu->arch.wbinvd_dirty_mask);\n\t} else\n\t\twbinvd();\n\treturn X86EMUL_CONTINUE;\n}\n\nint kvm_emulate_wbinvd(struct kvm_vcpu *vcpu)\n{\n\tkvm_emulate_wbinvd_noskip(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_wbinvd);\n\n\n\nstatic void emulator_wbinvd(struct x86_emulate_ctxt *ctxt)\n{\n\tkvm_emulate_wbinvd_noskip(emul_to_vcpu(ctxt));\n}\n\nstatic int emulator_get_dr(struct x86_emulate_ctxt *ctxt, int dr,\n\t\t\t   unsigned long *dest)\n{\n\treturn kvm_get_dr(emul_to_vcpu(ctxt), dr, dest);\n}\n\nstatic int emulator_set_dr(struct x86_emulate_ctxt *ctxt, int dr,\n\t\t\t   unsigned long value)\n{\n\n\treturn __kvm_set_dr(emul_to_vcpu(ctxt), dr, value);\n}\n\nstatic u64 mk_cr_64(u64 curr_cr, u32 new_val)\n{\n\treturn (curr_cr & ~((1ULL << 32) - 1)) | new_val;\n}\n\nstatic unsigned long emulator_get_cr(struct x86_emulate_ctxt *ctxt, int cr)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tunsigned long value;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tvalue = kvm_read_cr0(vcpu);\n\t\tbreak;\n\tcase 2:\n\t\tvalue = vcpu->arch.cr2;\n\t\tbreak;\n\tcase 3:\n\t\tvalue = kvm_read_cr3(vcpu);\n\t\tbreak;\n\tcase 4:\n\t\tvalue = kvm_read_cr4(vcpu);\n\t\tbreak;\n\tcase 8:\n\t\tvalue = kvm_get_cr8(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tkvm_err(\"%s: unexpected cr %u\\n\", __func__, cr);\n\t\treturn 0;\n\t}\n\n\treturn value;\n}\n\nstatic int emulator_set_cr(struct x86_emulate_ctxt *ctxt, int cr, ulong val)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint res = 0;\n\n\tswitch (cr) {\n\tcase 0:\n\t\tres = kvm_set_cr0(vcpu, mk_cr_64(kvm_read_cr0(vcpu), val));\n\t\tbreak;\n\tcase 2:\n\t\tvcpu->arch.cr2 = val;\n\t\tbreak;\n\tcase 3:\n\t\tres = kvm_set_cr3(vcpu, val);\n\t\tbreak;\n\tcase 4:\n\t\tres = kvm_set_cr4(vcpu, mk_cr_64(kvm_read_cr4(vcpu), val));\n\t\tbreak;\n\tcase 8:\n\t\tres = kvm_set_cr8(vcpu, val);\n\t\tbreak;\n\tdefault:\n\t\tkvm_err(\"%s: unexpected cr %u\\n\", __func__, cr);\n\t\tres = -1;\n\t}\n\n\treturn res;\n}\n\nstatic int emulator_get_cpl(struct x86_emulate_ctxt *ctxt)\n{\n\treturn kvm_x86_ops.get_cpl(emul_to_vcpu(ctxt));\n}\n\nstatic void emulator_get_gdt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tkvm_x86_ops.get_gdt(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_get_idt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tkvm_x86_ops.get_idt(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_set_gdt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tkvm_x86_ops.set_gdt(emul_to_vcpu(ctxt), dt);\n}\n\nstatic void emulator_set_idt(struct x86_emulate_ctxt *ctxt, struct desc_ptr *dt)\n{\n\tkvm_x86_ops.set_idt(emul_to_vcpu(ctxt), dt);\n}\n\nstatic unsigned long emulator_get_cached_segment_base(\n\tstruct x86_emulate_ctxt *ctxt, int seg)\n{\n\treturn get_segment_base(emul_to_vcpu(ctxt), seg);\n}\n\nstatic bool emulator_get_segment(struct x86_emulate_ctxt *ctxt, u16 *selector,\n\t\t\t\t struct desc_struct *desc, u32 *base3,\n\t\t\t\t int seg)\n{\n\tstruct kvm_segment var;\n\n\tkvm_get_segment(emul_to_vcpu(ctxt), &var, seg);\n\t*selector = var.selector;\n\n\tif (var.unusable) {\n\t\tmemset(desc, 0, sizeof(*desc));\n\t\tif (base3)\n\t\t\t*base3 = 0;\n\t\treturn false;\n\t}\n\n\tif (var.g)\n\t\tvar.limit >>= 12;\n\tset_desc_limit(desc, var.limit);\n\tset_desc_base(desc, (unsigned long)var.base);\n#ifdef CONFIG_X86_64\n\tif (base3)\n\t\t*base3 = var.base >> 32;\n#endif\n\tdesc->type = var.type;\n\tdesc->s = var.s;\n\tdesc->dpl = var.dpl;\n\tdesc->p = var.present;\n\tdesc->avl = var.avl;\n\tdesc->l = var.l;\n\tdesc->d = var.db;\n\tdesc->g = var.g;\n\n\treturn true;\n}\n\nstatic void emulator_set_segment(struct x86_emulate_ctxt *ctxt, u16 selector,\n\t\t\t\t struct desc_struct *desc, u32 base3,\n\t\t\t\t int seg)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tstruct kvm_segment var;\n\n\tvar.selector = selector;\n\tvar.base = get_desc_base(desc);\n#ifdef CONFIG_X86_64\n\tvar.base |= ((u64)base3) << 32;\n#endif\n\tvar.limit = get_desc_limit(desc);\n\tif (desc->g)\n\t\tvar.limit = (var.limit << 12) | 0xfff;\n\tvar.type = desc->type;\n\tvar.dpl = desc->dpl;\n\tvar.db = desc->d;\n\tvar.s = desc->s;\n\tvar.l = desc->l;\n\tvar.g = desc->g;\n\tvar.avl = desc->avl;\n\tvar.present = desc->p;\n\tvar.unusable = !var.present;\n\tvar.padding = 0;\n\n\tkvm_set_segment(vcpu, &var, seg);\n\treturn;\n}\n\nstatic int emulator_get_msr(struct x86_emulate_ctxt *ctxt,\n\t\t\t    u32 msr_index, u64 *pdata)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r;\n\n\tr = kvm_get_msr(vcpu, msr_index, pdata);\n\n\tif (r && kvm_get_msr_user_space(vcpu, msr_index, r)) {\n\t\t/* Bounce to user space */\n\t\treturn X86EMUL_IO_NEEDED;\n\t}\n\n\treturn r;\n}\n\nstatic int emulator_set_msr(struct x86_emulate_ctxt *ctxt,\n\t\t\t    u32 msr_index, u64 data)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tint r;\n\n\tr = kvm_set_msr(vcpu, msr_index, data);\n\n\tif (r && kvm_set_msr_user_space(vcpu, msr_index, data, r)) {\n\t\t/* Bounce to user space */\n\t\treturn X86EMUL_IO_NEEDED;\n\t}\n\n\treturn r;\n}\n\nstatic u64 emulator_get_smbase(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\n\treturn vcpu->arch.smbase;\n}\n\nstatic void emulator_set_smbase(struct x86_emulate_ctxt *ctxt, u64 smbase)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\n\tvcpu->arch.smbase = smbase;\n}\n\nstatic int emulator_check_pmc(struct x86_emulate_ctxt *ctxt,\n\t\t\t      u32 pmc)\n{\n\treturn kvm_pmu_is_valid_rdpmc_ecx(emul_to_vcpu(ctxt), pmc);\n}\n\nstatic int emulator_read_pmc(struct x86_emulate_ctxt *ctxt,\n\t\t\t     u32 pmc, u64 *pdata)\n{\n\treturn kvm_pmu_rdpmc(emul_to_vcpu(ctxt), pmc, pdata);\n}\n\nstatic void emulator_halt(struct x86_emulate_ctxt *ctxt)\n{\n\temul_to_vcpu(ctxt)->arch.halt_request = 1;\n}\n\nstatic int emulator_intercept(struct x86_emulate_ctxt *ctxt,\n\t\t\t      struct x86_instruction_info *info,\n\t\t\t      enum x86_intercept_stage stage)\n{\n\treturn kvm_x86_ops.check_intercept(emul_to_vcpu(ctxt), info, stage,\n\t\t\t\t\t    &ctxt->exception);\n}\n\nstatic bool emulator_get_cpuid(struct x86_emulate_ctxt *ctxt,\n\t\t\t      u32 *eax, u32 *ebx, u32 *ecx, u32 *edx,\n\t\t\t      bool exact_only)\n{\n\treturn kvm_cpuid(emul_to_vcpu(ctxt), eax, ebx, ecx, edx, exact_only);\n}\n\nstatic bool emulator_guest_has_long_mode(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_LM);\n}\n\nstatic bool emulator_guest_has_movbe(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_MOVBE);\n}\n\nstatic bool emulator_guest_has_fxsr(struct x86_emulate_ctxt *ctxt)\n{\n\treturn guest_cpuid_has(emul_to_vcpu(ctxt), X86_FEATURE_FXSR);\n}\n\nstatic ulong emulator_read_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg)\n{\n\treturn kvm_register_read(emul_to_vcpu(ctxt), reg);\n}\n\nstatic void emulator_write_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg, ulong val)\n{\n\tkvm_register_write(emul_to_vcpu(ctxt), reg, val);\n}\n\nstatic void emulator_set_nmi_mask(struct x86_emulate_ctxt *ctxt, bool masked)\n{\n\tkvm_x86_ops.set_nmi_mask(emul_to_vcpu(ctxt), masked);\n}\n\nstatic unsigned emulator_get_hflags(struct x86_emulate_ctxt *ctxt)\n{\n\treturn emul_to_vcpu(ctxt)->arch.hflags;\n}\n\nstatic void emulator_set_hflags(struct x86_emulate_ctxt *ctxt, unsigned emul_flags)\n{\n\temul_to_vcpu(ctxt)->arch.hflags = emul_flags;\n}\n\nstatic int emulator_pre_leave_smm(struct x86_emulate_ctxt *ctxt,\n\t\t\t\t  const char *smstate)\n{\n\treturn kvm_x86_ops.pre_leave_smm(emul_to_vcpu(ctxt), smstate);\n}\n\nstatic void emulator_post_leave_smm(struct x86_emulate_ctxt *ctxt)\n{\n\tkvm_smm_changed(emul_to_vcpu(ctxt));\n}\n\nstatic int emulator_set_xcr(struct x86_emulate_ctxt *ctxt, u32 index, u64 xcr)\n{\n\treturn __kvm_set_xcr(emul_to_vcpu(ctxt), index, xcr);\n}\n\nstatic const struct x86_emulate_ops emulate_ops = {\n\t.read_gpr            = emulator_read_gpr,\n\t.write_gpr           = emulator_write_gpr,\n\t.read_std            = emulator_read_std,\n\t.write_std           = emulator_write_std,\n\t.read_phys           = kvm_read_guest_phys_system,\n\t.fetch               = kvm_fetch_guest_virt,\n\t.read_emulated       = emulator_read_emulated,\n\t.write_emulated      = emulator_write_emulated,\n\t.cmpxchg_emulated    = emulator_cmpxchg_emulated,\n\t.invlpg              = emulator_invlpg,\n\t.pio_in_emulated     = emulator_pio_in_emulated,\n\t.pio_out_emulated    = emulator_pio_out_emulated,\n\t.get_segment         = emulator_get_segment,\n\t.set_segment         = emulator_set_segment,\n\t.get_cached_segment_base = emulator_get_cached_segment_base,\n\t.get_gdt             = emulator_get_gdt,\n\t.get_idt\t     = emulator_get_idt,\n\t.set_gdt             = emulator_set_gdt,\n\t.set_idt\t     = emulator_set_idt,\n\t.get_cr              = emulator_get_cr,\n\t.set_cr              = emulator_set_cr,\n\t.cpl                 = emulator_get_cpl,\n\t.get_dr              = emulator_get_dr,\n\t.set_dr              = emulator_set_dr,\n\t.get_smbase          = emulator_get_smbase,\n\t.set_smbase          = emulator_set_smbase,\n\t.set_msr             = emulator_set_msr,\n\t.get_msr             = emulator_get_msr,\n\t.check_pmc\t     = emulator_check_pmc,\n\t.read_pmc            = emulator_read_pmc,\n\t.halt                = emulator_halt,\n\t.wbinvd              = emulator_wbinvd,\n\t.fix_hypercall       = emulator_fix_hypercall,\n\t.intercept           = emulator_intercept,\n\t.get_cpuid           = emulator_get_cpuid,\n\t.guest_has_long_mode = emulator_guest_has_long_mode,\n\t.guest_has_movbe     = emulator_guest_has_movbe,\n\t.guest_has_fxsr      = emulator_guest_has_fxsr,\n\t.set_nmi_mask        = emulator_set_nmi_mask,\n\t.get_hflags          = emulator_get_hflags,\n\t.set_hflags          = emulator_set_hflags,\n\t.pre_leave_smm       = emulator_pre_leave_smm,\n\t.post_leave_smm      = emulator_post_leave_smm,\n\t.set_xcr             = emulator_set_xcr,\n};\n\nstatic void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask)\n{\n\tu32 int_shadow = kvm_x86_ops.get_interrupt_shadow(vcpu);\n\t/*\n\t * an sti; sti; sequence only disable interrupts for the first\n\t * instruction. So, if the last instruction, be it emulated or\n\t * not, left the system with the INT_STI flag enabled, it\n\t * means that the last instruction is an sti. We should not\n\t * leave the flag on in this case. The same goes for mov ss\n\t */\n\tif (int_shadow & mask)\n\t\tmask = 0;\n\tif (unlikely(int_shadow || mask)) {\n\t\tkvm_x86_ops.set_interrupt_shadow(vcpu, mask);\n\t\tif (!mask)\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n}\n\nstatic bool inject_emulated_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tif (ctxt->exception.vector == PF_VECTOR)\n\t\treturn kvm_inject_emulated_page_fault(vcpu, &ctxt->exception);\n\n\tif (ctxt->exception.error_code_valid)\n\t\tkvm_queue_exception_e(vcpu, ctxt->exception.vector,\n\t\t\t\t      ctxt->exception.error_code);\n\telse\n\t\tkvm_queue_exception(vcpu, ctxt->exception.vector);\n\treturn false;\n}\n\nstatic struct x86_emulate_ctxt *alloc_emulate_ctxt(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt;\n\n\tctxt = kmem_cache_zalloc(x86_emulator_cache, GFP_KERNEL_ACCOUNT);\n\tif (!ctxt) {\n\t\tpr_err(\"kvm: failed to allocate vcpu's emulator\\n\");\n\t\treturn NULL;\n\t}\n\n\tctxt->vcpu = vcpu;\n\tctxt->ops = &emulate_ops;\n\tvcpu->arch.emulate_ctxt = ctxt;\n\n\treturn ctxt;\n}\n\nstatic void init_emulate_ctxt(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint cs_db, cs_l;\n\n\tkvm_x86_ops.get_cs_db_l_bits(vcpu, &cs_db, &cs_l);\n\n\tctxt->gpa_available = false;\n\tctxt->eflags = kvm_get_rflags(vcpu);\n\tctxt->tf = (ctxt->eflags & X86_EFLAGS_TF) != 0;\n\n\tctxt->eip = kvm_rip_read(vcpu);\n\tctxt->mode = (!is_protmode(vcpu))\t\t? X86EMUL_MODE_REAL :\n\t\t     (ctxt->eflags & X86_EFLAGS_VM)\t? X86EMUL_MODE_VM86 :\n\t\t     (cs_l && is_long_mode(vcpu))\t? X86EMUL_MODE_PROT64 :\n\t\t     cs_db\t\t\t\t? X86EMUL_MODE_PROT32 :\n\t\t\t\t\t\t\t  X86EMUL_MODE_PROT16;\n\tBUILD_BUG_ON(HF_GUEST_MASK != X86EMUL_GUEST_MASK);\n\tBUILD_BUG_ON(HF_SMM_MASK != X86EMUL_SMM_MASK);\n\tBUILD_BUG_ON(HF_SMM_INSIDE_NMI_MASK != X86EMUL_SMM_INSIDE_NMI_MASK);\n\n\tinit_decode_cache(ctxt);\n\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n}\n\nvoid kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint ret;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tctxt->op_bytes = 2;\n\tctxt->ad_bytes = 2;\n\tctxt->_eip = ctxt->eip + inc_eip;\n\tret = emulate_int_real(ctxt, irq);\n\n\tif (ret != X86EMUL_CONTINUE) {\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t} else {\n\t\tctxt->eip = ctxt->_eip;\n\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\tkvm_set_rflags(vcpu, ctxt->eflags);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_inject_realmode_interrupt);\n\nstatic int handle_emulation_failure(struct kvm_vcpu *vcpu, int emulation_type)\n{\n\t++vcpu->stat.insn_emulation_fail;\n\ttrace_kvm_emulate_insn_failed(vcpu);\n\n\tif (emulation_type & EMULTYPE_VMWARE_GP) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\n\tif (!is_guest_mode(vcpu) && kvm_x86_ops.get_cpl(vcpu) == 0) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic bool reexecute_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t\t  bool write_fault_to_shadow_pgtable,\n\t\t\t\t  int emulation_type)\n{\n\tgpa_t gpa = cr2_or_gpa;\n\tkvm_pfn_t pfn;\n\n\tif (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))\n\t\treturn false;\n\n\tif (WARN_ON_ONCE(is_guest_mode(vcpu)) ||\n\t    WARN_ON_ONCE(!(emulation_type & EMULTYPE_PF)))\n\t\treturn false;\n\n\tif (!vcpu->arch.mmu->direct_map) {\n\t\t/*\n\t\t * Write permission should be allowed since only\n\t\t * write access need to be emulated.\n\t\t */\n\t\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);\n\n\t\t/*\n\t\t * If the mapping is invalid in guest, let cpu retry\n\t\t * it to generate fault.\n\t\t */\n\t\tif (gpa == UNMAPPED_GVA)\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * Do not retry the unhandleable instruction if it faults on the\n\t * readonly host memory, otherwise it will goto a infinite loop:\n\t * retry instruction -> write #PF -> emulation fail -> retry\n\t * instruction -> ...\n\t */\n\tpfn = gfn_to_pfn(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t/*\n\t * If the instruction failed on the error pfn, it can not be fixed,\n\t * report the error to userspace.\n\t */\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn false;\n\n\tkvm_release_pfn_clean(pfn);\n\n\t/* The instructions are well-emulated on direct mmu. */\n\tif (vcpu->arch.mmu->direct_map) {\n\t\tunsigned int indirect_shadow_pages;\n\n\t\tspin_lock(&vcpu->kvm->mmu_lock);\n\t\tindirect_shadow_pages = vcpu->kvm->arch.indirect_shadow_pages;\n\t\tspin_unlock(&vcpu->kvm->mmu_lock);\n\n\t\tif (indirect_shadow_pages)\n\t\t\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * if emulation was due to access to shadowed page table\n\t * and it failed try to unshadow page and re-enter the\n\t * guest to let CPU execute the instruction.\n\t */\n\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\t/*\n\t * If the access faults on its page table, it can not\n\t * be fixed by unprotecting shadow page and it should\n\t * be reported to userspace.\n\t */\n\treturn !write_fault_to_shadow_pgtable;\n}\n\nstatic bool retry_instruction(struct x86_emulate_ctxt *ctxt,\n\t\t\t      gpa_t cr2_or_gpa,  int emulation_type)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tunsigned long last_retry_eip, last_retry_addr, gpa = cr2_or_gpa;\n\n\tlast_retry_eip = vcpu->arch.last_retry_eip;\n\tlast_retry_addr = vcpu->arch.last_retry_addr;\n\n\t/*\n\t * If the emulation is caused by #PF and it is non-page_table\n\t * writing instruction, it means the VM-EXIT is caused by shadow\n\t * page protected, we can zap the shadow page and retry this\n\t * instruction directly.\n\t *\n\t * Note: if the guest uses a non-page-table modifying instruction\n\t * on the PDE that points to the instruction, then we will unmap\n\t * the instruction and go to an infinite loop. So, we cache the\n\t * last retried eip and the last fault address, if we meet the eip\n\t * and the address again, we can break out of the potential infinite\n\t * loop.\n\t */\n\tvcpu->arch.last_retry_eip = vcpu->arch.last_retry_addr = 0;\n\n\tif (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))\n\t\treturn false;\n\n\tif (WARN_ON_ONCE(is_guest_mode(vcpu)) ||\n\t    WARN_ON_ONCE(!(emulation_type & EMULTYPE_PF)))\n\t\treturn false;\n\n\tif (x86_page_table_writing_insn(ctxt))\n\t\treturn false;\n\n\tif (ctxt->eip == last_retry_eip && last_retry_addr == cr2_or_gpa)\n\t\treturn false;\n\n\tvcpu->arch.last_retry_eip = ctxt->eip;\n\tvcpu->arch.last_retry_addr = cr2_or_gpa;\n\n\tif (!vcpu->arch.mmu->direct_map)\n\t\tgpa = kvm_mmu_gva_to_gpa_write(vcpu, cr2_or_gpa, NULL);\n\n\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(gpa));\n\n\treturn true;\n}\n\nstatic int complete_emulated_mmio(struct kvm_vcpu *vcpu);\nstatic int complete_emulated_pio(struct kvm_vcpu *vcpu);\n\nstatic void kvm_smm_changed(struct kvm_vcpu *vcpu)\n{\n\tif (!(vcpu->arch.hflags & HF_SMM_MASK)) {\n\t\t/* This is a good place to trace that we are exiting SMM.  */\n\t\ttrace_kvm_enter_smm(vcpu->vcpu_id, vcpu->arch.smbase, false);\n\n\t\t/* Process a latched INIT or SMI, if any.  */\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n\n\tkvm_mmu_reset_context(vcpu);\n}\n\nstatic int kvm_vcpu_check_hw_bp(unsigned long addr, u32 type, u32 dr7,\n\t\t\t\tunsigned long *db)\n{\n\tu32 dr6 = 0;\n\tint i;\n\tu32 enable, rwlen;\n\n\tenable = dr7;\n\trwlen = dr7 >> 16;\n\tfor (i = 0; i < 4; i++, enable >>= 2, rwlen >>= 4)\n\t\tif ((enable & 3) && (rwlen & 15) == type && db[i] == addr)\n\t\t\tdr6 |= (1 << i);\n\treturn dr6;\n}\n\nstatic int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP) {\n\t\tkvm_run->debug.arch.dr6 = DR6_BS | DR6_FIXED_1 | DR6_RTM;\n\t\tkvm_run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\treturn 0;\n\t}\n\tkvm_queue_exception_p(vcpu, DB_VECTOR, DR6_BS);\n\treturn 1;\n}\n\nint kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags = kvm_x86_ops.get_rflags(vcpu);\n\tint r;\n\n\tr = kvm_x86_ops.skip_emulated_instruction(vcpu);\n\tif (unlikely(!r))\n\t\treturn 0;\n\n\t/*\n\t * rflags is the old, \"raw\" value of the flags.  The new value has\n\t * not been saved yet.\n\t *\n\t * This is correct even for TF set by the guest, because \"the\n\t * processor will not generate this exception after the instruction\n\t * that sets the TF flag\".\n\t */\n\tif (unlikely(rflags & X86_EFLAGS_TF))\n\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_skip_emulated_instruction);\n\nstatic bool kvm_vcpu_check_breakpoint(struct kvm_vcpu *vcpu, int *r)\n{\n\tif (unlikely(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) &&\n\t    (vcpu->arch.guest_debug_dr7 & DR7_BP_EN_MASK)) {\n\t\tstruct kvm_run *kvm_run = vcpu->run;\n\t\tunsigned long eip = kvm_get_linear_rip(vcpu);\n\t\tu32 dr6 = kvm_vcpu_check_hw_bp(eip, 0,\n\t\t\t\t\t   vcpu->arch.guest_debug_dr7,\n\t\t\t\t\t   vcpu->arch.eff_db);\n\n\t\tif (dr6 != 0) {\n\t\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1 | DR6_RTM;\n\t\t\tkvm_run->debug.arch.pc = eip;\n\t\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (unlikely(vcpu->arch.dr7 & DR7_BP_EN_MASK) &&\n\t    !(kvm_get_rflags(vcpu) & X86_EFLAGS_RF)) {\n\t\tunsigned long eip = kvm_get_linear_rip(vcpu);\n\t\tu32 dr6 = kvm_vcpu_check_hw_bp(eip, 0,\n\t\t\t\t\t   vcpu->arch.dr7,\n\t\t\t\t\t   vcpu->arch.db);\n\n\t\tif (dr6 != 0) {\n\t\t\tkvm_queue_exception_p(vcpu, DB_VECTOR, dr6);\n\t\t\t*r = 1;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic bool is_vmware_backdoor_opcode(struct x86_emulate_ctxt *ctxt)\n{\n\tswitch (ctxt->opcode_len) {\n\tcase 1:\n\t\tswitch (ctxt->b) {\n\t\tcase 0xe4:\t/* IN */\n\t\tcase 0xe5:\n\t\tcase 0xec:\n\t\tcase 0xed:\n\t\tcase 0xe6:\t/* OUT */\n\t\tcase 0xe7:\n\t\tcase 0xee:\n\t\tcase 0xef:\n\t\tcase 0x6c:\t/* INS */\n\t\tcase 0x6d:\n\t\tcase 0x6e:\t/* OUTS */\n\t\tcase 0x6f:\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\tswitch (ctxt->b) {\n\t\tcase 0x33:\t/* RDPMC */\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nint x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len)\n{\n\tint r;\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tbool writeback = true;\n\tbool write_fault_to_spt;\n\n\tif (unlikely(!kvm_x86_ops.can_emulate_instruction(vcpu, insn, insn_len)))\n\t\treturn 1;\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\t/*\n\t * Clear write_fault_to_shadow_pgtable here to ensure it is\n\t * never reused.\n\t */\n\twrite_fault_to_spt = vcpu->arch.write_fault_to_shadow_pgtable;\n\tvcpu->arch.write_fault_to_shadow_pgtable = false;\n\tkvm_clear_exception_queue(vcpu);\n\n\tif (!(emulation_type & EMULTYPE_NO_DECODE)) {\n\t\tinit_emulate_ctxt(vcpu);\n\n\t\t/*\n\t\t * We will reenter on the same instruction since\n\t\t * we do not set complete_userspace_io.  This does not\n\t\t * handle watchpoints yet, those would be handled in\n\t\t * the emulate_ops.\n\t\t */\n\t\tif (!(emulation_type & EMULTYPE_SKIP) &&\n\t\t    kvm_vcpu_check_breakpoint(vcpu, &r))\n\t\t\treturn r;\n\n\t\tctxt->interruptibility = 0;\n\t\tctxt->have_exception = false;\n\t\tctxt->exception.vector = -1;\n\t\tctxt->perm_ok = false;\n\n\t\tctxt->ud = emulation_type & EMULTYPE_TRAP_UD;\n\n\t\tr = x86_decode_insn(ctxt, insn, insn_len);\n\n\t\ttrace_kvm_emulate_insn_start(vcpu);\n\t\t++vcpu->stat.insn_emulation;\n\t\tif (r != EMULATION_OK)  {\n\t\t\tif ((emulation_type & EMULTYPE_TRAP_UD) ||\n\t\t\t    (emulation_type & EMULTYPE_TRAP_UD_FORCED)) {\n\t\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tif (reexecute_instruction(vcpu, cr2_or_gpa,\n\t\t\t\t\t\t  write_fault_to_spt,\n\t\t\t\t\t\t  emulation_type))\n\t\t\t\treturn 1;\n\t\t\tif (ctxt->have_exception) {\n\t\t\t\t/*\n\t\t\t\t * #UD should result in just EMULATION_FAILED, and trap-like\n\t\t\t\t * exception should not be encountered during decode.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(ctxt->exception.vector == UD_VECTOR ||\n\t\t\t\t\t     exception_type(ctxt->exception.vector) == EXCPT_TRAP);\n\t\t\t\tinject_emulated_exception(vcpu);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t\t}\n\t}\n\n\tif ((emulation_type & EMULTYPE_VMWARE_GP) &&\n\t    !is_vmware_backdoor_opcode(ctxt)) {\n\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * Note, EMULTYPE_SKIP is intended for use *only* by vendor callbacks\n\t * for kvm_skip_emulated_instruction().  The caller is responsible for\n\t * updating interruptibility state and injecting single-step #DBs.\n\t */\n\tif (emulation_type & EMULTYPE_SKIP) {\n\t\tkvm_rip_write(vcpu, ctxt->_eip);\n\t\tif (ctxt->eflags & X86_EFLAGS_RF)\n\t\t\tkvm_set_rflags(vcpu, ctxt->eflags & ~X86_EFLAGS_RF);\n\t\treturn 1;\n\t}\n\n\tif (retry_instruction(ctxt, cr2_or_gpa, emulation_type))\n\t\treturn 1;\n\n\t/* this is needed for vmware backdoor interface to work since it\n\t   changes registers values  during IO operation */\n\tif (vcpu->arch.emulate_regs_need_sync_from_vcpu) {\n\t\tvcpu->arch.emulate_regs_need_sync_from_vcpu = false;\n\t\temulator_invalidate_register_cache(ctxt);\n\t}\n\nrestart:\n\tif (emulation_type & EMULTYPE_PF) {\n\t\t/* Save the faulting GPA (cr2) in the address field */\n\t\tctxt->exception.address = cr2_or_gpa;\n\n\t\t/* With shadow page tables, cr2 contains a GVA or nGPA. */\n\t\tif (vcpu->arch.mmu->direct_map) {\n\t\t\tctxt->gpa_available = true;\n\t\t\tctxt->gpa_val = cr2_or_gpa;\n\t\t}\n\t} else {\n\t\t/* Sanitize the address out of an abundance of paranoia. */\n\t\tctxt->exception.address = 0;\n\t}\n\n\tr = x86_emulate_insn(ctxt);\n\n\tif (r == EMULATION_INTERCEPTED)\n\t\treturn 1;\n\n\tif (r == EMULATION_FAILED) {\n\t\tif (reexecute_instruction(vcpu, cr2_or_gpa, write_fault_to_spt,\n\t\t\t\t\temulation_type))\n\t\t\treturn 1;\n\n\t\treturn handle_emulation_failure(vcpu, emulation_type);\n\t}\n\n\tif (ctxt->have_exception) {\n\t\tr = 1;\n\t\tif (inject_emulated_exception(vcpu))\n\t\t\treturn r;\n\t} else if (vcpu->arch.pio.count) {\n\t\tif (!vcpu->arch.pio.in) {\n\t\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\t\tvcpu->arch.pio.count = 0;\n\t\t} else {\n\t\t\twriteback = false;\n\t\t\tvcpu->arch.complete_userspace_io = complete_emulated_pio;\n\t\t}\n\t\tr = 0;\n\t} else if (vcpu->mmio_needed) {\n\t\t++vcpu->stat.mmio_exits;\n\n\t\tif (!vcpu->mmio_is_write)\n\t\t\twriteback = false;\n\t\tr = 0;\n\t\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\t} else if (r == EMULATION_RESTART)\n\t\tgoto restart;\n\telse\n\t\tr = 1;\n\n\tif (writeback) {\n\t\tunsigned long rflags = kvm_x86_ops.get_rflags(vcpu);\n\t\ttoggle_interruptibility(vcpu, ctxt->interruptibility);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t\tif (!ctxt->have_exception ||\n\t\t    exception_type(ctxt->exception.vector) == EXCPT_TRAP) {\n\t\t\tkvm_rip_write(vcpu, ctxt->eip);\n\t\t\tif (r && (ctxt->tf || (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)))\n\t\t\t\tr = kvm_vcpu_do_singlestep(vcpu);\n\t\t\tif (kvm_x86_ops.update_emulated_instruction)\n\t\t\t\tkvm_x86_ops.update_emulated_instruction(vcpu);\n\t\t\t__kvm_set_rflags(vcpu, ctxt->eflags);\n\t\t}\n\n\t\t/*\n\t\t * For STI, interrupts are shadowed; so KVM_REQ_EVENT will\n\t\t * do nothing, and it will be requested again as soon as\n\t\t * the shadow expires.  But we still need to check here,\n\t\t * because POPF has no interrupt shadow.\n\t\t */\n\t\tif (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = true;\n\n\treturn r;\n}\n\nint kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)\n{\n\treturn x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_instruction);\n\nint kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,\n\t\t\t\t\tvoid *insn, int insn_len)\n{\n\treturn x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_instruction_from_buffer);\n\nstatic int complete_fast_pio_out_port_0x7e(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pio.count = 0;\n\treturn 1;\n}\n\nstatic int complete_fast_pio_out(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.pio.count = 0;\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.pio.linear_rip)))\n\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_fast_pio_out(struct kvm_vcpu *vcpu, int size,\n\t\t\t    unsigned short port)\n{\n\tunsigned long val = kvm_rax_read(vcpu);\n\tint ret = emulator_pio_out(vcpu, size, port, &val, 1);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Workaround userspace that relies on old KVM behavior of %rip being\n\t * incremented prior to exiting to userspace to handle \"OUT 0x7e\".\n\t */\n\tif (port == 0x7e &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_OUT_7E_INC_RIP)) {\n\t\tvcpu->arch.complete_userspace_io =\n\t\t\tcomplete_fast_pio_out_port_0x7e;\n\t\tkvm_skip_emulated_instruction(vcpu);\n\t} else {\n\t\tvcpu->arch.pio.linear_rip = kvm_get_linear_rip(vcpu);\n\t\tvcpu->arch.complete_userspace_io = complete_fast_pio_out;\n\t}\n\treturn 0;\n}\n\nstatic int complete_fast_pio_in(struct kvm_vcpu *vcpu)\n{\n\tunsigned long val;\n\n\t/* We should only ever be called with arch.pio.count equal to 1 */\n\tBUG_ON(vcpu->arch.pio.count != 1);\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.pio.linear_rip))) {\n\t\tvcpu->arch.pio.count = 0;\n\t\treturn 1;\n\t}\n\n\t/* For size less than 4 we merge, else we zero extend */\n\tval = (vcpu->arch.pio.size < 4) ? kvm_rax_read(vcpu) : 0;\n\n\t/*\n\t * Since vcpu->arch.pio.count == 1 let emulator_pio_in perform\n\t * the copy and tracing\n\t */\n\temulator_pio_in(vcpu, vcpu->arch.pio.size, vcpu->arch.pio.port, &val, 1);\n\tkvm_rax_write(vcpu, val);\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_fast_pio_in(struct kvm_vcpu *vcpu, int size,\n\t\t\t   unsigned short port)\n{\n\tunsigned long val;\n\tint ret;\n\n\t/* For size less than 4 we merge, else we zero extend */\n\tval = (size < 4) ? kvm_rax_read(vcpu) : 0;\n\n\tret = emulator_pio_in(vcpu, size, port, &val, 1);\n\tif (ret) {\n\t\tkvm_rax_write(vcpu, val);\n\t\treturn ret;\n\t}\n\n\tvcpu->arch.pio.linear_rip = kvm_get_linear_rip(vcpu);\n\tvcpu->arch.complete_userspace_io = complete_fast_pio_in;\n\n\treturn 0;\n}\n\nint kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in)\n{\n\tint ret;\n\n\tif (in)\n\t\tret = kvm_fast_pio_in(vcpu, size, port);\n\telse\n\t\tret = kvm_fast_pio_out(vcpu, size, port);\n\treturn ret && kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_fast_pio);\n\nstatic int kvmclock_cpu_down_prep(unsigned int cpu)\n{\n\t__this_cpu_write(cpu_tsc_khz, 0);\n\treturn 0;\n}\n\nstatic void tsc_khz_changed(void *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\tunsigned long khz = 0;\n\n\tif (data)\n\t\tkhz = freq->new;\n\telse if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC))\n\t\tkhz = cpufreq_quick_get(raw_smp_processor_id());\n\tif (!khz)\n\t\tkhz = tsc_khz;\n\t__this_cpu_write(cpu_tsc_khz, khz);\n}\n\n#ifdef CONFIG_X86_64\nstatic void kvm_hyperv_tsc_notifier(void)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint cpu;\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_make_mclock_inprogress_request(kvm);\n\n\thyperv_stop_tsc_emulation();\n\n\t/* TSC frequency always matches when on Hyper-V */\n\tfor_each_present_cpu(cpu)\n\t\tper_cpu(cpu_tsc_khz, cpu) = tsc_khz;\n\tkvm_max_guest_tsc_khz = tsc_khz;\n\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tstruct kvm_arch *ka = &kvm->arch;\n\n\t\tspin_lock(&ka->pvclock_gtod_sync_lock);\n\n\t\tpvclock_update_vm_gtod_copy(kvm);\n\n\t\tkvm_for_each_vcpu(cpu, vcpu, kvm)\n\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\t\tkvm_for_each_vcpu(cpu, vcpu, kvm)\n\t\t\tkvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);\n\n\t\tspin_unlock(&ka->pvclock_gtod_sync_lock);\n\t}\n\tmutex_unlock(&kvm_lock);\n}\n#endif\n\nstatic void __kvmclock_cpufreq_notifier(struct cpufreq_freqs *freq, int cpu)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint i, send_ipi = 0;\n\n\t/*\n\t * We allow guests to temporarily run on slowing clocks,\n\t * provided we notify them after, or to run on accelerating\n\t * clocks, provided we notify them before.  Thus time never\n\t * goes backwards.\n\t *\n\t * However, we have a problem.  We can't atomically update\n\t * the frequency of a given CPU from this function; it is\n\t * merely a notifier, which can be called from any CPU.\n\t * Changing the TSC frequency at arbitrary points in time\n\t * requires a recomputation of local variables related to\n\t * the TSC for each VCPU.  We must flag these local variables\n\t * to be updated and be sure the update takes place with the\n\t * new frequency before any guests proceed.\n\t *\n\t * Unfortunately, the combination of hotplug CPU and frequency\n\t * change creates an intractable locking scenario; the order\n\t * of when these callouts happen is undefined with respect to\n\t * CPU hotplug, and they can race with each other.  As such,\n\t * merely setting per_cpu(cpu_tsc_khz) = X during a hotadd is\n\t * undefined; you can actually have a CPU frequency change take\n\t * place in between the computation of X and the setting of the\n\t * variable.  To protect against this problem, all updates of\n\t * the per_cpu tsc_khz variable are done in an interrupt\n\t * protected IPI, and all callers wishing to update the value\n\t * must wait for a synchronous IPI to complete (which is trivial\n\t * if the caller is on the CPU already).  This establishes the\n\t * necessary total order on variable updates.\n\t *\n\t * Note that because a guest time update may take place\n\t * anytime after the setting of the VCPU's request bit, the\n\t * correct TSC value must be set before the request.  However,\n\t * to ensure the update actually makes it to any guest which\n\t * starts running in hardware virtualization between the set\n\t * and the acquisition of the spinlock, we must also ping the\n\t * CPU after setting the request bit.\n\t *\n\t */\n\n\tsmp_call_function_single(cpu, tsc_khz_changed, freq, 1);\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (vcpu->cpu != cpu)\n\t\t\t\tcontinue;\n\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\tif (vcpu->cpu != raw_smp_processor_id())\n\t\t\t\tsend_ipi = 1;\n\t\t}\n\t}\n\tmutex_unlock(&kvm_lock);\n\n\tif (freq->old < freq->new && send_ipi) {\n\t\t/*\n\t\t * We upscale the frequency.  Must make the guest\n\t\t * doesn't see old kvmclock values while running with\n\t\t * the new frequency, otherwise we risk the guest sees\n\t\t * time go backwards.\n\t\t *\n\t\t * In case we update the frequency for another cpu\n\t\t * (which might be in guest context) send an interrupt\n\t\t * to kick the cpu out of guest context.  Next time\n\t\t * guest context is entered kvmclock will be updated,\n\t\t * so the guest will not see stale values.\n\t\t */\n\t\tsmp_call_function_single(cpu, tsc_khz_changed, freq, 1);\n\t}\n}\n\nstatic int kvmclock_cpufreq_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t     void *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\tint cpu;\n\n\tif (val == CPUFREQ_PRECHANGE && freq->old > freq->new)\n\t\treturn 0;\n\tif (val == CPUFREQ_POSTCHANGE && freq->old < freq->new)\n\t\treturn 0;\n\n\tfor_each_cpu(cpu, freq->policy->cpus)\n\t\t__kvmclock_cpufreq_notifier(freq, cpu);\n\n\treturn 0;\n}\n\nstatic struct notifier_block kvmclock_cpufreq_notifier_block = {\n\t.notifier_call  = kvmclock_cpufreq_notifier\n};\n\nstatic int kvmclock_cpu_online(unsigned int cpu)\n{\n\ttsc_khz_changed(NULL);\n\treturn 0;\n}\n\nstatic void kvm_timer_init(void)\n{\n\tmax_tsc_khz = tsc_khz;\n\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {\n#ifdef CONFIG_CPU_FREQ\n\t\tstruct cpufreq_policy *policy;\n\t\tint cpu;\n\n\t\tcpu = get_cpu();\n\t\tpolicy = cpufreq_cpu_get(cpu);\n\t\tif (policy) {\n\t\t\tif (policy->cpuinfo.max_freq)\n\t\t\t\tmax_tsc_khz = policy->cpuinfo.max_freq;\n\t\t\tcpufreq_cpu_put(policy);\n\t\t}\n\t\tput_cpu();\n#endif\n\t\tcpufreq_register_notifier(&kvmclock_cpufreq_notifier_block,\n\t\t\t\t\t  CPUFREQ_TRANSITION_NOTIFIER);\n\t}\n\n\tcpuhp_setup_state(CPUHP_AP_X86_KVM_CLK_ONLINE, \"x86/kvm/clk:online\",\n\t\t\t  kvmclock_cpu_online, kvmclock_cpu_down_prep);\n}\n\nDEFINE_PER_CPU(struct kvm_vcpu *, current_vcpu);\nEXPORT_PER_CPU_SYMBOL_GPL(current_vcpu);\n\nint kvm_is_in_guest(void)\n{\n\treturn __this_cpu_read(current_vcpu) != NULL;\n}\n\nstatic int kvm_is_user_mode(void)\n{\n\tint user_mode = 3;\n\n\tif (__this_cpu_read(current_vcpu))\n\t\tuser_mode = kvm_x86_ops.get_cpl(__this_cpu_read(current_vcpu));\n\n\treturn user_mode != 0;\n}\n\nstatic unsigned long kvm_get_guest_ip(void)\n{\n\tunsigned long ip = 0;\n\n\tif (__this_cpu_read(current_vcpu))\n\t\tip = kvm_rip_read(__this_cpu_read(current_vcpu));\n\n\treturn ip;\n}\n\nstatic void kvm_handle_intel_pt_intr(void)\n{\n\tstruct kvm_vcpu *vcpu = __this_cpu_read(current_vcpu);\n\n\tkvm_make_request(KVM_REQ_PMI, vcpu);\n\t__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,\n\t\t\t(unsigned long *)&vcpu->arch.pmu.global_status);\n}\n\nstatic struct perf_guest_info_callbacks kvm_guest_cbs = {\n\t.is_in_guest\t\t= kvm_is_in_guest,\n\t.is_user_mode\t\t= kvm_is_user_mode,\n\t.get_guest_ip\t\t= kvm_get_guest_ip,\n\t.handle_intel_pt_intr\t= kvm_handle_intel_pt_intr,\n};\n\n#ifdef CONFIG_X86_64\nstatic void pvclock_gtod_update_fn(struct work_struct *work)\n{\n\tstruct kvm *kvm;\n\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\tmutex_lock(&kvm_lock);\n\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\tatomic_set(&kvm_guest_has_master_clock, 0);\n\tmutex_unlock(&kvm_lock);\n}\n\nstatic DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);\n\n/*\n * Notification about pvclock gtod data update.\n */\nstatic int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,\n\t\t\t       void *priv)\n{\n\tstruct pvclock_gtod_data *gtod = &pvclock_gtod_data;\n\tstruct timekeeper *tk = priv;\n\n\tupdate_pvclock_gtod(tk);\n\n\t/* disable master clock if host does not trust, or does not\n\t * use, TSC based clocksource.\n\t */\n\tif (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&\n\t    atomic_read(&kvm_guest_has_master_clock) != 0)\n\t\tqueue_work(system_long_wq, &pvclock_gtod_work);\n\n\treturn 0;\n}\n\nstatic struct notifier_block pvclock_gtod_notifier = {\n\t.notifier_call = pvclock_gtod_notify,\n};\n#endif\n\nint kvm_arch_init(void *opaque)\n{\n\tstruct kvm_x86_init_ops *ops = opaque;\n\tint r;\n\n\tif (kvm_x86_ops.hardware_enable) {\n\t\tprintk(KERN_ERR \"kvm: already loaded the other module\\n\");\n\t\tr = -EEXIST;\n\t\tgoto out;\n\t}\n\n\tif (!ops->cpu_has_kvm_support()) {\n\t\tpr_err_ratelimited(\"kvm: no hardware support\\n\");\n\t\tr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\tif (ops->disabled_by_bios()) {\n\t\tpr_err_ratelimited(\"kvm: disabled by bios\\n\");\n\t\tr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * KVM explicitly assumes that the guest has an FPU and\n\t * FXSAVE/FXRSTOR. For example, the KVM_GET_FPU explicitly casts the\n\t * vCPU's FPU state as a fxregs_state struct.\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_FPU) || !boot_cpu_has(X86_FEATURE_FXSR)) {\n\t\tprintk(KERN_ERR \"kvm: inadequate fpu\\n\");\n\t\tr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tr = -ENOMEM;\n\tx86_fpu_cache = kmem_cache_create(\"x86_fpu\", sizeof(struct fpu),\n\t\t\t\t\t  __alignof__(struct fpu), SLAB_ACCOUNT,\n\t\t\t\t\t  NULL);\n\tif (!x86_fpu_cache) {\n\t\tprintk(KERN_ERR \"kvm: failed to allocate cache for x86 fpu\\n\");\n\t\tgoto out;\n\t}\n\n\tx86_emulator_cache = kvm_alloc_emulator_cache();\n\tif (!x86_emulator_cache) {\n\t\tpr_err(\"kvm: failed to allocate cache for x86 emulator\\n\");\n\t\tgoto out_free_x86_fpu_cache;\n\t}\n\n\tuser_return_msrs = alloc_percpu(struct kvm_user_return_msrs);\n\tif (!user_return_msrs) {\n\t\tprintk(KERN_ERR \"kvm: failed to allocate percpu kvm_user_return_msrs\\n\");\n\t\tgoto out_free_x86_emulator_cache;\n\t}\n\n\tr = kvm_mmu_module_init();\n\tif (r)\n\t\tgoto out_free_percpu;\n\n\tkvm_mmu_set_mask_ptes(PT_USER_MASK, PT_ACCESSED_MASK,\n\t\t\tPT_DIRTY_MASK, PT64_NX_MASK, 0,\n\t\t\tPT_PRESENT_MASK, 0, sme_me_mask);\n\tkvm_timer_init();\n\n\tperf_register_guest_info_callbacks(&kvm_guest_cbs);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\thost_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\n\t\tsupported_xcr0 = host_xcr0 & KVM_SUPPORTED_XCR0;\n\t}\n\n\tkvm_lapic_init();\n\tif (pi_inject_timer == -1)\n\t\tpi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);\n#ifdef CONFIG_X86_64\n\tpvclock_gtod_register_notifier(&pvclock_gtod_notifier);\n\n\tif (hypervisor_is_type(X86_HYPER_MS_HYPERV))\n\t\tset_hv_tscchange_cb(kvm_hyperv_tsc_notifier);\n#endif\n\n\treturn 0;\n\nout_free_percpu:\n\tfree_percpu(user_return_msrs);\nout_free_x86_emulator_cache:\n\tkmem_cache_destroy(x86_emulator_cache);\nout_free_x86_fpu_cache:\n\tkmem_cache_destroy(x86_fpu_cache);\nout:\n\treturn r;\n}\n\nvoid kvm_arch_exit(void)\n{\n#ifdef CONFIG_X86_64\n\tif (hypervisor_is_type(X86_HYPER_MS_HYPERV))\n\t\tclear_hv_tscchange_cb();\n#endif\n\tkvm_lapic_exit();\n\tperf_unregister_guest_info_callbacks(&kvm_guest_cbs);\n\n\tif (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC))\n\t\tcpufreq_unregister_notifier(&kvmclock_cpufreq_notifier_block,\n\t\t\t\t\t    CPUFREQ_TRANSITION_NOTIFIER);\n\tcpuhp_remove_state_nocalls(CPUHP_AP_X86_KVM_CLK_ONLINE);\n#ifdef CONFIG_X86_64\n\tpvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);\n#endif\n\tkvm_x86_ops.hardware_enable = NULL;\n\tkvm_mmu_module_exit();\n\tfree_percpu(user_return_msrs);\n\tkmem_cache_destroy(x86_fpu_cache);\n}\n\nint kvm_vcpu_halt(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.halt_exits;\n\tif (lapic_in_kernel(vcpu)) {\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_HALTED;\n\t\treturn 1;\n\t} else {\n\t\tvcpu->run->exit_reason = KVM_EXIT_HLT;\n\t\treturn 0;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_halt);\n\nint kvm_emulate_halt(struct kvm_vcpu *vcpu)\n{\n\tint ret = kvm_skip_emulated_instruction(vcpu);\n\t/*\n\t * TODO: we might be squashing a GUESTDBG_SINGLESTEP-triggered\n\t * KVM_EXIT_DEBUG here.\n\t */\n\treturn kvm_vcpu_halt(vcpu) && ret;\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_halt);\n\n#ifdef CONFIG_X86_64\nstatic int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,\n\t\t\t        unsigned long clock_type)\n{\n\tstruct kvm_clock_pairing clock_pairing;\n\tstruct timespec64 ts;\n\tu64 cycle;\n\tint ret;\n\n\tif (clock_type != KVM_CLOCK_PAIRING_WALLCLOCK)\n\t\treturn -KVM_EOPNOTSUPP;\n\n\tif (kvm_get_walltime_and_clockread(&ts, &cycle) == false)\n\t\treturn -KVM_EOPNOTSUPP;\n\n\tclock_pairing.sec = ts.tv_sec;\n\tclock_pairing.nsec = ts.tv_nsec;\n\tclock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);\n\tclock_pairing.flags = 0;\n\tmemset(&clock_pairing.pad, 0, sizeof(clock_pairing.pad));\n\n\tret = 0;\n\tif (kvm_write_guest(vcpu->kvm, paddr, &clock_pairing,\n\t\t\t    sizeof(struct kvm_clock_pairing)))\n\t\tret = -KVM_EFAULT;\n\n\treturn ret;\n}\n#endif\n\n/*\n * kvm_pv_kick_cpu_op:  Kick a vcpu.\n *\n * @apicid - apicid of vcpu to be kicked.\n */\nstatic void kvm_pv_kick_cpu_op(struct kvm *kvm, unsigned long flags, int apicid)\n{\n\tstruct kvm_lapic_irq lapic_irq;\n\n\tlapic_irq.shorthand = APIC_DEST_NOSHORT;\n\tlapic_irq.dest_mode = APIC_DEST_PHYSICAL;\n\tlapic_irq.level = 0;\n\tlapic_irq.dest_id = apicid;\n\tlapic_irq.msi_redir_hint = false;\n\n\tlapic_irq.delivery_mode = APIC_DM_REMRD;\n\tkvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);\n}\n\nbool kvm_apicv_activated(struct kvm *kvm)\n{\n\treturn (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);\n}\nEXPORT_SYMBOL_GPL(kvm_apicv_activated);\n\nvoid kvm_apicv_init(struct kvm *kvm, bool enable)\n{\n\tif (enable)\n\t\tclear_bit(APICV_INHIBIT_REASON_DISABLE,\n\t\t\t  &kvm->arch.apicv_inhibit_reasons);\n\telse\n\t\tset_bit(APICV_INHIBIT_REASON_DISABLE,\n\t\t\t&kvm->arch.apicv_inhibit_reasons);\n}\nEXPORT_SYMBOL_GPL(kvm_apicv_init);\n\nstatic void kvm_sched_yield(struct kvm *kvm, unsigned long dest_id)\n{\n\tstruct kvm_vcpu *target = NULL;\n\tstruct kvm_apic_map *map;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tif (likely(map) && dest_id <= map->max_apic_id && map->phys_map[dest_id])\n\t\ttarget = map->phys_map[dest_id]->vcpu;\n\n\trcu_read_unlock();\n\n\tif (target && READ_ONCE(target->ready))\n\t\tkvm_vcpu_yield_to(target);\n}\n\nint kvm_emulate_hypercall(struct kvm_vcpu *vcpu)\n{\n\tunsigned long nr, a0, a1, a2, a3, ret;\n\tint op_64_bit;\n\n\tif (kvm_hv_hypercall_enabled(vcpu->kvm))\n\t\treturn kvm_hv_hypercall(vcpu);\n\n\tnr = kvm_rax_read(vcpu);\n\ta0 = kvm_rbx_read(vcpu);\n\ta1 = kvm_rcx_read(vcpu);\n\ta2 = kvm_rdx_read(vcpu);\n\ta3 = kvm_rsi_read(vcpu);\n\n\ttrace_kvm_hypercall(nr, a0, a1, a2, a3);\n\n\top_64_bit = is_64_bit_mode(vcpu);\n\tif (!op_64_bit) {\n\t\tnr &= 0xFFFFFFFF;\n\t\ta0 &= 0xFFFFFFFF;\n\t\ta1 &= 0xFFFFFFFF;\n\t\ta2 &= 0xFFFFFFFF;\n\t\ta3 &= 0xFFFFFFFF;\n\t}\n\n\tif (kvm_x86_ops.get_cpl(vcpu) != 0) {\n\t\tret = -KVM_EPERM;\n\t\tgoto out;\n\t}\n\n\tret = -KVM_ENOSYS;\n\n\tswitch (nr) {\n\tcase KVM_HC_VAPIC_POLL_IRQ:\n\t\tret = 0;\n\t\tbreak;\n\tcase KVM_HC_KICK_CPU:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_UNHALT))\n\t\t\tbreak;\n\n\t\tkvm_pv_kick_cpu_op(vcpu->kvm, a0, a1);\n\t\tkvm_sched_yield(vcpu->kvm, a1);\n\t\tret = 0;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase KVM_HC_CLOCK_PAIRING:\n\t\tret = kvm_pv_clock_pairing(vcpu, a0, a1);\n\t\tbreak;\n#endif\n\tcase KVM_HC_SEND_IPI:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_SEND_IPI))\n\t\t\tbreak;\n\n\t\tret = kvm_pv_send_ipi(vcpu->kvm, a0, a1, a2, a3, op_64_bit);\n\t\tbreak;\n\tcase KVM_HC_SCHED_YIELD:\n\t\tif (!guest_pv_has(vcpu, KVM_FEATURE_PV_SCHED_YIELD))\n\t\t\tbreak;\n\n\t\tkvm_sched_yield(vcpu->kvm, a0);\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -KVM_ENOSYS;\n\t\tbreak;\n\t}\nout:\n\tif (!op_64_bit)\n\t\tret = (u32)ret;\n\tkvm_rax_write(vcpu, ret);\n\n\t++vcpu->stat.hypercalls;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_hypercall);\n\nstatic int emulator_fix_hypercall(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);\n\tchar instruction[3];\n\tunsigned long rip = kvm_rip_read(vcpu);\n\n\tkvm_x86_ops.patch_hypercall(vcpu, instruction);\n\n\treturn emulator_write_emulated(ctxt, rip, instruction, 3,\n\t\t&ctxt->exception);\n}\n\nstatic int dm_request_for_irq_injection(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->run->request_interrupt_window &&\n\t\tlikely(!pic_in_kernel(vcpu->kvm));\n}\n\nstatic void post_kvm_run_save(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tkvm_run->if_flag = (kvm_get_rflags(vcpu) & X86_EFLAGS_IF) != 0;\n\tkvm_run->flags = is_smm(vcpu) ? KVM_RUN_X86_SMM : 0;\n\tkvm_run->cr8 = kvm_get_cr8(vcpu);\n\tkvm_run->apic_base = kvm_get_apic_base(vcpu);\n\tkvm_run->ready_for_interrupt_injection =\n\t\tpic_in_kernel(vcpu->kvm) ||\n\t\tkvm_vcpu_ready_for_interrupt_injection(vcpu);\n}\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu)\n{\n\tint max_irr, tpr;\n\n\tif (!kvm_x86_ops.update_cr8_intercept)\n\t\treturn;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tif (vcpu->arch.apicv_active)\n\t\treturn;\n\n\tif (!vcpu->arch.apic->vapic_addr)\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\telse\n\t\tmax_irr = -1;\n\n\tif (max_irr != -1)\n\t\tmax_irr >>= 4;\n\n\ttpr = kvm_lapic_get_cr8(vcpu);\n\n\tkvm_x86_ops.update_cr8_intercept(vcpu, tpr, max_irr);\n}\n\nstatic void inject_pending_event(struct kvm_vcpu *vcpu, bool *req_immediate_exit)\n{\n\tint r;\n\tbool can_inject = true;\n\n\t/* try to reinject previous events if any */\n\n\tif (vcpu->arch.exception.injected) {\n\t\tkvm_x86_ops.queue_exception(vcpu);\n\t\tcan_inject = false;\n\t}\n\t/*\n\t * Do not inject an NMI or interrupt if there is a pending\n\t * exception.  Exceptions and interrupts are recognized at\n\t * instruction boundaries, i.e. the start of an instruction.\n\t * Trap-like exceptions, e.g. #DB, have higher priority than\n\t * NMIs and interrupts, i.e. traps are recognized before an\n\t * NMI/interrupt that's pending on the same instruction.\n\t * Fault-like exceptions, e.g. #GP and #PF, are the lowest\n\t * priority, but are only generated (pended) during instruction\n\t * execution, i.e. a pending fault-like exception means the\n\t * fault occurred on the *previous* instruction and must be\n\t * serviced prior to recognizing any new events in order to\n\t * fully complete the previous instruction.\n\t */\n\telse if (!vcpu->arch.exception.pending) {\n\t\tif (vcpu->arch.nmi_injected) {\n\t\t\tkvm_x86_ops.set_nmi(vcpu);\n\t\t\tcan_inject = false;\n\t\t} else if (vcpu->arch.interrupt.injected) {\n\t\t\tkvm_x86_ops.set_irq(vcpu);\n\t\t\tcan_inject = false;\n\t\t}\n\t}\n\n\tWARN_ON_ONCE(vcpu->arch.exception.injected &&\n\t\t     vcpu->arch.exception.pending);\n\n\t/*\n\t * Call check_nested_events() even if we reinjected a previous event\n\t * in order for caller to determine if it should require immediate-exit\n\t * from L2 to L1 due to pending L1 events which require exit\n\t * from L2 to L1.\n\t */\n\tif (is_guest_mode(vcpu)) {\n\t\tr = kvm_x86_ops.nested_ops->check_events(vcpu);\n\t\tif (r < 0)\n\t\t\tgoto busy;\n\t}\n\n\t/* try to inject new event if pending */\n\tif (vcpu->arch.exception.pending) {\n\t\ttrace_kvm_inj_exception(vcpu->arch.exception.nr,\n\t\t\t\t\tvcpu->arch.exception.has_error_code,\n\t\t\t\t\tvcpu->arch.exception.error_code);\n\n\t\tvcpu->arch.exception.pending = false;\n\t\tvcpu->arch.exception.injected = true;\n\n\t\tif (exception_type(vcpu->arch.exception.nr) == EXCPT_FAULT)\n\t\t\t__kvm_set_rflags(vcpu, kvm_get_rflags(vcpu) |\n\t\t\t\t\t     X86_EFLAGS_RF);\n\n\t\tif (vcpu->arch.exception.nr == DB_VECTOR) {\n\t\t\tkvm_deliver_exception_payload(vcpu);\n\t\t\tif (vcpu->arch.dr7 & DR7_GD) {\n\t\t\t\tvcpu->arch.dr7 &= ~DR7_GD;\n\t\t\t\tkvm_update_dr7(vcpu);\n\t\t\t}\n\t\t}\n\n\t\tkvm_x86_ops.queue_exception(vcpu);\n\t\tcan_inject = false;\n\t}\n\n\t/*\n\t * Finally, inject interrupt events.  If an event cannot be injected\n\t * due to architectural conditions (e.g. IF=0) a window-open exit\n\t * will re-request KVM_REQ_EVENT.  Sometimes however an event is pending\n\t * and can architecturally be injected, but we cannot do it right now:\n\t * an interrupt could have arrived just now and we have to inject it\n\t * as a vmexit, or there could already an event in the queue, which is\n\t * indicated by can_inject.  In that case we request an immediate exit\n\t * in order to make progress and get back here for another iteration.\n\t * The kvm_x86_ops hooks communicate this by returning -EBUSY.\n\t */\n\tif (vcpu->arch.smi_pending) {\n\t\tr = can_inject ? kvm_x86_ops.smi_allowed(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto busy;\n\t\tif (r) {\n\t\t\tvcpu->arch.smi_pending = false;\n\t\t\t++vcpu->arch.smi_count;\n\t\t\tenter_smm(vcpu);\n\t\t\tcan_inject = false;\n\t\t} else\n\t\t\tkvm_x86_ops.enable_smi_window(vcpu);\n\t}\n\n\tif (vcpu->arch.nmi_pending) {\n\t\tr = can_inject ? kvm_x86_ops.nmi_allowed(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto busy;\n\t\tif (r) {\n\t\t\t--vcpu->arch.nmi_pending;\n\t\t\tvcpu->arch.nmi_injected = true;\n\t\t\tkvm_x86_ops.set_nmi(vcpu);\n\t\t\tcan_inject = false;\n\t\t\tWARN_ON(kvm_x86_ops.nmi_allowed(vcpu, true) < 0);\n\t\t}\n\t\tif (vcpu->arch.nmi_pending)\n\t\t\tkvm_x86_ops.enable_nmi_window(vcpu);\n\t}\n\n\tif (kvm_cpu_has_injectable_intr(vcpu)) {\n\t\tr = can_inject ? kvm_x86_ops.interrupt_allowed(vcpu, true) : -EBUSY;\n\t\tif (r < 0)\n\t\t\tgoto busy;\n\t\tif (r) {\n\t\t\tkvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu), false);\n\t\t\tkvm_x86_ops.set_irq(vcpu);\n\t\t\tWARN_ON(kvm_x86_ops.interrupt_allowed(vcpu, true) < 0);\n\t\t}\n\t\tif (kvm_cpu_has_injectable_intr(vcpu))\n\t\t\tkvm_x86_ops.enable_irq_window(vcpu);\n\t}\n\n\tif (is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->hv_timer_pending &&\n\t    kvm_x86_ops.nested_ops->hv_timer_pending(vcpu))\n\t\t*req_immediate_exit = true;\n\n\tWARN_ON(vcpu->arch.exception.pending);\n\treturn;\n\nbusy:\n\t*req_immediate_exit = true;\n\treturn;\n}\n\nstatic void process_nmi(struct kvm_vcpu *vcpu)\n{\n\tunsigned limit = 2;\n\n\t/*\n\t * x86 is limited to one NMI running, and one NMI pending after it.\n\t * If an NMI is already in progress, limit further NMIs to just one.\n\t * Otherwise, allow two (and we'll inject the first one immediately).\n\t */\n\tif (kvm_x86_ops.get_nmi_mask(vcpu) || vcpu->arch.nmi_injected)\n\t\tlimit = 1;\n\n\tvcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);\n\tvcpu->arch.nmi_pending = min(vcpu->arch.nmi_pending, limit);\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nstatic u32 enter_smm_get_segment_flags(struct kvm_segment *seg)\n{\n\tu32 flags = 0;\n\tflags |= seg->g       << 23;\n\tflags |= seg->db      << 22;\n\tflags |= seg->l       << 21;\n\tflags |= seg->avl     << 20;\n\tflags |= seg->present << 15;\n\tflags |= seg->dpl     << 13;\n\tflags |= seg->s       << 12;\n\tflags |= seg->type    << 8;\n\treturn flags;\n}\n\nstatic void enter_smm_save_seg_32(struct kvm_vcpu *vcpu, char *buf, int n)\n{\n\tstruct kvm_segment seg;\n\tint offset;\n\n\tkvm_get_segment(vcpu, &seg, n);\n\tput_smstate(u32, buf, 0x7fa8 + n * 4, seg.selector);\n\n\tif (n < 3)\n\t\toffset = 0x7f84 + n * 12;\n\telse\n\t\toffset = 0x7f2c + (n - 3) * 12;\n\n\tput_smstate(u32, buf, offset + 8, seg.base);\n\tput_smstate(u32, buf, offset + 4, seg.limit);\n\tput_smstate(u32, buf, offset, enter_smm_get_segment_flags(&seg));\n}\n\n#ifdef CONFIG_X86_64\nstatic void enter_smm_save_seg_64(struct kvm_vcpu *vcpu, char *buf, int n)\n{\n\tstruct kvm_segment seg;\n\tint offset;\n\tu16 flags;\n\n\tkvm_get_segment(vcpu, &seg, n);\n\toffset = 0x7e00 + n * 16;\n\n\tflags = enter_smm_get_segment_flags(&seg) >> 8;\n\tput_smstate(u16, buf, offset, seg.selector);\n\tput_smstate(u16, buf, offset + 2, flags);\n\tput_smstate(u32, buf, offset + 4, seg.limit);\n\tput_smstate(u64, buf, offset + 8, seg.base);\n}\n#endif\n\nstatic void enter_smm_save_state_32(struct kvm_vcpu *vcpu, char *buf)\n{\n\tstruct desc_ptr dt;\n\tstruct kvm_segment seg;\n\tunsigned long val;\n\tint i;\n\n\tput_smstate(u32, buf, 0x7ffc, kvm_read_cr0(vcpu));\n\tput_smstate(u32, buf, 0x7ff8, kvm_read_cr3(vcpu));\n\tput_smstate(u32, buf, 0x7ff4, kvm_get_rflags(vcpu));\n\tput_smstate(u32, buf, 0x7ff0, kvm_rip_read(vcpu));\n\n\tfor (i = 0; i < 8; i++)\n\t\tput_smstate(u32, buf, 0x7fd0 + i * 4, kvm_register_read(vcpu, i));\n\n\tkvm_get_dr(vcpu, 6, &val);\n\tput_smstate(u32, buf, 0x7fcc, (u32)val);\n\tkvm_get_dr(vcpu, 7, &val);\n\tput_smstate(u32, buf, 0x7fc8, (u32)val);\n\n\tkvm_get_segment(vcpu, &seg, VCPU_SREG_TR);\n\tput_smstate(u32, buf, 0x7fc4, seg.selector);\n\tput_smstate(u32, buf, 0x7f64, seg.base);\n\tput_smstate(u32, buf, 0x7f60, seg.limit);\n\tput_smstate(u32, buf, 0x7f5c, enter_smm_get_segment_flags(&seg));\n\n\tkvm_get_segment(vcpu, &seg, VCPU_SREG_LDTR);\n\tput_smstate(u32, buf, 0x7fc0, seg.selector);\n\tput_smstate(u32, buf, 0x7f80, seg.base);\n\tput_smstate(u32, buf, 0x7f7c, seg.limit);\n\tput_smstate(u32, buf, 0x7f78, enter_smm_get_segment_flags(&seg));\n\n\tkvm_x86_ops.get_gdt(vcpu, &dt);\n\tput_smstate(u32, buf, 0x7f74, dt.address);\n\tput_smstate(u32, buf, 0x7f70, dt.size);\n\n\tkvm_x86_ops.get_idt(vcpu, &dt);\n\tput_smstate(u32, buf, 0x7f58, dt.address);\n\tput_smstate(u32, buf, 0x7f54, dt.size);\n\n\tfor (i = 0; i < 6; i++)\n\t\tenter_smm_save_seg_32(vcpu, buf, i);\n\n\tput_smstate(u32, buf, 0x7f14, kvm_read_cr4(vcpu));\n\n\t/* revision id */\n\tput_smstate(u32, buf, 0x7efc, 0x00020000);\n\tput_smstate(u32, buf, 0x7ef8, vcpu->arch.smbase);\n}\n\n#ifdef CONFIG_X86_64\nstatic void enter_smm_save_state_64(struct kvm_vcpu *vcpu, char *buf)\n{\n\tstruct desc_ptr dt;\n\tstruct kvm_segment seg;\n\tunsigned long val;\n\tint i;\n\n\tfor (i = 0; i < 16; i++)\n\t\tput_smstate(u64, buf, 0x7ff8 - i * 8, kvm_register_read(vcpu, i));\n\n\tput_smstate(u64, buf, 0x7f78, kvm_rip_read(vcpu));\n\tput_smstate(u32, buf, 0x7f70, kvm_get_rflags(vcpu));\n\n\tkvm_get_dr(vcpu, 6, &val);\n\tput_smstate(u64, buf, 0x7f68, val);\n\tkvm_get_dr(vcpu, 7, &val);\n\tput_smstate(u64, buf, 0x7f60, val);\n\n\tput_smstate(u64, buf, 0x7f58, kvm_read_cr0(vcpu));\n\tput_smstate(u64, buf, 0x7f50, kvm_read_cr3(vcpu));\n\tput_smstate(u64, buf, 0x7f48, kvm_read_cr4(vcpu));\n\n\tput_smstate(u32, buf, 0x7f00, vcpu->arch.smbase);\n\n\t/* revision id */\n\tput_smstate(u32, buf, 0x7efc, 0x00020064);\n\n\tput_smstate(u64, buf, 0x7ed0, vcpu->arch.efer);\n\n\tkvm_get_segment(vcpu, &seg, VCPU_SREG_TR);\n\tput_smstate(u16, buf, 0x7e90, seg.selector);\n\tput_smstate(u16, buf, 0x7e92, enter_smm_get_segment_flags(&seg) >> 8);\n\tput_smstate(u32, buf, 0x7e94, seg.limit);\n\tput_smstate(u64, buf, 0x7e98, seg.base);\n\n\tkvm_x86_ops.get_idt(vcpu, &dt);\n\tput_smstate(u32, buf, 0x7e84, dt.size);\n\tput_smstate(u64, buf, 0x7e88, dt.address);\n\n\tkvm_get_segment(vcpu, &seg, VCPU_SREG_LDTR);\n\tput_smstate(u16, buf, 0x7e70, seg.selector);\n\tput_smstate(u16, buf, 0x7e72, enter_smm_get_segment_flags(&seg) >> 8);\n\tput_smstate(u32, buf, 0x7e74, seg.limit);\n\tput_smstate(u64, buf, 0x7e78, seg.base);\n\n\tkvm_x86_ops.get_gdt(vcpu, &dt);\n\tput_smstate(u32, buf, 0x7e64, dt.size);\n\tput_smstate(u64, buf, 0x7e68, dt.address);\n\n\tfor (i = 0; i < 6; i++)\n\t\tenter_smm_save_seg_64(vcpu, buf, i);\n}\n#endif\n\nstatic void enter_smm(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs, ds;\n\tstruct desc_ptr dt;\n\tchar buf[512];\n\tu32 cr0;\n\n\ttrace_kvm_enter_smm(vcpu->vcpu_id, vcpu->arch.smbase, true);\n\tmemset(buf, 0, 512);\n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\tenter_smm_save_state_64(vcpu, buf);\n\telse\n#endif\n\t\tenter_smm_save_state_32(vcpu, buf);\n\n\t/*\n\t * Give pre_enter_smm() a chance to make ISA-specific changes to the\n\t * vCPU state (e.g. leave guest mode) after we've saved the state into\n\t * the SMM state-save area.\n\t */\n\tkvm_x86_ops.pre_enter_smm(vcpu, buf);\n\n\tvcpu->arch.hflags |= HF_SMM_MASK;\n\tkvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, buf, sizeof(buf));\n\n\tif (kvm_x86_ops.get_nmi_mask(vcpu))\n\t\tvcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;\n\telse\n\t\tkvm_x86_ops.set_nmi_mask(vcpu, true);\n\n\tkvm_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tkvm_rip_write(vcpu, 0x8000);\n\n\tcr0 = vcpu->arch.cr0 & ~(X86_CR0_PE | X86_CR0_EM | X86_CR0_TS | X86_CR0_PG);\n\tkvm_x86_ops.set_cr0(vcpu, cr0);\n\tvcpu->arch.cr0 = cr0;\n\n\tkvm_x86_ops.set_cr4(vcpu, 0);\n\n\t/* Undocumented: IDT limit is set to zero on entry to SMM.  */\n\tdt.address = dt.size = 0;\n\tkvm_x86_ops.set_idt(vcpu, &dt);\n\n\t__kvm_set_dr(vcpu, 7, DR7_FIXED_1);\n\n\tcs.selector = (vcpu->arch.smbase >> 4) & 0xffff;\n\tcs.base = vcpu->arch.smbase;\n\n\tds.selector = 0;\n\tds.base = 0;\n\n\tcs.limit    = ds.limit = 0xffffffff;\n\tcs.type     = ds.type = 0x3;\n\tcs.dpl      = ds.dpl = 0;\n\tcs.db       = ds.db = 0;\n\tcs.s        = ds.s = 1;\n\tcs.l        = ds.l = 0;\n\tcs.g        = ds.g = 1;\n\tcs.avl      = ds.avl = 0;\n\tcs.present  = ds.present = 1;\n\tcs.unusable = ds.unusable = 0;\n\tcs.padding  = ds.padding = 0;\n\n\tkvm_set_segment(vcpu, &cs, VCPU_SREG_CS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_DS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_ES);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_FS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_GS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_SS);\n\n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\tkvm_x86_ops.set_efer(vcpu, 0);\n#endif\n\n\tkvm_update_cpuid_runtime(vcpu);\n\tkvm_mmu_reset_context(vcpu);\n}\n\nstatic void process_smi(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.smi_pending = true;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nvoid kvm_make_scan_ioapic_request_mask(struct kvm *kvm,\n\t\t\t\t       unsigned long *vcpu_bitmap)\n{\n\tcpumask_var_t cpus;\n\n\tzalloc_cpumask_var(&cpus, GFP_ATOMIC);\n\n\tkvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC,\n\t\t\t\t    NULL, vcpu_bitmap, cpus);\n\n\tfree_cpumask_var(cpus);\n}\n\nvoid kvm_make_scan_ioapic_request(struct kvm *kvm)\n{\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);\n}\n\nvoid kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tvcpu->arch.apicv_active = kvm_apicv_activated(vcpu->kvm);\n\tkvm_apic_update_apicv(vcpu);\n\tkvm_x86_ops.refresh_apicv_exec_ctrl(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_update_apicv);\n\n/*\n * NOTE: Do not hold any lock prior to calling this.\n *\n * In particular, kvm_request_apicv_update() expects kvm->srcu not to be\n * locked, because it calls __x86_set_memory_region() which does\n * synchronize_srcu(&kvm->srcu).\n */\nvoid kvm_request_apicv_update(struct kvm *kvm, bool activate, ulong bit)\n{\n\tstruct kvm_vcpu *except;\n\tunsigned long old, new, expected;\n\n\tif (!kvm_x86_ops.check_apicv_inhibit_reasons ||\n\t    !kvm_x86_ops.check_apicv_inhibit_reasons(bit))\n\t\treturn;\n\n\told = READ_ONCE(kvm->arch.apicv_inhibit_reasons);\n\tdo {\n\t\texpected = new = old;\n\t\tif (activate)\n\t\t\t__clear_bit(bit, &new);\n\t\telse\n\t\t\t__set_bit(bit, &new);\n\t\tif (new == old)\n\t\t\tbreak;\n\t\told = cmpxchg(&kvm->arch.apicv_inhibit_reasons, expected, new);\n\t} while (old != expected);\n\n\tif (!!old == !!new)\n\t\treturn;\n\n\ttrace_kvm_apicv_update_request(activate, bit);\n\tif (kvm_x86_ops.pre_update_apicv_exec_ctrl)\n\t\tkvm_x86_ops.pre_update_apicv_exec_ctrl(kvm, activate);\n\n\t/*\n\t * Sending request to update APICV for all other vcpus,\n\t * while update the calling vcpu immediately instead of\n\t * waiting for another #VMEXIT to handle the request.\n\t */\n\texcept = kvm_get_running_vcpu();\n\tkvm_make_all_cpus_request_except(kvm, KVM_REQ_APICV_UPDATE,\n\t\t\t\t\t except);\n\tif (except)\n\t\tkvm_vcpu_update_apicv(except);\n}\nEXPORT_SYMBOL_GPL(kvm_request_apicv_update);\n\nstatic void vcpu_scan_ioapic(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_apic_present(vcpu))\n\t\treturn;\n\n\tbitmap_zero(vcpu->arch.ioapic_handled_vectors, 256);\n\n\tif (irqchip_split(vcpu->kvm))\n\t\tkvm_scan_ioapic_routes(vcpu, vcpu->arch.ioapic_handled_vectors);\n\telse {\n\t\tif (vcpu->arch.apicv_active)\n\t\t\tkvm_x86_ops.sync_pir_to_irr(vcpu);\n\t\tif (ioapic_in_kernel(vcpu->kvm))\n\t\t\tkvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);\n\t}\n\n\tif (is_guest_mode(vcpu))\n\t\tvcpu->arch.load_eoi_exitmap_pending = true;\n\telse\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n}\n\nstatic void vcpu_load_eoi_exitmap(struct kvm_vcpu *vcpu)\n{\n\tu64 eoi_exit_bitmap[4];\n\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn;\n\n\tbitmap_or((ulong *)eoi_exit_bitmap, vcpu->arch.ioapic_handled_vectors,\n\t\t  vcpu_to_synic(vcpu)->vec_bitmap, 256);\n\tkvm_x86_ops.load_eoi_exitmap(vcpu, eoi_exit_bitmap);\n}\n\nvoid kvm_arch_mmu_notifier_invalidate_range(struct kvm *kvm,\n\t\t\t\t\t    unsigned long start, unsigned long end)\n{\n\tunsigned long apic_address;\n\n\t/*\n\t * The physical address of apic access page is stored in the VMCS.\n\t * Update it when it becomes invalid.\n\t */\n\tapic_address = gfn_to_hva(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);\n\tif (start <= apic_address && apic_address < end)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);\n}\n\nvoid kvm_vcpu_reload_apic_access_page(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tif (!kvm_x86_ops.set_apic_access_page_addr)\n\t\treturn;\n\n\tkvm_x86_ops.set_apic_access_page_addr(vcpu);\n}\n\nvoid __kvm_request_immediate_exit(struct kvm_vcpu *vcpu)\n{\n\tsmp_send_reschedule(vcpu->cpu);\n}\nEXPORT_SYMBOL_GPL(__kvm_request_immediate_exit);\n\n/*\n * Returns 1 to let vcpu_run() continue the guest execution loop without\n * exiting to the userspace.  Otherwise, the value will be returned to the\n * userspace.\n */\nstatic int vcpu_enter_guest(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\tbool req_int_win =\n\t\tdm_request_for_irq_injection(vcpu) &&\n\t\tkvm_cpu_accept_dm_intr(vcpu);\n\tfastpath_t exit_fastpath;\n\n\tbool req_immediate_exit = false;\n\n\tif (kvm_request_pending(vcpu)) {\n\t\tif (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {\n\t\t\tif (unlikely(!kvm_x86_ops.nested_ops->get_nested_state_pages(vcpu))) {\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_RELOAD, vcpu))\n\t\t\tkvm_mmu_unload(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))\n\t\t\t__kvm_migrate_timers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))\n\t\t\tkvm_gen_update_masterclock(vcpu->kvm);\n\t\tif (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))\n\t\t\tkvm_gen_kvmclock_update(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {\n\t\t\tr = kvm_guest_time_update(vcpu);\n\t\t\tif (unlikely(r))\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_MMU_SYNC, vcpu))\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_LOAD_MMU_PGD, vcpu))\n\t\t\tkvm_mmu_load_pgd(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu)) {\n\t\t\tkvm_vcpu_flush_tlb_all(vcpu);\n\n\t\t\t/* Flushing all ASIDs flushes the current ASID... */\n\t\t\tkvm_clear_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu))\n\t\t\tkvm_vcpu_flush_tlb_current(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_HV_TLB_FLUSH, vcpu))\n\t\t\tkvm_vcpu_flush_tlb_guest(vcpu);\n\n\t\tif (kvm_check_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_TPR_ACCESS;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_TRIPLE_FAULT, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\t\t\tvcpu->mmio_needed = 0;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {\n\t\t\t/* Page is swapped out. Do synthetic halt */\n\t\t\tvcpu->arch.apf.halted = true;\n\t\t\tr = 1;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))\n\t\t\trecord_steal_time(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_SMI, vcpu))\n\t\t\tprocess_smi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_NMI, vcpu))\n\t\t\tprocess_nmi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_PMU, vcpu))\n\t\t\tkvm_pmu_handle_event(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_PMI, vcpu))\n\t\t\tkvm_pmu_deliver_pmi(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {\n\t\t\tBUG_ON(vcpu->arch.pending_ioapic_eoi > 255);\n\t\t\tif (test_bit(vcpu->arch.pending_ioapic_eoi,\n\t\t\t\t     vcpu->arch.ioapic_handled_vectors)) {\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_IOAPIC_EOI;\n\t\t\t\tvcpu->run->eoi.vector =\n\t\t\t\t\t\tvcpu->arch.pending_ioapic_eoi;\n\t\t\t\tr = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_SCAN_IOAPIC, vcpu))\n\t\t\tvcpu_scan_ioapic(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu))\n\t\t\tvcpu_load_eoi_exitmap(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu))\n\t\t\tkvm_vcpu_reload_apic_access_page(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_HV_CRASH, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_CRASH;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_HV_RESET, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_RESET;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tif (kvm_check_request(KVM_REQ_HV_EXIT, vcpu)) {\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_HYPERV;\n\t\t\tvcpu->run->hyperv = vcpu->arch.hyperv.exit;\n\t\t\tr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * KVM_REQ_HV_STIMER has to be processed after\n\t\t * KVM_REQ_CLOCK_UPDATE, because Hyper-V SynIC timers\n\t\t * depend on the guest clock being up-to-date\n\t\t */\n\t\tif (kvm_check_request(KVM_REQ_HV_STIMER, vcpu))\n\t\t\tkvm_hv_process_stimers(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))\n\t\t\tkvm_vcpu_update_apicv(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_APF_READY, vcpu))\n\t\t\tkvm_check_async_pf_completion(vcpu);\n\t\tif (kvm_check_request(KVM_REQ_MSR_FILTER_CHANGED, vcpu))\n\t\t\tkvm_x86_ops.msr_filter_changed(vcpu);\n\t}\n\n\tif (kvm_check_request(KVM_REQ_EVENT, vcpu) || req_int_win) {\n\t\t++vcpu->stat.req_event;\n\t\tkvm_apic_accept_events(vcpu);\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\tr = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tinject_pending_event(vcpu, &req_immediate_exit);\n\t\tif (req_int_win)\n\t\t\tkvm_x86_ops.enable_irq_window(vcpu);\n\n\t\tif (kvm_lapic_enabled(vcpu)) {\n\t\t\tupdate_cr8_intercept(vcpu);\n\t\t\tkvm_lapic_sync_to_vapic(vcpu);\n\t\t}\n\t}\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r)) {\n\t\tgoto cancel_injection;\n\t}\n\n\tpreempt_disable();\n\n\tkvm_x86_ops.prepare_guest_switch(vcpu);\n\n\t/*\n\t * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt\n\t * IPI are then delayed after guest entry, which ensures that they\n\t * result in virtual interrupt delivery.\n\t */\n\tlocal_irq_disable();\n\tvcpu->mode = IN_GUEST_MODE;\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\n\t/*\n\t * 1) We should set ->mode before checking ->requests.  Please see\n\t * the comment in kvm_vcpu_exiting_guest_mode().\n\t *\n\t * 2) For APICv, we should set ->mode before checking PID.ON. This\n\t * pairs with the memory barrier implicit in pi_test_and_set_on\n\t * (see vmx_deliver_posted_interrupt).\n\t *\n\t * 3) This also orders the write to mode from any reads to the page\n\t * tables done while the VCPU is running.  Please see the comment\n\t * in kvm_flush_remote_tlbs.\n\t */\n\tsmp_mb__after_srcu_read_unlock();\n\n\t/*\n\t * This handles the case where a posted interrupt was\n\t * notified with kvm_vcpu_kick.\n\t */\n\tif (kvm_lapic_enabled(vcpu) && vcpu->arch.apicv_active)\n\t\tkvm_x86_ops.sync_pir_to_irr(vcpu);\n\n\tif (kvm_vcpu_exit_request(vcpu)) {\n\t\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\t\tsmp_wmb();\n\t\tlocal_irq_enable();\n\t\tpreempt_enable();\n\t\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\t\tr = 1;\n\t\tgoto cancel_injection;\n\t}\n\n\tif (req_immediate_exit) {\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_x86_ops.request_immediate_exit(vcpu);\n\t}\n\n\ttrace_kvm_entry(vcpu);\n\n\tfpregs_assert_state_consistent();\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tswitch_fpu_return();\n\n\tif (unlikely(vcpu->arch.switch_db_regs)) {\n\t\tset_debugreg(0, 7);\n\t\tset_debugreg(vcpu->arch.eff_db[0], 0);\n\t\tset_debugreg(vcpu->arch.eff_db[1], 1);\n\t\tset_debugreg(vcpu->arch.eff_db[2], 2);\n\t\tset_debugreg(vcpu->arch.eff_db[3], 3);\n\t\tset_debugreg(vcpu->arch.dr6, 6);\n\t\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;\n\t}\n\n\texit_fastpath = kvm_x86_ops.run(vcpu);\n\n\t/*\n\t * Do this here before restoring debug registers on the host.  And\n\t * since we do this before handling the vmexit, a DR access vmexit\n\t * can (a) read the correct value of the debug registers, (b) set\n\t * KVM_DEBUGREG_WONT_EXIT again.\n\t */\n\tif (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)) {\n\t\tWARN_ON(vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP);\n\t\tkvm_x86_ops.sync_dirty_debug_regs(vcpu);\n\t\tkvm_update_dr0123(vcpu);\n\t\tkvm_update_dr7(vcpu);\n\t\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_RELOAD;\n\t}\n\n\t/*\n\t * If the guest has used debug registers, at least dr7\n\t * will be disabled while returning to the host.\n\t * If we don't have active breakpoints in the host, we don't\n\t * care about the messed up debug address registers. But if\n\t * we have some of them active, restore the old state.\n\t */\n\tif (hw_breakpoint_active())\n\t\thw_breakpoint_restore();\n\n\tvcpu->arch.last_vmentry_cpu = vcpu->cpu;\n\tvcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\n\tvcpu->mode = OUTSIDE_GUEST_MODE;\n\tsmp_wmb();\n\n\tkvm_x86_ops.handle_exit_irqoff(vcpu);\n\n\t/*\n\t * Consume any pending interrupts, including the possible source of\n\t * VM-Exit on SVM and any ticks that occur between VM-Exit and now.\n\t * An instruction is required after local_irq_enable() to fully unblock\n\t * interrupts on processors that implement an interrupt shadow, the\n\t * stat.exits increment will do nicely.\n\t */\n\tkvm_before_interrupt(vcpu);\n\tlocal_irq_enable();\n\t++vcpu->stat.exits;\n\tlocal_irq_disable();\n\tkvm_after_interrupt(vcpu);\n\n\tif (lapic_in_kernel(vcpu)) {\n\t\ts64 delta = vcpu->arch.apic->lapic_timer.advance_expire_delta;\n\t\tif (delta != S64_MIN) {\n\t\t\ttrace_kvm_wait_lapic_expire(vcpu->vcpu_id, delta);\n\t\t\tvcpu->arch.apic->lapic_timer.advance_expire_delta = S64_MIN;\n\t\t}\n\t}\n\n\tlocal_irq_enable();\n\tpreempt_enable();\n\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\t/*\n\t * Profile KVM exit RIPs:\n\t */\n\tif (unlikely(prof_on == KVM_PROFILING)) {\n\t\tunsigned long rip = kvm_rip_read(vcpu);\n\t\tprofile_hit(KVM_PROFILING, (void *)rip);\n\t}\n\n\tif (unlikely(vcpu->arch.tsc_always_catchup))\n\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\tif (vcpu->arch.apic_attention)\n\t\tkvm_lapic_sync_from_vapic(vcpu);\n\n\tr = kvm_x86_ops.handle_exit(vcpu, exit_fastpath);\n\treturn r;\n\ncancel_injection:\n\tif (req_immediate_exit)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tkvm_x86_ops.cancel_injection(vcpu);\n\tif (unlikely(vcpu->arch.apic_attention))\n\t\tkvm_lapic_sync_from_vapic(vcpu);\nout:\n\treturn r;\n}\n\nstatic inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_arch_vcpu_runnable(vcpu) &&\n\t    (!kvm_x86_ops.pre_block || kvm_x86_ops.pre_block(vcpu) == 0)) {\n\t\tsrcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);\n\t\tkvm_vcpu_block(vcpu);\n\t\tvcpu->srcu_idx = srcu_read_lock(&kvm->srcu);\n\n\t\tif (kvm_x86_ops.post_block)\n\t\t\tkvm_x86_ops.post_block(vcpu);\n\n\t\tif (!kvm_check_request(KVM_REQ_UNHALT, vcpu))\n\t\t\treturn 1;\n\t}\n\n\tkvm_apic_accept_events(vcpu);\n\tswitch(vcpu->arch.mp_state) {\n\tcase KVM_MP_STATE_HALTED:\n\t\tvcpu->arch.pv.pv_unhalted = false;\n\t\tvcpu->arch.mp_state =\n\t\t\tKVM_MP_STATE_RUNNABLE;\n\t\tfallthrough;\n\tcase KVM_MP_STATE_RUNNABLE:\n\t\tvcpu->arch.apf.halted = false;\n\t\tbreak;\n\tcase KVM_MP_STATE_INIT_RECEIVED:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINTR;\n\t}\n\treturn 1;\n}\n\nstatic inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu))\n\t\tkvm_x86_ops.nested_ops->check_events(vcpu);\n\n\treturn (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&\n\t\t!vcpu->arch.apf.halted);\n}\n\nstatic int vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tvcpu->srcu_idx = srcu_read_lock(&kvm->srcu);\n\tvcpu->arch.l1tf_flush_l1d = true;\n\n\tfor (;;) {\n\t\tif (kvm_vcpu_running(vcpu)) {\n\t\t\tr = vcpu_enter_guest(vcpu);\n\t\t} else {\n\t\t\tr = vcpu_block(kvm, vcpu);\n\t\t}\n\n\t\tif (r <= 0)\n\t\t\tbreak;\n\n\t\tkvm_clear_request(KVM_REQ_PENDING_TIMER, vcpu);\n\t\tif (kvm_cpu_has_pending_timer(vcpu))\n\t\t\tkvm_inject_pending_timer_irqs(vcpu);\n\n\t\tif (dm_request_for_irq_injection(vcpu) &&\n\t\t\tkvm_vcpu_ready_for_interrupt_injection(vcpu)) {\n\t\t\tr = 0;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;\n\t\t\t++vcpu->stat.request_irq_exits;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (__xfer_to_guest_mode_work_pending()) {\n\t\t\tsrcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);\n\t\t\tr = xfer_to_guest_mode_handle_work(vcpu);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tvcpu->srcu_idx = srcu_read_lock(&kvm->srcu);\n\t\t}\n\t}\n\n\tsrcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);\n\n\treturn r;\n}\n\nstatic inline int complete_emulated_io(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\tr = kvm_emulate_instruction(vcpu, EMULTYPE_NO_DECODE);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\treturn r;\n}\n\nstatic int complete_emulated_pio(struct kvm_vcpu *vcpu)\n{\n\tBUG_ON(!vcpu->arch.pio.count);\n\n\treturn complete_emulated_io(vcpu);\n}\n\n/*\n * Implements the following, as a state machine:\n *\n * read:\n *   for each fragment\n *     for each mmio piece in the fragment\n *       write gpa, len\n *       exit\n *       copy data\n *   execute insn\n *\n * write:\n *   for each fragment\n *     for each mmio piece in the fragment\n *       write gpa, len\n *       copy data\n *       exit\n */\nstatic int complete_emulated_mmio(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *run = vcpu->run;\n\tstruct kvm_mmio_fragment *frag;\n\tunsigned len;\n\n\tBUG_ON(!vcpu->mmio_needed);\n\n\t/* Complete previous fragment */\n\tfrag = &vcpu->mmio_fragments[vcpu->mmio_cur_fragment];\n\tlen = min(8u, frag->len);\n\tif (!vcpu->mmio_is_write)\n\t\tmemcpy(frag->data, run->mmio.data, len);\n\n\tif (frag->len <= 8) {\n\t\t/* Switch to the next fragment. */\n\t\tfrag++;\n\t\tvcpu->mmio_cur_fragment++;\n\t} else {\n\t\t/* Go forward to the next mmio piece. */\n\t\tfrag->data += len;\n\t\tfrag->gpa += len;\n\t\tfrag->len -= len;\n\t}\n\n\tif (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {\n\t\tvcpu->mmio_needed = 0;\n\n\t\t/* FIXME: return into emulator if single-stepping.  */\n\t\tif (vcpu->mmio_is_write)\n\t\t\treturn 1;\n\t\tvcpu->mmio_read_completed = 1;\n\t\treturn complete_emulated_io(vcpu);\n\t}\n\n\trun->exit_reason = KVM_EXIT_MMIO;\n\trun->mmio.phys_addr = frag->gpa;\n\tif (vcpu->mmio_is_write)\n\t\tmemcpy(run->mmio.data, frag->data, min(8u, frag->len));\n\trun->mmio.len = min(8u, frag->len);\n\trun->mmio.is_write = vcpu->mmio_is_write;\n\tvcpu->arch.complete_userspace_io = complete_emulated_mmio;\n\treturn 0;\n}\n\nstatic void kvm_save_current_fpu(struct fpu *fpu)\n{\n\t/*\n\t * If the target FPU state is not resident in the CPU registers, just\n\t * memcpy() from current, else save CPU state directly to the target.\n\t */\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tmemcpy(&fpu->state, &current->thread.fpu.state,\n\t\t       fpu_kernel_xstate_size);\n\telse\n\t\tcopy_fpregs_to_fpstate(fpu);\n}\n\n/* Swap (qemu) user FPU context for the guest FPU context. */\nstatic void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)\n{\n\tfpregs_lock();\n\n\tkvm_save_current_fpu(vcpu->arch.user_fpu);\n\n\t/* PKRU is separately restored in kvm_x86_ops.run.  */\n\t__copy_kernel_to_fpregs(&vcpu->arch.guest_fpu->state,\n\t\t\t\t~XFEATURE_MASK_PKRU);\n\n\tfpregs_mark_activate();\n\tfpregs_unlock();\n\n\ttrace_kvm_fpu(1);\n}\n\n/* When vcpu_run ends, restore user space FPU context. */\nstatic void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)\n{\n\tfpregs_lock();\n\n\tkvm_save_current_fpu(vcpu->arch.guest_fpu);\n\n\tcopy_kernel_to_fpregs(&vcpu->arch.user_fpu->state);\n\n\tfpregs_mark_activate();\n\tfpregs_unlock();\n\n\t++vcpu->stat.fpu_reload;\n\ttrace_kvm_fpu(0);\n}\n\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tint r;\n\n\tvcpu_load(vcpu);\n\tkvm_sigset_activate(vcpu);\n\tkvm_load_guest_fpu(vcpu);\n\n\tif (unlikely(vcpu->arch.mp_state == KVM_MP_STATE_UNINITIALIZED)) {\n\t\tif (kvm_run->immediate_exit) {\n\t\t\tr = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t\tkvm_vcpu_block(vcpu);\n\t\tkvm_apic_accept_events(vcpu);\n\t\tkvm_clear_request(KVM_REQ_UNHALT, vcpu);\n\t\tr = -EAGAIN;\n\t\tif (signal_pending(current)) {\n\t\t\tr = -EINTR;\n\t\t\tkvm_run->exit_reason = KVM_EXIT_INTR;\n\t\t\t++vcpu->stat.signal_exits;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (kvm_run->kvm_valid_regs & ~KVM_SYNC_X86_VALID_FIELDS) {\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (kvm_run->kvm_dirty_regs) {\n\t\tr = sync_regs(vcpu);\n\t\tif (r != 0)\n\t\t\tgoto out;\n\t}\n\n\t/* re-sync apic's tpr */\n\tif (!lapic_in_kernel(vcpu)) {\n\t\tif (kvm_set_cr8(vcpu, kvm_run->cr8) != 0) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (unlikely(vcpu->arch.complete_userspace_io)) {\n\t\tint (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;\n\t\tvcpu->arch.complete_userspace_io = NULL;\n\t\tr = cui(vcpu);\n\t\tif (r <= 0)\n\t\t\tgoto out;\n\t} else\n\t\tWARN_ON(vcpu->arch.pio.count || vcpu->mmio_needed);\n\n\tif (kvm_run->immediate_exit)\n\t\tr = -EINTR;\n\telse\n\t\tr = vcpu_run(vcpu);\n\nout:\n\tkvm_put_guest_fpu(vcpu);\n\tif (kvm_run->kvm_valid_regs)\n\t\tstore_regs(vcpu);\n\tpost_kvm_run_save(vcpu);\n\tkvm_sigset_deactivate(vcpu);\n\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\nstatic void __get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tif (vcpu->arch.emulate_regs_need_sync_to_vcpu) {\n\t\t/*\n\t\t * We are here if userspace calls get_regs() in the middle of\n\t\t * instruction emulation. Registers state needs to be copied\n\t\t * back from emulation context to vcpu. Userspace shouldn't do\n\t\t * that usually, but some bad designed PV devices (vmware\n\t\t * backdoor interface) need this to work\n\t\t */\n\t\temulator_writeback_register_cache(vcpu->arch.emulate_ctxt);\n\t\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\t}\n\tregs->rax = kvm_rax_read(vcpu);\n\tregs->rbx = kvm_rbx_read(vcpu);\n\tregs->rcx = kvm_rcx_read(vcpu);\n\tregs->rdx = kvm_rdx_read(vcpu);\n\tregs->rsi = kvm_rsi_read(vcpu);\n\tregs->rdi = kvm_rdi_read(vcpu);\n\tregs->rsp = kvm_rsp_read(vcpu);\n\tregs->rbp = kvm_rbp_read(vcpu);\n#ifdef CONFIG_X86_64\n\tregs->r8 = kvm_r8_read(vcpu);\n\tregs->r9 = kvm_r9_read(vcpu);\n\tregs->r10 = kvm_r10_read(vcpu);\n\tregs->r11 = kvm_r11_read(vcpu);\n\tregs->r12 = kvm_r12_read(vcpu);\n\tregs->r13 = kvm_r13_read(vcpu);\n\tregs->r14 = kvm_r14_read(vcpu);\n\tregs->r15 = kvm_r15_read(vcpu);\n#endif\n\n\tregs->rip = kvm_rip_read(vcpu);\n\tregs->rflags = kvm_get_rflags(vcpu);\n}\n\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu_load(vcpu);\n\t__get_regs(vcpu, regs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void __set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu->arch.emulate_regs_need_sync_from_vcpu = true;\n\tvcpu->arch.emulate_regs_need_sync_to_vcpu = false;\n\n\tkvm_rax_write(vcpu, regs->rax);\n\tkvm_rbx_write(vcpu, regs->rbx);\n\tkvm_rcx_write(vcpu, regs->rcx);\n\tkvm_rdx_write(vcpu, regs->rdx);\n\tkvm_rsi_write(vcpu, regs->rsi);\n\tkvm_rdi_write(vcpu, regs->rdi);\n\tkvm_rsp_write(vcpu, regs->rsp);\n\tkvm_rbp_write(vcpu, regs->rbp);\n#ifdef CONFIG_X86_64\n\tkvm_r8_write(vcpu, regs->r8);\n\tkvm_r9_write(vcpu, regs->r9);\n\tkvm_r10_write(vcpu, regs->r10);\n\tkvm_r11_write(vcpu, regs->r11);\n\tkvm_r12_write(vcpu, regs->r12);\n\tkvm_r13_write(vcpu, regs->r13);\n\tkvm_r14_write(vcpu, regs->r14);\n\tkvm_r15_write(vcpu, regs->r15);\n#endif\n\n\tkvm_rip_write(vcpu, regs->rip);\n\tkvm_set_rflags(vcpu, regs->rflags | X86_EFLAGS_FIXED);\n\n\tvcpu->arch.exception.pending = false;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\tvcpu_load(vcpu);\n\t__set_regs(vcpu, regs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nvoid kvm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)\n{\n\tstruct kvm_segment cs;\n\n\tkvm_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\t*db = cs.db;\n\t*l = cs.l;\n}\nEXPORT_SYMBOL_GPL(kvm_get_cs_db_l_bits);\n\nstatic void __get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tstruct desc_ptr dt;\n\n\tkvm_get_segment(vcpu, &sregs->cs, VCPU_SREG_CS);\n\tkvm_get_segment(vcpu, &sregs->ds, VCPU_SREG_DS);\n\tkvm_get_segment(vcpu, &sregs->es, VCPU_SREG_ES);\n\tkvm_get_segment(vcpu, &sregs->fs, VCPU_SREG_FS);\n\tkvm_get_segment(vcpu, &sregs->gs, VCPU_SREG_GS);\n\tkvm_get_segment(vcpu, &sregs->ss, VCPU_SREG_SS);\n\n\tkvm_get_segment(vcpu, &sregs->tr, VCPU_SREG_TR);\n\tkvm_get_segment(vcpu, &sregs->ldt, VCPU_SREG_LDTR);\n\n\tkvm_x86_ops.get_idt(vcpu, &dt);\n\tsregs->idt.limit = dt.size;\n\tsregs->idt.base = dt.address;\n\tkvm_x86_ops.get_gdt(vcpu, &dt);\n\tsregs->gdt.limit = dt.size;\n\tsregs->gdt.base = dt.address;\n\n\tsregs->cr0 = kvm_read_cr0(vcpu);\n\tsregs->cr2 = vcpu->arch.cr2;\n\tsregs->cr3 = kvm_read_cr3(vcpu);\n\tsregs->cr4 = kvm_read_cr4(vcpu);\n\tsregs->cr8 = kvm_get_cr8(vcpu);\n\tsregs->efer = vcpu->arch.efer;\n\tsregs->apic_base = kvm_get_apic_base(vcpu);\n\n\tmemset(sregs->interrupt_bitmap, 0, sizeof(sregs->interrupt_bitmap));\n\n\tif (vcpu->arch.interrupt.injected && !vcpu->arch.interrupt.soft)\n\t\tset_bit(vcpu->arch.interrupt.nr,\n\t\t\t(unsigned long *)sregs->interrupt_bitmap);\n}\n\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\tvcpu_load(vcpu);\n\t__get_sregs(vcpu, sregs);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\tvcpu_load(vcpu);\n\tif (kvm_mpx_supported())\n\t\tkvm_load_guest_fpu(vcpu);\n\n\tkvm_apic_accept_events(vcpu);\n\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED &&\n\t\t\t\t\tvcpu->arch.pv.pv_unhalted)\n\t\tmp_state->mp_state = KVM_MP_STATE_RUNNABLE;\n\telse\n\t\tmp_state->mp_state = vcpu->arch.mp_state;\n\n\tif (kvm_mpx_supported())\n\t\tkvm_put_guest_fpu(vcpu);\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state)\n{\n\tint ret = -EINVAL;\n\n\tvcpu_load(vcpu);\n\n\tif (!lapic_in_kernel(vcpu) &&\n\t    mp_state->mp_state != KVM_MP_STATE_RUNNABLE)\n\t\tgoto out;\n\n\t/*\n\t * KVM_MP_STATE_INIT_RECEIVED means the processor is in\n\t * INIT state; latched init should be reported using\n\t * KVM_SET_VCPU_EVENTS, so reject it here.\n\t */\n\tif ((kvm_vcpu_latch_init(vcpu) || vcpu->arch.smi_pending) &&\n\t    (mp_state->mp_state == KVM_MP_STATE_SIPI_RECEIVED ||\n\t     mp_state->mp_state == KVM_MP_STATE_INIT_RECEIVED))\n\t\tgoto out;\n\n\tif (mp_state->mp_state == KVM_MP_STATE_SIPI_RECEIVED) {\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t\tset_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);\n\t} else\n\t\tvcpu->arch.mp_state = mp_state->mp_state;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tret = 0;\nout:\n\tvcpu_put(vcpu);\n\treturn ret;\n}\n\nint kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,\n\t\t    int reason, bool has_error_code, u32 error_code)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\tint ret;\n\n\tinit_emulate_ctxt(vcpu);\n\n\tret = emulator_task_switch(ctxt, tss_selector, idt_index, reason,\n\t\t\t\t   has_error_code, error_code);\n\tif (ret) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\tkvm_rip_write(vcpu, ctxt->eip);\n\tkvm_set_rflags(vcpu, ctxt->eflags);\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(kvm_task_switch);\n\nstatic int kvm_valid_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tif ((sregs->efer & EFER_LME) && (sregs->cr0 & X86_CR0_PG)) {\n\t\t/*\n\t\t * When EFER.LME and CR0.PG are set, the processor is in\n\t\t * 64-bit mode (though maybe in a 32-bit code segment).\n\t\t * CR4.PAE and EFER.LMA must be set.\n\t\t */\n\t\tif (!(sregs->cr4 & X86_CR4_PAE)\n\t\t    || !(sregs->efer & EFER_LMA))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\t/*\n\t\t * Not in 64-bit mode: EFER.LMA is clear and the code\n\t\t * segment cannot be 64-bit.\n\t\t */\n\t\tif (sregs->efer & EFER_LMA || sregs->cs.l)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn kvm_valid_cr4(vcpu, sregs->cr4);\n}\n\nstatic int __set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\n{\n\tstruct msr_data apic_base_msr;\n\tint mmu_reset_needed = 0;\n\tint cpuid_update_needed = 0;\n\tint pending_vec, max_bits, idx;\n\tstruct desc_ptr dt;\n\tint ret = -EINVAL;\n\n\tif (kvm_valid_sregs(vcpu, sregs))\n\t\tgoto out;\n\n\tapic_base_msr.data = sregs->apic_base;\n\tapic_base_msr.host_initiated = true;\n\tif (kvm_set_apic_base(vcpu, &apic_base_msr))\n\t\tgoto out;\n\n\tdt.size = sregs->idt.limit;\n\tdt.address = sregs->idt.base;\n\tkvm_x86_ops.set_idt(vcpu, &dt);\n\tdt.size = sregs->gdt.limit;\n\tdt.address = sregs->gdt.base;\n\tkvm_x86_ops.set_gdt(vcpu, &dt);\n\n\tvcpu->arch.cr2 = sregs->cr2;\n\tmmu_reset_needed |= kvm_read_cr3(vcpu) != sregs->cr3;\n\tvcpu->arch.cr3 = sregs->cr3;\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR3);\n\n\tkvm_set_cr8(vcpu, sregs->cr8);\n\n\tmmu_reset_needed |= vcpu->arch.efer != sregs->efer;\n\tkvm_x86_ops.set_efer(vcpu, sregs->efer);\n\n\tmmu_reset_needed |= kvm_read_cr0(vcpu) != sregs->cr0;\n\tkvm_x86_ops.set_cr0(vcpu, sregs->cr0);\n\tvcpu->arch.cr0 = sregs->cr0;\n\n\tmmu_reset_needed |= kvm_read_cr4(vcpu) != sregs->cr4;\n\tcpuid_update_needed |= ((kvm_read_cr4(vcpu) ^ sregs->cr4) &\n\t\t\t\t(X86_CR4_OSXSAVE | X86_CR4_PKE));\n\tkvm_x86_ops.set_cr4(vcpu, sregs->cr4);\n\tif (cpuid_update_needed)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tif (is_pae_paging(vcpu)) {\n\t\tload_pdptrs(vcpu, vcpu->arch.walk_mmu, kvm_read_cr3(vcpu));\n\t\tmmu_reset_needed = 1;\n\t}\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\n\tif (mmu_reset_needed)\n\t\tkvm_mmu_reset_context(vcpu);\n\n\tmax_bits = KVM_NR_INTERRUPTS;\n\tpending_vec = find_first_bit(\n\t\t(const unsigned long *)sregs->interrupt_bitmap, max_bits);\n\tif (pending_vec < max_bits) {\n\t\tkvm_queue_interrupt(vcpu, pending_vec, false);\n\t\tpr_debug(\"Set back pending irq %d\\n\", pending_vec);\n\t}\n\n\tkvm_set_segment(vcpu, &sregs->cs, VCPU_SREG_CS);\n\tkvm_set_segment(vcpu, &sregs->ds, VCPU_SREG_DS);\n\tkvm_set_segment(vcpu, &sregs->es, VCPU_SREG_ES);\n\tkvm_set_segment(vcpu, &sregs->fs, VCPU_SREG_FS);\n\tkvm_set_segment(vcpu, &sregs->gs, VCPU_SREG_GS);\n\tkvm_set_segment(vcpu, &sregs->ss, VCPU_SREG_SS);\n\n\tkvm_set_segment(vcpu, &sregs->tr, VCPU_SREG_TR);\n\tkvm_set_segment(vcpu, &sregs->ldt, VCPU_SREG_LDTR);\n\n\tupdate_cr8_intercept(vcpu);\n\n\t/* Older userspace won't unhalt the vcpu on reset. */\n\tif (kvm_vcpu_is_bsp(vcpu) && kvm_rip_read(vcpu) == 0xfff0 &&\n\t    sregs->cs.selector == 0xf000 && sregs->cs.base == 0xffff0000 &&\n\t    !is_protmode(vcpu))\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\tint ret;\n\n\tvcpu_load(vcpu);\n\tret = __set_sregs(vcpu, sregs);\n\tvcpu_put(vcpu);\n\treturn ret;\n}\n\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\tunsigned long rflags;\n\tint i, r;\n\n\tvcpu_load(vcpu);\n\n\tif (dbg->control & (KVM_GUESTDBG_INJECT_DB | KVM_GUESTDBG_INJECT_BP)) {\n\t\tr = -EBUSY;\n\t\tif (vcpu->arch.exception.pending)\n\t\t\tgoto out;\n\t\tif (dbg->control & KVM_GUESTDBG_INJECT_DB)\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\telse\n\t\t\tkvm_queue_exception(vcpu, BP_VECTOR);\n\t}\n\n\t/*\n\t * Read rflags as long as potentially injected trace flags are still\n\t * filtered out.\n\t */\n\trflags = kvm_get_rflags(vcpu);\n\n\tvcpu->guest_debug = dbg->control;\n\tif (!(vcpu->guest_debug & KVM_GUESTDBG_ENABLE))\n\t\tvcpu->guest_debug = 0;\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; ++i)\n\t\t\tvcpu->arch.eff_db[i] = dbg->arch.debugreg[i];\n\t\tvcpu->arch.guest_debug_dr7 = dbg->arch.debugreg[7];\n\t} else {\n\t\tfor (i = 0; i < KVM_NR_DB_REGS; i++)\n\t\t\tvcpu->arch.eff_db[i] = vcpu->arch.db[i];\n\t}\n\tkvm_update_dr7(vcpu);\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvcpu->arch.singlestep_rip = kvm_rip_read(vcpu) +\n\t\t\tget_segment_base(vcpu, VCPU_SREG_CS);\n\n\t/*\n\t * Trigger an rflags update that will inject or remove the trace\n\t * flags.\n\t */\n\tkvm_set_rflags(vcpu, rflags);\n\n\tkvm_x86_ops.update_exception_bitmap(vcpu);\n\n\tr = 0;\n\nout:\n\tvcpu_put(vcpu);\n\treturn r;\n}\n\n/*\n * Translate a guest virtual address to a guest physical address.\n */\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_translation *tr)\n{\n\tunsigned long vaddr = tr->linear_address;\n\tgpa_t gpa;\n\tint idx;\n\n\tvcpu_load(vcpu);\n\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tgpa = kvm_mmu_gva_to_gpa_system(vcpu, vaddr, NULL);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\ttr->physical_address = gpa;\n\ttr->valid = gpa != UNMAPPED_GVA;\n\ttr->writeable = 1;\n\ttr->usermode = 0;\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\tstruct fxregs_state *fxsave;\n\n\tvcpu_load(vcpu);\n\n\tfxsave = &vcpu->arch.guest_fpu->state.fxsave;\n\tmemcpy(fpu->fpr, fxsave->st_space, 128);\n\tfpu->fcw = fxsave->cwd;\n\tfpu->fsw = fxsave->swd;\n\tfpu->ftwx = fxsave->twd;\n\tfpu->last_opcode = fxsave->fop;\n\tfpu->last_ip = fxsave->rip;\n\tfpu->last_dp = fxsave->rdp;\n\tmemcpy(fpu->xmm, fxsave->xmm_space, sizeof(fxsave->xmm_space));\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\tstruct fxregs_state *fxsave;\n\n\tvcpu_load(vcpu);\n\n\tfxsave = &vcpu->arch.guest_fpu->state.fxsave;\n\n\tmemcpy(fxsave->st_space, fpu->fpr, 128);\n\tfxsave->cwd = fpu->fcw;\n\tfxsave->swd = fpu->fsw;\n\tfxsave->twd = fpu->ftwx;\n\tfxsave->fop = fpu->last_opcode;\n\tfxsave->rip = fpu->last_ip;\n\tfxsave->rdp = fpu->last_dp;\n\tmemcpy(fxsave->xmm_space, fpu->xmm, sizeof(fxsave->xmm_space));\n\n\tvcpu_put(vcpu);\n\treturn 0;\n}\n\nstatic void store_regs(struct kvm_vcpu *vcpu)\n{\n\tBUILD_BUG_ON(sizeof(struct kvm_sync_regs) > SYNC_REGS_SIZE_BYTES);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_REGS)\n\t\t__get_regs(vcpu, &vcpu->run->s.regs.regs);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_SREGS)\n\t\t__get_sregs(vcpu, &vcpu->run->s.regs.sregs);\n\n\tif (vcpu->run->kvm_valid_regs & KVM_SYNC_X86_EVENTS)\n\t\tkvm_vcpu_ioctl_x86_get_vcpu_events(\n\t\t\t\tvcpu, &vcpu->run->s.regs.events);\n}\n\nstatic int sync_regs(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->run->kvm_dirty_regs & ~KVM_SYNC_X86_VALID_FIELDS)\n\t\treturn -EINVAL;\n\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_REGS) {\n\t\t__set_regs(vcpu, &vcpu->run->s.regs.regs);\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_REGS;\n\t}\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_SREGS) {\n\t\tif (__set_sregs(vcpu, &vcpu->run->s.regs.sregs))\n\t\t\treturn -EINVAL;\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_SREGS;\n\t}\n\tif (vcpu->run->kvm_dirty_regs & KVM_SYNC_X86_EVENTS) {\n\t\tif (kvm_vcpu_ioctl_x86_set_vcpu_events(\n\t\t\t\tvcpu, &vcpu->run->s.regs.events))\n\t\t\treturn -EINVAL;\n\t\tvcpu->run->kvm_dirty_regs &= ~KVM_SYNC_X86_EVENTS;\n\t}\n\n\treturn 0;\n}\n\nstatic void fx_init(struct kvm_vcpu *vcpu)\n{\n\tfpstate_init(&vcpu->arch.guest_fpu->state);\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\tvcpu->arch.guest_fpu->state.xsave.header.xcomp_bv =\n\t\t\thost_xcr0 | XSTATE_COMPACTION_ENABLED;\n\n\t/*\n\t * Ensure guest xcr0 is valid for loading\n\t */\n\tvcpu->arch.xcr0 = XFEATURE_MASK_FP;\n\n\tvcpu->arch.cr0 |= X86_CR0_ET;\n}\n\nint kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)\n{\n\tif (kvm_check_tsc_unstable() && atomic_read(&kvm->online_vcpus) != 0)\n\t\tpr_warn_once(\"kvm: SMP vm created on host with unstable TSC; \"\n\t\t\t     \"guest TSC will not be reliable\\n\");\n\n\treturn 0;\n}\n\nint kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)\n{\n\tstruct page *page;\n\tint r;\n\n\tif (!irqchip_in_kernel(vcpu->kvm) || kvm_vcpu_is_reset_bsp(vcpu))\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\telse\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;\n\n\tkvm_set_tsc_khz(vcpu, max_tsc_khz);\n\n\tr = kvm_mmu_create(vcpu);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (irqchip_in_kernel(vcpu->kvm)) {\n\t\tr = kvm_create_lapic(vcpu, lapic_timer_advance_ns);\n\t\tif (r < 0)\n\t\t\tgoto fail_mmu_destroy;\n\t\tif (kvm_apicv_activated(vcpu->kvm))\n\t\t\tvcpu->arch.apicv_active = true;\n\t} else\n\t\tstatic_key_slow_inc(&kvm_no_apic_vcpu);\n\n\tr = -ENOMEM;\n\n\tpage = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!page)\n\t\tgoto fail_free_lapic;\n\tvcpu->arch.pio_data = page_address(page);\n\n\tvcpu->arch.mce_banks = kzalloc(KVM_MAX_MCE_BANKS * sizeof(u64) * 4,\n\t\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!vcpu->arch.mce_banks)\n\t\tgoto fail_free_pio_data;\n\tvcpu->arch.mcg_cap = KVM_MAX_MCE_BANKS;\n\n\tif (!zalloc_cpumask_var(&vcpu->arch.wbinvd_dirty_mask,\n\t\t\t\tGFP_KERNEL_ACCOUNT))\n\t\tgoto fail_free_mce_banks;\n\n\tif (!alloc_emulate_ctxt(vcpu))\n\t\tgoto free_wbinvd_dirty_mask;\n\n\tvcpu->arch.user_fpu = kmem_cache_zalloc(x86_fpu_cache,\n\t\t\t\t\t\tGFP_KERNEL_ACCOUNT);\n\tif (!vcpu->arch.user_fpu) {\n\t\tpr_err(\"kvm: failed to allocate userspace's fpu\\n\");\n\t\tgoto free_emulate_ctxt;\n\t}\n\n\tvcpu->arch.guest_fpu = kmem_cache_zalloc(x86_fpu_cache,\n\t\t\t\t\t\t GFP_KERNEL_ACCOUNT);\n\tif (!vcpu->arch.guest_fpu) {\n\t\tpr_err(\"kvm: failed to allocate vcpu's fpu\\n\");\n\t\tgoto free_user_fpu;\n\t}\n\tfx_init(vcpu);\n\n\tvcpu->arch.maxphyaddr = cpuid_query_maxphyaddr(vcpu);\n\n\tvcpu->arch.pat = MSR_IA32_CR_PAT_DEFAULT;\n\n\tkvm_async_pf_hash_reset(vcpu);\n\tkvm_pmu_init(vcpu);\n\n\tvcpu->arch.pending_external_vector = -1;\n\tvcpu->arch.preempted_in_kernel = false;\n\n\tkvm_hv_vcpu_init(vcpu);\n\n\tr = kvm_x86_ops.vcpu_create(vcpu);\n\tif (r)\n\t\tgoto free_guest_fpu;\n\n\tvcpu->arch.arch_capabilities = kvm_get_arch_capabilities();\n\tvcpu->arch.msr_platform_info = MSR_PLATFORM_INFO_CPUID_FAULT;\n\tkvm_vcpu_mtrr_init(vcpu);\n\tvcpu_load(vcpu);\n\tkvm_vcpu_reset(vcpu, false);\n\tkvm_init_mmu(vcpu, false);\n\tvcpu_put(vcpu);\n\treturn 0;\n\nfree_guest_fpu:\n\tkmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);\nfree_user_fpu:\n\tkmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);\nfree_emulate_ctxt:\n\tkmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);\nfree_wbinvd_dirty_mask:\n\tfree_cpumask_var(vcpu->arch.wbinvd_dirty_mask);\nfail_free_mce_banks:\n\tkfree(vcpu->arch.mce_banks);\nfail_free_pio_data:\n\tfree_page((unsigned long)vcpu->arch.pio_data);\nfail_free_lapic:\n\tkvm_free_lapic(vcpu);\nfail_mmu_destroy:\n\tkvm_mmu_destroy(vcpu);\n\treturn r;\n}\n\nvoid kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tkvm_hv_vcpu_postcreate(vcpu);\n\n\tif (mutex_lock_killable(&vcpu->mutex))\n\t\treturn;\n\tvcpu_load(vcpu);\n\tkvm_synchronize_tsc(vcpu, 0);\n\tvcpu_put(vcpu);\n\n\t/* poll control enabled by default */\n\tvcpu->arch.msr_kvm_poll_control = 1;\n\n\tmutex_unlock(&vcpu->mutex);\n\n\tif (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)\n\t\tschedule_delayed_work(&kvm->arch.kvmclock_sync_work,\n\t\t\t\t\t\tKVMCLOCK_SYNC_PERIOD);\n}\n\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\n{\n\tstruct gfn_to_pfn_cache *cache = &vcpu->arch.st.cache;\n\tint idx;\n\n\tkvm_release_pfn(cache->pfn, cache->dirty, cache);\n\n\tkvmclock_reset(vcpu);\n\n\tkvm_x86_ops.vcpu_free(vcpu);\n\n\tkmem_cache_free(x86_emulator_cache, vcpu->arch.emulate_ctxt);\n\tfree_cpumask_var(vcpu->arch.wbinvd_dirty_mask);\n\tkmem_cache_free(x86_fpu_cache, vcpu->arch.user_fpu);\n\tkmem_cache_free(x86_fpu_cache, vcpu->arch.guest_fpu);\n\n\tkvm_hv_vcpu_uninit(vcpu);\n\tkvm_pmu_destroy(vcpu);\n\tkfree(vcpu->arch.mce_banks);\n\tkvm_free_lapic(vcpu);\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\tkvm_mmu_destroy(vcpu);\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\tfree_page((unsigned long)vcpu->arch.pio_data);\n\tkvfree(vcpu->arch.cpuid_entries);\n\tif (!lapic_in_kernel(vcpu))\n\t\tstatic_key_slow_dec(&kvm_no_apic_vcpu);\n}\n\nvoid kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tkvm_lapic_reset(vcpu, init_event);\n\n\tvcpu->arch.hflags = 0;\n\n\tvcpu->arch.smi_pending = 0;\n\tvcpu->arch.smi_count = 0;\n\tatomic_set(&vcpu->arch.nmi_queued, 0);\n\tvcpu->arch.nmi_pending = 0;\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_interrupt_queue(vcpu);\n\tkvm_clear_exception_queue(vcpu);\n\n\tmemset(vcpu->arch.db, 0, sizeof(vcpu->arch.db));\n\tkvm_update_dr0123(vcpu);\n\tvcpu->arch.dr6 = DR6_INIT;\n\tvcpu->arch.dr7 = DR7_FIXED_1;\n\tkvm_update_dr7(vcpu);\n\n\tvcpu->arch.cr2 = 0;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tvcpu->arch.apf.msr_en_val = 0;\n\tvcpu->arch.apf.msr_int_val = 0;\n\tvcpu->arch.st.msr_val = 0;\n\n\tkvmclock_reset(vcpu);\n\n\tkvm_clear_async_pf_completion_queue(vcpu);\n\tkvm_async_pf_hash_reset(vcpu);\n\tvcpu->arch.apf.halted = false;\n\n\tif (kvm_mpx_supported()) {\n\t\tvoid *mpx_state_buffer;\n\n\t\t/*\n\t\t * To avoid have the INIT path from kvm_apic_has_events() that be\n\t\t * called with loaded FPU and does not let userspace fix the state.\n\t\t */\n\t\tif (init_event)\n\t\t\tkvm_put_guest_fpu(vcpu);\n\t\tmpx_state_buffer = get_xsave_addr(&vcpu->arch.guest_fpu->state.xsave,\n\t\t\t\t\tXFEATURE_BNDREGS);\n\t\tif (mpx_state_buffer)\n\t\t\tmemset(mpx_state_buffer, 0, sizeof(struct mpx_bndreg_state));\n\t\tmpx_state_buffer = get_xsave_addr(&vcpu->arch.guest_fpu->state.xsave,\n\t\t\t\t\tXFEATURE_BNDCSR);\n\t\tif (mpx_state_buffer)\n\t\t\tmemset(mpx_state_buffer, 0, sizeof(struct mpx_bndcsr));\n\t\tif (init_event)\n\t\t\tkvm_load_guest_fpu(vcpu);\n\t}\n\n\tif (!init_event) {\n\t\tkvm_pmu_reset(vcpu);\n\t\tvcpu->arch.smbase = 0x30000;\n\n\t\tvcpu->arch.msr_misc_features_enables = 0;\n\n\t\tvcpu->arch.xcr0 = XFEATURE_MASK_FP;\n\t}\n\n\tmemset(vcpu->arch.regs, 0, sizeof(vcpu->arch.regs));\n\tvcpu->arch.regs_avail = ~0;\n\tvcpu->arch.regs_dirty = ~0;\n\n\tvcpu->arch.ia32_xss = 0;\n\n\tkvm_x86_ops.vcpu_reset(vcpu, init_event);\n}\n\nvoid kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)\n{\n\tstruct kvm_segment cs;\n\n\tkvm_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tcs.selector = vector << 8;\n\tcs.base = vector << 12;\n\tkvm_set_segment(vcpu, &cs, VCPU_SREG_CS);\n\tkvm_rip_write(vcpu, 0);\n}\n\nint kvm_arch_hardware_enable(void)\n{\n\tstruct kvm *kvm;\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\tint ret;\n\tu64 local_tsc;\n\tu64 max_tsc = 0;\n\tbool stable, backwards_tsc = false;\n\n\tkvm_user_return_msr_cpu_online();\n\tret = kvm_x86_ops.hardware_enable();\n\tif (ret != 0)\n\t\treturn ret;\n\n\tlocal_tsc = rdtsc();\n\tstable = !kvm_check_tsc_unstable();\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!stable && vcpu->cpu == smp_processor_id())\n\t\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\t\tif (stable && vcpu->arch.last_host_tsc > local_tsc) {\n\t\t\t\tbackwards_tsc = true;\n\t\t\t\tif (vcpu->arch.last_host_tsc > max_tsc)\n\t\t\t\t\tmax_tsc = vcpu->arch.last_host_tsc;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Sometimes, even reliable TSCs go backwards.  This happens on\n\t * platforms that reset TSC during suspend or hibernate actions, but\n\t * maintain synchronization.  We must compensate.  Fortunately, we can\n\t * detect that condition here, which happens early in CPU bringup,\n\t * before any KVM threads can be running.  Unfortunately, we can't\n\t * bring the TSCs fully up to date with real time, as we aren't yet far\n\t * enough into CPU bringup that we know how much real time has actually\n\t * elapsed; our helper function, ktime_get_boottime_ns() will be using boot\n\t * variables that haven't been updated yet.\n\t *\n\t * So we simply find the maximum observed TSC above, then record the\n\t * adjustment to TSC in each VCPU.  When the VCPU later gets loaded,\n\t * the adjustment will be applied.  Note that we accumulate\n\t * adjustments, in case multiple suspend cycles happen before some VCPU\n\t * gets a chance to run again.  In the event that no KVM threads get a\n\t * chance to run, we will miss the entire elapsed period, as we'll have\n\t * reset last_host_tsc, so VCPUs will not have the TSC adjusted and may\n\t * loose cycle time.  This isn't too big a deal, since the loss will be\n\t * uniform across all VCPUs (not to mention the scenario is extremely\n\t * unlikely). It is possible that a second hibernate recovery happens\n\t * much faster than a first, causing the observed TSC here to be\n\t * smaller; this would require additional padding adjustment, which is\n\t * why we set last_host_tsc to the local tsc observed here.\n\t *\n\t * N.B. - this code below runs only on platforms with reliable TSC,\n\t * as that is the only way backwards_tsc is set above.  Also note\n\t * that this runs for ALL vcpus, which is not a bug; all VCPUs should\n\t * have the same delta_cyc adjustment applied if backwards_tsc\n\t * is detected.  Note further, this adjustment is only done once,\n\t * as we reset last_host_tsc on all VCPUs to stop this from being\n\t * called multiple times (one for each physical CPU bringup).\n\t *\n\t * Platforms with unreliable TSCs don't have to deal with this, they\n\t * will be compensated by the logic in vcpu_load, which sets the TSC to\n\t * catchup mode.  This will catchup all VCPUs to real time, but cannot\n\t * guarantee that they stay in perfect synchronization.\n\t */\n\tif (backwards_tsc) {\n\t\tu64 delta_cyc = max_tsc - local_tsc;\n\t\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\t\tkvm->arch.backwards_tsc_observed = true;\n\t\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\t\tvcpu->arch.tsc_offset_adjustment += delta_cyc;\n\t\t\t\tvcpu->arch.last_host_tsc = local_tsc;\n\t\t\t\tkvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We have to disable TSC offset matching.. if you were\n\t\t\t * booting a VM while issuing an S4 host suspend....\n\t\t\t * you may have some problem.  Solving this issue is\n\t\t\t * left as an exercise to the reader.\n\t\t\t */\n\t\t\tkvm->arch.last_tsc_nsec = 0;\n\t\t\tkvm->arch.last_tsc_write = 0;\n\t\t}\n\n\t}\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_disable(void)\n{\n\tkvm_x86_ops.hardware_disable();\n\tdrop_user_return_notifiers();\n}\n\nint kvm_arch_hardware_setup(void *opaque)\n{\n\tstruct kvm_x86_init_ops *ops = opaque;\n\tint r;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\trdmsrl(MSR_IA32_XSS, host_xss);\n\n\tr = ops->hardware_setup();\n\tif (r != 0)\n\t\treturn r;\n\n\tmemcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));\n\n\tif (!kvm_cpu_cap_has(X86_FEATURE_XSAVES))\n\t\tsupported_xss = 0;\n\n#define __kvm_cpu_cap_has(UNUSED_, f) kvm_cpu_cap_has(f)\n\tcr4_reserved_bits = __cr4_reserved_bits(__kvm_cpu_cap_has, UNUSED_);\n#undef __kvm_cpu_cap_has\n\n\tif (kvm_has_tsc_control) {\n\t\t/*\n\t\t * Make sure the user can only configure tsc_khz values that\n\t\t * fit into a signed integer.\n\t\t * A min value is not calculated because it will always\n\t\t * be 1 on all machines.\n\t\t */\n\t\tu64 max = min(0x7fffffffULL,\n\t\t\t      __scale_tsc(kvm_max_tsc_scaling_ratio, tsc_khz));\n\t\tkvm_max_guest_tsc_khz = max;\n\n\t\tkvm_default_tsc_scaling_ratio = 1ULL << kvm_tsc_scaling_ratio_frac_bits;\n\t}\n\n\tkvm_init_msr_list();\n\treturn 0;\n}\n\nvoid kvm_arch_hardware_unsetup(void)\n{\n\tkvm_x86_ops.hardware_unsetup();\n}\n\nint kvm_arch_check_processor_compat(void *opaque)\n{\n\tstruct cpuinfo_x86 *c = &cpu_data(smp_processor_id());\n\tstruct kvm_x86_init_ops *ops = opaque;\n\n\tWARN_ON(!irqs_disabled());\n\n\tif (__cr4_reserved_bits(cpu_has, c) !=\n\t    __cr4_reserved_bits(cpu_has, &boot_cpu_data))\n\t\treturn -EIO;\n\n\treturn ops->check_processor_compatibility();\n}\n\nbool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->kvm->arch.bsp_vcpu_id == vcpu->vcpu_id;\n}\nEXPORT_SYMBOL_GPL(kvm_vcpu_is_reset_bsp);\n\nbool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.apic_base & MSR_IA32_APICBASE_BSP) != 0;\n}\n\nstruct static_key kvm_no_apic_vcpu __read_mostly;\nEXPORT_SYMBOL_GPL(kvm_no_apic_vcpu);\n\nvoid kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\tif (pmu->version && unlikely(pmu->event_count)) {\n\t\tpmu->need_cleanup = true;\n\t\tkvm_make_request(KVM_REQ_PMU, vcpu);\n\t}\n\tkvm_x86_ops.sched_in(vcpu, cpu);\n}\n\nvoid kvm_arch_free_vm(struct kvm *kvm)\n{\n\tkfree(kvm->arch.hyperv.hv_pa_pg);\n\tvfree(kvm);\n}\n\n\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\n{\n\tif (type)\n\t\treturn -EINVAL;\n\n\tINIT_HLIST_HEAD(&kvm->arch.mask_notifier_list);\n\tINIT_LIST_HEAD(&kvm->arch.active_mmu_pages);\n\tINIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);\n\tINIT_LIST_HEAD(&kvm->arch.lpage_disallowed_mmu_pages);\n\tINIT_LIST_HEAD(&kvm->arch.assigned_dev_head);\n\tatomic_set(&kvm->arch.noncoherent_dma_count, 0);\n\n\t/* Reserve bit 0 of irq_sources_bitmap for userspace irq source */\n\tset_bit(KVM_USERSPACE_IRQ_SOURCE_ID, &kvm->arch.irq_sources_bitmap);\n\t/* Reserve bit 1 of irq_sources_bitmap for irqfd-resampler */\n\tset_bit(KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t&kvm->arch.irq_sources_bitmap);\n\n\traw_spin_lock_init(&kvm->arch.tsc_write_lock);\n\tmutex_init(&kvm->arch.apic_map_lock);\n\tspin_lock_init(&kvm->arch.pvclock_gtod_sync_lock);\n\n\tkvm->arch.kvmclock_offset = -get_kvmclock_base_ns();\n\tpvclock_update_vm_gtod_copy(kvm);\n\n\tkvm->arch.guest_can_read_msr_platform_info = true;\n\n\tINIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);\n\tINIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);\n\n\tkvm_hv_init_vm(kvm);\n\tkvm_page_track_init(kvm);\n\tkvm_mmu_init_vm(kvm);\n\n\treturn kvm_x86_ops.vm_init(kvm);\n}\n\nint kvm_arch_post_init_vm(struct kvm *kvm)\n{\n\treturn kvm_mmu_post_init_vm(kvm);\n}\n\nstatic void kvm_unload_vcpu_mmu(struct kvm_vcpu *vcpu)\n{\n\tvcpu_load(vcpu);\n\tkvm_mmu_unload(vcpu);\n\tvcpu_put(vcpu);\n}\n\nstatic void kvm_free_vcpus(struct kvm *kvm)\n{\n\tunsigned int i;\n\tstruct kvm_vcpu *vcpu;\n\n\t/*\n\t * Unpin any mmu pages first.\n\t */\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tkvm_clear_async_pf_completion_queue(vcpu);\n\t\tkvm_unload_vcpu_mmu(vcpu);\n\t}\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_destroy(vcpu);\n\n\tmutex_lock(&kvm->lock);\n\tfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++)\n\t\tkvm->vcpus[i] = NULL;\n\n\tatomic_set(&kvm->online_vcpus, 0);\n\tmutex_unlock(&kvm->lock);\n}\n\nvoid kvm_arch_sync_events(struct kvm *kvm)\n{\n\tcancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);\n\tcancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);\n\tkvm_free_pit(kvm);\n}\n\nint __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size)\n{\n\tint i, r;\n\tunsigned long hva, old_npages;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *slot;\n\n\t/* Called with kvm->slots_lock held.  */\n\tif (WARN_ON(id >= KVM_MEM_SLOTS_NUM))\n\t\treturn -EINVAL;\n\n\tslot = id_to_memslot(slots, id);\n\tif (size) {\n\t\tif (slot && slot->npages)\n\t\t\treturn -EEXIST;\n\n\t\t/*\n\t\t * MAP_SHARED to prevent internal slot pages from being moved\n\t\t * by fork()/COW.\n\t\t */\n\t\thva = vm_mmap(NULL, 0, size, PROT_READ | PROT_WRITE,\n\t\t\t      MAP_SHARED | MAP_ANONYMOUS, 0);\n\t\tif (IS_ERR((void *)hva))\n\t\t\treturn PTR_ERR((void *)hva);\n\t} else {\n\t\tif (!slot || !slot->npages)\n\t\t\treturn 0;\n\n\t\told_npages = slot->npages;\n\t\thva = 0;\n\t}\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tstruct kvm_userspace_memory_region m;\n\n\t\tm.slot = id | (i << 16);\n\t\tm.flags = 0;\n\t\tm.guest_phys_addr = gpa;\n\t\tm.userspace_addr = hva;\n\t\tm.memory_size = size;\n\t\tr = __kvm_set_memory_region(kvm, &m);\n\t\tif (r < 0)\n\t\t\treturn r;\n\t}\n\n\tif (!size)\n\t\tvm_munmap(hva, old_npages * PAGE_SIZE);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__x86_set_memory_region);\n\nvoid kvm_arch_pre_destroy_vm(struct kvm *kvm)\n{\n\tkvm_mmu_pre_destroy_vm(kvm);\n}\n\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\n{\n\tu32 i;\n\n\tif (current->mm == kvm->mm) {\n\t\t/*\n\t\t * Free memory regions allocated on behalf of userspace,\n\t\t * unless the the memory map has changed due to process exit\n\t\t * or fd copying.\n\t\t */\n\t\tmutex_lock(&kvm->slots_lock);\n\t\t__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,\n\t\t\t\t\t0, 0);\n\t\t__x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t\t0, 0);\n\t\t__x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);\n\t\tmutex_unlock(&kvm->slots_lock);\n\t}\n\tif (kvm_x86_ops.vm_destroy)\n\t\tkvm_x86_ops.vm_destroy(kvm);\n\tfor (i = 0; i < kvm->arch.msr_filter.count; i++)\n\t\tkfree(kvm->arch.msr_filter.ranges[i].bitmap);\n\tkvm_pic_destroy(kvm);\n\tkvm_ioapic_destroy(kvm);\n\tkvm_free_vcpus(kvm);\n\tkvfree(rcu_dereference_check(kvm->arch.apic_map, 1));\n\tkfree(srcu_dereference_check(kvm->arch.pmu_event_filter, &kvm->srcu, 1));\n\tkvm_mmu_uninit_vm(kvm);\n\tkvm_page_track_cleanup(kvm);\n\tkvm_hv_destroy_vm(kvm);\n}\n\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot)\n{\n\tint i;\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.rmap[i]);\n\t\tslot->arch.rmap[i] = NULL;\n\n\t\tif (i == 0)\n\t\t\tcontinue;\n\n\t\tkvfree(slot->arch.lpage_info[i - 1]);\n\t\tslot->arch.lpage_info[i - 1] = NULL;\n\t}\n\n\tkvm_page_track_free_memslot(slot);\n}\n\nstatic int kvm_alloc_memslot_metadata(struct kvm_memory_slot *slot,\n\t\t\t\t      unsigned long npages)\n{\n\tint i;\n\n\t/*\n\t * Clear out the previous array pointers for the KVM_MR_MOVE case.  The\n\t * old arrays will be freed by __kvm_set_memory_region() if installing\n\t * the new memslot is successful.\n\t */\n\tmemset(&slot->arch, 0, sizeof(slot->arch));\n\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tstruct kvm_lpage_info *linfo;\n\t\tunsigned long ugfn;\n\t\tint lpages;\n\t\tint level = i + 1;\n\n\t\tlpages = gfn_to_index(slot->base_gfn + npages - 1,\n\t\t\t\t      slot->base_gfn, level) + 1;\n\n\t\tslot->arch.rmap[i] =\n\t\t\tkvcalloc(lpages, sizeof(*slot->arch.rmap[i]),\n\t\t\t\t GFP_KERNEL_ACCOUNT);\n\t\tif (!slot->arch.rmap[i])\n\t\t\tgoto out_free;\n\t\tif (i == 0)\n\t\t\tcontinue;\n\n\t\tlinfo = kvcalloc(lpages, sizeof(*linfo), GFP_KERNEL_ACCOUNT);\n\t\tif (!linfo)\n\t\t\tgoto out_free;\n\n\t\tslot->arch.lpage_info[i - 1] = linfo;\n\n\t\tif (slot->base_gfn & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tlinfo[0].disallow_lpage = 1;\n\t\tif ((slot->base_gfn + npages) & (KVM_PAGES_PER_HPAGE(level) - 1))\n\t\t\tlinfo[lpages - 1].disallow_lpage = 1;\n\t\tugfn = slot->userspace_addr >> PAGE_SHIFT;\n\t\t/*\n\t\t * If the gfn and userspace address are not aligned wrt each\n\t\t * other, disable large page support for this slot.\n\t\t */\n\t\tif ((slot->base_gfn ^ ugfn) & (KVM_PAGES_PER_HPAGE(level) - 1)) {\n\t\t\tunsigned long j;\n\n\t\t\tfor (j = 0; j < lpages; ++j)\n\t\t\t\tlinfo[j].disallow_lpage = 1;\n\t\t}\n\t}\n\n\tif (kvm_page_track_create_memslot(slot, npages))\n\t\tgoto out_free;\n\n\treturn 0;\n\nout_free:\n\tfor (i = 0; i < KVM_NR_PAGE_SIZES; ++i) {\n\t\tkvfree(slot->arch.rmap[i]);\n\t\tslot->arch.rmap[i] = NULL;\n\t\tif (i == 0)\n\t\t\tcontinue;\n\n\t\tkvfree(slot->arch.lpage_info[i - 1]);\n\t\tslot->arch.lpage_info[i - 1] = NULL;\n\t}\n\treturn -ENOMEM;\n}\n\nvoid kvm_arch_memslots_updated(struct kvm *kvm, u64 gen)\n{\n\tstruct kvm_vcpu *vcpu;\n\tint i;\n\n\t/*\n\t * memslots->generation has been incremented.\n\t * mmio generation may have reached its maximum value.\n\t */\n\tkvm_mmu_invalidate_mmio_sptes(kvm, gen);\n\n\t/* Force re-initialization of steal_time cache */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\tstruct kvm_memory_slot *memslot,\n\t\t\t\tconst struct kvm_userspace_memory_region *mem,\n\t\t\t\tenum kvm_mr_change change)\n{\n\tif (change == KVM_MR_CREATE || change == KVM_MR_MOVE)\n\t\treturn kvm_alloc_memslot_metadata(memslot,\n\t\t\t\t\t\t  mem->memory_size >> PAGE_SHIFT);\n\treturn 0;\n}\n\nstatic void kvm_mmu_slot_apply_flags(struct kvm *kvm,\n\t\t\t\t     struct kvm_memory_slot *old,\n\t\t\t\t     struct kvm_memory_slot *new,\n\t\t\t\t     enum kvm_mr_change change)\n{\n\t/*\n\t * Nothing to do for RO slots or CREATE/MOVE/DELETE of a slot.\n\t * See comments below.\n\t */\n\tif ((change != KVM_MR_FLAGS_ONLY) || (new->flags & KVM_MEM_READONLY))\n\t\treturn;\n\n\t/*\n\t * Dirty logging tracks sptes in 4k granularity, meaning that large\n\t * sptes have to be split.  If live migration is successful, the guest\n\t * in the source machine will be destroyed and large sptes will be\n\t * created in the destination. However, if the guest continues to run\n\t * in the source machine (for example if live migration fails), small\n\t * sptes will remain around and cause bad performance.\n\t *\n\t * Scan sptes if dirty logging has been stopped, dropping those\n\t * which can be collapsed into a single large-page spte.  Later\n\t * page faults will create the large-page sptes.\n\t *\n\t * There is no need to do this in any of the following cases:\n\t * CREATE:      No dirty mappings will already exist.\n\t * MOVE/DELETE: The old mappings will already have been cleaned up by\n\t *\t\tkvm_arch_flush_shadow_memslot()\n\t */\n\tif ((old->flags & KVM_MEM_LOG_DIRTY_PAGES) &&\n\t    !(new->flags & KVM_MEM_LOG_DIRTY_PAGES))\n\t\tkvm_mmu_zap_collapsible_sptes(kvm, new);\n\n\t/*\n\t * Enable or disable dirty logging for the slot.\n\t *\n\t * For KVM_MR_DELETE and KVM_MR_MOVE, the shadow pages of the old\n\t * slot have been zapped so no dirty logging updates are needed for\n\t * the old slot.\n\t * For KVM_MR_CREATE and KVM_MR_MOVE, once the new slot is visible\n\t * any mappings that might be created in it will consume the\n\t * properties of the new slot and do not need to be updated here.\n\t *\n\t * When PML is enabled, the kvm_x86_ops dirty logging hooks are\n\t * called to enable/disable dirty logging.\n\t *\n\t * When disabling dirty logging with PML enabled, the D-bit is set\n\t * for sptes in the slot in order to prevent unnecessary GPA\n\t * logging in the PML buffer (and potential PML buffer full VMEXIT).\n\t * This guarantees leaving PML enabled for the guest's lifetime\n\t * won't have any additional overhead from PML when the guest is\n\t * running with dirty logging disabled.\n\t *\n\t * When enabling dirty logging, large sptes are write-protected\n\t * so they can be split on first write.  New large sptes cannot\n\t * be created for this slot until the end of the logging.\n\t * See the comments in fast_page_fault().\n\t * For small sptes, nothing is done if the dirty log is in the\n\t * initial-all-set state.  Otherwise, depending on whether pml\n\t * is enabled the D-bit or the W-bit will be cleared.\n\t */\n\tif (new->flags & KVM_MEM_LOG_DIRTY_PAGES) {\n\t\tif (kvm_x86_ops.slot_enable_log_dirty) {\n\t\t\tkvm_x86_ops.slot_enable_log_dirty(kvm, new);\n\t\t} else {\n\t\t\tint level =\n\t\t\t\tkvm_dirty_log_manual_protect_and_init_set(kvm) ?\n\t\t\t\tPG_LEVEL_2M : PG_LEVEL_4K;\n\n\t\t\t/*\n\t\t\t * If we're with initial-all-set, we don't need\n\t\t\t * to write protect any small page because\n\t\t\t * they're reported as dirty already.  However\n\t\t\t * we still need to write-protect huge pages\n\t\t\t * so that the page split can happen lazily on\n\t\t\t * the first write to the huge page.\n\t\t\t */\n\t\t\tkvm_mmu_slot_remove_write_access(kvm, new, level);\n\t\t}\n\t} else {\n\t\tif (kvm_x86_ops.slot_disable_log_dirty)\n\t\t\tkvm_x86_ops.slot_disable_log_dirty(kvm, new);\n\t}\n}\n\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\tconst struct kvm_userspace_memory_region *mem,\n\t\t\t\tstruct kvm_memory_slot *old,\n\t\t\t\tconst struct kvm_memory_slot *new,\n\t\t\t\tenum kvm_mr_change change)\n{\n\tif (!kvm->arch.n_requested_mmu_pages)\n\t\tkvm_mmu_change_mmu_pages(kvm,\n\t\t\t\tkvm_mmu_calculate_default_mmu_pages(kvm));\n\n\t/*\n\t * FIXME: const-ify all uses of struct kvm_memory_slot.\n\t */\n\tkvm_mmu_slot_apply_flags(kvm, old, (struct kvm_memory_slot *) new, change);\n\n\t/* Free the arrays associated with the old memslot. */\n\tif (change == KVM_MR_MOVE)\n\t\tkvm_arch_free_memslot(kvm, old);\n}\n\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\n{\n\tkvm_mmu_zap_all(kvm);\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n\tkvm_page_track_flush_slot(kvm, slot);\n}\n\nstatic inline bool kvm_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn (is_guest_mode(vcpu) &&\n\t\t\tkvm_x86_ops.guest_apic_has_interrupt &&\n\t\t\tkvm_x86_ops.guest_apic_has_interrupt(vcpu));\n}\n\nstatic inline bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)\n{\n\tif (!list_empty_careful(&vcpu->async_pf.done))\n\t\treturn true;\n\n\tif (kvm_apic_has_events(vcpu))\n\t\treturn true;\n\n\tif (vcpu->arch.pv.pv_unhalted)\n\t\treturn true;\n\n\tif (vcpu->arch.exception.pending)\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_NMI, vcpu) ||\n\t    (vcpu->arch.nmi_pending &&\n\t     kvm_x86_ops.nmi_allowed(vcpu, false)))\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_SMI, vcpu) ||\n\t    (vcpu->arch.smi_pending &&\n\t     kvm_x86_ops.smi_allowed(vcpu, false)))\n\t\treturn true;\n\n\tif (kvm_arch_interrupt_allowed(vcpu) &&\n\t    (kvm_cpu_has_interrupt(vcpu) ||\n\t    kvm_guest_apic_has_interrupt(vcpu)))\n\t\treturn true;\n\n\tif (kvm_hv_has_stimer_pending(vcpu))\n\t\treturn true;\n\n\tif (is_guest_mode(vcpu) &&\n\t    kvm_x86_ops.nested_ops->hv_timer_pending &&\n\t    kvm_x86_ops.nested_ops->hv_timer_pending(vcpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_running(vcpu) || kvm_vcpu_has_events(vcpu);\n}\n\nbool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)\n{\n\tif (READ_ONCE(vcpu->arch.pv.pv_unhalted))\n\t\treturn true;\n\n\tif (kvm_test_request(KVM_REQ_NMI, vcpu) ||\n\t\tkvm_test_request(KVM_REQ_SMI, vcpu) ||\n\t\t kvm_test_request(KVM_REQ_EVENT, vcpu))\n\t\treturn true;\n\n\tif (vcpu->arch.apicv_active && kvm_x86_ops.dy_apicv_has_pending_interrupt(vcpu))\n\t\treturn true;\n\n\treturn false;\n}\n\nbool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.preempted_in_kernel;\n}\n\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;\n}\n\nint kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_x86_ops.interrupt_allowed(vcpu, false);\n}\n\nunsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu)\n{\n\tif (is_64_bit_mode(vcpu))\n\t\treturn kvm_rip_read(vcpu);\n\treturn (u32)(get_segment_base(vcpu, VCPU_SREG_CS) +\n\t\t     kvm_rip_read(vcpu));\n}\nEXPORT_SYMBOL_GPL(kvm_get_linear_rip);\n\nbool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip)\n{\n\treturn kvm_get_linear_rip(vcpu) == linear_rip;\n}\nEXPORT_SYMBOL_GPL(kvm_is_linear_rip);\n\nunsigned long kvm_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags;\n\n\trflags = kvm_x86_ops.get_rflags(vcpu);\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\trflags &= ~X86_EFLAGS_TF;\n\treturn rflags;\n}\nEXPORT_SYMBOL_GPL(kvm_get_rflags);\n\nstatic void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP &&\n\t    kvm_is_linear_rip(vcpu, vcpu->arch.singlestep_rip))\n\t\trflags |= X86_EFLAGS_TF;\n\tkvm_x86_ops.set_rflags(vcpu, rflags);\n}\n\nvoid kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\t__kvm_set_rflags(vcpu, rflags);\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_set_rflags);\n\nvoid kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)\n{\n\tint r;\n\n\tif ((vcpu->arch.mmu->direct_map != work->arch.direct_map) ||\n\t      work->wakeup_all)\n\t\treturn;\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r))\n\t\treturn;\n\n\tif (!vcpu->arch.mmu->direct_map &&\n\t      work->arch.cr3 != vcpu->arch.mmu->get_guest_pgd(vcpu))\n\t\treturn;\n\n\tkvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true);\n}\n\nstatic inline u32 kvm_async_pf_hash_fn(gfn_t gfn)\n{\n\tBUILD_BUG_ON(!is_power_of_2(ASYNC_PF_PER_VCPU));\n\n\treturn hash_32(gfn & 0xffffffff, order_base_2(ASYNC_PF_PER_VCPU));\n}\n\nstatic inline u32 kvm_async_pf_next_probe(u32 key)\n{\n\treturn (key + 1) & (ASYNC_PF_PER_VCPU - 1);\n}\n\nstatic void kvm_add_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tu32 key = kvm_async_pf_hash_fn(gfn);\n\n\twhile (vcpu->arch.apf.gfns[key] != ~0)\n\t\tkey = kvm_async_pf_next_probe(key);\n\n\tvcpu->arch.apf.gfns[key] = gfn;\n}\n\nstatic u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tint i;\n\tu32 key = kvm_async_pf_hash_fn(gfn);\n\n\tfor (i = 0; i < ASYNC_PF_PER_VCPU &&\n\t\t     (vcpu->arch.apf.gfns[key] != gfn &&\n\t\t      vcpu->arch.apf.gfns[key] != ~0); i++)\n\t\tkey = kvm_async_pf_next_probe(key);\n\n\treturn key;\n}\n\nbool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\treturn vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;\n}\n\nstatic void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\n{\n\tu32 i, j, k;\n\n\ti = j = kvm_async_pf_gfn_slot(vcpu, gfn);\n\n\tif (WARN_ON_ONCE(vcpu->arch.apf.gfns[i] != gfn))\n\t\treturn;\n\n\twhile (true) {\n\t\tvcpu->arch.apf.gfns[i] = ~0;\n\t\tdo {\n\t\t\tj = kvm_async_pf_next_probe(j);\n\t\t\tif (vcpu->arch.apf.gfns[j] == ~0)\n\t\t\t\treturn;\n\t\t\tk = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);\n\t\t\t/*\n\t\t\t * k lies cyclically in ]i,j]\n\t\t\t * |    i.k.j |\n\t\t\t * |....j i.k.| or  |.k..j i...|\n\t\t\t */\n\t\t} while ((i <= j) ? (i < k && k <= j) : (i < k || k <= j));\n\t\tvcpu->arch.apf.gfns[i] = vcpu->arch.apf.gfns[j];\n\t\ti = j;\n\t}\n}\n\nstatic inline int apf_put_user_notpresent(struct kvm_vcpu *vcpu)\n{\n\tu32 reason = KVM_PV_REASON_PAGE_NOT_PRESENT;\n\n\treturn kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,\n\t\t\t\t      sizeof(reason));\n}\n\nstatic inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)\n{\n\tunsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);\n\n\treturn kvm_write_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,\n\t\t\t\t\t     &token, offset, sizeof(token));\n}\n\nstatic inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)\n{\n\tunsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);\n\tu32 val;\n\n\tif (kvm_read_guest_offset_cached(vcpu->kvm, &vcpu->arch.apf.data,\n\t\t\t\t\t &val, offset, sizeof(val)))\n\t\treturn false;\n\n\treturn !val;\n}\n\nstatic bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)\n{\n\tif (!vcpu->arch.apf.delivery_as_pf_vmexit && is_guest_mode(vcpu))\n\t\treturn false;\n\n\tif (!kvm_pv_async_pf_enabled(vcpu) ||\n\t    (vcpu->arch.apf.send_user_only && kvm_x86_ops.get_cpl(vcpu) == 0))\n\t\treturn false;\n\n\treturn true;\n}\n\nbool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)\n{\n\tif (unlikely(!lapic_in_kernel(vcpu) ||\n\t\t     kvm_event_needs_reinjection(vcpu) ||\n\t\t     vcpu->arch.exception.pending))\n\t\treturn false;\n\n\tif (kvm_hlt_in_guest(vcpu->kvm) && !kvm_can_deliver_async_pf(vcpu))\n\t\treturn false;\n\n\t/*\n\t * If interrupts are off we cannot even use an artificial\n\t * halt state.\n\t */\n\treturn kvm_arch_interrupt_allowed(vcpu);\n}\n\nbool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_async_pf *work)\n{\n\tstruct x86_exception fault;\n\n\ttrace_kvm_async_pf_not_present(work->arch.token, work->cr2_or_gpa);\n\tkvm_add_async_pf_gfn(vcpu, work->arch.gfn);\n\n\tif (kvm_can_deliver_async_pf(vcpu) &&\n\t    !apf_put_user_notpresent(vcpu)) {\n\t\tfault.vector = PF_VECTOR;\n\t\tfault.error_code_valid = true;\n\t\tfault.error_code = 0;\n\t\tfault.nested_page_fault = false;\n\t\tfault.address = work->arch.token;\n\t\tfault.async_page_fault = true;\n\t\tkvm_inject_page_fault(vcpu, &fault);\n\t\treturn true;\n\t} else {\n\t\t/*\n\t\t * It is not possible to deliver a paravirtualized asynchronous\n\t\t * page fault, but putting the guest in an artificial halt state\n\t\t * can be beneficial nevertheless: if an interrupt arrives, we\n\t\t * can deliver it timely and perhaps the guest will schedule\n\t\t * another process.  When the instruction that triggered a page\n\t\t * fault is retried, hopefully the page will be ready in the host.\n\t\t */\n\t\tkvm_make_request(KVM_REQ_APF_HALT, vcpu);\n\t\treturn false;\n\t}\n}\n\nvoid kvm_arch_async_page_present(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_async_pf *work)\n{\n\tstruct kvm_lapic_irq irq = {\n\t\t.delivery_mode = APIC_DM_FIXED,\n\t\t.vector = vcpu->arch.apf.vec\n\t};\n\n\tif (work->wakeup_all)\n\t\twork->arch.token = ~0; /* broadcast wakeup */\n\telse\n\t\tkvm_del_async_pf_gfn(vcpu, work->arch.gfn);\n\ttrace_kvm_async_pf_ready(work->arch.token, work->cr2_or_gpa);\n\n\tif ((work->wakeup_all || work->notpresent_injected) &&\n\t    kvm_pv_async_pf_enabled(vcpu) &&\n\t    !apf_put_user_ready(vcpu, work->arch.token)) {\n\t\tvcpu->arch.apf.pageready_pending = true;\n\t\tkvm_apic_set_irq(vcpu, &irq, NULL);\n\t}\n\n\tvcpu->arch.apf.halted = false;\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n}\n\nvoid kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)\n{\n\tkvm_make_request(KVM_REQ_APF_READY, vcpu);\n\tif (!vcpu->arch.apf.pageready_pending)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nbool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_pv_async_pf_enabled(vcpu))\n\t\treturn true;\n\telse\n\t\treturn apf_pageready_slot_free(vcpu);\n}\n\nvoid kvm_arch_start_assignment(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_start_assignment);\n\nvoid kvm_arch_end_assignment(struct kvm *kvm)\n{\n\tatomic_dec(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_end_assignment);\n\nbool kvm_arch_has_assigned_device(struct kvm *kvm)\n{\n\treturn atomic_read(&kvm->arch.assigned_device_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_has_assigned_device);\n\nvoid kvm_arch_register_noncoherent_dma(struct kvm *kvm)\n{\n\tatomic_inc(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_register_noncoherent_dma);\n\nvoid kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)\n{\n\tatomic_dec(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_unregister_noncoherent_dma);\n\nbool kvm_arch_has_noncoherent_dma(struct kvm *kvm)\n{\n\treturn atomic_read(&kvm->arch.noncoherent_dma_count);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_has_noncoherent_dma);\n\nbool kvm_arch_has_irq_bypass(void)\n{\n\treturn true;\n}\n\nint kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\tint ret;\n\n\tirqfd->producer = prod;\n\tkvm_arch_start_assignment(irqfd->kvm);\n\tret = kvm_x86_ops.update_pi_irte(irqfd->kvm,\n\t\t\t\t\t prod->irq, irqfd->gsi, 1);\n\n\tif (ret)\n\t\tkvm_arch_end_assignment(irqfd->kvm);\n\n\treturn ret;\n}\n\nvoid kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,\n\t\t\t\t      struct irq_bypass_producer *prod)\n{\n\tint ret;\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(cons, struct kvm_kernel_irqfd, consumer);\n\n\tWARN_ON(irqfd->producer != prod);\n\tirqfd->producer = NULL;\n\n\t/*\n\t * When producer of consumer is unregistered, we change back to\n\t * remapped mode, so we can re-use the current implementation\n\t * when the irq is masked/disabled or the consumer side (KVM\n\t * int this case doesn't want to receive the interrupts.\n\t*/\n\tret = kvm_x86_ops.update_pi_irte(irqfd->kvm, prod->irq, irqfd->gsi, 0);\n\tif (ret)\n\t\tprintk(KERN_INFO \"irq bypass consumer (token %p) unregistration\"\n\t\t       \" fails: %d\\n\", irqfd->consumer.token, ret);\n\n\tkvm_arch_end_assignment(irqfd->kvm);\n}\n\nint kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,\n\t\t\t\t   uint32_t guest_irq, bool set)\n{\n\treturn kvm_x86_ops.update_pi_irte(kvm, host_irq, guest_irq, set);\n}\n\nbool kvm_vector_hashing_enabled(void)\n{\n\treturn vector_hashing;\n}\n\nbool kvm_arch_no_poll(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu->arch.msr_kvm_poll_control & 1) == 0;\n}\nEXPORT_SYMBOL_GPL(kvm_arch_no_poll);\n\n\nint kvm_spec_ctrl_test_value(u64 value)\n{\n\t/*\n\t * test that setting IA32_SPEC_CTRL to given value\n\t * is allowed by the host processor\n\t */\n\n\tu64 saved_value;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tlocal_irq_save(flags);\n\n\tif (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))\n\t\tret = 1;\n\telse if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))\n\t\tret = 1;\n\telse\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, saved_value);\n\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kvm_spec_ctrl_test_value);\n\nvoid kvm_fixup_and_inject_pf_error(struct kvm_vcpu *vcpu, gva_t gva, u16 error_code)\n{\n\tstruct x86_exception fault;\n\tu32 access = error_code &\n\t\t(PFERR_WRITE_MASK | PFERR_FETCH_MASK | PFERR_USER_MASK);\n\n\tif (!(error_code & PFERR_PRESENT_MASK) ||\n\t    vcpu->arch.walk_mmu->gva_to_gpa(vcpu, gva, access, &fault) != UNMAPPED_GVA) {\n\t\t/*\n\t\t * If vcpu->arch.walk_mmu->gva_to_gpa succeeded, the page\n\t\t * tables probably do not match the TLB.  Just proceed\n\t\t * with the error code that the processor gave.\n\t\t */\n\t\tfault.vector = PF_VECTOR;\n\t\tfault.error_code_valid = true;\n\t\tfault.error_code = error_code;\n\t\tfault.nested_page_fault = false;\n\t\tfault.address = gva;\n\t}\n\tvcpu->arch.walk_mmu->inject_page_fault(vcpu, &fault);\n}\nEXPORT_SYMBOL_GPL(kvm_fixup_and_inject_pf_error);\n\n/*\n * Handles kvm_read/write_guest_virt*() result and either injects #PF or returns\n * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value\n * indicates whether exit to userspace is needed.\n */\nint kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,\n\t\t\t      struct x86_exception *e)\n{\n\tif (r == X86EMUL_PROPAGATE_FAULT) {\n\t\tkvm_inject_emulated_page_fault(vcpu, e);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * In case kvm_read/write_guest_virt*() failed with X86EMUL_IO_NEEDED\n\t * while handling a VMX instruction KVM could've handled the request\n\t * correctly by exiting to userspace and performing I/O but there\n\t * doesn't seem to be a real use-case behind such requests, just return\n\t * KVM_EXIT_INTERNAL_ERROR for now.\n\t */\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\tvcpu->run->internal.ndata = 0;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(kvm_handle_memory_failure);\n\nint kvm_handle_invpcid(struct kvm_vcpu *vcpu, unsigned long type, gva_t gva)\n{\n\tbool pcid_enabled;\n\tstruct x86_exception e;\n\tunsigned i;\n\tunsigned long roots_to_free = 0;\n\tstruct {\n\t\tu64 pcid;\n\t\tu64 gla;\n\t} operand;\n\tint r;\n\n\tr = kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\n\tif (operand.pcid >> 12 != 0) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tpcid_enabled = kvm_read_cr4_bits(vcpu, X86_CR4_PCIDE);\n\n\tswitch (type) {\n\tcase INVPCID_TYPE_INDIV_ADDR:\n\t\tif ((!pcid_enabled && (operand.pcid != 0)) ||\n\t\t    is_noncanonical_address(operand.gla, vcpu)) {\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_mmu_invpcid_gva(vcpu, operand.gla, operand.pcid);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tcase INVPCID_TYPE_SINGLE_CTXT:\n\t\tif (!pcid_enabled && (operand.pcid != 0)) {\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (kvm_get_active_pcid(vcpu) == operand.pcid) {\n\t\t\tkvm_mmu_sync_roots(vcpu);\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t\t}\n\n\t\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\t\tif (kvm_get_pcid(vcpu, vcpu->arch.mmu->prev_roots[i].pgd)\n\t\t\t    == operand.pcid)\n\t\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\n\t\tkvm_mmu_free_roots(vcpu, vcpu->arch.mmu, roots_to_free);\n\t\t/*\n\t\t * If neither the current cr3 nor any of the prev_roots use the\n\t\t * given PCID, then nothing needs to be done here because a\n\t\t * resync will happen anyway before switching to any other CR3.\n\t\t */\n\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tcase INVPCID_TYPE_ALL_NON_GLOBAL:\n\t\t/*\n\t\t * Currently, KVM doesn't mark global entries in the shadow\n\t\t * page tables, so a non-global flush just degenerates to a\n\t\t * global flush. If needed, we could optimize this later by\n\t\t * keeping track of global entries in shadow page tables.\n\t\t */\n\n\t\tfallthrough;\n\tcase INVPCID_TYPE_ALL_INCL_GLOBAL:\n\t\tkvm_mmu_unload(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tdefault:\n\t\tBUG(); /* We have already checked above that type <= 3 */\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_handle_invpcid);\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_exit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_fast_mmio);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_inj_virq);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_page_fault);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_msr);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_cr);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmrun);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmexit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmexit_inject);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_intr_vmexit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_vmenter_failed);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_invlpga);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_skinit);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_nested_intercepts);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_write_tsc_offset);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ple_window_update);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_pml_full);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_pi_irte_update);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_unaccelerated_access);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_incomplete_ipi);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_avic_ga_log);\nEXPORT_TRACEPOINT_SYMBOL_GPL(kvm_apicv_update_request);\n"}}, "reports": [{"events": [{"location": {"col": 5, "file": 0, "line": 7990}, "message": "WARNING: Comparison to bool"}], "macros": [], "notes": [], "path": "/src/arch/x86/kvm/x86.c", "reportHash": "3e24c787f297f473ef02b108ab03307f", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 19, "file": 0, "line": 197}, "message": "WARNING: Assignment of 0/1 to bool variable"}], "macros": [], "notes": [], "path": "/src/arch/x86/kvm/x86.c", "reportHash": "58e15dcce4a1f2c2c23efc8b30a7f8d5", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 26, "file": 0, "line": 116}, "message": "WARNING: Assignment of 0/1 to bool variable"}], "macros": [], "notes": [], "path": "/src/arch/x86/kvm/x86.c", "reportHash": "5463f660b059f9bd25f2ec5e526b2108", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
