<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"1": {"id": 1, "path": "/src/include/linux/mm.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_MM_H\n#define _LINUX_MM_H\n\n#include <linux/errno.h>\n\n#ifdef __KERNEL__\n\n#include <linux/mmdebug.h>\n#include <linux/gfp.h>\n#include <linux/bug.h>\n#include <linux/list.h>\n#include <linux/mmzone.h>\n#include <linux/rbtree.h>\n#include <linux/atomic.h>\n#include <linux/debug_locks.h>\n#include <linux/mm_types.h>\n#include <linux/mmap_lock.h>\n#include <linux/range.h>\n#include <linux/pfn.h>\n#include <linux/percpu-refcount.h>\n#include <linux/bit_spinlock.h>\n#include <linux/shrinker.h>\n#include <linux/resource.h>\n#include <linux/page_ext.h>\n#include <linux/err.h>\n#include <linux/page-flags.h>\n#include <linux/page_ref.h>\n#include <linux/memremap.h>\n#include <linux/overflow.h>\n#include <linux/sizes.h>\n#include <linux/sched.h>\n#include <linux/pgtable.h>\n#include <linux/kasan.h>\n\nstruct mempolicy;\nstruct anon_vma;\nstruct anon_vma_chain;\nstruct file_ra_state;\nstruct user_struct;\nstruct writeback_control;\nstruct bdi_writeback;\nstruct pt_regs;\n\nextern int sysctl_page_lock_unfairness;\n\nvoid init_mm_internals(void);\n\n#ifndef CONFIG_NEED_MULTIPLE_NODES\t/* Don't use mapnrs, do it properly */\nextern unsigned long max_mapnr;\n\nstatic inline void set_max_mapnr(unsigned long limit)\n{\n\tmax_mapnr = limit;\n}\n#else\nstatic inline void set_max_mapnr(unsigned long limit) { }\n#endif\n\nextern atomic_long_t _totalram_pages;\nstatic inline unsigned long totalram_pages(void)\n{\n\treturn (unsigned long)atomic_long_read(&_totalram_pages);\n}\n\nstatic inline void totalram_pages_inc(void)\n{\n\tatomic_long_inc(&_totalram_pages);\n}\n\nstatic inline void totalram_pages_dec(void)\n{\n\tatomic_long_dec(&_totalram_pages);\n}\n\nstatic inline void totalram_pages_add(long count)\n{\n\tatomic_long_add(count, &_totalram_pages);\n}\n\nextern void * high_memory;\nextern int page_cluster;\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_legacy_va_layout;\n#else\n#define sysctl_legacy_va_layout 0\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nextern const int mmap_rnd_bits_min;\nextern const int mmap_rnd_bits_max;\nextern int mmap_rnd_bits __read_mostly;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nextern const int mmap_rnd_compat_bits_min;\nextern const int mmap_rnd_compat_bits_max;\nextern int mmap_rnd_compat_bits __read_mostly;\n#endif\n\n#include <asm/page.h>\n#include <asm/processor.h>\n\n/*\n * Architectures that support memory tagging (assigning tags to memory regions,\n * embedding these tags into addresses that point to these memory regions, and\n * checking that the memory and the pointer tags match on memory accesses)\n * redefine this macro to strip tags from pointers.\n * It's defined as noop for arcitectures that don't support memory tagging.\n */\n#ifndef untagged_addr\n#define untagged_addr(addr) (addr)\n#endif\n\n#ifndef __pa_symbol\n#define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))\n#endif\n\n#ifndef page_to_virt\n#define page_to_virt(x)\t__va(PFN_PHYS(page_to_pfn(x)))\n#endif\n\n#ifndef lm_alias\n#define lm_alias(x)\t__va(__pa_symbol(x))\n#endif\n\n/*\n * To prevent common memory management code establishing\n * a zero page mapping on a read fault.\n * This macro should be defined within <asm/pgtable.h>.\n * s390 does this to prevent multiplexing of hardware bits\n * related to the physical page in case of virtualization.\n */\n#ifndef mm_forbids_zeropage\n#define mm_forbids_zeropage(X)\t(0)\n#endif\n\n/*\n * On some architectures it is expensive to call memset() for small sizes.\n * If an architecture decides to implement their own version of\n * mm_zero_struct_page they should wrap the defines below in a #ifndef and\n * define their own version of this macro in <asm/pgtable.h>\n */\n#if BITS_PER_LONG == 64\n/* This function must be updated when the size of struct page grows above 80\n * or reduces below 56. The idea that compiler optimizes out switch()\n * statement, and only leaves move/store instructions. Also the compiler can\n * combine write statments if they are both assignments and can be reordered,\n * this can result in several of the writes here being dropped.\n */\n#define\tmm_zero_struct_page(pp) __mm_zero_struct_page(pp)\nstatic inline void __mm_zero_struct_page(struct page *page)\n{\n\tunsigned long *_pp = (void *)page;\n\n\t /* Check that struct page is either 56, 64, 72, or 80 bytes */\n\tBUILD_BUG_ON(sizeof(struct page) & 7);\n\tBUILD_BUG_ON(sizeof(struct page) < 56);\n\tBUILD_BUG_ON(sizeof(struct page) > 80);\n\n\tswitch (sizeof(struct page)) {\n\tcase 80:\n\t\t_pp[9] = 0;\n\t\tfallthrough;\n\tcase 72:\n\t\t_pp[8] = 0;\n\t\tfallthrough;\n\tcase 64:\n\t\t_pp[7] = 0;\n\t\tfallthrough;\n\tcase 56:\n\t\t_pp[6] = 0;\n\t\t_pp[5] = 0;\n\t\t_pp[4] = 0;\n\t\t_pp[3] = 0;\n\t\t_pp[2] = 0;\n\t\t_pp[1] = 0;\n\t\t_pp[0] = 0;\n\t}\n}\n#else\n#define mm_zero_struct_page(pp)  ((void)memset((pp), 0, sizeof(struct page)))\n#endif\n\n/*\n * Default maximum number of active map areas, this limits the number of vmas\n * per mm struct. Users can overwrite this number by sysctl but there is a\n * problem.\n *\n * When a program's coredump is generated as ELF format, a section is created\n * per a vma. In ELF, the number of sections is represented in unsigned short.\n * This means the number of sections should be smaller than 65535 at coredump.\n * Because the kernel adds some informative sections to a image of program at\n * generating coredump, we need some margin. The number of extra sections is\n * 1-3 now and depends on arch. We use \"5\" as safe margin, here.\n *\n * ELF extended numbering allows more than 65535 sections, so 16-bit bound is\n * not a hard limit any more. Although some userspace tools can be surprised by\n * that.\n */\n#define MAPCOUNT_ELF_CORE_MARGIN\t(5)\n#define DEFAULT_MAX_MAP_COUNT\t(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\n\nextern int sysctl_max_map_count;\n\nextern unsigned long sysctl_user_reserve_kbytes;\nextern unsigned long sysctl_admin_reserve_kbytes;\n\nextern int sysctl_overcommit_memory;\nextern int sysctl_overcommit_ratio;\nextern unsigned long sysctl_overcommit_kbytes;\n\nint overcommit_ratio_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\nint overcommit_kbytes_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\nint overcommit_policy_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\n\n#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\n\n/* to align the pointer to the (next) page boundary */\n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n\n/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */\n#define PAGE_ALIGNED(addr)\tIS_ALIGNED((unsigned long)(addr), PAGE_SIZE)\n\n#define lru_to_page(head) (list_entry((head)->prev, struct page, lru))\n\n/*\n * Linux kernel virtual memory manager primitives.\n * The idea being to have a \"virtual\" mm in the same way\n * we have a virtual fs - giving a cleaner interface to the\n * mm details, and allowing different kinds of memory mappings\n * (from shared memory to executable loading to arbitrary\n * mmap() functions).\n */\n\nstruct vm_area_struct *vm_area_alloc(struct mm_struct *);\nstruct vm_area_struct *vm_area_dup(struct vm_area_struct *);\nvoid vm_area_free(struct vm_area_struct *);\n\n#ifndef CONFIG_MMU\nextern struct rb_root nommu_region_tree;\nextern struct rw_semaphore nommu_region_sem;\n\nextern unsigned int kobjsize(const void *objp);\n#endif\n\n/*\n * vm_flags in vm_area_struct, see mm_types.h.\n * When changing, update also include/trace/events/mmflags.h\n */\n#define VM_NONE\t\t0x00000000\n\n#define VM_READ\t\t0x00000001\t/* currently active flags */\n#define VM_WRITE\t0x00000002\n#define VM_EXEC\t\t0x00000004\n#define VM_SHARED\t0x00000008\n\n/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */\n#define VM_MAYREAD\t0x00000010\t/* limits for mprotect() etc */\n#define VM_MAYWRITE\t0x00000020\n#define VM_MAYEXEC\t0x00000040\n#define VM_MAYSHARE\t0x00000080\n\n#define VM_GROWSDOWN\t0x00000100\t/* general info on the segment */\n#define VM_UFFD_MISSING\t0x00000200\t/* missing pages tracking */\n#define VM_PFNMAP\t0x00000400\t/* Page-ranges managed without \"struct page\", just pure PFN */\n#define VM_DENYWRITE\t0x00000800\t/* ETXTBSY on write attempts.. */\n#define VM_UFFD_WP\t0x00001000\t/* wrprotect pages tracking */\n\n#define VM_LOCKED\t0x00002000\n#define VM_IO           0x00004000\t/* Memory mapped I/O or similar */\n\n\t\t\t\t\t/* Used by sys_madvise() */\n#define VM_SEQ_READ\t0x00008000\t/* App will access data sequentially */\n#define VM_RAND_READ\t0x00010000\t/* App will not benefit from clustered reads */\n\n#define VM_DONTCOPY\t0x00020000      /* Do not copy this vma on fork */\n#define VM_DONTEXPAND\t0x00040000\t/* Cannot expand with mremap() */\n#define VM_LOCKONFAULT\t0x00080000\t/* Lock the pages covered when they are faulted in */\n#define VM_ACCOUNT\t0x00100000\t/* Is a VM accounted object */\n#define VM_NORESERVE\t0x00200000\t/* should the VM suppress accounting */\n#define VM_HUGETLB\t0x00400000\t/* Huge TLB Page VM */\n#define VM_SYNC\t\t0x00800000\t/* Synchronous page faults */\n#define VM_ARCH_1\t0x01000000\t/* Architecture-specific flag */\n#define VM_WIPEONFORK\t0x02000000\t/* Wipe VMA contents in child. */\n#define VM_DONTDUMP\t0x04000000\t/* Do not include in the core dump */\n\n#ifdef CONFIG_MEM_SOFT_DIRTY\n# define VM_SOFTDIRTY\t0x08000000\t/* Not soft dirty clean area */\n#else\n# define VM_SOFTDIRTY\t0\n#endif\n\n#define VM_MIXEDMAP\t0x10000000\t/* Can contain \"struct page\" and pure PFN pages */\n#define VM_HUGEPAGE\t0x20000000\t/* MADV_HUGEPAGE marked this vma */\n#define VM_NOHUGEPAGE\t0x40000000\t/* MADV_NOHUGEPAGE marked this vma */\n#define VM_MERGEABLE\t0x80000000\t/* KSM may merge identical pages */\n\n#ifdef CONFIG_ARCH_USES_HIGH_VMA_FLAGS\n#define VM_HIGH_ARCH_BIT_0\t32\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_1\t33\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_2\t34\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_3\t35\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_BIT_4\t36\t/* bit only usable on 64-bit architectures */\n#define VM_HIGH_ARCH_0\tBIT(VM_HIGH_ARCH_BIT_0)\n#define VM_HIGH_ARCH_1\tBIT(VM_HIGH_ARCH_BIT_1)\n#define VM_HIGH_ARCH_2\tBIT(VM_HIGH_ARCH_BIT_2)\n#define VM_HIGH_ARCH_3\tBIT(VM_HIGH_ARCH_BIT_3)\n#define VM_HIGH_ARCH_4\tBIT(VM_HIGH_ARCH_BIT_4)\n#endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */\n\n#ifdef CONFIG_ARCH_HAS_PKEYS\n# define VM_PKEY_SHIFT\tVM_HIGH_ARCH_BIT_0\n# define VM_PKEY_BIT0\tVM_HIGH_ARCH_0\t/* A protection key is a 4-bit value */\n# define VM_PKEY_BIT1\tVM_HIGH_ARCH_1\t/* on x86 and 5-bit value on ppc64   */\n# define VM_PKEY_BIT2\tVM_HIGH_ARCH_2\n# define VM_PKEY_BIT3\tVM_HIGH_ARCH_3\n#ifdef CONFIG_PPC\n# define VM_PKEY_BIT4  VM_HIGH_ARCH_4\n#else\n# define VM_PKEY_BIT4  0\n#endif\n#endif /* CONFIG_ARCH_HAS_PKEYS */\n\n#if defined(CONFIG_X86)\n# define VM_PAT\t\tVM_ARCH_1\t/* PAT reserves whole VMA at once (x86) */\n#elif defined(CONFIG_PPC)\n# define VM_SAO\t\tVM_ARCH_1\t/* Strong Access Ordering (powerpc) */\n#elif defined(CONFIG_PARISC)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_IA64)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_SPARC64)\n# define VM_SPARC_ADI\tVM_ARCH_1\t/* Uses ADI tag for access control */\n# define VM_ARCH_CLEAR\tVM_SPARC_ADI\n#elif defined(CONFIG_ARM64)\n# define VM_ARM64_BTI\tVM_ARCH_1\t/* BTI guarded page, a.k.a. GP bit */\n# define VM_ARCH_CLEAR\tVM_ARM64_BTI\n#elif !defined(CONFIG_MMU)\n# define VM_MAPPED_COPY\tVM_ARCH_1\t/* T if mapped copy of data (nommu mmap) */\n#endif\n\n#if defined(CONFIG_ARM64_MTE)\n# define VM_MTE\t\tVM_HIGH_ARCH_0\t/* Use Tagged memory for access control */\n# define VM_MTE_ALLOWED\tVM_HIGH_ARCH_1\t/* Tagged memory permitted */\n#else\n# define VM_MTE\t\tVM_NONE\n# define VM_MTE_ALLOWED\tVM_NONE\n#endif\n\n#ifndef VM_GROWSUP\n# define VM_GROWSUP\tVM_NONE\n#endif\n\n/* Bits set in the VMA until the stack is in its final location */\n#define VM_STACK_INCOMPLETE_SETUP\t(VM_RAND_READ | VM_SEQ_READ)\n\n#define TASK_EXEC ((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0)\n\n/* Common data flag combinations */\n#define VM_DATA_FLAGS_TSK_EXEC\t(VM_READ | VM_WRITE | TASK_EXEC | \\\n\t\t\t\t VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)\n#define VM_DATA_FLAGS_NON_EXEC\t(VM_READ | VM_WRITE | VM_MAYREAD | \\\n\t\t\t\t VM_MAYWRITE | VM_MAYEXEC)\n#define VM_DATA_FLAGS_EXEC\t(VM_READ | VM_WRITE | VM_EXEC | \\\n\t\t\t\t VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)\n\n#ifndef VM_DATA_DEFAULT_FLAGS\t\t/* arch can override this */\n#define VM_DATA_DEFAULT_FLAGS  VM_DATA_FLAGS_EXEC\n#endif\n\n#ifndef VM_STACK_DEFAULT_FLAGS\t\t/* arch can override this */\n#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\n#endif\n\n#ifdef CONFIG_STACK_GROWSUP\n#define VM_STACK\tVM_GROWSUP\n#else\n#define VM_STACK\tVM_GROWSDOWN\n#endif\n\n#define VM_STACK_FLAGS\t(VM_STACK | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n\n/* VMA basic access permission flags */\n#define VM_ACCESS_FLAGS (VM_READ | VM_WRITE | VM_EXEC)\n\n\n/*\n * Special vmas that are non-mergable, non-mlock()able.\n */\n#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)\n\n/* This mask prevents VMA from being scanned with khugepaged */\n#define VM_NO_KHUGEPAGED (VM_SPECIAL | VM_HUGETLB)\n\n/* This mask defines which mm->def_flags a process can inherit its parent */\n#define VM_INIT_DEF_MASK\tVM_NOHUGEPAGE\n\n/* This mask is used to clear all the VMA flags used by mlock */\n#define VM_LOCKED_CLEAR_MASK\t(~(VM_LOCKED | VM_LOCKONFAULT))\n\n/* Arch-specific flags to clear when updating VM flags on protection change */\n#ifndef VM_ARCH_CLEAR\n# define VM_ARCH_CLEAR\tVM_NONE\n#endif\n#define VM_FLAGS_CLEAR\t(ARCH_VM_PKEY_FLAGS | VM_ARCH_CLEAR)\n\n/*\n * mapping from the currently active vm_flags protection bits (the\n * low four bits) to a page protection mask..\n */\nextern pgprot_t protection_map[16];\n\n/**\n * Fault flag definitions.\n *\n * @FAULT_FLAG_WRITE: Fault was a write fault.\n * @FAULT_FLAG_MKWRITE: Fault was mkwrite of existing PTE.\n * @FAULT_FLAG_ALLOW_RETRY: Allow to retry the fault if blocked.\n * @FAULT_FLAG_RETRY_NOWAIT: Don't drop mmap_lock and wait when retrying.\n * @FAULT_FLAG_KILLABLE: The fault task is in SIGKILL killable region.\n * @FAULT_FLAG_TRIED: The fault has been tried once.\n * @FAULT_FLAG_USER: The fault originated in userspace.\n * @FAULT_FLAG_REMOTE: The fault is not for current task/mm.\n * @FAULT_FLAG_INSTRUCTION: The fault was during an instruction fetch.\n * @FAULT_FLAG_INTERRUPTIBLE: The fault can be interrupted by non-fatal signals.\n *\n * About @FAULT_FLAG_ALLOW_RETRY and @FAULT_FLAG_TRIED: we can specify\n * whether we would allow page faults to retry by specifying these two\n * fault flags correctly.  Currently there can be three legal combinations:\n *\n * (a) ALLOW_RETRY and !TRIED:  this means the page fault allows retry, and\n *                              this is the first try\n *\n * (b) ALLOW_RETRY and TRIED:   this means the page fault allows retry, and\n *                              we've already tried at least once\n *\n * (c) !ALLOW_RETRY and !TRIED: this means the page fault does not allow retry\n *\n * The unlisted combination (!ALLOW_RETRY && TRIED) is illegal and should never\n * be used.  Note that page faults can be allowed to retry for multiple times,\n * in which case we'll have an initial fault with flags (a) then later on\n * continuous faults with flags (b).  We should always try to detect pending\n * signals before a retry to make sure the continuous page faults can still be\n * interrupted if necessary.\n */\n#define FAULT_FLAG_WRITE\t\t\t0x01\n#define FAULT_FLAG_MKWRITE\t\t\t0x02\n#define FAULT_FLAG_ALLOW_RETRY\t\t\t0x04\n#define FAULT_FLAG_RETRY_NOWAIT\t\t\t0x08\n#define FAULT_FLAG_KILLABLE\t\t\t0x10\n#define FAULT_FLAG_TRIED\t\t\t0x20\n#define FAULT_FLAG_USER\t\t\t\t0x40\n#define FAULT_FLAG_REMOTE\t\t\t0x80\n#define FAULT_FLAG_INSTRUCTION  \t\t0x100\n#define FAULT_FLAG_INTERRUPTIBLE\t\t0x200\n\n/*\n * The default fault flags that should be used by most of the\n * arch-specific page fault handlers.\n */\n#define FAULT_FLAG_DEFAULT  (FAULT_FLAG_ALLOW_RETRY | \\\n\t\t\t     FAULT_FLAG_KILLABLE | \\\n\t\t\t     FAULT_FLAG_INTERRUPTIBLE)\n\n/**\n * fault_flag_allow_retry_first - check ALLOW_RETRY the first time\n *\n * This is mostly used for places where we want to try to avoid taking\n * the mmap_lock for too long a time when waiting for another condition\n * to change, in which case we can try to be polite to release the\n * mmap_lock in the first round to avoid potential starvation of other\n * processes that would also want the mmap_lock.\n *\n * Return: true if the page fault allows retry and this is the first\n * attempt of the fault handling; false otherwise.\n */\nstatic inline bool fault_flag_allow_retry_first(unsigned int flags)\n{\n\treturn (flags & FAULT_FLAG_ALLOW_RETRY) &&\n\t    (!(flags & FAULT_FLAG_TRIED));\n}\n\n#define FAULT_FLAG_TRACE \\\n\t{ FAULT_FLAG_WRITE,\t\t\"WRITE\" }, \\\n\t{ FAULT_FLAG_MKWRITE,\t\t\"MKWRITE\" }, \\\n\t{ FAULT_FLAG_ALLOW_RETRY,\t\"ALLOW_RETRY\" }, \\\n\t{ FAULT_FLAG_RETRY_NOWAIT,\t\"RETRY_NOWAIT\" }, \\\n\t{ FAULT_FLAG_KILLABLE,\t\t\"KILLABLE\" }, \\\n\t{ FAULT_FLAG_TRIED,\t\t\"TRIED\" }, \\\n\t{ FAULT_FLAG_USER,\t\t\"USER\" }, \\\n\t{ FAULT_FLAG_REMOTE,\t\t\"REMOTE\" }, \\\n\t{ FAULT_FLAG_INSTRUCTION,\t\"INSTRUCTION\" }, \\\n\t{ FAULT_FLAG_INTERRUPTIBLE,\t\"INTERRUPTIBLE\" }\n\n/*\n * vm_fault is filled by the pagefault handler and passed to the vma's\n * ->fault function. The vma's ->fault is responsible for returning a bitmask\n * of VM_FAULT_xxx flags that give details about how the fault was handled.\n *\n * MM layer fills up gfp_mask for page allocations but fault handler might\n * alter it if its implementation requires a different allocation context.\n *\n * pgoff should be used in favour of virtual_address, if possible.\n */\nstruct vm_fault {\n\tstruct vm_area_struct *vma;\t/* Target VMA */\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tgfp_t gfp_mask;\t\t\t/* gfp mask to be used for allocations */\n\tpgoff_t pgoff;\t\t\t/* Logical page offset based on vma */\n\tunsigned long address;\t\t/* Faulting virtual address */\n\tpmd_t *pmd;\t\t\t/* Pointer to pmd entry matching\n\t\t\t\t\t * the 'address' */\n\tpud_t *pud;\t\t\t/* Pointer to pud entry matching\n\t\t\t\t\t * the 'address'\n\t\t\t\t\t */\n\tpte_t orig_pte;\t\t\t/* Value of PTE at the time of fault */\n\n\tstruct page *cow_page;\t\t/* Page handler may use for COW fault */\n\tstruct page *page;\t\t/* ->fault handlers should return a\n\t\t\t\t\t * page here, unless VM_FAULT_NOPAGE\n\t\t\t\t\t * is set (which is also implied by\n\t\t\t\t\t * VM_FAULT_ERROR).\n\t\t\t\t\t */\n\t/* These three entries are valid only while holding ptl lock */\n\tpte_t *pte;\t\t\t/* Pointer to pte entry matching\n\t\t\t\t\t * the 'address'. NULL if the page\n\t\t\t\t\t * table hasn't been allocated.\n\t\t\t\t\t */\n\tspinlock_t *ptl;\t\t/* Page table lock.\n\t\t\t\t\t * Protects pte page table if 'pte'\n\t\t\t\t\t * is not NULL, otherwise pmd.\n\t\t\t\t\t */\n\tpgtable_t prealloc_pte;\t\t/* Pre-allocated pte page table.\n\t\t\t\t\t * vm_ops->map_pages() calls\n\t\t\t\t\t * alloc_set_pte() from atomic context.\n\t\t\t\t\t * do_fault_around() pre-allocates\n\t\t\t\t\t * page table to avoid allocation from\n\t\t\t\t\t * atomic context.\n\t\t\t\t\t */\n};\n\n/* page entry size for vm->huge_fault() */\nenum page_entry_size {\n\tPE_SIZE_PTE = 0,\n\tPE_SIZE_PMD,\n\tPE_SIZE_PUD,\n};\n\n/*\n * These are the virtual MM functions - opening of an area, closing and\n * unmapping it (needed to keep files on disk up-to-date etc), pointer\n * to the functions called when a no-page or a wp-page exception occurs.\n */\nstruct vm_operations_struct {\n\tvoid (*open)(struct vm_area_struct * area);\n\tvoid (*close)(struct vm_area_struct * area);\n\t/* Called any time before splitting to check if it's allowed */\n\tint (*may_split)(struct vm_area_struct *area, unsigned long addr);\n\tint (*mremap)(struct vm_area_struct *area, unsigned long flags);\n\tvm_fault_t (*fault)(struct vm_fault *vmf);\n\tvm_fault_t (*huge_fault)(struct vm_fault *vmf,\n\t\t\tenum page_entry_size pe_size);\n\tvoid (*map_pages)(struct vm_fault *vmf,\n\t\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\n\tunsigned long (*pagesize)(struct vm_area_struct * area);\n\n\t/* notification that a previously read-only page is about to become\n\t * writable, if an error is returned it will cause a SIGBUS */\n\tvm_fault_t (*page_mkwrite)(struct vm_fault *vmf);\n\n\t/* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */\n\tvm_fault_t (*pfn_mkwrite)(struct vm_fault *vmf);\n\n\t/* called by access_process_vm when get_user_pages() fails, typically\n\t * for use by special VMAs that can switch between memory and hardware\n\t */\n\tint (*access)(struct vm_area_struct *vma, unsigned long addr,\n\t\t      void *buf, int len, int write);\n\n\t/* Called by the /proc/PID/maps code to ask the vma whether it\n\t * has a special name.  Returning non-NULL will also cause this\n\t * vma to be dumped unconditionally. */\n\tconst char *(*name)(struct vm_area_struct *vma);\n\n#ifdef CONFIG_NUMA\n\t/*\n\t * set_policy() op must add a reference to any non-NULL @new mempolicy\n\t * to hold the policy upon return.  Caller should pass NULL @new to\n\t * remove a policy and fall back to surrounding context--i.e. do not\n\t * install a MPOL_DEFAULT policy, nor the task or system default\n\t * mempolicy.\n\t */\n\tint (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\n\n\t/*\n\t * get_policy() op must add reference [mpol_get()] to any policy at\n\t * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure\n\t * in mm/mempolicy.c will do this automatically.\n\t * get_policy() must NOT add a ref if the policy at (vma,addr) is not\n\t * marked as MPOL_SHARED. vma policies are protected by the mmap_lock.\n\t * If no [shared/vma] mempolicy exists at the addr, get_policy() op\n\t * must return NULL--i.e., do not \"fallback\" to task or system default\n\t * policy.\n\t */\n\tstruct mempolicy *(*get_policy)(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr);\n#endif\n\t/*\n\t * Called by vm_normal_page() for special PTEs to find the\n\t * page for @addr.  This is useful if the default behavior\n\t * (using pte_page()) would not find the correct page.\n\t */\n\tstruct page *(*find_special_page)(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr);\n};\n\nstatic inline void vma_init(struct vm_area_struct *vma, struct mm_struct *mm)\n{\n\tstatic const struct vm_operations_struct dummy_vm_ops = {};\n\n\tmemset(vma, 0, sizeof(*vma));\n\tvma->vm_mm = mm;\n\tvma->vm_ops = &dummy_vm_ops;\n\tINIT_LIST_HEAD(&vma->anon_vma_chain);\n}\n\nstatic inline void vma_set_anonymous(struct vm_area_struct *vma)\n{\n\tvma->vm_ops = NULL;\n}\n\nstatic inline bool vma_is_anonymous(struct vm_area_struct *vma)\n{\n\treturn !vma->vm_ops;\n}\n\nstatic inline bool vma_is_temporary_stack(struct vm_area_struct *vma)\n{\n\tint maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);\n\n\tif (!maybe_stack)\n\t\treturn false;\n\n\tif ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==\n\t\t\t\t\t\tVM_STACK_INCOMPLETE_SETUP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool vma_is_foreign(struct vm_area_struct *vma)\n{\n\tif (!current->mm)\n\t\treturn true;\n\n\tif (current->mm != vma->vm_mm)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool vma_is_accessible(struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & VM_ACCESS_FLAGS;\n}\n\n#ifdef CONFIG_SHMEM\n/*\n * The vma_is_shmem is not inline because it is used only by slow\n * paths in userfault.\n */\nbool vma_is_shmem(struct vm_area_struct *vma);\n#else\nstatic inline bool vma_is_shmem(struct vm_area_struct *vma) { return false; }\n#endif\n\nint vma_is_stack_for_current(struct vm_area_struct *vma);\n\n/* flush_tlb_range() takes a vma, not a mm, and can care about flags */\n#define TLB_FLUSH_VMA(mm,flags) { .vm_mm = (mm), .vm_flags = (flags) }\n\nstruct mmu_gather;\nstruct inode;\n\n#include <linux/huge_mm.h>\n\n/*\n * Methods to modify the page usage count.\n *\n * What counts for a page usage:\n * - cache mapping   (page->mapping)\n * - private data    (page->private)\n * - page mapped in a task's page tables, each mapping\n *   is counted separately\n *\n * Also, many kernel routines increase the page count before a critical\n * routine so they can be sure the page doesn't go away from under them.\n */\n\n/*\n * Drop a ref, return true if the refcount fell to zero (the page has no users)\n */\nstatic inline int put_page_testzero(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\n\treturn page_ref_dec_and_test(page);\n}\n\n/*\n * Try to grab a ref unless the page has a refcount of zero, return false if\n * that is the case.\n * This can be called when MMU is off so it must not access\n * any of the virtual mappings.\n */\nstatic inline int get_page_unless_zero(struct page *page)\n{\n\treturn page_ref_add_unless(page, 1, 0);\n}\n\nextern int page_is_ram(unsigned long pfn);\n\nenum {\n\tREGION_INTERSECTS,\n\tREGION_DISJOINT,\n\tREGION_MIXED,\n};\n\nint region_intersects(resource_size_t offset, size_t size, unsigned long flags,\n\t\t      unsigned long desc);\n\n/* Support for virtually mapped pages */\nstruct page *vmalloc_to_page(const void *addr);\nunsigned long vmalloc_to_pfn(const void *addr);\n\n/*\n * Determine if an address is within the vmalloc range\n *\n * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there\n * is no special casing required.\n */\n\n#ifndef is_ioremap_addr\n#define is_ioremap_addr(x) is_vmalloc_addr(x)\n#endif\n\n#ifdef CONFIG_MMU\nextern bool is_vmalloc_addr(const void *x);\nextern int is_vmalloc_or_module_addr(const void *x);\n#else\nstatic inline bool is_vmalloc_addr(const void *x)\n{\n\treturn false;\n}\nstatic inline int is_vmalloc_or_module_addr(const void *x)\n{\n\treturn 0;\n}\n#endif\n\nextern void *kvmalloc_node(size_t size, gfp_t flags, int node);\nstatic inline void *kvmalloc(size_t size, gfp_t flags)\n{\n\treturn kvmalloc_node(size, flags, NUMA_NO_NODE);\n}\nstatic inline void *kvzalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn kvmalloc_node(size, flags | __GFP_ZERO, node);\n}\nstatic inline void *kvzalloc(size_t size, gfp_t flags)\n{\n\treturn kvmalloc(size, flags | __GFP_ZERO);\n}\n\nstatic inline void *kvmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\n\treturn kvmalloc(bytes, flags);\n}\n\nstatic inline void *kvcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn kvmalloc_array(n, size, flags | __GFP_ZERO);\n}\n\nextern void kvfree(const void *addr);\nextern void kvfree_sensitive(const void *addr, size_t len);\n\nstatic inline int head_compound_mapcount(struct page *head)\n{\n\treturn atomic_read(compound_mapcount_ptr(head)) + 1;\n}\n\n/*\n * Mapcount of compound page as a whole, does not include mapped sub-pages.\n *\n * Must be called only for compound pages or any their tail sub-pages.\n */\nstatic inline int compound_mapcount(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageCompound(page), page);\n\tpage = compound_head(page);\n\treturn head_compound_mapcount(page);\n}\n\n/*\n * The atomic page->_mapcount, starts from -1: so that transitions\n * both from it and to it can be tracked, using atomic_inc_and_test\n * and atomic_add_negative(-1).\n */\nstatic inline void page_mapcount_reset(struct page *page)\n{\n\tatomic_set(&(page)->_mapcount, -1);\n}\n\nint __page_mapcount(struct page *page);\n\n/*\n * Mapcount of 0-order page; when compound sub-page, includes\n * compound_mapcount().\n *\n * Result is undefined for pages which cannot be mapped into userspace.\n * For example SLAB or special types of pages. See function page_has_type().\n * They use this place in struct page differently.\n */\nstatic inline int page_mapcount(struct page *page)\n{\n\tif (unlikely(PageCompound(page)))\n\t\treturn __page_mapcount(page);\n\treturn atomic_read(&page->_mapcount) + 1;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nint total_mapcount(struct page *page);\nint page_trans_huge_mapcount(struct page *page, int *total_mapcount);\n#else\nstatic inline int total_mapcount(struct page *page)\n{\n\treturn page_mapcount(page);\n}\nstatic inline int page_trans_huge_mapcount(struct page *page,\n\t\t\t\t\t   int *total_mapcount)\n{\n\tint mapcount = page_mapcount(page);\n\tif (total_mapcount)\n\t\t*total_mapcount = mapcount;\n\treturn mapcount;\n}\n#endif\n\nstatic inline struct page *virt_to_head_page(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\n\treturn compound_head(page);\n}\n\nvoid __put_page(struct page *page);\n\nvoid put_pages_list(struct list_head *pages);\n\nvoid split_page(struct page *page, unsigned int order);\n\n/*\n * Compound pages have a destructor function.  Provide a\n * prototype for that function and accessor functions.\n * These are _only_ valid on the head of a compound page.\n */\ntypedef void compound_page_dtor(struct page *);\n\n/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */\nenum compound_dtor_id {\n\tNULL_COMPOUND_DTOR,\n\tCOMPOUND_PAGE_DTOR,\n#ifdef CONFIG_HUGETLB_PAGE\n\tHUGETLB_PAGE_DTOR,\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tTRANSHUGE_PAGE_DTOR,\n#endif\n\tNR_COMPOUND_DTORS,\n};\nextern compound_page_dtor * const compound_page_dtors[NR_COMPOUND_DTORS];\n\nstatic inline void set_compound_page_dtor(struct page *page,\n\t\tenum compound_dtor_id compound_dtor)\n{\n\tVM_BUG_ON_PAGE(compound_dtor >= NR_COMPOUND_DTORS, page);\n\tpage[1].compound_dtor = compound_dtor;\n}\n\nstatic inline void destroy_compound_page(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page[1].compound_dtor >= NR_COMPOUND_DTORS, page);\n\tcompound_page_dtors[page[1].compound_dtor](page);\n}\n\nstatic inline unsigned int compound_order(struct page *page)\n{\n\tif (!PageHead(page))\n\t\treturn 0;\n\treturn page[1].compound_order;\n}\n\nstatic inline bool hpage_pincount_available(struct page *page)\n{\n\t/*\n\t * Can the page->hpage_pinned_refcount field be used? That field is in\n\t * the 3rd page of the compound page, so the smallest (2-page) compound\n\t * pages cannot support it.\n\t */\n\tpage = compound_head(page);\n\treturn PageCompound(page) && compound_order(page) > 1;\n}\n\nstatic inline int head_compound_pincount(struct page *head)\n{\n\treturn atomic_read(compound_pincount_ptr(head));\n}\n\nstatic inline int compound_pincount(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!hpage_pincount_available(page), page);\n\tpage = compound_head(page);\n\treturn head_compound_pincount(page);\n}\n\nstatic inline void set_compound_order(struct page *page, unsigned int order)\n{\n\tpage[1].compound_order = order;\n\tpage[1].compound_nr = 1U << order;\n}\n\n/* Returns the number of pages in this potentially compound page. */\nstatic inline unsigned long compound_nr(struct page *page)\n{\n\tif (!PageHead(page))\n\t\treturn 1;\n\treturn page[1].compound_nr;\n}\n\n/* Returns the number of bytes in this potentially compound page. */\nstatic inline unsigned long page_size(struct page *page)\n{\n\treturn PAGE_SIZE << compound_order(page);\n}\n\n/* Returns the number of bits needed for the number of bytes in a page */\nstatic inline unsigned int page_shift(struct page *page)\n{\n\treturn PAGE_SHIFT + compound_order(page);\n}\n\nvoid free_compound_page(struct page *page);\n\n#ifdef CONFIG_MMU\n/*\n * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when\n * servicing faults for write access.  In the normal case, do always want\n * pte_mkwrite.  But get_user_pages can cause write faults for mappings\n * that do not have writing enabled, when used by access_process_vm.\n */\nstatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpte = pte_mkwrite(pte);\n\treturn pte;\n}\n\nvm_fault_t alloc_set_pte(struct vm_fault *vmf, struct page *page);\nvm_fault_t finish_fault(struct vm_fault *vmf);\nvm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);\n#endif\n\n/*\n * Multiple processes may \"see\" the same page. E.g. for untouched\n * mappings of /dev/null, all processes see the same page full of\n * zeroes, and text pages of executables and shared libraries have\n * only one copy in memory, at most, normally.\n *\n * For the non-reserved pages, page_count(page) denotes a reference count.\n *   page_count() == 0 means the page is free. page->lru is then used for\n *   freelist management in the buddy allocator.\n *   page_count() > 0  means the page has been allocated.\n *\n * Pages are allocated by the slab allocator in order to provide memory\n * to kmalloc and kmem_cache_alloc. In this case, the management of the\n * page, and the fields in 'struct page' are the responsibility of mm/slab.c\n * unless a particular usage is carefully commented. (the responsibility of\n * freeing the kmalloc memory is the caller's, of course).\n *\n * A page may be used by anyone else who does a __get_free_page().\n * In this case, page_count still tracks the references, and should only\n * be used through the normal accessor functions. The top bits of page->flags\n * and page->virtual store page management information, but all other fields\n * are unused and could be used privately, carefully. The management of this\n * page is the responsibility of the one who allocated it, and those who have\n * subsequently been given references to it.\n *\n * The other pages (we may call them \"pagecache pages\") are completely\n * managed by the Linux memory manager: I/O, buffers, swapping etc.\n * The following discussion applies only to them.\n *\n * A pagecache page contains an opaque `private' member, which belongs to the\n * page's address_space. Usually, this is the address of a circular list of\n * the page's disk buffers. PG_private must be set to tell the VM to call\n * into the filesystem to release these pages.\n *\n * A page may belong to an inode's memory mapping. In this case, page->mapping\n * is the pointer to the inode, and page->index is the file offset of the page,\n * in units of PAGE_SIZE.\n *\n * If pagecache pages are not associated with an inode, they are said to be\n * anonymous pages. These may become associated with the swapcache, and in that\n * case PG_swapcache is set, and page->private is an offset into the swapcache.\n *\n * In either case (swapcache or inode backed), the pagecache itself holds one\n * reference to the page. Setting PG_private should also increment the\n * refcount. The each user mapping also has a reference to the page.\n *\n * The pagecache pages are stored in a per-mapping radix tree, which is\n * rooted at mapping->i_pages, and indexed by offset.\n * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space\n * lists, we instead now tag pages as dirty/writeback in the radix tree.\n *\n * All pagecache pages may be subject to I/O:\n * - inode pages may need to be read from disk,\n * - inode pages which have been modified and are MAP_SHARED may need\n *   to be written back to the inode on disk,\n * - anonymous pages (including MAP_PRIVATE file mappings) which have been\n *   modified may need to be swapped out to swap space and (later) to be read\n *   back into memory.\n */\n\n/*\n * The zone field is never updated after free_area_init_core()\n * sets it, so none of the operations on it need to be atomic.\n */\n\n/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */\n#define SECTIONS_PGOFF\t\t((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\n#define NODES_PGOFF\t\t(SECTIONS_PGOFF - NODES_WIDTH)\n#define ZONES_PGOFF\t\t(NODES_PGOFF - ZONES_WIDTH)\n#define LAST_CPUPID_PGOFF\t(ZONES_PGOFF - LAST_CPUPID_WIDTH)\n#define KASAN_TAG_PGOFF\t\t(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)\n\n/*\n * Define the bit shifts to access each section.  For non-existent\n * sections we define the shift as 0; that plus a 0 mask ensures\n * the compiler will optimise away reference to them.\n */\n#define SECTIONS_PGSHIFT\t(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\n#define NODES_PGSHIFT\t\t(NODES_PGOFF * (NODES_WIDTH != 0))\n#define ZONES_PGSHIFT\t\t(ZONES_PGOFF * (ZONES_WIDTH != 0))\n#define LAST_CPUPID_PGSHIFT\t(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))\n#define KASAN_TAG_PGSHIFT\t(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))\n\n/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n#define ZONEID_SHIFT\t\t(SECTIONS_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((SECTIONS_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tSECTIONS_PGOFF : ZONES_PGOFF)\n#else\n#define ZONEID_SHIFT\t\t(NODES_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((NODES_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tNODES_PGOFF : ZONES_PGOFF)\n#endif\n\n#define ZONEID_PGSHIFT\t\t(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\n\n#define ZONES_MASK\t\t((1UL << ZONES_WIDTH) - 1)\n#define NODES_MASK\t\t((1UL << NODES_WIDTH) - 1)\n#define SECTIONS_MASK\t\t((1UL << SECTIONS_WIDTH) - 1)\n#define LAST_CPUPID_MASK\t((1UL << LAST_CPUPID_SHIFT) - 1)\n#define KASAN_TAG_MASK\t\t((1UL << KASAN_TAG_WIDTH) - 1)\n#define ZONEID_MASK\t\t((1UL << ZONEID_SHIFT) - 1)\n\nstatic inline enum zone_type page_zonenum(const struct page *page)\n{\n\tASSERT_EXCLUSIVE_BITS(page->flags, ZONES_MASK << ZONES_PGSHIFT);\n\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn page_zonenum(page) == ZONE_DEVICE;\n}\nextern void memmap_init_zone_device(struct zone *, unsigned long,\n\t\t\t\t    unsigned long, struct dev_pagemap *);\n#else\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn false;\n}\n#endif\n\n#ifdef CONFIG_DEV_PAGEMAP_OPS\nvoid free_devmap_managed_page(struct page *page);\nDECLARE_STATIC_KEY_FALSE(devmap_managed_key);\n\nstatic inline bool page_is_devmap_managed(struct page *page)\n{\n\tif (!static_branch_unlikely(&devmap_managed_key))\n\t\treturn false;\n\tif (!is_zone_device_page(page))\n\t\treturn false;\n\tswitch (page->pgmap->type) {\n\tcase MEMORY_DEVICE_PRIVATE:\n\tcase MEMORY_DEVICE_FS_DAX:\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n}\n\nvoid put_devmap_managed_page(struct page *page);\n\n#else /* CONFIG_DEV_PAGEMAP_OPS */\nstatic inline bool page_is_devmap_managed(struct page *page)\n{\n\treturn false;\n}\n\nstatic inline void put_devmap_managed_page(struct page *page)\n{\n}\n#endif /* CONFIG_DEV_PAGEMAP_OPS */\n\nstatic inline bool is_device_private_page(const struct page *page)\n{\n\treturn IS_ENABLED(CONFIG_DEV_PAGEMAP_OPS) &&\n\t\tIS_ENABLED(CONFIG_DEVICE_PRIVATE) &&\n\t\tis_zone_device_page(page) &&\n\t\tpage->pgmap->type == MEMORY_DEVICE_PRIVATE;\n}\n\nstatic inline bool is_pci_p2pdma_page(const struct page *page)\n{\n\treturn IS_ENABLED(CONFIG_DEV_PAGEMAP_OPS) &&\n\t\tIS_ENABLED(CONFIG_PCI_P2PDMA) &&\n\t\tis_zone_device_page(page) &&\n\t\tpage->pgmap->type == MEMORY_DEVICE_PCI_P2PDMA;\n}\n\n/* 127: arbitrary random number, small enough to assemble well */\n#define page_ref_zero_or_close_to_overflow(page) \\\n\t((unsigned int) page_ref_count(page) + 127u <= 127u)\n\nstatic inline void get_page(struct page *page)\n{\n\tpage = compound_head(page);\n\t/*\n\t * Getting a normal page or the head of a compound page\n\t * requires to already have an elevated page->_refcount.\n\t */\n\tVM_BUG_ON_PAGE(page_ref_zero_or_close_to_overflow(page), page);\n\tpage_ref_inc(page);\n}\n\nbool __must_check try_grab_page(struct page *page, unsigned int flags);\n\nstatic inline __must_check bool try_get_page(struct page *page)\n{\n\tpage = compound_head(page);\n\tif (WARN_ON_ONCE(page_ref_count(page) <= 0))\n\t\treturn false;\n\tpage_ref_inc(page);\n\treturn true;\n}\n\nstatic inline void put_page(struct page *page)\n{\n\tpage = compound_head(page);\n\n\t/*\n\t * For devmap managed pages we need to catch refcount transition from\n\t * 2 to 1, when refcount reach one it means the page is free and we\n\t * need to inform the device driver through callback. See\n\t * include/linux/memremap.h and HMM for details.\n\t */\n\tif (page_is_devmap_managed(page)) {\n\t\tput_devmap_managed_page(page);\n\t\treturn;\n\t}\n\n\tif (put_page_testzero(page))\n\t\t__put_page(page);\n}\n\n/*\n * GUP_PIN_COUNTING_BIAS, and the associated functions that use it, overload\n * the page's refcount so that two separate items are tracked: the original page\n * reference count, and also a new count of how many pin_user_pages() calls were\n * made against the page. (\"gup-pinned\" is another term for the latter).\n *\n * With this scheme, pin_user_pages() becomes special: such pages are marked as\n * distinct from normal pages. As such, the unpin_user_page() call (and its\n * variants) must be used in order to release gup-pinned pages.\n *\n * Choice of value:\n *\n * By making GUP_PIN_COUNTING_BIAS a power of two, debugging of page reference\n * counts with respect to pin_user_pages() and unpin_user_page() becomes\n * simpler, due to the fact that adding an even power of two to the page\n * refcount has the effect of using only the upper N bits, for the code that\n * counts up using the bias value. This means that the lower bits are left for\n * the exclusive use of the original code that increments and decrements by one\n * (or at least, by much smaller values than the bias value).\n *\n * Of course, once the lower bits overflow into the upper bits (and this is\n * OK, because subtraction recovers the original values), then visual inspection\n * no longer suffices to directly view the separate counts. However, for normal\n * applications that don't have huge page reference counts, this won't be an\n * issue.\n *\n * Locking: the lockless algorithm described in page_cache_get_speculative()\n * and page_cache_gup_pin_speculative() provides safe operation for\n * get_user_pages and page_mkclean and other calls that race to set up page\n * table entries.\n */\n#define GUP_PIN_COUNTING_BIAS (1U << 10)\n\nvoid unpin_user_page(struct page *page);\nvoid unpin_user_pages_dirty_lock(struct page **pages, unsigned long npages,\n\t\t\t\t bool make_dirty);\nvoid unpin_user_pages(struct page **pages, unsigned long npages);\n\n/**\n * page_maybe_dma_pinned() - report if a page is pinned for DMA.\n *\n * This function checks if a page has been pinned via a call to\n * pin_user_pages*().\n *\n * For non-huge pages, the return value is partially fuzzy: false is not fuzzy,\n * because it means \"definitely not pinned for DMA\", but true means \"probably\n * pinned for DMA, but possibly a false positive due to having at least\n * GUP_PIN_COUNTING_BIAS worth of normal page references\".\n *\n * False positives are OK, because: a) it's unlikely for a page to get that many\n * refcounts, and b) all the callers of this routine are expected to be able to\n * deal gracefully with a false positive.\n *\n * For huge pages, the result will be exactly correct. That's because we have\n * more tracking data available: the 3rd struct page in the compound page is\n * used to track the pincount (instead using of the GUP_PIN_COUNTING_BIAS\n * scheme).\n *\n * For more information, please see Documentation/core-api/pin_user_pages.rst.\n *\n * @page:\tpointer to page to be queried.\n * @Return:\tTrue, if it is likely that the page has been \"dma-pinned\".\n *\t\tFalse, if the page is definitely not dma-pinned.\n */\nstatic inline bool page_maybe_dma_pinned(struct page *page)\n{\n\tif (hpage_pincount_available(page))\n\t\treturn compound_pincount(page) > 0;\n\n\t/*\n\t * page_ref_count() is signed. If that refcount overflows, then\n\t * page_ref_count() returns a negative value, and callers will avoid\n\t * further incrementing the refcount.\n\t *\n\t * Here, for that overflow case, use the signed bit to count a little\n\t * bit higher via unsigned math, and thus still get an accurate result.\n\t */\n\treturn ((unsigned int)page_ref_count(compound_head(page))) >=\n\t\tGUP_PIN_COUNTING_BIAS;\n}\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define SECTION_IN_PAGE_FLAGS\n#endif\n\n/*\n * The identification function is mainly used by the buddy allocator for\n * determining if two pages could be buddies. We are not really identifying\n * the zone since we could be using the section number id if we do not have\n * node id available in page flags.\n * We only guarantee that it will return the same value for two combinable\n * pages in a zone.\n */\nstatic inline int page_zone_id(struct page *page)\n{\n\treturn (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\n}\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\nextern int page_to_nid(const struct page *page);\n#else\nstatic inline int page_to_nid(const struct page *page)\n{\n\tstruct page *p = (struct page *)page;\n\n\treturn (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline int cpu_pid_to_cpupid(int cpu, int pid)\n{\n\treturn ((cpu & LAST__CPU_MASK) << LAST__PID_SHIFT) | (pid & LAST__PID_MASK);\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn cpupid & LAST__PID_MASK;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn (cpupid >> LAST__PID_SHIFT) & LAST__CPU_MASK;\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn cpu_to_node(cpupid_to_cpu(cpupid));\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn cpupid_to_pid(cpupid) == (-1 & LAST__PID_MASK);\n}\n\nstatic inline bool cpupid_cpu_unset(int cpupid)\n{\n\treturn cpupid_to_cpu(cpupid) == (-1 & LAST__CPU_MASK);\n}\n\nstatic inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)\n{\n\treturn (task_pid & LAST__PID_MASK) == cpupid_to_pid(cpupid);\n}\n\n#define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task->pid, cpupid)\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn xchg(&page->_last_cpupid, cpupid & LAST_CPUPID_MASK);\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page->_last_cpupid;\n}\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->_last_cpupid = -1 & LAST_CPUPID_MASK;\n}\n#else\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;\n}\n\nextern int page_cpupid_xchg_last(struct page *page, int cpupid);\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->flags |= LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT;\n}\n#endif /* LAST_CPUPID_NOT_IN_PAGE_FLAGS */\n#else /* !CONFIG_NUMA_BALANCING */\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn page_to_nid(page); /* XXX */\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page_to_nid(page); /* XXX */\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpu_pid_to_cpupid(int nid, int pid)\n{\n\treturn -1;\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn true;\n}\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n}\n\nstatic inline bool cpupid_match_pid(struct task_struct *task, int cpupid)\n{\n\treturn false;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#if defined(CONFIG_KASAN_SW_TAGS) || defined(CONFIG_KASAN_HW_TAGS)\n\nstatic inline u8 page_kasan_tag(const struct page *page)\n{\n\tif (kasan_enabled())\n\t\treturn (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;\n\treturn 0xff;\n}\n\nstatic inline void page_kasan_tag_set(struct page *page, u8 tag)\n{\n\tif (kasan_enabled()) {\n\t\tpage->flags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);\n\t\tpage->flags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;\n\t}\n}\n\nstatic inline void page_kasan_tag_reset(struct page *page)\n{\n\tif (kasan_enabled())\n\t\tpage_kasan_tag_set(page, 0xff);\n}\n\n#else /* CONFIG_KASAN_SW_TAGS || CONFIG_KASAN_HW_TAGS */\n\nstatic inline u8 page_kasan_tag(const struct page *page)\n{\n\treturn 0xff;\n}\n\nstatic inline void page_kasan_tag_set(struct page *page, u8 tag) { }\nstatic inline void page_kasan_tag_reset(struct page *page) { }\n\n#endif /* CONFIG_KASAN_SW_TAGS || CONFIG_KASAN_HW_TAGS */\n\nstatic inline struct zone *page_zone(const struct page *page)\n{\n\treturn &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\n}\n\nstatic inline pg_data_t *page_pgdat(const struct page *page)\n{\n\treturn NODE_DATA(page_to_nid(page));\n}\n\n#ifdef SECTION_IN_PAGE_FLAGS\nstatic inline void set_page_section(struct page *page, unsigned long section)\n{\n\tpage->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tpage->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\n}\n\nstatic inline unsigned long page_to_section(const struct page *page)\n{\n\treturn (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\n}\n#endif\n\nstatic inline void set_page_zone(struct page *page, enum zone_type zone)\n{\n\tpage->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\n\tpage->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\n}\n\nstatic inline void set_page_node(struct page *page, unsigned long node)\n{\n\tpage->flags &= ~(NODES_MASK << NODES_PGSHIFT);\n\tpage->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\n}\n\nstatic inline void set_page_links(struct page *page, enum zone_type zone,\n\tunsigned long node, unsigned long pfn)\n{\n\tset_page_zone(page, zone);\n\tset_page_node(page, node);\n#ifdef SECTION_IN_PAGE_FLAGS\n\tset_page_section(page, pfn_to_section_nr(pfn));\n#endif\n}\n\n/*\n * Some inline functions in vmstat.h depend on page_zone()\n */\n#include <linux/vmstat.h>\n\nstatic __always_inline void *lowmem_page_address(const struct page *page)\n{\n\treturn page_to_virt(page);\n}\n\n#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\n#define HASHED_PAGE_VIRTUAL\n#endif\n\n#if defined(WANT_PAGE_VIRTUAL)\nstatic inline void *page_address(const struct page *page)\n{\n\treturn page->virtual;\n}\nstatic inline void set_page_address(struct page *page, void *address)\n{\n\tpage->virtual = address;\n}\n#define page_address_init()  do { } while(0)\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\nvoid *page_address(const struct page *page);\nvoid set_page_address(struct page *page, void *virtual);\nvoid page_address_init(void);\n#endif\n\n#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) lowmem_page_address(page)\n#define set_page_address(page, address)  do { } while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\nextern void *page_rmapping(struct page *page);\nextern struct anon_vma *page_anon_vma(struct page *page);\nextern struct address_space *page_mapping(struct page *page);\n\nextern struct address_space *__page_file_mapping(struct page *);\n\nstatic inline\nstruct address_space *page_file_mapping(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_mapping(page);\n\n\treturn page->mapping;\n}\n\nextern pgoff_t __page_file_index(struct page *page);\n\n/*\n * Return the pagecache index of the passed page.  Regular pagecache pages\n * use ->index whereas swapcache pages use swp_offset(->private)\n */\nstatic inline pgoff_t page_index(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_index(page);\n\treturn page->index;\n}\n\nbool page_mapped(struct page *page);\nstruct address_space *page_mapping(struct page *page);\nstruct address_space *page_mapping_file(struct page *page);\n\n/*\n * Return true only if the page has been allocated with\n * ALLOC_NO_WATERMARKS and the low watermark was not\n * met implying that the system is under some pressure.\n */\nstatic inline bool page_is_pfmemalloc(struct page *page)\n{\n\t/*\n\t * Page index cannot be this large so this must be\n\t * a pfmemalloc page.\n\t */\n\treturn page->index == -1UL;\n}\n\n/*\n * Only to be called by the page allocator on a freshly allocated\n * page.\n */\nstatic inline void set_page_pfmemalloc(struct page *page)\n{\n\tpage->index = -1UL;\n}\n\nstatic inline void clear_page_pfmemalloc(struct page *page)\n{\n\tpage->index = 0;\n}\n\n/*\n * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.\n */\nextern void pagefault_out_of_memory(void);\n\n#define offset_in_page(p)\t((unsigned long)(p) & ~PAGE_MASK)\n#define offset_in_thp(page, p)\t((unsigned long)(p) & (thp_size(page) - 1))\n\n/*\n * Flags passed to show_mem() and show_free_areas() to suppress output in\n * various contexts.\n */\n#define SHOW_MEM_FILTER_NODES\t\t(0x0001u)\t/* disallowed nodes */\n\nextern void show_free_areas(unsigned int flags, nodemask_t *nodemask);\n\n#ifdef CONFIG_MMU\nextern bool can_do_mlock(void);\n#else\nstatic inline bool can_do_mlock(void) { return false; }\n#endif\nextern int user_shm_lock(size_t, struct user_struct *);\nextern void user_shm_unlock(size_t, struct user_struct *);\n\n/*\n * Parameter block passed down to zap_pte_range in exceptional cases.\n */\nstruct zap_details {\n\tstruct address_space *check_mapping;\t/* Check page->mapping if set */\n\tpgoff_t\tfirst_index;\t\t\t/* Lowest page->index to unmap */\n\tpgoff_t last_index;\t\t\t/* Highest page->index to unmap */\n};\n\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t     pte_t pte);\nstruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tpmd_t pmd);\n\nvoid zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\t  unsigned long size);\nvoid zap_page_range(struct vm_area_struct *vma, unsigned long address,\n\t\t    unsigned long size);\nvoid unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,\n\t\tunsigned long start, unsigned long end);\n\nstruct mmu_notifier_range;\n\nvoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\n\t\tunsigned long end, unsigned long floor, unsigned long ceiling);\nint\ncopy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma);\nint follow_pte(struct mm_struct *mm, unsigned long address,\n\t\tstruct mmu_notifier_range *range, pte_t **ptepp, pmd_t **pmdpp,\n\t\tspinlock_t **ptlp);\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn);\nint follow_phys(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags, unsigned long *prot, resource_size_t *phys);\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write);\n\nextern void truncate_pagecache(struct inode *inode, loff_t new);\nextern void truncate_setsize(struct inode *inode, loff_t newsize);\nvoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);\nvoid truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);\nint truncate_inode_page(struct address_space *mapping, struct page *page);\nint generic_error_remove_page(struct address_space *mapping, struct page *page);\nint invalidate_inode_page(struct page *page);\n\n#ifdef CONFIG_MMU\nextern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, unsigned int flags,\n\t\t\t\t  struct pt_regs *regs);\nextern int fixup_user_fault(struct mm_struct *mm,\n\t\t\t    unsigned long address, unsigned int fault_flags,\n\t\t\t    bool *unlocked);\nvoid unmap_mapping_pages(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t nr, bool even_cows);\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows);\n#else\nstatic inline vm_fault_t handle_mm_fault(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address, unsigned int flags,\n\t\t\t\t\t struct pt_regs *regs)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn VM_FAULT_SIGBUS;\n}\nstatic inline int fixup_user_fault(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int fault_flags, bool *unlocked)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn -EFAULT;\n}\nstatic inline void unmap_mapping_pages(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t nr, bool even_cows) { }\nstatic inline void unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows) { }\n#endif\n\nstatic inline void unmap_shared_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen)\n{\n\tunmap_mapping_range(mapping, holebegin, holelen, 0);\n}\n\nextern int access_process_vm(struct task_struct *tsk, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags);\nextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags);\nextern int __access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\t\t      void *buf, int len, unsigned int gup_flags);\n\nlong get_user_pages_remote(struct mm_struct *mm,\n\t\t\t    unsigned long start, unsigned long nr_pages,\n\t\t\t    unsigned int gup_flags, struct page **pages,\n\t\t\t    struct vm_area_struct **vmas, int *locked);\nlong pin_user_pages_remote(struct mm_struct *mm,\n\t\t\t   unsigned long start, unsigned long nr_pages,\n\t\t\t   unsigned int gup_flags, struct page **pages,\n\t\t\t   struct vm_area_struct **vmas, int *locked);\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t\t    unsigned int gup_flags, struct page **pages,\n\t\t\t    struct vm_area_struct **vmas);\nlong pin_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages,\n\t\t    struct vm_area_struct **vmas);\nlong get_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages, int *locked);\nlong pin_user_pages_locked(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages, int *locked);\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t    struct page **pages, unsigned int gup_flags);\nlong pin_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t    struct page **pages, unsigned int gup_flags);\n\nint get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\tunsigned int gup_flags, struct page **pages);\nint pin_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\tunsigned int gup_flags, struct page **pages);\n\nint account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc);\nint __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,\n\t\t\tstruct task_struct *task, bool bypass_rlim);\n\n/* Container for pinned pfns / pages */\nstruct frame_vector {\n\tunsigned int nr_allocated;\t/* Number of frames we have space for */\n\tunsigned int nr_frames;\t/* Number of frames stored in ptrs array */\n\tbool got_ref;\t\t/* Did we pin pages by getting page ref? */\n\tbool is_pfns;\t\t/* Does array contain pages or pfns? */\n\tvoid *ptrs[];\t\t/* Array of pinned pfns / pages. Use\n\t\t\t\t * pfns_vector_pages() or pfns_vector_pfns()\n\t\t\t\t * for access */\n};\n\nstruct frame_vector *frame_vector_create(unsigned int nr_frames);\nvoid frame_vector_destroy(struct frame_vector *vec);\nint get_vaddr_frames(unsigned long start, unsigned int nr_pfns,\n\t\t     unsigned int gup_flags, struct frame_vector *vec);\nvoid put_vaddr_frames(struct frame_vector *vec);\nint frame_vector_to_pages(struct frame_vector *vec);\nvoid frame_vector_to_pfns(struct frame_vector *vec);\n\nstatic inline unsigned int frame_vector_count(struct frame_vector *vec)\n{\n\treturn vec->nr_frames;\n}\n\nstatic inline struct page **frame_vector_pages(struct frame_vector *vec)\n{\n\tif (vec->is_pfns) {\n\t\tint err = frame_vector_to_pages(vec);\n\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t}\n\treturn (struct page **)(vec->ptrs);\n}\n\nstatic inline unsigned long *frame_vector_pfns(struct frame_vector *vec)\n{\n\tif (!vec->is_pfns)\n\t\tframe_vector_to_pfns(vec);\n\treturn (unsigned long *)(vec->ptrs);\n}\n\nstruct kvec;\nint get_kernel_pages(const struct kvec *iov, int nr_pages, int write,\n\t\t\tstruct page **pages);\nint get_kernel_page(unsigned long start, int write, struct page **pages);\nstruct page *get_dump_page(unsigned long addr);\n\nextern int try_to_release_page(struct page * page, gfp_t gfp_mask);\nextern void do_invalidatepage(struct page *page, unsigned int offset,\n\t\t\t      unsigned int length);\n\nvoid __set_page_dirty(struct page *, struct address_space *, int warn);\nint __set_page_dirty_nobuffers(struct page *page);\nint __set_page_dirty_no_writeback(struct page *page);\nint redirty_page_for_writepage(struct writeback_control *wbc,\n\t\t\t\tstruct page *page);\nvoid account_page_dirtied(struct page *page, struct address_space *mapping);\nvoid account_page_cleaned(struct page *page, struct address_space *mapping,\n\t\t\t  struct bdi_writeback *wb);\nint set_page_dirty(struct page *page);\nint set_page_dirty_lock(struct page *page);\nvoid __cancel_dirty_page(struct page *page);\nstatic inline void cancel_dirty_page(struct page *page)\n{\n\t/* Avoid atomic ops, locking, etc. when not actually needed. */\n\tif (PageDirty(page))\n\t\t__cancel_dirty_page(page);\n}\nint clear_page_dirty_for_io(struct page *page);\n\nint get_cmdline(struct task_struct *task, char *buffer, int buflen);\n\nextern unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks);\n\n/*\n * Flags used by change_protection().  For now we make it a bitmap so\n * that we can pass in multiple flags just like parameters.  However\n * for now all the callers are only use one of the flags at the same\n * time.\n */\n/* Whether we should allow dirty bit accounting */\n#define  MM_CP_DIRTY_ACCT                  (1UL << 0)\n/* Whether this protection change is for NUMA hints */\n#define  MM_CP_PROT_NUMA                   (1UL << 1)\n/* Whether this change is for write protecting */\n#define  MM_CP_UFFD_WP                     (1UL << 2) /* do wp */\n#define  MM_CP_UFFD_WP_RESOLVE             (1UL << 3) /* Resolve wp */\n#define  MM_CP_UFFD_WP_ALL                 (MM_CP_UFFD_WP | \\\n\t\t\t\t\t    MM_CP_UFFD_WP_RESOLVE)\n\nextern unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,\n\t\t\t      unsigned long end, pgprot_t newprot,\n\t\t\t      unsigned long cp_flags);\nextern int mprotect_fixup(struct vm_area_struct *vma,\n\t\t\t  struct vm_area_struct **pprev, unsigned long start,\n\t\t\t  unsigned long end, unsigned long newflags);\n\n/*\n * doesn't attempt to fault and will return short.\n */\nint get_user_pages_fast_only(unsigned long start, int nr_pages,\n\t\t\t     unsigned int gup_flags, struct page **pages);\nint pin_user_pages_fast_only(unsigned long start, int nr_pages,\n\t\t\t     unsigned int gup_flags, struct page **pages);\n\nstatic inline bool get_user_page_fast_only(unsigned long addr,\n\t\t\tunsigned int gup_flags, struct page **pagep)\n{\n\treturn get_user_pages_fast_only(addr, 1, gup_flags, pagep) == 1;\n}\n/*\n * per-process(per-mm_struct) statistics.\n */\nstatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n{\n\tlong val = atomic_long_read(&mm->rss_stat.count[member]);\n\n#ifdef SPLIT_RSS_COUNTING\n\t/*\n\t * counter is updated in asynchronous manner and may go to minus.\n\t * But it's never be expected number for users.\n\t */\n\tif (val < 0)\n\t\tval = 0;\n#endif\n\treturn (unsigned long)val;\n}\n\nvoid mm_trace_rss_stat(struct mm_struct *mm, int member, long count);\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tlong count = atomic_long_add_return(value, &mm->rss_stat.count[member]);\n\n\tmm_trace_rss_stat(mm, member, count);\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tlong count = atomic_long_inc_return(&mm->rss_stat.count[member]);\n\n\tmm_trace_rss_stat(mm, member, count);\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tlong count = atomic_long_dec_return(&mm->rss_stat.count[member]);\n\n\tmm_trace_rss_stat(mm, member, count);\n}\n\n/* Optimized variant when page is already known not to be PageAnon */\nstatic inline int mm_counter_file(struct page *page)\n{\n\tif (PageSwapBacked(page))\n\t\treturn MM_SHMEMPAGES;\n\treturn MM_FILEPAGES;\n}\n\nstatic inline int mm_counter(struct page *page)\n{\n\tif (PageAnon(page))\n\t\treturn MM_ANONPAGES;\n\treturn mm_counter_file(page);\n}\n\nstatic inline unsigned long get_mm_rss(struct mm_struct *mm)\n{\n\treturn get_mm_counter(mm, MM_FILEPAGES) +\n\t\tget_mm_counter(mm, MM_ANONPAGES) +\n\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n}\n\nstatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_rss, get_mm_rss(mm));\n}\n\nstatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_vm, mm->total_vm);\n}\n\nstatic inline void update_hiwater_rss(struct mm_struct *mm)\n{\n\tunsigned long _rss = get_mm_rss(mm);\n\n\tif ((mm)->hiwater_rss < _rss)\n\t\t(mm)->hiwater_rss = _rss;\n}\n\nstatic inline void update_hiwater_vm(struct mm_struct *mm)\n{\n\tif (mm->hiwater_vm < mm->total_vm)\n\t\tmm->hiwater_vm = mm->total_vm;\n}\n\nstatic inline void reset_mm_hiwater_rss(struct mm_struct *mm)\n{\n\tmm->hiwater_rss = get_mm_rss(mm);\n}\n\nstatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\n\t\t\t\t\t struct mm_struct *mm)\n{\n\tunsigned long hiwater_rss = get_mm_hiwater_rss(mm);\n\n\tif (*maxrss < hiwater_rss)\n\t\t*maxrss = hiwater_rss;\n}\n\n#if defined(SPLIT_RSS_COUNTING)\nvoid sync_mm_rss(struct mm_struct *mm);\n#else\nstatic inline void sync_mm_rss(struct mm_struct *mm)\n{\n}\n#endif\n\n#ifndef CONFIG_ARCH_HAS_PTE_SPECIAL\nstatic inline int pte_special(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline pte_t pte_mkspecial(pte_t pte)\n{\n\treturn pte;\n}\n#endif\n\n#ifndef CONFIG_ARCH_HAS_PTE_DEVMAP\nstatic inline int pte_devmap(pte_t pte)\n{\n\treturn 0;\n}\n#endif\n\nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);\n\nextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t       spinlock_t **ptl);\nstatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    spinlock_t **ptl)\n{\n\tpte_t *ptep;\n\t__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\n\treturn ptep;\n}\n\n#ifdef __PAGETABLE_P4D_FOLDED\nstatic inline int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\n#endif\n\n#if defined(__PAGETABLE_PUD_FOLDED) || !defined(CONFIG_MMU)\nstatic inline int __pud_alloc(struct mm_struct *mm, p4d_t *p4d,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\nstatic inline void mm_inc_nr_puds(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_puds(struct mm_struct *mm) {}\n\n#else\nint __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address);\n\nstatic inline void mm_inc_nr_puds(struct mm_struct *mm)\n{\n\tif (mm_pud_folded(mm))\n\t\treturn;\n\tatomic_long_add(PTRS_PER_PUD * sizeof(pud_t), &mm->pgtables_bytes);\n}\n\nstatic inline void mm_dec_nr_puds(struct mm_struct *mm)\n{\n\tif (mm_pud_folded(mm))\n\t\treturn;\n\tatomic_long_sub(PTRS_PER_PUD * sizeof(pud_t), &mm->pgtables_bytes);\n}\n#endif\n\n#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)\nstatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm) {}\n\n#else\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm)\n{\n\tif (mm_pmd_folded(mm))\n\t\treturn;\n\tatomic_long_add(PTRS_PER_PMD * sizeof(pmd_t), &mm->pgtables_bytes);\n}\n\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm)\n{\n\tif (mm_pmd_folded(mm))\n\t\treturn;\n\tatomic_long_sub(PTRS_PER_PMD * sizeof(pmd_t), &mm->pgtables_bytes);\n}\n#endif\n\n#ifdef CONFIG_MMU\nstatic inline void mm_pgtables_bytes_init(struct mm_struct *mm)\n{\n\tatomic_long_set(&mm->pgtables_bytes, 0);\n}\n\nstatic inline unsigned long mm_pgtables_bytes(const struct mm_struct *mm)\n{\n\treturn atomic_long_read(&mm->pgtables_bytes);\n}\n\nstatic inline void mm_inc_nr_ptes(struct mm_struct *mm)\n{\n\tatomic_long_add(PTRS_PER_PTE * sizeof(pte_t), &mm->pgtables_bytes);\n}\n\nstatic inline void mm_dec_nr_ptes(struct mm_struct *mm)\n{\n\tatomic_long_sub(PTRS_PER_PTE * sizeof(pte_t), &mm->pgtables_bytes);\n}\n#else\n\nstatic inline void mm_pgtables_bytes_init(struct mm_struct *mm) {}\nstatic inline unsigned long mm_pgtables_bytes(const struct mm_struct *mm)\n{\n\treturn 0;\n}\n\nstatic inline void mm_inc_nr_ptes(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_ptes(struct mm_struct *mm) {}\n#endif\n\nint __pte_alloc(struct mm_struct *mm, pmd_t *pmd);\nint __pte_alloc_kernel(pmd_t *pmd);\n\n#if defined(CONFIG_MMU)\n\nstatic inline p4d_t *p4d_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\tunsigned long address)\n{\n\treturn (unlikely(pgd_none(*pgd)) && __p4d_alloc(mm, pgd, address)) ?\n\t\tNULL : p4d_offset(pgd, address);\n}\n\nstatic inline pud_t *pud_alloc(struct mm_struct *mm, p4d_t *p4d,\n\t\tunsigned long address)\n{\n\treturn (unlikely(p4d_none(*p4d)) && __pud_alloc(mm, p4d, address)) ?\n\t\tNULL : pud_offset(p4d, address);\n}\n\nstatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\treturn (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\n\t\tNULL: pmd_offset(pud, address);\n}\n#endif /* CONFIG_MMU */\n\n#if USE_SPLIT_PTE_PTLOCKS\n#if ALLOC_SPLIT_PTLOCKS\nvoid __init ptlock_cache_init(void);\nextern bool ptlock_alloc(struct page *page);\nextern void ptlock_free(struct page *page);\n\nstatic inline spinlock_t *ptlock_ptr(struct page *page)\n{\n\treturn page->ptl;\n}\n#else /* ALLOC_SPLIT_PTLOCKS */\nstatic inline void ptlock_cache_init(void)\n{\n}\n\nstatic inline bool ptlock_alloc(struct page *page)\n{\n\treturn true;\n}\n\nstatic inline void ptlock_free(struct page *page)\n{\n}\n\nstatic inline spinlock_t *ptlock_ptr(struct page *page)\n{\n\treturn &page->ptl;\n}\n#endif /* ALLOC_SPLIT_PTLOCKS */\n\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_page(*pmd));\n}\n\nstatic inline bool ptlock_init(struct page *page)\n{\n\t/*\n\t * prep_new_page() initialize page->private (and therefore page->ptl)\n\t * with 0. Make sure nobody took it in use in between.\n\t *\n\t * It can happen if arch try to use slab for page table allocation:\n\t * slab code uses page->slab_cache, which share storage with page->ptl.\n\t */\n\tVM_BUG_ON_PAGE(*(unsigned long *)&page->ptl, page);\n\tif (!ptlock_alloc(page))\n\t\treturn false;\n\tspin_lock_init(ptlock_ptr(page));\n\treturn true;\n}\n\n#else\t/* !USE_SPLIT_PTE_PTLOCKS */\n/*\n * We use mm->page_table_lock to guard all pagetable pages of the mm.\n */\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\nstatic inline void ptlock_cache_init(void) {}\nstatic inline bool ptlock_init(struct page *page) { return true; }\nstatic inline void ptlock_free(struct page *page) {}\n#endif /* USE_SPLIT_PTE_PTLOCKS */\n\nstatic inline void pgtable_init(void)\n{\n\tptlock_cache_init();\n\tpgtable_cache_init();\n}\n\nstatic inline bool pgtable_pte_page_ctor(struct page *page)\n{\n\tif (!ptlock_init(page))\n\t\treturn false;\n\t__SetPageTable(page);\n\tinc_zone_page_state(page, NR_PAGETABLE);\n\treturn true;\n}\n\nstatic inline void pgtable_pte_page_dtor(struct page *page)\n{\n\tptlock_free(page);\n\t__ClearPageTable(page);\n\tdec_zone_page_state(page, NR_PAGETABLE);\n}\n\n#define pte_offset_map_lock(mm, pmd, address, ptlp)\t\\\n({\t\t\t\t\t\t\t\\\n\tspinlock_t *__ptl = pte_lockptr(mm, pmd);\t\\\n\tpte_t *__pte = pte_offset_map(pmd, address);\t\\\n\t*(ptlp) = __ptl;\t\t\t\t\\\n\tspin_lock(__ptl);\t\t\t\t\\\n\t__pte;\t\t\t\t\t\t\\\n})\n\n#define pte_unmap_unlock(pte, ptl)\tdo {\t\t\\\n\tspin_unlock(ptl);\t\t\t\t\\\n\tpte_unmap(pte);\t\t\t\t\t\\\n} while (0)\n\n#define pte_alloc(mm, pmd) (unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, pmd))\n\n#define pte_alloc_map(mm, pmd, address)\t\t\t\\\n\t(pte_alloc(mm, pmd) ? NULL : pte_offset_map(pmd, address))\n\n#define pte_alloc_map_lock(mm, pmd, address, ptlp)\t\\\n\t(pte_alloc(mm, pmd) ?\t\t\t\\\n\t\t NULL : pte_offset_map_lock(mm, pmd, address, ptlp))\n\n#define pte_alloc_kernel(pmd, address)\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd))? \\\n\t\tNULL: pte_offset_kernel(pmd, address))\n\n#if USE_SPLIT_PMD_PTLOCKS\n\nstatic struct page *pmd_to_page(pmd_t *pmd)\n{\n\tunsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);\n\treturn virt_to_page((void *)((unsigned long) pmd & mask));\n}\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_to_page(pmd));\n}\n\nstatic inline bool pmd_ptlock_init(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tpage->pmd_huge_pte = NULL;\n#endif\n\treturn ptlock_init(page);\n}\n\nstatic inline void pmd_ptlock_free(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tVM_BUG_ON_PAGE(page->pmd_huge_pte, page);\n#endif\n\tptlock_free(page);\n}\n\n#define pmd_huge_pte(mm, pmd) (pmd_to_page(pmd)->pmd_huge_pte)\n\n#else\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline bool pmd_ptlock_init(struct page *page) { return true; }\nstatic inline void pmd_ptlock_free(struct page *page) {}\n\n#define pmd_huge_pte(mm, pmd) ((mm)->pmd_huge_pte)\n\n#endif\n\nstatic inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\nstatic inline bool pgtable_pmd_page_ctor(struct page *page)\n{\n\tif (!pmd_ptlock_init(page))\n\t\treturn false;\n\t__SetPageTable(page);\n\tinc_zone_page_state(page, NR_PAGETABLE);\n\treturn true;\n}\n\nstatic inline void pgtable_pmd_page_dtor(struct page *page)\n{\n\tpmd_ptlock_free(page);\n\t__ClearPageTable(page);\n\tdec_zone_page_state(page, NR_PAGETABLE);\n}\n\n/*\n * No scalability reason to split PUD locks yet, but follow the same pattern\n * as the PMD locks to make it easier if we decide to.  The VM should not be\n * considered ready to switch to split PUD locks yet; there may be places\n * which need to be converted from page_table_lock.\n */\nstatic inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)\n{\n\tspinlock_t *ptl = pud_lockptr(mm, pud);\n\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\nextern void __init pagecache_init(void);\nextern void __init free_area_init_memoryless_node(int nid);\nextern void free_initmem(void);\n\n/*\n * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)\n * into the buddy system. The freed pages will be poisoned with pattern\n * \"poison\" if it's within range [0, UCHAR_MAX].\n * Return pages freed into the buddy system.\n */\nextern unsigned long free_reserved_area(void *start, void *end,\n\t\t\t\t\tint poison, const char *s);\n\n#ifdef\tCONFIG_HIGHMEM\n/*\n * Free a highmem page into the buddy system, adjusting totalhigh_pages\n * and totalram_pages.\n */\nextern void free_highmem_page(struct page *page);\n#endif\n\nextern void adjust_managed_page_count(struct page *page, long count);\nextern void mem_init_print_info(const char *str);\n\nextern void reserve_bootmem_region(phys_addr_t start, phys_addr_t end);\n\n/* Free the reserved page into the buddy system, so it gets managed. */\nstatic inline void __free_reserved_page(struct page *page)\n{\n\tClearPageReserved(page);\n\tinit_page_count(page);\n\t__free_page(page);\n}\n\nstatic inline void free_reserved_page(struct page *page)\n{\n\t__free_reserved_page(page);\n\tadjust_managed_page_count(page, 1);\n}\n\nstatic inline void mark_page_reserved(struct page *page)\n{\n\tSetPageReserved(page);\n\tadjust_managed_page_count(page, -1);\n}\n\n/*\n * Default method to free all the __init memory into the buddy system.\n * The freed pages will be poisoned with pattern \"poison\" if it's within\n * range [0, UCHAR_MAX].\n * Return pages freed into the buddy system.\n */\nstatic inline unsigned long free_initmem_default(int poison)\n{\n\textern char __init_begin[], __init_end[];\n\n\treturn free_reserved_area(&__init_begin, &__init_end,\n\t\t\t\t  poison, \"unused kernel\");\n}\n\nstatic inline unsigned long get_num_physpages(void)\n{\n\tint nid;\n\tunsigned long phys_pages = 0;\n\n\tfor_each_online_node(nid)\n\t\tphys_pages += node_present_pages(nid);\n\n\treturn phys_pages;\n}\n\n/*\n * Using memblock node mappings, an architecture may initialise its\n * zones, allocate the backing mem_map and account for memory holes in an\n * architecture independent manner.\n *\n * An architecture is expected to register range of page frames backed by\n * physical memory with memblock_add[_node]() before calling\n * free_area_init() passing in the PFN each zone ends at. At a basic\n * usage, an architecture is expected to do something like\n *\n * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,\n * \t\t\t\t\t\t\t max_highmem_pfn};\n * for_each_valid_physical_page_range()\n * \tmemblock_add_node(base, size, nid)\n * free_area_init(max_zone_pfns);\n */\nvoid free_area_init(unsigned long *max_zone_pfn);\nunsigned long node_map_pfn_alignment(void);\nunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern unsigned long absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern void get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn);\nextern unsigned long find_min_pfn_with_active_regions(void);\n\n#ifndef CONFIG_NEED_MULTIPLE_NODES\nstatic inline int early_pfn_to_nid(unsigned long pfn)\n{\n\treturn 0;\n}\n#else\n/* please see mm/page_alloc.c */\nextern int __meminit early_pfn_to_nid(unsigned long pfn);\n#endif\n\nextern void set_dma_reserve(unsigned long new_dma_reserve);\nextern void memmap_init_zone(unsigned long, int, unsigned long, unsigned long,\n\t\tenum meminit_context, struct vmem_altmap *, int migratetype);\nextern void setup_per_zone_wmarks(void);\nextern int __meminit init_per_zone_wmark_min(void);\nextern void mem_init(void);\nextern void __init mmap_init(void);\nextern void show_mem(unsigned int flags, nodemask_t *nodemask);\nextern long si_mem_available(void);\nextern void si_meminfo(struct sysinfo * val);\nextern void si_meminfo_node(struct sysinfo *val, int nid);\n#ifdef __HAVE_ARCH_RESERVED_KERNEL_PAGES\nextern unsigned long arch_reserved_kernel_pages(void);\n#endif\n\nextern __printf(3, 4)\nvoid warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...);\n\nextern void setup_per_cpu_pageset(void);\n\n/* page_alloc.c */\nextern int min_free_kbytes;\nextern int watermark_boost_factor;\nextern int watermark_scale_factor;\nextern bool arch_has_descending_max_zone_pfns(void);\n\n/* nommu.c */\nextern atomic_long_t mmap_pages_allocated;\nextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\n\n/* interval_tree.c */\nvoid vma_interval_tree_insert(struct vm_area_struct *node,\n\t\t\t      struct rb_root_cached *root);\nvoid vma_interval_tree_insert_after(struct vm_area_struct *node,\n\t\t\t\t    struct vm_area_struct *prev,\n\t\t\t\t    struct rb_root_cached *root);\nvoid vma_interval_tree_remove(struct vm_area_struct *node,\n\t\t\t      struct rb_root_cached *root);\nstruct vm_area_struct *vma_interval_tree_iter_first(struct rb_root_cached *root,\n\t\t\t\tunsigned long start, unsigned long last);\nstruct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,\n\t\t\t\tunsigned long start, unsigned long last);\n\n#define vma_interval_tree_foreach(vma, root, start, last)\t\t\\\n\tfor (vma = vma_interval_tree_iter_first(root, start, last);\t\\\n\t     vma; vma = vma_interval_tree_iter_next(vma, start, last))\n\nvoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root);\nvoid anon_vma_interval_tree_remove(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root);\nstruct anon_vma_chain *\nanon_vma_interval_tree_iter_first(struct rb_root_cached *root,\n\t\t\t\t  unsigned long start, unsigned long last);\nstruct anon_vma_chain *anon_vma_interval_tree_iter_next(\n\tstruct anon_vma_chain *node, unsigned long start, unsigned long last);\n#ifdef CONFIG_DEBUG_VM_RB\nvoid anon_vma_interval_tree_verify(struct anon_vma_chain *node);\n#endif\n\n#define anon_vma_interval_tree_foreach(avc, root, start, last)\t\t \\\n\tfor (avc = anon_vma_interval_tree_iter_first(root, start, last); \\\n\t     avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))\n\n/* mmap.c */\nextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\nextern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,\n\tstruct vm_area_struct *expand);\nstatic inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)\n{\n\treturn __vma_adjust(vma, start, end, pgoff, insert, NULL);\n}\nextern struct vm_area_struct *vma_merge(struct mm_struct *,\n\tstruct vm_area_struct *prev, unsigned long addr, unsigned long end,\n\tunsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,\n\tstruct mempolicy *, struct vm_userfaultfd_ctx);\nextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\nextern int __split_vma(struct mm_struct *, struct vm_area_struct *,\n\tunsigned long addr, int new_below);\nextern int split_vma(struct mm_struct *, struct vm_area_struct *,\n\tunsigned long addr, int new_below);\nextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\nextern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,\n\tstruct rb_node **, struct rb_node *);\nextern void unlink_file_vma(struct vm_area_struct *);\nextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks);\nextern void exit_mmap(struct mm_struct *);\n\nstatic inline int check_data_rlimit(unsigned long rlim,\n\t\t\t\t    unsigned long new,\n\t\t\t\t    unsigned long start,\n\t\t\t\t    unsigned long end_data,\n\t\t\t\t    unsigned long start_data)\n{\n\tif (rlim < RLIM_INFINITY) {\n\t\tif (((new - start) + (end_data - start_data)) > rlim)\n\t\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nextern int mm_take_all_locks(struct mm_struct *mm);\nextern void mm_drop_all_locks(struct mm_struct *mm);\n\nextern void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);\nextern struct file *get_mm_exe_file(struct mm_struct *mm);\nextern struct file *get_task_exe_file(struct task_struct *task);\n\nextern bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long npages);\nextern void vm_stat_account(struct mm_struct *, vm_flags_t, long npages);\n\nextern bool vma_is_special_mapping(const struct vm_area_struct *vma,\n\t\t\t\t   const struct vm_special_mapping *sm);\nextern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags,\n\t\t\t\t   const struct vm_special_mapping *spec);\n/* This is an obsolete alternative to _install_special_mapping. */\nextern int install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags, struct page **pages);\n\nunsigned long randomize_stack_top(unsigned long stack_top);\n\nextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\n\nextern unsigned long mmap_region(struct file *file, unsigned long addr,\n\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff,\n\tstruct list_head *uf);\nextern unsigned long do_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot, unsigned long flags,\n\tunsigned long pgoff, unsigned long *populate, struct list_head *uf);\nextern int __do_munmap(struct mm_struct *, unsigned long, size_t,\n\t\t       struct list_head *uf, bool downgrade);\nextern int do_munmap(struct mm_struct *, unsigned long, size_t,\n\t\t     struct list_head *uf);\nextern int do_madvise(struct mm_struct *mm, unsigned long start, size_t len_in, int behavior);\n\n#ifdef CONFIG_MMU\nextern int __mm_populate(unsigned long addr, unsigned long len,\n\t\t\t int ignore_errors);\nstatic inline void mm_populate(unsigned long addr, unsigned long len)\n{\n\t/* Ignore errors */\n\t(void) __mm_populate(addr, len, 1);\n}\n#else\nstatic inline void mm_populate(unsigned long addr, unsigned long len) {}\n#endif\n\n/* These take the mm semaphore themselves */\nextern int __must_check vm_brk(unsigned long, unsigned long);\nextern int __must_check vm_brk_flags(unsigned long, unsigned long, unsigned long);\nextern int vm_munmap(unsigned long, size_t);\nextern unsigned long __must_check vm_mmap(struct file *, unsigned long,\n        unsigned long, unsigned long,\n        unsigned long, unsigned long);\n\nstruct vm_unmapped_area_info {\n#define VM_UNMAPPED_AREA_TOPDOWN 1\n\tunsigned long flags;\n\tunsigned long length;\n\tunsigned long low_limit;\n\tunsigned long high_limit;\n\tunsigned long align_mask;\n\tunsigned long align_offset;\n};\n\nextern unsigned long vm_unmapped_area(struct vm_unmapped_area_info *info);\n\n/* truncate.c */\nextern void truncate_inode_pages(struct address_space *, loff_t);\nextern void truncate_inode_pages_range(struct address_space *,\n\t\t\t\t       loff_t lstart, loff_t lend);\nextern void truncate_inode_pages_final(struct address_space *);\n\n/* generic vm_area_ops exported for stackable file systems */\nextern vm_fault_t filemap_fault(struct vm_fault *vmf);\nextern void filemap_map_pages(struct vm_fault *vmf,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\nextern vm_fault_t filemap_page_mkwrite(struct vm_fault *vmf);\n\n/* mm/page-writeback.c */\nint __must_check write_one_page(struct page *page);\nvoid task_dirty_inc(struct task_struct *tsk);\n\nextern unsigned long stack_guard_gap;\n/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */\nextern int expand_stack(struct vm_area_struct *vma, unsigned long address);\n\n/* CONFIG_STACK_GROWSUP still needs to grow downwards at some places */\nextern int expand_downwards(struct vm_area_struct *vma,\n\t\tunsigned long address);\n#if VM_GROWSUP\nextern int expand_upwards(struct vm_area_struct *vma, unsigned long address);\n#else\n  #define expand_upwards(vma, address) (0)\n#endif\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\nextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\n\t\t\t\t\t     struct vm_area_struct **pprev);\n\n/* Look up the first VMA which intersects the interval start_addr..end_addr-1,\n   NULL if none.  Assume start_addr < end_addr. */\nstatic inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)\n{\n\tstruct vm_area_struct * vma = find_vma(mm,start_addr);\n\n\tif (vma && end_addr <= vma->vm_start)\n\t\tvma = NULL;\n\treturn vma;\n}\n\nstatic inline unsigned long vm_start_gap(struct vm_area_struct *vma)\n{\n\tunsigned long vm_start = vma->vm_start;\n\n\tif (vma->vm_flags & VM_GROWSDOWN) {\n\t\tvm_start -= stack_guard_gap;\n\t\tif (vm_start > vma->vm_start)\n\t\t\tvm_start = 0;\n\t}\n\treturn vm_start;\n}\n\nstatic inline unsigned long vm_end_gap(struct vm_area_struct *vma)\n{\n\tunsigned long vm_end = vma->vm_end;\n\n\tif (vma->vm_flags & VM_GROWSUP) {\n\t\tvm_end += stack_guard_gap;\n\t\tif (vm_end < vma->vm_end)\n\t\t\tvm_end = -PAGE_SIZE;\n\t}\n\treturn vm_end;\n}\n\nstatic inline unsigned long vma_pages(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n}\n\n/* Look up the first VMA which exactly match the interval vm_start ... vm_end */\nstatic inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,\n\t\t\t\tunsigned long vm_start, unsigned long vm_end)\n{\n\tstruct vm_area_struct *vma = find_vma(mm, vm_start);\n\n\tif (vma && (vma->vm_start != vm_start || vma->vm_end != vm_end))\n\t\tvma = NULL;\n\n\treturn vma;\n}\n\nstatic inline bool range_in_vma(struct vm_area_struct *vma,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\treturn (vma && vma->vm_start <= start && end <= vma->vm_end);\n}\n\n#ifdef CONFIG_MMU\npgprot_t vm_get_page_prot(unsigned long vm_flags);\nvoid vma_set_page_prot(struct vm_area_struct *vma);\n#else\nstatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\treturn __pgprot(0);\n}\nstatic inline void vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n}\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\nunsigned long change_prot_numa(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end);\n#endif\n\nstruct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);\nint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\n\t\t\tunsigned long pfn, unsigned long size, pgprot_t);\nint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\nint vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page **pages, unsigned long *num);\nint vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num);\nint vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num);\nvm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\nvm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn, pgprot_t pgprot);\nvm_fault_t vmf_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn);\nvm_fault_t vmf_insert_mixed_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn, pgprot_t pgprot);\nvm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,\n\t\tunsigned long addr, pfn_t pfn);\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);\n\nstatic inline vm_fault_t vmf_insert_page(struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, struct page *page)\n{\n\tint err = vm_insert_page(vma, addr, page);\n\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\tif (err < 0 && err != -EBUSY)\n\t\treturn VM_FAULT_SIGBUS;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\n#ifndef io_remap_pfn_range\nstatic inline int io_remap_pfn_range(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, unsigned long pfn,\n\t\t\t\t     unsigned long size, pgprot_t prot)\n{\n\treturn remap_pfn_range(vma, addr, pfn, size, pgprot_decrypted(prot));\n}\n#endif\n\nstatic inline vm_fault_t vmf_error(int err)\n{\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\treturn VM_FAULT_SIGBUS;\n}\n\nstruct page *follow_page(struct vm_area_struct *vma, unsigned long address,\n\t\t\t unsigned int foll_flags);\n\n#define FOLL_WRITE\t0x01\t/* check pte is writable */\n#define FOLL_TOUCH\t0x02\t/* mark page accessed */\n#define FOLL_GET\t0x04\t/* do get_page on page */\n#define FOLL_DUMP\t0x08\t/* give error on hole if it would be zero */\n#define FOLL_FORCE\t0x10\t/* get_user_pages read/write w/o permission */\n#define FOLL_NOWAIT\t0x20\t/* if a disk transfer is needed, start the IO\n\t\t\t\t * and return without waiting upon it */\n#define FOLL_POPULATE\t0x40\t/* fault in page */\n#define FOLL_SPLIT\t0x80\t/* don't return transhuge pages, split them */\n#define FOLL_HWPOISON\t0x100\t/* check page is hwpoisoned */\n#define FOLL_NUMA\t0x200\t/* force NUMA hinting page fault */\n#define FOLL_MIGRATION\t0x400\t/* wait for page to replace migration entry */\n#define FOLL_TRIED\t0x800\t/* a retry, previous pass started an IO */\n#define FOLL_MLOCK\t0x1000\t/* lock present pages */\n#define FOLL_REMOTE\t0x2000\t/* we are working on non-current tsk/mm */\n#define FOLL_COW\t0x4000\t/* internal GUP flag */\n#define FOLL_ANON\t0x8000\t/* don't do file mappings */\n#define FOLL_LONGTERM\t0x10000\t/* mapping lifetime is indefinite: see below */\n#define FOLL_SPLIT_PMD\t0x20000\t/* split huge pmd before returning */\n#define FOLL_PIN\t0x40000\t/* pages must be released via unpin_user_page */\n#define FOLL_FAST_ONLY\t0x80000\t/* gup_fast: prevent fall-back to slow gup */\n\n/*\n * FOLL_PIN and FOLL_LONGTERM may be used in various combinations with each\n * other. Here is what they mean, and how to use them:\n *\n * FOLL_LONGTERM indicates that the page will be held for an indefinite time\n * period _often_ under userspace control.  This is in contrast to\n * iov_iter_get_pages(), whose usages are transient.\n *\n * FIXME: For pages which are part of a filesystem, mappings are subject to the\n * lifetime enforced by the filesystem and we need guarantees that longterm\n * users like RDMA and V4L2 only establish mappings which coordinate usage with\n * the filesystem.  Ideas for this coordination include revoking the longterm\n * pin, delaying writeback, bounce buffer page writeback, etc.  As FS DAX was\n * added after the problem with filesystems was found FS DAX VMAs are\n * specifically failed.  Filesystem pages are still subject to bugs and use of\n * FOLL_LONGTERM should be avoided on those pages.\n *\n * FIXME: Also NOTE that FOLL_LONGTERM is not supported in every GUP call.\n * Currently only get_user_pages() and get_user_pages_fast() support this flag\n * and calls to get_user_pages_[un]locked are specifically not allowed.  This\n * is due to an incompatibility with the FS DAX check and\n * FAULT_FLAG_ALLOW_RETRY.\n *\n * In the CMA case: long term pins in a CMA region would unnecessarily fragment\n * that region.  And so, CMA attempts to migrate the page before pinning, when\n * FOLL_LONGTERM is specified.\n *\n * FOLL_PIN indicates that a special kind of tracking (not just page->_refcount,\n * but an additional pin counting system) will be invoked. This is intended for\n * anything that gets a page reference and then touches page data (for example,\n * Direct IO). This lets the filesystem know that some non-file-system entity is\n * potentially changing the pages' data. In contrast to FOLL_GET (whose pages\n * are released via put_page()), FOLL_PIN pages must be released, ultimately, by\n * a call to unpin_user_page().\n *\n * FOLL_PIN is similar to FOLL_GET: both of these pin pages. They use different\n * and separate refcounting mechanisms, however, and that means that each has\n * its own acquire and release mechanisms:\n *\n *     FOLL_GET: get_user_pages*() to acquire, and put_page() to release.\n *\n *     FOLL_PIN: pin_user_pages*() to acquire, and unpin_user_pages to release.\n *\n * FOLL_PIN and FOLL_GET are mutually exclusive for a given function call.\n * (The underlying pages may experience both FOLL_GET-based and FOLL_PIN-based\n * calls applied to them, and that's perfectly OK. This is a constraint on the\n * callers, not on the pages.)\n *\n * FOLL_PIN should be set internally by the pin_user_pages*() APIs, never\n * directly by the caller. That's in order to help avoid mismatches when\n * releasing pages: get_user_pages*() pages must be released via put_page(),\n * while pin_user_pages*() pages must be released via unpin_user_page().\n *\n * Please see Documentation/core-api/pin_user_pages.rst for more information.\n */\n\nstatic inline int vm_fault_to_errno(vm_fault_t vm_fault, int foll_flags)\n{\n\tif (vm_fault & VM_FAULT_OOM)\n\t\treturn -ENOMEM;\n\tif (vm_fault & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\treturn (foll_flags & FOLL_HWPOISON) ? -EHWPOISON : -EFAULT;\n\tif (vm_fault & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\ntypedef int (*pte_fn_t)(pte_t *pte, unsigned long addr, void *data);\nextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\n\t\t\t       unsigned long size, pte_fn_t fn, void *data);\nextern int apply_to_existing_page_range(struct mm_struct *mm,\n\t\t\t\t   unsigned long address, unsigned long size,\n\t\t\t\t   pte_fn_t fn, void *data);\n\n#ifdef CONFIG_PAGE_POISONING\nextern bool page_poisoning_enabled(void);\nextern void kernel_poison_pages(struct page *page, int numpages, int enable);\n#else\nstatic inline bool page_poisoning_enabled(void) { return false; }\nstatic inline void kernel_poison_pages(struct page *page, int numpages,\n\t\t\t\t\tint enable) { }\n#endif\n\n#ifdef CONFIG_INIT_ON_ALLOC_DEFAULT_ON\nDECLARE_STATIC_KEY_TRUE(init_on_alloc);\n#else\nDECLARE_STATIC_KEY_FALSE(init_on_alloc);\n#endif\nstatic inline bool want_init_on_alloc(gfp_t flags)\n{\n\tif (static_branch_unlikely(&init_on_alloc) &&\n\t    !page_poisoning_enabled())\n\t\treturn true;\n\treturn flags & __GFP_ZERO;\n}\n\n#ifdef CONFIG_INIT_ON_FREE_DEFAULT_ON\nDECLARE_STATIC_KEY_TRUE(init_on_free);\n#else\nDECLARE_STATIC_KEY_FALSE(init_on_free);\n#endif\nstatic inline bool want_init_on_free(void)\n{\n\treturn static_branch_unlikely(&init_on_free) &&\n\t       !page_poisoning_enabled();\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern void init_debug_pagealloc(void);\n#else\nstatic inline void init_debug_pagealloc(void) {}\n#endif\nextern bool _debug_pagealloc_enabled_early;\nDECLARE_STATIC_KEY_FALSE(_debug_pagealloc_enabled);\n\nstatic inline bool debug_pagealloc_enabled(void)\n{\n\treturn IS_ENABLED(CONFIG_DEBUG_PAGEALLOC) &&\n\t\t_debug_pagealloc_enabled_early;\n}\n\n/*\n * For use in fast paths after init_debug_pagealloc() has run, or when a\n * false negative result is not harmful when called too early.\n */\nstatic inline bool debug_pagealloc_enabled_static(void)\n{\n\tif (!IS_ENABLED(CONFIG_DEBUG_PAGEALLOC))\n\t\treturn false;\n\n\treturn static_branch_unlikely(&_debug_pagealloc_enabled);\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\n/*\n * To support DEBUG_PAGEALLOC architecture must ensure that\n * __kernel_map_pages() never fails\n */\nextern void __kernel_map_pages(struct page *page, int numpages, int enable);\n\nstatic inline void debug_pagealloc_map_pages(struct page *page, int numpages)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\t__kernel_map_pages(page, numpages, 1);\n}\n\nstatic inline void debug_pagealloc_unmap_pages(struct page *page, int numpages)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\t__kernel_map_pages(page, numpages, 0);\n}\n#else\t/* CONFIG_DEBUG_PAGEALLOC */\nstatic inline void debug_pagealloc_map_pages(struct page *page, int numpages) {}\nstatic inline void debug_pagealloc_unmap_pages(struct page *page, int numpages) {}\n#endif\t/* CONFIG_DEBUG_PAGEALLOC */\n\n#ifdef __HAVE_ARCH_GATE_AREA\nextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\nextern int in_gate_area_no_mm(unsigned long addr);\nextern int in_gate_area(struct mm_struct *mm, unsigned long addr);\n#else\nstatic inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)\n{\n\treturn NULL;\n}\nstatic inline int in_gate_area_no_mm(unsigned long addr) { return 0; }\nstatic inline int in_gate_area(struct mm_struct *mm, unsigned long addr)\n{\n\treturn 0;\n}\n#endif\t/* __HAVE_ARCH_GATE_AREA */\n\nextern bool process_shares_mm(struct task_struct *p, struct mm_struct *mm);\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_drop_caches;\nint drop_caches_sysctl_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\n#endif\n\nvoid drop_slab(void);\nvoid drop_slab_node(int nid);\n\n#ifndef CONFIG_MMU\n#define randomize_va_space 0\n#else\nextern int randomize_va_space;\n#endif\n\nconst char * arch_vma_name(struct vm_area_struct *vma);\n#ifdef CONFIG_MMU\nvoid print_vma_addr(char *prefix, unsigned long rip);\n#else\nstatic inline void print_vma_addr(char *prefix, unsigned long rip)\n{\n}\n#endif\n\nvoid *sparse_buffer_alloc(unsigned long size);\nstruct page * __populate_section_memmap(unsigned long pfn,\n\t\tunsigned long nr_pages, int nid, struct vmem_altmap *altmap);\npgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\np4d_t *vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node);\npud_t *vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node);\npmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\npte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,\n\t\t\t    struct vmem_altmap *altmap);\nvoid *vmemmap_alloc_block(unsigned long size, int node);\nstruct vmem_altmap;\nvoid *vmemmap_alloc_block_buf(unsigned long size, int node,\n\t\t\t      struct vmem_altmap *altmap);\nvoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\nint vmemmap_populate_basepages(unsigned long start, unsigned long end,\n\t\t\t       int node, struct vmem_altmap *altmap);\nint vmemmap_populate(unsigned long start, unsigned long end, int node,\n\t\tstruct vmem_altmap *altmap);\nvoid vmemmap_populate_print_last(void);\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid vmemmap_free(unsigned long start, unsigned long end,\n\t\tstruct vmem_altmap *altmap);\n#endif\nvoid register_page_bootmem_memmap(unsigned long section_nr, struct page *map,\n\t\t\t\t  unsigned long nr_pages);\n\nenum mf_flags {\n\tMF_COUNT_INCREASED = 1 << 0,\n\tMF_ACTION_REQUIRED = 1 << 1,\n\tMF_MUST_KILL = 1 << 2,\n\tMF_SOFT_OFFLINE = 1 << 3,\n};\nextern int memory_failure(unsigned long pfn, int flags);\nextern void memory_failure_queue(unsigned long pfn, int flags);\nextern void memory_failure_queue_kick(int cpu);\nextern int unpoison_memory(unsigned long pfn);\nextern int sysctl_memory_failure_early_kill;\nextern int sysctl_memory_failure_recovery;\nextern void shake_page(struct page *p, int access);\nextern atomic_long_t num_poisoned_pages __read_mostly;\nextern int soft_offline_page(unsigned long pfn, int flags);\n\n\n/*\n * Error handlers for various types of pages.\n */\nenum mf_result {\n\tMF_IGNORED,\t/* Error: cannot be handled */\n\tMF_FAILED,\t/* Error: handling failed */\n\tMF_DELAYED,\t/* Will be handled later */\n\tMF_RECOVERED,\t/* Successfully recovered */\n};\n\nenum mf_action_page_type {\n\tMF_MSG_KERNEL,\n\tMF_MSG_KERNEL_HIGH_ORDER,\n\tMF_MSG_SLAB,\n\tMF_MSG_DIFFERENT_COMPOUND,\n\tMF_MSG_POISONED_HUGE,\n\tMF_MSG_HUGE,\n\tMF_MSG_FREE_HUGE,\n\tMF_MSG_NON_PMD_HUGE,\n\tMF_MSG_UNMAP_FAILED,\n\tMF_MSG_DIRTY_SWAPCACHE,\n\tMF_MSG_CLEAN_SWAPCACHE,\n\tMF_MSG_DIRTY_MLOCKED_LRU,\n\tMF_MSG_CLEAN_MLOCKED_LRU,\n\tMF_MSG_DIRTY_UNEVICTABLE_LRU,\n\tMF_MSG_CLEAN_UNEVICTABLE_LRU,\n\tMF_MSG_DIRTY_LRU,\n\tMF_MSG_CLEAN_LRU,\n\tMF_MSG_TRUNCATED_LRU,\n\tMF_MSG_BUDDY,\n\tMF_MSG_BUDDY_2ND,\n\tMF_MSG_DAX,\n\tMF_MSG_UNSPLIT_THP,\n\tMF_MSG_UNKNOWN,\n};\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\nextern void clear_huge_page(struct page *page,\n\t\t\t    unsigned long addr_hint,\n\t\t\t    unsigned int pages_per_huge_page);\nextern void copy_user_huge_page(struct page *dst, struct page *src,\n\t\t\t\tunsigned long addr_hint,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned int pages_per_huge_page);\nextern long copy_huge_page_from_user(struct page *dst_page,\n\t\t\t\tconst void __user *usr_src,\n\t\t\t\tunsigned int pages_per_huge_page,\n\t\t\t\tbool allow_pagefault);\n\n/**\n * vma_is_special_huge - Are transhuge page-table entries considered special?\n * @vma: Pointer to the struct vm_area_struct to consider\n *\n * Whether transhuge page-table entries are considered \"special\" following\n * the definition in vm_normal_page().\n *\n * Return: true if transhuge page-table entries should be considered special,\n * false otherwise.\n */\nstatic inline bool vma_is_special_huge(const struct vm_area_struct *vma)\n{\n\treturn vma_is_dax(vma) || (vma->vm_file &&\n\t\t\t\t   (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)));\n}\n\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern unsigned int _debug_guardpage_minorder;\nDECLARE_STATIC_KEY_FALSE(_debug_guardpage_enabled);\n\nstatic inline unsigned int debug_guardpage_minorder(void)\n{\n\treturn _debug_guardpage_minorder;\n}\n\nstatic inline bool debug_guardpage_enabled(void)\n{\n\treturn static_branch_unlikely(&_debug_guardpage_enabled);\n}\n\nstatic inline bool page_is_guard(struct page *page)\n{\n\tif (!debug_guardpage_enabled())\n\t\treturn false;\n\n\treturn PageGuard(page);\n}\n#else\nstatic inline unsigned int debug_guardpage_minorder(void) { return 0; }\nstatic inline bool debug_guardpage_enabled(void) { return false; }\nstatic inline bool page_is_guard(struct page *page) { return false; }\n#endif /* CONFIG_DEBUG_PAGEALLOC */\n\n#if MAX_NUMNODES > 1\nvoid __init setup_nr_node_ids(void);\n#else\nstatic inline void setup_nr_node_ids(void) {}\n#endif\n\nextern int memcmp_pages(struct page *page1, struct page *page2);\n\nstatic inline int pages_identical(struct page *page1, struct page *page2)\n{\n\treturn !memcmp_pages(page1, page2);\n}\n\n#ifdef CONFIG_MAPPING_DIRTY_HELPERS\nunsigned long clean_record_shared_mapping_range(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t first_index, pgoff_t nr,\n\t\t\t\t\t\tpgoff_t bitmap_pgoff,\n\t\t\t\t\t\tunsigned long *bitmap,\n\t\t\t\t\t\tpgoff_t *start,\n\t\t\t\t\t\tpgoff_t *end);\n\nunsigned long wp_shared_mapping_range(struct address_space *mapping,\n\t\t\t\t      pgoff_t first_index, pgoff_t nr);\n#endif\n\nextern int sysctl_nr_trim_pages;\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_MM_H */\n"}, "0": {"id": 0, "path": "/src/mm/memory.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/mm/memory.c\n *\n *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds\n */\n\n/*\n * demand-loading started 01.12.91 - seems it is high on the list of\n * things wanted, and it should be easy to implement. - Linus\n */\n\n/*\n * Ok, demand-loading was easy, shared pages a little bit tricker. Shared\n * pages started 02.12.91, seems to work. - Linus.\n *\n * Tested sharing by executing about 30 /bin/sh: under the old kernel it\n * would have taken more than the 6M I have free, but it worked well as\n * far as I could see.\n *\n * Also corrected some \"invalidate()\"s - I wasn't doing enough of them.\n */\n\n/*\n * Real VM (paging to/from disk) started 18.12.91. Much more work and\n * thought has to go into this. Oh, well..\n * 19.12.91  -  works, somewhat. Sometimes I get faults, don't know why.\n *\t\tFound it. Everything seems to work now.\n * 20.12.91  -  Ok, making the swap-device changeable like the root.\n */\n\n/*\n * 05.04.94  -  Multi-page memory management added for v1.1.\n *              Idea by Alex Bligh (alex@cconcepts.co.uk)\n *\n * 16.07.99  -  Support of BIGMEM added by Gerhard Wichert, Siemens AG\n *\t\t(Gerhard.Wichert@pdb.siemens.de)\n *\n * Aug/Sep 2004 Changed to four level page tables (Andi Kleen)\n */\n\n#include <linux/kernel_stat.h>\n#include <linux/mm.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/numa_balancing.h>\n#include <linux/sched/task.h>\n#include <linux/hugetlb.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/memremap.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/export.h>\n#include <linux/delayacct.h>\n#include <linux/init.h>\n#include <linux/pfn_t.h>\n#include <linux/writeback.h>\n#include <linux/memcontrol.h>\n#include <linux/mmu_notifier.h>\n#include <linux/swapops.h>\n#include <linux/elf.h>\n#include <linux/gfp.h>\n#include <linux/migrate.h>\n#include <linux/string.h>\n#include <linux/debugfs.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/dax.h>\n#include <linux/oom.h>\n#include <linux/numa.h>\n#include <linux/perf_event.h>\n#include <linux/ptrace.h>\n#include <linux/vmalloc.h>\n\n#include <trace/events/kmem.h>\n\n#include <asm/io.h>\n#include <asm/mmu_context.h>\n#include <asm/pgalloc.h>\n#include <linux/uaccess.h>\n#include <asm/tlb.h>\n#include <asm/tlbflush.h>\n\n#include \"pgalloc-track.h\"\n#include \"internal.h\"\n\n#if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)\n#warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.\n#endif\n\n#ifndef CONFIG_NEED_MULTIPLE_NODES\n/* use the per-pgdat data instead for discontigmem - mbligh */\nunsigned long max_mapnr;\nEXPORT_SYMBOL(max_mapnr);\n\nstruct page *mem_map;\nEXPORT_SYMBOL(mem_map);\n#endif\n\n/*\n * A number of key systems in x86 including ioremap() rely on the assumption\n * that high_memory defines the upper bound on direct map memory, then end\n * of ZONE_NORMAL.  Under CONFIG_DISCONTIG this means that max_low_pfn and\n * highstart_pfn must be the same; there must be no gap between ZONE_NORMAL\n * and ZONE_HIGHMEM.\n */\nvoid *high_memory;\nEXPORT_SYMBOL(high_memory);\n\n/*\n * Randomize the address space (stacks, mmaps, brk, etc.).\n *\n * ( When CONFIG_COMPAT_BRK=y we exclude brk from randomization,\n *   as ancient (libc5 based) binaries can segfault. )\n */\nint randomize_va_space __read_mostly =\n#ifdef CONFIG_COMPAT_BRK\n\t\t\t\t\t1;\n#else\n\t\t\t\t\t2;\n#endif\n\n#ifndef arch_faults_on_old_pte\nstatic inline bool arch_faults_on_old_pte(void)\n{\n\t/*\n\t * Those arches which don't have hw access flag feature need to\n\t * implement their own helper. By default, \"true\" means pagefault\n\t * will be hit on old pte.\n\t */\n\treturn true;\n}\n#endif\n\nstatic int __init disable_randmaps(char *s)\n{\n\trandomize_va_space = 0;\n\treturn 1;\n}\n__setup(\"norandmaps\", disable_randmaps);\n\nunsigned long zero_pfn __read_mostly;\nEXPORT_SYMBOL(zero_pfn);\n\nunsigned long highest_memmap_pfn __read_mostly;\n\n/*\n * CONFIG_MMU architectures set up ZERO_PAGE in their paging_init()\n */\nstatic int __init init_zero_pfn(void)\n{\n\tzero_pfn = page_to_pfn(ZERO_PAGE(0));\n\treturn 0;\n}\ncore_initcall(init_zero_pfn);\n\nvoid mm_trace_rss_stat(struct mm_struct *mm, int member, long count)\n{\n\ttrace_rss_stat(mm, member, count);\n}\n\n#if defined(SPLIT_RSS_COUNTING)\n\nvoid sync_mm_rss(struct mm_struct *mm)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_MM_COUNTERS; i++) {\n\t\tif (current->rss_stat.count[i]) {\n\t\t\tadd_mm_counter(mm, i, current->rss_stat.count[i]);\n\t\t\tcurrent->rss_stat.count[i] = 0;\n\t\t}\n\t}\n\tcurrent->rss_stat.events = 0;\n}\n\nstatic void add_mm_counter_fast(struct mm_struct *mm, int member, int val)\n{\n\tstruct task_struct *task = current;\n\n\tif (likely(task->mm == mm))\n\t\ttask->rss_stat.count[member] += val;\n\telse\n\t\tadd_mm_counter(mm, member, val);\n}\n#define inc_mm_counter_fast(mm, member) add_mm_counter_fast(mm, member, 1)\n#define dec_mm_counter_fast(mm, member) add_mm_counter_fast(mm, member, -1)\n\n/* sync counter once per 64 page faults */\n#define TASK_RSS_EVENTS_THRESH\t(64)\nstatic void check_sync_rss_stat(struct task_struct *task)\n{\n\tif (unlikely(task != current))\n\t\treturn;\n\tif (unlikely(task->rss_stat.events++ > TASK_RSS_EVENTS_THRESH))\n\t\tsync_mm_rss(task->mm);\n}\n#else /* SPLIT_RSS_COUNTING */\n\n#define inc_mm_counter_fast(mm, member) inc_mm_counter(mm, member)\n#define dec_mm_counter_fast(mm, member) dec_mm_counter(mm, member)\n\nstatic void check_sync_rss_stat(struct task_struct *task)\n{\n}\n\n#endif /* SPLIT_RSS_COUNTING */\n\n/*\n * Note: this doesn't free the actual pages themselves. That\n * has been handled earlier when unmapping all the memory regions.\n */\nstatic void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,\n\t\t\t   unsigned long addr)\n{\n\tpgtable_t token = pmd_pgtable(*pmd);\n\tpmd_clear(pmd);\n\tpte_free_tlb(tlb, token, addr);\n\tmm_dec_nr_ptes(tlb->mm);\n}\n\nstatic inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tunsigned long start;\n\n\tstart = addr;\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none_or_clear_bad(pmd))\n\t\t\tcontinue;\n\t\tfree_pte_range(tlb, pmd, addr);\n\t} while (pmd++, addr = next, addr != end);\n\n\tstart &= PUD_MASK;\n\tif (start < floor)\n\t\treturn;\n\tif (ceiling) {\n\t\tceiling &= PUD_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\treturn;\n\n\tpmd = pmd_offset(pud, start);\n\tpud_clear(pud);\n\tpmd_free_tlb(tlb, pmd, start);\n\tmm_dec_nr_pmds(tlb->mm);\n}\n\nstatic inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tunsigned long start;\n\n\tstart = addr;\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tfree_pmd_range(tlb, pud, addr, next, floor, ceiling);\n\t} while (pud++, addr = next, addr != end);\n\n\tstart &= P4D_MASK;\n\tif (start < floor)\n\t\treturn;\n\tif (ceiling) {\n\t\tceiling &= P4D_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\treturn;\n\n\tpud = pud_offset(p4d, start);\n\tp4d_clear(p4d);\n\tpud_free_tlb(tlb, pud, start);\n\tmm_dec_nr_puds(tlb->mm);\n}\n\nstatic inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tunsigned long start;\n\n\tstart = addr;\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tfree_pud_range(tlb, p4d, addr, next, floor, ceiling);\n\t} while (p4d++, addr = next, addr != end);\n\n\tstart &= PGDIR_MASK;\n\tif (start < floor)\n\t\treturn;\n\tif (ceiling) {\n\t\tceiling &= PGDIR_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\treturn;\n\n\tp4d = p4d_offset(pgd, start);\n\tpgd_clear(pgd);\n\tp4d_free_tlb(tlb, p4d, start);\n}\n\n/*\n * This function frees user-level page tables of a process.\n */\nvoid free_pgd_range(struct mmu_gather *tlb,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\n\t/*\n\t * The next few lines have given us lots of grief...\n\t *\n\t * Why are we testing PMD* at this top level?  Because often\n\t * there will be no work to do at all, and we'd prefer not to\n\t * go all the way down to the bottom just to discover that.\n\t *\n\t * Why all these \"- 1\"s?  Because 0 represents both the bottom\n\t * of the address space and the top of it (using -1 for the\n\t * top wouldn't help much: the masks would do the wrong thing).\n\t * The rule is that addr 0 and floor 0 refer to the bottom of\n\t * the address space, but end 0 and ceiling 0 refer to the top\n\t * Comparisons need to use \"end - 1\" and \"ceiling - 1\" (though\n\t * that end 0 case should be mythical).\n\t *\n\t * Wherever addr is brought up or ceiling brought down, we must\n\t * be careful to reject \"the opposite 0\" before it confuses the\n\t * subsequent tests.  But what about where end is brought down\n\t * by PMD_SIZE below? no, end can't go down to 0 there.\n\t *\n\t * Whereas we round start (addr) and ceiling down, by different\n\t * masks at different levels, in order to test whether a table\n\t * now has no other vmas using it, so can be freed, we don't\n\t * bother to round floor or end up - the tests don't need that.\n\t */\n\n\taddr &= PMD_MASK;\n\tif (addr < floor) {\n\t\taddr += PMD_SIZE;\n\t\tif (!addr)\n\t\t\treturn;\n\t}\n\tif (ceiling) {\n\t\tceiling &= PMD_MASK;\n\t\tif (!ceiling)\n\t\t\treturn;\n\t}\n\tif (end - 1 > ceiling - 1)\n\t\tend -= PMD_SIZE;\n\tif (addr > end - 1)\n\t\treturn;\n\t/*\n\t * We add page table cache pages with PAGE_SIZE,\n\t * (see pte_free_tlb()), flush the tlb if we need\n\t */\n\ttlb_change_page_size(tlb, PAGE_SIZE);\n\tpgd = pgd_offset(tlb->mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tfree_p4d_range(tlb, pgd, addr, next, floor, ceiling);\n\t} while (pgd++, addr = next, addr != end);\n}\n\nvoid free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\tunsigned long floor, unsigned long ceiling)\n{\n\twhile (vma) {\n\t\tstruct vm_area_struct *next = vma->vm_next;\n\t\tunsigned long addr = vma->vm_start;\n\n\t\t/*\n\t\t * Hide vma from rmap and truncate_pagecache before freeing\n\t\t * pgtables\n\t\t */\n\t\tunlink_anon_vmas(vma);\n\t\tunlink_file_vma(vma);\n\n\t\tif (is_vm_hugetlb_page(vma)) {\n\t\t\thugetlb_free_pgd_range(tlb, addr, vma->vm_end,\n\t\t\t\tfloor, next ? next->vm_start : ceiling);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Optimization: gather nearby vmas into one call down\n\t\t\t */\n\t\t\twhile (next && next->vm_start <= vma->vm_end + PMD_SIZE\n\t\t\t       && !is_vm_hugetlb_page(next)) {\n\t\t\t\tvma = next;\n\t\t\t\tnext = vma->vm_next;\n\t\t\t\tunlink_anon_vmas(vma);\n\t\t\t\tunlink_file_vma(vma);\n\t\t\t}\n\t\t\tfree_pgd_range(tlb, addr, vma->vm_end,\n\t\t\t\tfloor, next ? next->vm_start : ceiling);\n\t\t}\n\t\tvma = next;\n\t}\n}\n\nint __pte_alloc(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl;\n\tpgtable_t new = pte_alloc_one(mm);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Ensure all pte setup (eg. pte page lock and page clearing) are\n\t * visible before the pte is made visible to other CPUs by being\n\t * put into page tables.\n\t *\n\t * The other side of the story is the pointer chasing in the page\n\t * table walking code (when walking the page table without locking;\n\t * ie. most of the time). Fortunately, these data accesses consist\n\t * of a chain of data-dependent loads, meaning most CPUs (alpha\n\t * being the notable exception) will already guarantee loads are\n\t * seen in-order. See the alpha page table accessors for the\n\t * smp_rmb() barriers in page table walking code.\n\t */\n\tsmp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */\n\n\tptl = pmd_lock(mm, pmd);\n\tif (likely(pmd_none(*pmd))) {\t/* Has another populated it ? */\n\t\tmm_inc_nr_ptes(mm);\n\t\tpmd_populate(mm, pmd, new);\n\t\tnew = NULL;\n\t}\n\tspin_unlock(ptl);\n\tif (new)\n\t\tpte_free(mm, new);\n\treturn 0;\n}\n\nint __pte_alloc_kernel(pmd_t *pmd)\n{\n\tpte_t *new = pte_alloc_one_kernel(&init_mm);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tsmp_wmb(); /* See comment in __pte_alloc */\n\n\tspin_lock(&init_mm.page_table_lock);\n\tif (likely(pmd_none(*pmd))) {\t/* Has another populated it ? */\n\t\tpmd_populate_kernel(&init_mm, pmd, new);\n\t\tnew = NULL;\n\t}\n\tspin_unlock(&init_mm.page_table_lock);\n\tif (new)\n\t\tpte_free_kernel(&init_mm, new);\n\treturn 0;\n}\n\nstatic inline void init_rss_vec(int *rss)\n{\n\tmemset(rss, 0, sizeof(int) * NR_MM_COUNTERS);\n}\n\nstatic inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)\n{\n\tint i;\n\n\tif (current->mm == mm)\n\t\tsync_mm_rss(mm);\n\tfor (i = 0; i < NR_MM_COUNTERS; i++)\n\t\tif (rss[i])\n\t\t\tadd_mm_counter(mm, i, rss[i]);\n}\n\n/*\n * This function is called to print an error when a bad pte\n * is found. For example, we might have a PFN-mapped pte in\n * a region that doesn't allow it.\n *\n * The calling function must still handle the error.\n */\nstatic void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t  pte_t pte, struct page *page)\n{\n\tpgd_t *pgd = pgd_offset(vma->vm_mm, addr);\n\tp4d_t *p4d = p4d_offset(pgd, addr);\n\tpud_t *pud = pud_offset(p4d, addr);\n\tpmd_t *pmd = pmd_offset(pud, addr);\n\tstruct address_space *mapping;\n\tpgoff_t index;\n\tstatic unsigned long resume;\n\tstatic unsigned long nr_shown;\n\tstatic unsigned long nr_unshown;\n\n\t/*\n\t * Allow a burst of 60 reports, then keep quiet for that minute;\n\t * or allow a steady drip of one report per second.\n\t */\n\tif (nr_shown == 60) {\n\t\tif (time_before(jiffies, resume)) {\n\t\t\tnr_unshown++;\n\t\t\treturn;\n\t\t}\n\t\tif (nr_unshown) {\n\t\t\tpr_alert(\"BUG: Bad page map: %lu messages suppressed\\n\",\n\t\t\t\t nr_unshown);\n\t\t\tnr_unshown = 0;\n\t\t}\n\t\tnr_shown = 0;\n\t}\n\tif (nr_shown++ == 0)\n\t\tresume = jiffies + 60 * HZ;\n\n\tmapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;\n\tindex = linear_page_index(vma, addr);\n\n\tpr_alert(\"BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\\n\",\n\t\t current->comm,\n\t\t (long long)pte_val(pte), (long long)pmd_val(*pmd));\n\tif (page)\n\t\tdump_page(page, \"bad pte\");\n\tpr_alert(\"addr:%px vm_flags:%08lx anon_vma:%px mapping:%px index:%lx\\n\",\n\t\t (void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);\n\tpr_alert(\"file:%pD fault:%ps mmap:%ps readpage:%ps\\n\",\n\t\t vma->vm_file,\n\t\t vma->vm_ops ? vma->vm_ops->fault : NULL,\n\t\t vma->vm_file ? vma->vm_file->f_op->mmap : NULL,\n\t\t mapping ? mapping->a_ops->readpage : NULL);\n\tdump_stack();\n\tadd_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);\n}\n\n/*\n * vm_normal_page -- This function gets the \"struct page\" associated with a pte.\n *\n * \"Special\" mappings do not wish to be associated with a \"struct page\" (either\n * it doesn't exist, or it exists but they don't want to touch it). In this\n * case, NULL is returned here. \"Normal\" mappings do have a struct page.\n *\n * There are 2 broad cases. Firstly, an architecture may define a pte_special()\n * pte bit, in which case this function is trivial. Secondly, an architecture\n * may not have a spare pte bit, which requires a more complicated scheme,\n * described below.\n *\n * A raw VM_PFNMAP mapping (ie. one that is not COWed) is always considered a\n * special mapping (even if there are underlying and valid \"struct pages\").\n * COWed pages of a VM_PFNMAP are always normal.\n *\n * The way we recognize COWed pages within VM_PFNMAP mappings is through the\n * rules set up by \"remap_pfn_range()\": the vma will have the VM_PFNMAP bit\n * set, and the vm_pgoff will point to the first PFN mapped: thus every special\n * mapping will always honor the rule\n *\n *\tpfn_of_page == vma->vm_pgoff + ((addr - vma->vm_start) >> PAGE_SHIFT)\n *\n * And for normal mappings this is false.\n *\n * This restricts such mappings to be a linear translation from virtual address\n * to pfn. To get around this restriction, we allow arbitrary mappings so long\n * as the vma is not a COW mapping; in that case, we know that all ptes are\n * special (because none can have been COWed).\n *\n *\n * In order to support COW of arbitrary special mappings, we have VM_MIXEDMAP.\n *\n * VM_MIXEDMAP mappings can likewise contain memory with or without \"struct\n * page\" backing, however the difference is that _all_ pages with a struct\n * page (that is, those where pfn_valid is true) are refcounted and considered\n * normal pages by the VM. The disadvantage is that pages are refcounted\n * (which can be slower and simply not an option for some PFNMAP users). The\n * advantage is that we don't have to follow the strict linearity rule of\n * PFNMAP mappings in order to support COWable mappings.\n *\n */\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t    pte_t pte)\n{\n\tunsigned long pfn = pte_pfn(pte);\n\n\tif (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL)) {\n\t\tif (likely(!pte_special(pte)))\n\t\t\tgoto check_pfn;\n\t\tif (vma->vm_ops && vma->vm_ops->find_special_page)\n\t\t\treturn vma->vm_ops->find_special_page(vma, addr);\n\t\tif (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))\n\t\t\treturn NULL;\n\t\tif (is_zero_pfn(pfn))\n\t\t\treturn NULL;\n\t\tif (pte_devmap(pte))\n\t\t\treturn NULL;\n\n\t\tprint_bad_pte(vma, addr, pte, NULL);\n\t\treturn NULL;\n\t}\n\n\t/* !CONFIG_ARCH_HAS_PTE_SPECIAL case follows: */\n\n\tif (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {\n\t\tif (vma->vm_flags & VM_MIXEDMAP) {\n\t\t\tif (!pfn_valid(pfn))\n\t\t\t\treturn NULL;\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tunsigned long off;\n\t\t\toff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\t\tif (pfn == vma->vm_pgoff + off)\n\t\t\t\treturn NULL;\n\t\t\tif (!is_cow_mapping(vma->vm_flags))\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (is_zero_pfn(pfn))\n\t\treturn NULL;\n\ncheck_pfn:\n\tif (unlikely(pfn > highest_memmap_pfn)) {\n\t\tprint_bad_pte(vma, addr, pte, NULL);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * NOTE! We still have PageReserved() pages in the page tables.\n\t * eg. VDSO mappings can cause them to exist.\n\t */\nout:\n\treturn pfn_to_page(pfn);\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tpmd_t pmd)\n{\n\tunsigned long pfn = pmd_pfn(pmd);\n\n\t/*\n\t * There is no pmd_special() but there may be special pmds, e.g.\n\t * in a direct-access (dax) mapping, so let's just replicate the\n\t * !CONFIG_ARCH_HAS_PTE_SPECIAL case from vm_normal_page() here.\n\t */\n\tif (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {\n\t\tif (vma->vm_flags & VM_MIXEDMAP) {\n\t\t\tif (!pfn_valid(pfn))\n\t\t\t\treturn NULL;\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tunsigned long off;\n\t\t\toff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\t\tif (pfn == vma->vm_pgoff + off)\n\t\t\t\treturn NULL;\n\t\t\tif (!is_cow_mapping(vma->vm_flags))\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (pmd_devmap(pmd))\n\t\treturn NULL;\n\tif (is_huge_zero_pmd(pmd))\n\t\treturn NULL;\n\tif (unlikely(pfn > highest_memmap_pfn))\n\t\treturn NULL;\n\n\t/*\n\t * NOTE! We still have PageReserved() pages in the page tables.\n\t * eg. VDSO mappings can cause them to exist.\n\t */\nout:\n\treturn pfn_to_page(pfn);\n}\n#endif\n\n/*\n * copy one vm_area from one task to the other. Assumes the page tables\n * already present in the new task to be cleared in the whole range\n * covered by this vma.\n */\n\nstatic unsigned long\ncopy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\tpte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,\n\t\tunsigned long addr, int *rss)\n{\n\tunsigned long vm_flags = vma->vm_flags;\n\tpte_t pte = *src_pte;\n\tstruct page *page;\n\tswp_entry_t entry = pte_to_swp_entry(pte);\n\n\tif (likely(!non_swap_entry(entry))) {\n\t\tif (swap_duplicate(entry) < 0)\n\t\t\treturn entry.val;\n\n\t\t/* make sure dst_mm is on swapoff's mmlist. */\n\t\tif (unlikely(list_empty(&dst_mm->mmlist))) {\n\t\t\tspin_lock(&mmlist_lock);\n\t\t\tif (list_empty(&dst_mm->mmlist))\n\t\t\t\tlist_add(&dst_mm->mmlist,\n\t\t\t\t\t\t&src_mm->mmlist);\n\t\t\tspin_unlock(&mmlist_lock);\n\t\t}\n\t\trss[MM_SWAPENTS]++;\n\t} else if (is_migration_entry(entry)) {\n\t\tpage = migration_entry_to_page(entry);\n\n\t\trss[mm_counter(page)]++;\n\n\t\tif (is_write_migration_entry(entry) &&\n\t\t\t\tis_cow_mapping(vm_flags)) {\n\t\t\t/*\n\t\t\t * COW mappings require pages in both\n\t\t\t * parent and child to be set to read.\n\t\t\t */\n\t\t\tmake_migration_entry_read(&entry);\n\t\t\tpte = swp_entry_to_pte(entry);\n\t\t\tif (pte_swp_soft_dirty(*src_pte))\n\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n\t\t\tif (pte_swp_uffd_wp(*src_pte))\n\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n\t\t\tset_pte_at(src_mm, addr, src_pte, pte);\n\t\t}\n\t} else if (is_device_private_entry(entry)) {\n\t\tpage = device_private_entry_to_page(entry);\n\n\t\t/*\n\t\t * Update rss count even for unaddressable pages, as\n\t\t * they should treated just like normal pages in this\n\t\t * respect.\n\t\t *\n\t\t * We will likely want to have some new rss counters\n\t\t * for unaddressable pages, at some point. But for now\n\t\t * keep things as they are.\n\t\t */\n\t\tget_page(page);\n\t\trss[mm_counter(page)]++;\n\t\tpage_dup_rmap(page, false);\n\n\t\t/*\n\t\t * We do not preserve soft-dirty information, because so\n\t\t * far, checkpoint/restore is the only feature that\n\t\t * requires that. And checkpoint/restore does not work\n\t\t * when a device driver is involved (you cannot easily\n\t\t * save and restore device driver state).\n\t\t */\n\t\tif (is_write_device_private_entry(entry) &&\n\t\t    is_cow_mapping(vm_flags)) {\n\t\t\tmake_device_private_entry_read(&entry);\n\t\t\tpte = swp_entry_to_pte(entry);\n\t\t\tif (pte_swp_uffd_wp(*src_pte))\n\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n\t\t\tset_pte_at(src_mm, addr, src_pte, pte);\n\t\t}\n\t}\n\tset_pte_at(dst_mm, addr, dst_pte, pte);\n\treturn 0;\n}\n\n/*\n * Copy a present and normal page if necessary.\n *\n * NOTE! The usual case is that this doesn't need to do\n * anything, and can just return a positive value. That\n * will let the caller know that it can just increase\n * the page refcount and re-use the pte the traditional\n * way.\n *\n * But _if_ we need to copy it because it needs to be\n * pinned in the parent (and the child should get its own\n * copy rather than just a reference to the same page),\n * we'll do that here and return zero to let the caller\n * know we're done.\n *\n * And if we need a pre-allocated page but don't yet have\n * one, return a negative error to let the preallocation\n * code know so that it can do so outside the page table\n * lock.\n */\nstatic inline int\ncopy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t  pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t  struct page **prealloc, pte_t pte, struct page *page)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tstruct page *new_page;\n\n\tif (!is_cow_mapping(src_vma->vm_flags))\n\t\treturn 1;\n\n\t/*\n\t * What we want to do is to check whether this page may\n\t * have been pinned by the parent process.  If so,\n\t * instead of wrprotect the pte on both sides, we copy\n\t * the page immediately so that we'll always guarantee\n\t * the pinned page won't be randomly replaced in the\n\t * future.\n\t *\n\t * The page pinning checks are just \"has this mm ever\n\t * seen pinning\", along with the (inexact) check of\n\t * the page count. That might give false positives for\n\t * for pinning, but it will work correctly.\n\t */\n\tif (likely(!atomic_read(&src_mm->has_pinned)))\n\t\treturn 1;\n\tif (likely(!page_maybe_dma_pinned(page)))\n\t\treturn 1;\n\n\tnew_page = *prealloc;\n\tif (!new_page)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * We have a prealloc page, all good!  Take it\n\t * over and copy the page & arm it.\n\t */\n\t*prealloc = NULL;\n\tcopy_user_highpage(new_page, page, addr, src_vma);\n\t__SetPageUptodate(new_page);\n\tpage_add_new_anon_rmap(new_page, dst_vma, addr, false);\n\tlru_cache_add_inactive_or_unevictable(new_page, dst_vma);\n\trss[mm_counter(new_page)]++;\n\n\t/* All done, just insert the new page copy in the child */\n\tpte = mk_pte(new_page, dst_vma->vm_page_prot);\n\tpte = maybe_mkwrite(pte_mkdirty(pte), dst_vma);\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}\n\n/*\n * Copy one pte.  Returns 0 if succeeded, or -EAGAIN if one preallocated page\n * is required to copy this pte.\n */\nstatic inline int\ncopy_present_pte(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t\t pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,\n\t\t struct page **prealloc)\n{\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tunsigned long vm_flags = src_vma->vm_flags;\n\tpte_t pte = *src_pte;\n\tstruct page *page;\n\n\tpage = vm_normal_page(src_vma, addr, pte);\n\tif (page) {\n\t\tint retval;\n\n\t\tretval = copy_present_page(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t\t   addr, rss, prealloc, pte, page);\n\t\tif (retval <= 0)\n\t\t\treturn retval;\n\n\t\tget_page(page);\n\t\tpage_dup_rmap(page, false);\n\t\trss[mm_counter(page)]++;\n\t}\n\n\t/*\n\t * If it's a COW mapping, write protect it both\n\t * in the parent and the child\n\t */\n\tif (is_cow_mapping(vm_flags) && pte_write(pte)) {\n\t\tptep_set_wrprotect(src_mm, addr, src_pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\n\t/*\n\t * If it's a shared mapping, mark it clean in\n\t * the child\n\t */\n\tif (vm_flags & VM_SHARED)\n\t\tpte = pte_mkclean(pte);\n\tpte = pte_mkold(pte);\n\n\t/*\n\t * Make sure the _PAGE_UFFD_WP bit is cleared if the new VMA\n\t * does not have the VM_UFFD_WP, which means that the uffd\n\t * fork event is not enabled.\n\t */\n\tif (!(vm_flags & VM_UFFD_WP))\n\t\tpte = pte_clear_uffd_wp(pte);\n\n\tset_pte_at(dst_vma->vm_mm, addr, dst_pte, pte);\n\treturn 0;\n}\n\nstatic inline struct page *\npage_copy_prealloc(struct mm_struct *src_mm, struct vm_area_struct *vma,\n\t\t   unsigned long addr)\n{\n\tstruct page *new_page;\n\n\tnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, addr);\n\tif (!new_page)\n\t\treturn NULL;\n\n\tif (mem_cgroup_charge(new_page, src_mm, GFP_KERNEL)) {\n\t\tput_page(new_page);\n\t\treturn NULL;\n\t}\n\tcgroup_throttle_swaprate(new_page, GFP_KERNEL);\n\n\treturn new_page;\n}\n\nstatic int\ncopy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpte_t *orig_src_pte, *orig_dst_pte;\n\tpte_t *src_pte, *dst_pte;\n\tspinlock_t *src_ptl, *dst_ptl;\n\tint progress, ret = 0;\n\tint rss[NR_MM_COUNTERS];\n\tswp_entry_t entry = (swp_entry_t){0};\n\tstruct page *prealloc = NULL;\n\nagain:\n\tprogress = 0;\n\tinit_rss_vec(rss);\n\n\tdst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);\n\tif (!dst_pte) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsrc_pte = pte_offset_map(src_pmd, addr);\n\tsrc_ptl = pte_lockptr(src_mm, src_pmd);\n\tspin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);\n\torig_src_pte = src_pte;\n\torig_dst_pte = dst_pte;\n\tarch_enter_lazy_mmu_mode();\n\n\tdo {\n\t\t/*\n\t\t * We are holding two locks at this point - either of them\n\t\t * could generate latencies in another task on another CPU.\n\t\t */\n\t\tif (progress >= 32) {\n\t\t\tprogress = 0;\n\t\t\tif (need_resched() ||\n\t\t\t    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (pte_none(*src_pte)) {\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(!pte_present(*src_pte))) {\n\t\t\tentry.val = copy_nonpresent_pte(dst_mm, src_mm,\n\t\t\t\t\t\t\tdst_pte, src_pte,\n\t\t\t\t\t\t\tsrc_vma, addr, rss);\n\t\t\tif (entry.val)\n\t\t\t\tbreak;\n\t\t\tprogress += 8;\n\t\t\tcontinue;\n\t\t}\n\t\t/* copy_present_pte() will clear `*prealloc' if consumed */\n\t\tret = copy_present_pte(dst_vma, src_vma, dst_pte, src_pte,\n\t\t\t\t       addr, rss, &prealloc);\n\t\t/*\n\t\t * If we need a pre-allocated page for this pte, drop the\n\t\t * locks, allocate, and try again.\n\t\t */\n\t\tif (unlikely(ret == -EAGAIN))\n\t\t\tbreak;\n\t\tif (unlikely(prealloc)) {\n\t\t\t/*\n\t\t\t * pre-alloc page cannot be reused by next time so as\n\t\t\t * to strictly follow mempolicy (e.g., alloc_page_vma()\n\t\t\t * will allocate page according to address).  This\n\t\t\t * could only happen if one pinned pte changed.\n\t\t\t */\n\t\t\tput_page(prealloc);\n\t\t\tprealloc = NULL;\n\t\t}\n\t\tprogress += 8;\n\t} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);\n\n\tarch_leave_lazy_mmu_mode();\n\tspin_unlock(src_ptl);\n\tpte_unmap(orig_src_pte);\n\tadd_mm_rss_vec(dst_mm, rss);\n\tpte_unmap_unlock(orig_dst_pte, dst_ptl);\n\tcond_resched();\n\n\tif (entry.val) {\n\t\tif (add_swap_count_continuation(entry, GFP_KERNEL) < 0) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tentry.val = 0;\n\t} else if (ret) {\n\t\tWARN_ON_ONCE(ret != -EAGAIN);\n\t\tprealloc = page_copy_prealloc(src_mm, src_vma, addr);\n\t\tif (!prealloc)\n\t\t\treturn -ENOMEM;\n\t\t/* We've captured and resolved the error. Reset, try again. */\n\t\tret = 0;\n\t}\n\tif (addr != end)\n\t\tgoto again;\nout:\n\tif (unlikely(prealloc))\n\t\tput_page(prealloc);\n\treturn ret;\n}\n\nstatic inline int\ncopy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pud_t *dst_pud, pud_t *src_pud, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpmd_t *src_pmd, *dst_pmd;\n\tunsigned long next;\n\n\tdst_pmd = pmd_alloc(dst_mm, dst_pud, addr);\n\tif (!dst_pmd)\n\t\treturn -ENOMEM;\n\tsrc_pmd = pmd_offset(src_pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)\n\t\t\t|| pmd_devmap(*src_pmd)) {\n\t\t\tint err;\n\t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, src_vma);\n\t\t\terr = copy_huge_pmd(dst_mm, src_mm,\n\t\t\t\t\t    dst_pmd, src_pmd, addr, src_vma);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pmd_none_or_clear_bad(src_pmd))\n\t\t\tcontinue;\n\t\tif (copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_pmd++, src_pmd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int\ncopy_pud_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       p4d_t *dst_p4d, p4d_t *src_p4d, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tpud_t *src_pud, *dst_pud;\n\tunsigned long next;\n\n\tdst_pud = pud_alloc(dst_mm, dst_p4d, addr);\n\tif (!dst_pud)\n\t\treturn -ENOMEM;\n\tsrc_pud = pud_offset(src_p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {\n\t\t\tint err;\n\n\t\t\tVM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, src_vma);\n\t\t\terr = copy_huge_pud(dst_mm, src_mm,\n\t\t\t\t\t    dst_pud, src_pud, addr, src_vma);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\treturn -ENOMEM;\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pud_none_or_clear_bad(src_pud))\n\t\t\tcontinue;\n\t\tif (copy_pmd_range(dst_vma, src_vma, dst_pud, src_pud,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_pud++, src_pud++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int\ncopy_p4d_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,\n\t       pgd_t *dst_pgd, pgd_t *src_pgd, unsigned long addr,\n\t       unsigned long end)\n{\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tp4d_t *src_p4d, *dst_p4d;\n\tunsigned long next;\n\n\tdst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);\n\tif (!dst_p4d)\n\t\treturn -ENOMEM;\n\tsrc_p4d = p4d_offset(src_pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(src_p4d))\n\t\t\tcontinue;\n\t\tif (copy_pud_range(dst_vma, src_vma, dst_p4d, src_p4d,\n\t\t\t\t   addr, next))\n\t\t\treturn -ENOMEM;\n\t} while (dst_p4d++, src_p4d++, addr = next, addr != end);\n\treturn 0;\n}\n\nint\ncopy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)\n{\n\tpgd_t *src_pgd, *dst_pgd;\n\tunsigned long next;\n\tunsigned long addr = src_vma->vm_start;\n\tunsigned long end = src_vma->vm_end;\n\tstruct mm_struct *dst_mm = dst_vma->vm_mm;\n\tstruct mm_struct *src_mm = src_vma->vm_mm;\n\tstruct mmu_notifier_range range;\n\tbool is_cow;\n\tint ret;\n\n\t/*\n\t * Don't copy ptes where a page fault will fill them correctly.\n\t * Fork becomes much lighter when there are big shared or private\n\t * readonly mappings. The tradeoff is that copy_page_range is more\n\t * efficient than faulting.\n\t */\n\tif (!(src_vma->vm_flags & (VM_HUGETLB | VM_PFNMAP | VM_MIXEDMAP)) &&\n\t    !src_vma->anon_vma)\n\t\treturn 0;\n\n\tif (is_vm_hugetlb_page(src_vma))\n\t\treturn copy_hugetlb_page_range(dst_mm, src_mm, src_vma);\n\n\tif (unlikely(src_vma->vm_flags & VM_PFNMAP)) {\n\t\t/*\n\t\t * We do not free on error cases below as remove_vma\n\t\t * gets called on error from higher level routine\n\t\t */\n\t\tret = track_pfn_copy(src_vma);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/*\n\t * We need to invalidate the secondary MMU mappings only when\n\t * there could be a permission downgrade on the ptes of the\n\t * parent mm. And a permission downgrade will only happen if\n\t * is_cow_mapping() returns true.\n\t */\n\tis_cow = is_cow_mapping(src_vma->vm_flags);\n\n\tif (is_cow) {\n\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,\n\t\t\t\t\t0, src_vma, src_mm, addr, end);\n\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t/*\n\t\t * Disabling preemption is not needed for the write side, as\n\t\t * the read side doesn't spin, but goes to the mmap_lock.\n\t\t *\n\t\t * Use the raw variant of the seqcount_t write API to avoid\n\t\t * lockdep complaining about preemptibility.\n\t\t */\n\t\tmmap_assert_write_locked(src_mm);\n\t\traw_write_seqcount_begin(&src_mm->write_protect_seq);\n\t}\n\n\tret = 0;\n\tdst_pgd = pgd_offset(dst_mm, addr);\n\tsrc_pgd = pgd_offset(src_mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(src_pgd))\n\t\t\tcontinue;\n\t\tif (unlikely(copy_p4d_range(dst_vma, src_vma, dst_pgd, src_pgd,\n\t\t\t\t\t    addr, next))) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t} while (dst_pgd++, src_pgd++, addr = next, addr != end);\n\n\tif (is_cow) {\n\t\traw_write_seqcount_end(&src_mm->write_protect_seq);\n\t\tmmu_notifier_invalidate_range_end(&range);\n\t}\n\treturn ret;\n}\n\nstatic unsigned long zap_pte_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, pmd_t *pmd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tstruct mm_struct *mm = tlb->mm;\n\tint force_flush = 0;\n\tint rss[NR_MM_COUNTERS];\n\tspinlock_t *ptl;\n\tpte_t *start_pte;\n\tpte_t *pte;\n\tswp_entry_t entry;\n\n\ttlb_change_page_size(tlb, PAGE_SIZE);\nagain:\n\tinit_rss_vec(rss);\n\tstart_pte = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\tpte = start_pte;\n\tflush_tlb_batched_pending(mm);\n\tarch_enter_lazy_mmu_mode();\n\tdo {\n\t\tpte_t ptent = *pte;\n\t\tif (pte_none(ptent))\n\t\t\tcontinue;\n\n\t\tif (need_resched())\n\t\t\tbreak;\n\n\t\tif (pte_present(ptent)) {\n\t\t\tstruct page *page;\n\n\t\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\t\tif (unlikely(details) && page) {\n\t\t\t\t/*\n\t\t\t\t * unmap_shared_mapping_pages() wants to\n\t\t\t\t * invalidate cache without truncating:\n\t\t\t\t * unmap shared but keep private pages.\n\t\t\t\t */\n\t\t\t\tif (details->check_mapping &&\n\t\t\t\t    details->check_mapping != page_rmapping(page))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tptent = ptep_get_and_clear_full(mm, addr, pte,\n\t\t\t\t\t\t\ttlb->fullmm);\n\t\t\ttlb_remove_tlb_entry(tlb, pte, addr);\n\t\t\tif (unlikely(!page))\n\t\t\t\tcontinue;\n\n\t\t\tif (!PageAnon(page)) {\n\t\t\t\tif (pte_dirty(ptent)) {\n\t\t\t\t\tforce_flush = 1;\n\t\t\t\t\tset_page_dirty(page);\n\t\t\t\t}\n\t\t\t\tif (pte_young(ptent) &&\n\t\t\t\t    likely(!(vma->vm_flags & VM_SEQ_READ)))\n\t\t\t\t\tmark_page_accessed(page);\n\t\t\t}\n\t\t\trss[mm_counter(page)]--;\n\t\t\tpage_remove_rmap(page, false);\n\t\t\tif (unlikely(page_mapcount(page) < 0))\n\t\t\t\tprint_bad_pte(vma, addr, ptent, page);\n\t\t\tif (unlikely(__tlb_remove_page(tlb, page))) {\n\t\t\t\tforce_flush = 1;\n\t\t\t\taddr += PAGE_SIZE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tentry = pte_to_swp_entry(ptent);\n\t\tif (is_device_private_entry(entry)) {\n\t\t\tstruct page *page = device_private_entry_to_page(entry);\n\n\t\t\tif (unlikely(details && details->check_mapping)) {\n\t\t\t\t/*\n\t\t\t\t * unmap_shared_mapping_pages() wants to\n\t\t\t\t * invalidate cache without truncating:\n\t\t\t\t * unmap shared but keep private pages.\n\t\t\t\t */\n\t\t\t\tif (details->check_mapping !=\n\t\t\t\t    page_rmapping(page))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tpte_clear_not_present_full(mm, addr, pte, tlb->fullmm);\n\t\t\trss[mm_counter(page)]--;\n\t\t\tpage_remove_rmap(page, false);\n\t\t\tput_page(page);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* If details->check_mapping, we leave swap entries. */\n\t\tif (unlikely(details))\n\t\t\tcontinue;\n\n\t\tif (!non_swap_entry(entry))\n\t\t\trss[MM_SWAPENTS]--;\n\t\telse if (is_migration_entry(entry)) {\n\t\t\tstruct page *page;\n\n\t\t\tpage = migration_entry_to_page(entry);\n\t\t\trss[mm_counter(page)]--;\n\t\t}\n\t\tif (unlikely(!free_swap_and_cache(entry)))\n\t\t\tprint_bad_pte(vma, addr, ptent, NULL);\n\t\tpte_clear_not_present_full(mm, addr, pte, tlb->fullmm);\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\n\tadd_mm_rss_vec(mm, rss);\n\tarch_leave_lazy_mmu_mode();\n\n\t/* Do the actual TLB flush before dropping ptl */\n\tif (force_flush)\n\t\ttlb_flush_mmu_tlbonly(tlb);\n\tpte_unmap_unlock(start_pte, ptl);\n\n\t/*\n\t * If we forced a TLB flush (either due to running out of\n\t * batch buffers or because we needed to flush dirty TLB\n\t * entries before releasing the ptl), free the batched\n\t * memory too. Restart if we didn't do everything.\n\t */\n\tif (force_flush) {\n\t\tforce_flush = 0;\n\t\ttlb_flush_mmu(tlb);\n\t}\n\n\tif (addr != end) {\n\t\tcond_resched();\n\t\tgoto again;\n\t}\n\n\treturn addr;\n}\n\nstatic inline unsigned long zap_pmd_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, pud_t *pud,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {\n\t\t\tif (next - addr != HPAGE_PMD_SIZE)\n\t\t\t\t__split_huge_pmd(vma, pmd, addr, false, NULL);\n\t\t\telse if (zap_huge_pmd(tlb, vma, pmd, addr))\n\t\t\t\tgoto next;\n\t\t\t/* fall through */\n\t\t}\n\t\t/*\n\t\t * Here there can be other concurrent MADV_DONTNEED or\n\t\t * trans huge page faults running, and if the pmd is\n\t\t * none or trans huge it can change under us. This is\n\t\t * because MADV_DONTNEED holds the mmap_lock in read\n\t\t * mode.\n\t\t */\n\t\tif (pmd_none_or_trans_huge_or_clear_bad(pmd))\n\t\t\tgoto next;\n\t\tnext = zap_pte_range(tlb, vma, pmd, addr, next, details);\nnext:\n\t\tcond_resched();\n\t} while (pmd++, addr = next, addr != end);\n\n\treturn addr;\n}\n\nstatic inline unsigned long zap_pud_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, p4d_t *p4d,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_trans_huge(*pud) || pud_devmap(*pud)) {\n\t\t\tif (next - addr != HPAGE_PUD_SIZE) {\n\t\t\t\tmmap_assert_locked(tlb->mm);\n\t\t\t\tsplit_huge_pud(vma, pud, addr);\n\t\t\t} else if (zap_huge_pud(tlb, vma, pud, addr))\n\t\t\t\tgoto next;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pud_none_or_clear_bad(pud))\n\t\t\tcontinue;\n\t\tnext = zap_pmd_range(tlb, vma, pud, addr, next, details);\nnext:\n\t\tcond_resched();\n\t} while (pud++, addr = next, addr != end);\n\n\treturn addr;\n}\n\nstatic inline unsigned long zap_p4d_range(struct mmu_gather *tlb,\n\t\t\t\tstruct vm_area_struct *vma, pgd_t *pgd,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tstruct zap_details *details)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d))\n\t\t\tcontinue;\n\t\tnext = zap_pud_range(tlb, vma, p4d, addr, next, details);\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn addr;\n}\n\nvoid unmap_page_range(struct mmu_gather *tlb,\n\t\t\t     struct vm_area_struct *vma,\n\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t     struct zap_details *details)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\n\tBUG_ON(addr >= end);\n\ttlb_start_vma(tlb, vma);\n\tpgd = pgd_offset(vma->vm_mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\tnext = zap_p4d_range(tlb, vma, pgd, addr, next, details);\n\t} while (pgd++, addr = next, addr != end);\n\ttlb_end_vma(tlb, vma);\n}\n\n\nstatic void unmap_single_vma(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, unsigned long start_addr,\n\t\tunsigned long end_addr,\n\t\tstruct zap_details *details)\n{\n\tunsigned long start = max(vma->vm_start, start_addr);\n\tunsigned long end;\n\n\tif (start >= vma->vm_end)\n\t\treturn;\n\tend = min(vma->vm_end, end_addr);\n\tif (end <= vma->vm_start)\n\t\treturn;\n\n\tif (vma->vm_file)\n\t\tuprobe_munmap(vma, start, end);\n\n\tif (unlikely(vma->vm_flags & VM_PFNMAP))\n\t\tuntrack_pfn(vma, 0, 0);\n\n\tif (start != end) {\n\t\tif (unlikely(is_vm_hugetlb_page(vma))) {\n\t\t\t/*\n\t\t\t * It is undesirable to test vma->vm_file as it\n\t\t\t * should be non-null for valid hugetlb area.\n\t\t\t * However, vm_file will be NULL in the error\n\t\t\t * cleanup path of mmap_region. When\n\t\t\t * hugetlbfs ->mmap method fails,\n\t\t\t * mmap_region() nullifies vma->vm_file\n\t\t\t * before calling this function to clean up.\n\t\t\t * Since no pte has actually been setup, it is\n\t\t\t * safe to do nothing in this case.\n\t\t\t */\n\t\t\tif (vma->vm_file) {\n\t\t\t\ti_mmap_lock_write(vma->vm_file->f_mapping);\n\t\t\t\t__unmap_hugepage_range_final(tlb, vma, start, end, NULL);\n\t\t\t\ti_mmap_unlock_write(vma->vm_file->f_mapping);\n\t\t\t}\n\t\t} else\n\t\t\tunmap_page_range(tlb, vma, start, end, details);\n\t}\n}\n\n/**\n * unmap_vmas - unmap a range of memory covered by a list of vma's\n * @tlb: address of the caller's struct mmu_gather\n * @vma: the starting vma\n * @start_addr: virtual address at which to start unmapping\n * @end_addr: virtual address at which to end unmapping\n *\n * Unmap all pages in the vma list.\n *\n * Only addresses between `start' and `end' will be unmapped.\n *\n * The VMA list must be sorted in ascending virtual address order.\n *\n * unmap_vmas() assumes that the caller will flush the whole unmapped address\n * range after unmap_vmas() returns.  So the only responsibility here is to\n * ensure that any thus-far unmapped pages are flushed before unmap_vmas()\n * drops the lock and schedules.\n */\nvoid unmap_vmas(struct mmu_gather *tlb,\n\t\tstruct vm_area_struct *vma, unsigned long start_addr,\n\t\tunsigned long end_addr)\n{\n\tstruct mmu_notifier_range range;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_UNMAP, 0, vma, vma->vm_mm,\n\t\t\t\tstart_addr, end_addr);\n\tmmu_notifier_invalidate_range_start(&range);\n\tfor ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)\n\t\tunmap_single_vma(tlb, vma, start_addr, end_addr, NULL);\n\tmmu_notifier_invalidate_range_end(&range);\n}\n\n/**\n * zap_page_range - remove user pages in a given range\n * @vma: vm_area_struct holding the applicable pages\n * @start: starting address of pages to zap\n * @size: number of bytes to zap\n *\n * Caller must protect the VMA list\n */\nvoid zap_page_range(struct vm_area_struct *vma, unsigned long start,\n\t\tunsigned long size)\n{\n\tstruct mmu_notifier_range range;\n\tstruct mmu_gather tlb;\n\n\tlru_add_drain();\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\tstart, start + size);\n\ttlb_gather_mmu(&tlb, vma->vm_mm, start, range.end);\n\tupdate_hiwater_rss(vma->vm_mm);\n\tmmu_notifier_invalidate_range_start(&range);\n\tfor ( ; vma && vma->vm_start < range.end; vma = vma->vm_next)\n\t\tunmap_single_vma(&tlb, vma, start, range.end, NULL);\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_finish_mmu(&tlb, start, range.end);\n}\n\n/**\n * zap_page_range_single - remove user pages in a given range\n * @vma: vm_area_struct holding the applicable pages\n * @address: starting address of pages to zap\n * @size: number of bytes to zap\n * @details: details of shared cache invalidation\n *\n * The range must fit into one VMA.\n */\nstatic void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size, struct zap_details *details)\n{\n\tstruct mmu_notifier_range range;\n\tstruct mmu_gather tlb;\n\n\tlru_add_drain();\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress, address + size);\n\ttlb_gather_mmu(&tlb, vma->vm_mm, address, range.end);\n\tupdate_hiwater_rss(vma->vm_mm);\n\tmmu_notifier_invalidate_range_start(&range);\n\tunmap_single_vma(&tlb, vma, address, range.end, details);\n\tmmu_notifier_invalidate_range_end(&range);\n\ttlb_finish_mmu(&tlb, address, range.end);\n}\n\n/**\n * zap_vma_ptes - remove ptes mapping the vma\n * @vma: vm_area_struct holding ptes to be zapped\n * @address: starting address of pages to zap\n * @size: number of bytes to zap\n *\n * This function only unmaps ptes assigned to VM_PFNMAP vmas.\n *\n * The entire address range must be fully contained within the vma.\n *\n */\nvoid zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size)\n{\n\tif (address < vma->vm_start || address + size > vma->vm_end ||\n\t    \t\t!(vma->vm_flags & VM_PFNMAP))\n\t\treturn;\n\n\tzap_page_range_single(vma, address, size, NULL);\n}\nEXPORT_SYMBOL_GPL(zap_vma_ptes);\n\nstatic pmd_t *walk_to_pmd(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn NULL;\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (!pud)\n\t\treturn NULL;\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\treturn pmd;\n}\n\npte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\tspinlock_t **ptl)\n{\n\tpmd_t *pmd = walk_to_pmd(mm, addr);\n\n\tif (!pmd)\n\t\treturn NULL;\n\treturn pte_alloc_map_lock(mm, pmd, addr, ptl);\n}\n\nstatic int validate_page_before_insert(struct page *page)\n{\n\tif (PageAnon(page) || PageSlab(page) || page_has_type(page))\n\t\treturn -EINVAL;\n\tflush_dcache_page(page);\n\treturn 0;\n}\n\nstatic int insert_page_into_pte_locked(struct mm_struct *mm, pte_t *pte,\n\t\t\tunsigned long addr, struct page *page, pgprot_t prot)\n{\n\tif (!pte_none(*pte))\n\t\treturn -EBUSY;\n\t/* Ok, finally just insert the thing.. */\n\tget_page(page);\n\tinc_mm_counter_fast(mm, mm_counter_file(page));\n\tpage_add_file_rmap(page, false);\n\tset_pte_at(mm, addr, pte, mk_pte(page, prot));\n\treturn 0;\n}\n\n/*\n * This is the old fallback for page remapping.\n *\n * For historical reasons, it only allows reserved pages. Only\n * old drivers should use this, and they needed to mark their\n * pages reserved for the old functions anyway.\n */\nstatic int insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page *page, pgprot_t prot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint retval;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tretval = validate_page_before_insert(page);\n\tif (retval)\n\t\tgoto out;\n\tretval = -ENOMEM;\n\tpte = get_locked_pte(mm, addr, &ptl);\n\tif (!pte)\n\t\tgoto out;\n\tretval = insert_page_into_pte_locked(mm, pte, addr, page, prot);\n\tpte_unmap_unlock(pte, ptl);\nout:\n\treturn retval;\n}\n\n#ifdef pte_index\nstatic int insert_page_in_batch_locked(struct mm_struct *mm, pte_t *pte,\n\t\t\tunsigned long addr, struct page *page, pgprot_t prot)\n{\n\tint err;\n\n\tif (!page_count(page))\n\t\treturn -EINVAL;\n\terr = validate_page_before_insert(page);\n\tif (err)\n\t\treturn err;\n\treturn insert_page_into_pte_locked(mm, pte, addr, page, prot);\n}\n\n/* insert_pages() amortizes the cost of spinlock operations\n * when inserting pages in a loop. Arch *must* define pte_index.\n */\nstatic int insert_pages(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page **pages, unsigned long *num, pgprot_t prot)\n{\n\tpmd_t *pmd = NULL;\n\tpte_t *start_pte, *pte;\n\tspinlock_t *pte_lock;\n\tstruct mm_struct *const mm = vma->vm_mm;\n\tunsigned long curr_page_idx = 0;\n\tunsigned long remaining_pages_total = *num;\n\tunsigned long pages_to_write_in_pmd;\n\tint ret;\nmore:\n\tret = -EFAULT;\n\tpmd = walk_to_pmd(mm, addr);\n\tif (!pmd)\n\t\tgoto out;\n\n\tpages_to_write_in_pmd = min_t(unsigned long,\n\t\tremaining_pages_total, PTRS_PER_PTE - pte_index(addr));\n\n\t/* Allocate the PTE if necessary; takes PMD lock once only. */\n\tret = -ENOMEM;\n\tif (pte_alloc(mm, pmd))\n\t\tgoto out;\n\n\twhile (pages_to_write_in_pmd) {\n\t\tint pte_idx = 0;\n\t\tconst int batch_size = min_t(int, pages_to_write_in_pmd, 8);\n\n\t\tstart_pte = pte_offset_map_lock(mm, pmd, addr, &pte_lock);\n\t\tfor (pte = start_pte; pte_idx < batch_size; ++pte, ++pte_idx) {\n\t\t\tint err = insert_page_in_batch_locked(mm, pte,\n\t\t\t\taddr, pages[curr_page_idx], prot);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tpte_unmap_unlock(start_pte, pte_lock);\n\t\t\t\tret = err;\n\t\t\t\tremaining_pages_total -= pte_idx;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\taddr += PAGE_SIZE;\n\t\t\t++curr_page_idx;\n\t\t}\n\t\tpte_unmap_unlock(start_pte, pte_lock);\n\t\tpages_to_write_in_pmd -= batch_size;\n\t\tremaining_pages_total -= batch_size;\n\t}\n\tif (remaining_pages_total)\n\t\tgoto more;\n\tret = 0;\nout:\n\t*num = remaining_pages_total;\n\treturn ret;\n}\n#endif  /* ifdef pte_index */\n\n/**\n * vm_insert_pages - insert multiple pages into user vma, batching the pmd lock.\n * @vma: user vma to map to\n * @addr: target start user address of these pages\n * @pages: source kernel pages\n * @num: in: number of pages to map. out: number of pages that were *not*\n * mapped. (0 means all pages were successfully mapped).\n *\n * Preferred over vm_insert_page() when inserting multiple pages.\n *\n * In case of error, we may have mapped a subset of the provided\n * pages. It is the caller's responsibility to account for this case.\n *\n * The same restrictions apply as in vm_insert_page().\n */\nint vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page **pages, unsigned long *num)\n{\n#ifdef pte_index\n\tconst unsigned long end_addr = addr + (*num * PAGE_SIZE) - 1;\n\n\tif (addr < vma->vm_start || end_addr >= vma->vm_end)\n\t\treturn -EFAULT;\n\tif (!(vma->vm_flags & VM_MIXEDMAP)) {\n\t\tBUG_ON(mmap_read_trylock(vma->vm_mm));\n\t\tBUG_ON(vma->vm_flags & VM_PFNMAP);\n\t\tvma->vm_flags |= VM_MIXEDMAP;\n\t}\n\t/* Defer page refcount checking till we're about to map that page. */\n\treturn insert_pages(vma, addr, pages, num, vma->vm_page_prot);\n#else\n\tunsigned long idx = 0, pgcount = *num;\n\tint err = -EINVAL;\n\n\tfor (; idx < pgcount; ++idx) {\n\t\terr = vm_insert_page(vma, addr + (PAGE_SIZE * idx), pages[idx]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\t*num = pgcount - idx;\n\treturn err;\n#endif  /* ifdef pte_index */\n}\nEXPORT_SYMBOL(vm_insert_pages);\n\n/**\n * vm_insert_page - insert single page into user vma\n * @vma: user vma to map to\n * @addr: target user address of this page\n * @page: source kernel page\n *\n * This allows drivers to insert individual pages they've allocated\n * into a user vma.\n *\n * The page has to be a nice clean _individual_ kernel allocation.\n * If you allocate a compound page, you need to have marked it as\n * such (__GFP_COMP), or manually just split the page up yourself\n * (see split_page()).\n *\n * NOTE! Traditionally this was done with \"remap_pfn_range()\" which\n * took an arbitrary page protection parameter. This doesn't allow\n * that. Your vma protection will have to be set up correctly, which\n * means that if you want a shared writable mapping, you'd better\n * ask for a shared writable mapping!\n *\n * The page does not need to be reserved.\n *\n * Usually this function is called from f_op->mmap() handler\n * under mm->mmap_lock write-lock, so it can change vma->vm_flags.\n * Caller must set VM_MIXEDMAP on vma if it wants to call this\n * function from other places, for example from page-fault handler.\n *\n * Return: %0 on success, negative error code otherwise.\n */\nint vm_insert_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page *page)\n{\n\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\treturn -EFAULT;\n\tif (!page_count(page))\n\t\treturn -EINVAL;\n\tif (!(vma->vm_flags & VM_MIXEDMAP)) {\n\t\tBUG_ON(mmap_read_trylock(vma->vm_mm));\n\t\tBUG_ON(vma->vm_flags & VM_PFNMAP);\n\t\tvma->vm_flags |= VM_MIXEDMAP;\n\t}\n\treturn insert_page(vma, addr, page, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vm_insert_page);\n\n/*\n * __vm_map_pages - maps range of kernel pages into user vma\n * @vma: user vma to map to\n * @pages: pointer to array of source kernel pages\n * @num: number of pages in page array\n * @offset: user's requested vm_pgoff\n *\n * This allows drivers to map range of kernel pages into a user vma.\n *\n * Return: 0 on success and error code otherwise.\n */\nstatic int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num, unsigned long offset)\n{\n\tunsigned long count = vma_pages(vma);\n\tunsigned long uaddr = vma->vm_start;\n\tint ret, i;\n\n\t/* Fail if the user requested offset is beyond the end of the object */\n\tif (offset >= num)\n\t\treturn -ENXIO;\n\n\t/* Fail if the user requested size exceeds available object size */\n\tif (count > num - offset)\n\t\treturn -ENXIO;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = vm_insert_page(vma, uaddr, pages[offset + i]);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tuaddr += PAGE_SIZE;\n\t}\n\n\treturn 0;\n}\n\n/**\n * vm_map_pages - maps range of kernel pages starts with non zero offset\n * @vma: user vma to map to\n * @pages: pointer to array of source kernel pages\n * @num: number of pages in page array\n *\n * Maps an object consisting of @num pages, catering for the user's\n * requested vm_pgoff\n *\n * If we fail to insert any page into the vma, the function will return\n * immediately leaving any previously inserted pages present.  Callers\n * from the mmap handler may immediately return the error as their caller\n * will destroy the vma, removing any successfully inserted pages. Other\n * callers should make their own arrangements for calling unmap_region().\n *\n * Context: Process context. Called by mmap handlers.\n * Return: 0 on success and error code otherwise.\n */\nint vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num)\n{\n\treturn __vm_map_pages(vma, pages, num, vma->vm_pgoff);\n}\nEXPORT_SYMBOL(vm_map_pages);\n\n/**\n * vm_map_pages_zero - map range of kernel pages starts with zero offset\n * @vma: user vma to map to\n * @pages: pointer to array of source kernel pages\n * @num: number of pages in page array\n *\n * Similar to vm_map_pages(), except that it explicitly sets the offset\n * to 0. This function is intended for the drivers that did not consider\n * vm_pgoff.\n *\n * Context: Process context. Called by mmap handlers.\n * Return: 0 on success and error code otherwise.\n */\nint vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num)\n{\n\treturn __vm_map_pages(vma, pages, num, 0);\n}\nEXPORT_SYMBOL(vm_map_pages_zero);\n\nstatic vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn, pgprot_t prot, bool mkwrite)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *pte, entry;\n\tspinlock_t *ptl;\n\n\tpte = get_locked_pte(mm, addr, &ptl);\n\tif (!pte)\n\t\treturn VM_FAULT_OOM;\n\tif (!pte_none(*pte)) {\n\t\tif (mkwrite) {\n\t\t\t/*\n\t\t\t * For read faults on private mappings the PFN passed\n\t\t\t * in may not match the PFN we have mapped if the\n\t\t\t * mapped PFN is a writeable COW page.  In the mkwrite\n\t\t\t * case we are creating a writable PTE for a shared\n\t\t\t * mapping and we expect the PFNs to match. If they\n\t\t\t * don't match, we are likely racing with block\n\t\t\t * allocation and mapping invalidation so just skip the\n\t\t\t * update.\n\t\t\t */\n\t\t\tif (pte_pfn(*pte) != pfn_t_to_pfn(pfn)) {\n\t\t\t\tWARN_ON_ONCE(!is_zero_pfn(pte_pfn(*pte)));\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tentry = pte_mkyoung(*pte);\n\t\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\t\tif (ptep_set_access_flags(vma, addr, pte, entry, 1))\n\t\t\t\tupdate_mmu_cache(vma, addr, pte);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\n\t/* Ok, finally just insert the thing.. */\n\tif (pfn_t_devmap(pfn))\n\t\tentry = pte_mkdevmap(pfn_t_pte(pfn, prot));\n\telse\n\t\tentry = pte_mkspecial(pfn_t_pte(pfn, prot));\n\n\tif (mkwrite) {\n\t\tentry = pte_mkyoung(entry);\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t}\n\n\tset_pte_at(mm, addr, pte, entry);\n\tupdate_mmu_cache(vma, addr, pte); /* XXX: why not for insert_page? */\n\nout_unlock:\n\tpte_unmap_unlock(pte, ptl);\n\treturn VM_FAULT_NOPAGE;\n}\n\n/**\n * vmf_insert_pfn_prot - insert single pfn into user vma with specified pgprot\n * @vma: user vma to map to\n * @addr: target user address of this page\n * @pfn: source kernel pfn\n * @pgprot: pgprot flags for the inserted page\n *\n * This is exactly like vmf_insert_pfn(), except that it allows drivers\n * to override pgprot on a per-page basis.\n *\n * This only makes sense for IO mappings, and it makes no sense for\n * COW mappings.  In general, using multiple vmas is preferable;\n * vmf_insert_pfn_prot should only be used if using multiple VMAs is\n * impractical.\n *\n * See vmf_insert_mixed_prot() for a discussion of the implication of using\n * a value of @pgprot different from that of @vma->vm_page_prot.\n *\n * Context: Process context.  May allocate using %GFP_KERNEL.\n * Return: vm_fault_t value.\n */\nvm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn, pgprot_t pgprot)\n{\n\t/*\n\t * Technically, architectures with pte_special can avoid all these\n\t * restrictions (same for remap_pfn_range).  However we would like\n\t * consistency in testing and feature parity among all, so we should\n\t * try to keep these invariants in place for everybody.\n\t */\n\tBUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));\n\tBUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==\n\t\t\t\t\t\t(VM_PFNMAP|VM_MIXEDMAP));\n\tBUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));\n\tBUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));\n\n\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tif (!pfn_modify_allowed(pfn, pgprot))\n\t\treturn VM_FAULT_SIGBUS;\n\n\ttrack_pfn_insert(vma, &pgprot, __pfn_to_pfn_t(pfn, PFN_DEV));\n\n\treturn insert_pfn(vma, addr, __pfn_to_pfn_t(pfn, PFN_DEV), pgprot,\n\t\t\tfalse);\n}\nEXPORT_SYMBOL(vmf_insert_pfn_prot);\n\n/**\n * vmf_insert_pfn - insert single pfn into user vma\n * @vma: user vma to map to\n * @addr: target user address of this page\n * @pfn: source kernel pfn\n *\n * Similar to vm_insert_page, this allows drivers to insert individual pages\n * they've allocated into a user vma. Same comments apply.\n *\n * This function should only be called from a vm_ops->fault handler, and\n * in that case the handler should return the result of this function.\n *\n * vma cannot be a COW mapping.\n *\n * As this is called only for pages that do not currently exist, we\n * do not need to flush old virtual caches or the TLB.\n *\n * Context: Process context.  May allocate using %GFP_KERNEL.\n * Return: vm_fault_t value.\n */\nvm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn)\n{\n\treturn vmf_insert_pfn_prot(vma, addr, pfn, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vmf_insert_pfn);\n\nstatic bool vm_mixed_ok(struct vm_area_struct *vma, pfn_t pfn)\n{\n\t/* these checks mirror the abort conditions in vm_normal_page */\n\tif (vma->vm_flags & VM_MIXEDMAP)\n\t\treturn true;\n\tif (pfn_t_devmap(pfn))\n\t\treturn true;\n\tif (pfn_t_special(pfn))\n\t\treturn true;\n\tif (is_zero_pfn(pfn_t_to_pfn(pfn)))\n\t\treturn true;\n\treturn false;\n}\n\nstatic vm_fault_t __vm_insert_mixed(struct vm_area_struct *vma,\n\t\tunsigned long addr, pfn_t pfn, pgprot_t pgprot,\n\t\tbool mkwrite)\n{\n\tint err;\n\n\tBUG_ON(!vm_mixed_ok(vma, pfn));\n\n\tif (addr < vma->vm_start || addr >= vma->vm_end)\n\t\treturn VM_FAULT_SIGBUS;\n\n\ttrack_pfn_insert(vma, &pgprot, pfn);\n\n\tif (!pfn_modify_allowed(pfn_t_to_pfn(pfn), pgprot))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t/*\n\t * If we don't have pte special, then we have to use the pfn_valid()\n\t * based VM_MIXEDMAP scheme (see vm_normal_page), and thus we *must*\n\t * refcount the page if pfn_valid is true (hence insert_page rather\n\t * than insert_pfn).  If a zero_pfn were inserted into a VM_MIXEDMAP\n\t * without pte special, it would there be refcounted as a normal page.\n\t */\n\tif (!IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL) &&\n\t    !pfn_t_devmap(pfn) && pfn_t_valid(pfn)) {\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * At this point we are committed to insert_page()\n\t\t * regardless of whether the caller specified flags that\n\t\t * result in pfn_t_has_page() == false.\n\t\t */\n\t\tpage = pfn_to_page(pfn_t_to_pfn(pfn));\n\t\terr = insert_page(vma, addr, page, pgprot);\n\t} else {\n\t\treturn insert_pfn(vma, addr, pfn, pgprot, mkwrite);\n\t}\n\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\tif (err < 0 && err != -EBUSY)\n\t\treturn VM_FAULT_SIGBUS;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\n/**\n * vmf_insert_mixed_prot - insert single pfn into user vma with specified pgprot\n * @vma: user vma to map to\n * @addr: target user address of this page\n * @pfn: source kernel pfn\n * @pgprot: pgprot flags for the inserted page\n *\n * This is exactly like vmf_insert_mixed(), except that it allows drivers\n * to override pgprot on a per-page basis.\n *\n * Typically this function should be used by drivers to set caching- and\n * encryption bits different than those of @vma->vm_page_prot, because\n * the caching- or encryption mode may not be known at mmap() time.\n * This is ok as long as @vma->vm_page_prot is not used by the core vm\n * to set caching and encryption bits for those vmas (except for COW pages).\n * This is ensured by core vm only modifying these page table entries using\n * functions that don't touch caching- or encryption bits, using pte_modify()\n * if needed. (See for example mprotect()).\n * Also when new page-table entries are created, this is only done using the\n * fault() callback, and never using the value of vma->vm_page_prot,\n * except for page-table entries that point to anonymous pages as the result\n * of COW.\n *\n * Context: Process context.  May allocate using %GFP_KERNEL.\n * Return: vm_fault_t value.\n */\nvm_fault_t vmf_insert_mixed_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t pfn_t pfn, pgprot_t pgprot)\n{\n\treturn __vm_insert_mixed(vma, addr, pfn, pgprot, false);\n}\nEXPORT_SYMBOL(vmf_insert_mixed_prot);\n\nvm_fault_t vmf_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\tpfn_t pfn)\n{\n\treturn __vm_insert_mixed(vma, addr, pfn, vma->vm_page_prot, false);\n}\nEXPORT_SYMBOL(vmf_insert_mixed);\n\n/*\n *  If the insertion of PTE failed because someone else already added a\n *  different entry in the mean time, we treat that as success as we assume\n *  the same entry was actually inserted.\n */\nvm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,\n\t\tunsigned long addr, pfn_t pfn)\n{\n\treturn __vm_insert_mixed(vma, addr, pfn, vma->vm_page_prot, true);\n}\nEXPORT_SYMBOL(vmf_insert_mixed_mkwrite);\n\n/*\n * maps a range of physical memory into the requested pages. the old\n * mappings are removed. any references to nonexistent pages results\n * in null mappings (currently treated as \"copy-on-access\")\n */\nstatic int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\tint err = 0;\n\n\tpte = pte_alloc_map_lock(mm, pmd, addr, &ptl);\n\tif (!pte)\n\t\treturn -ENOMEM;\n\tarch_enter_lazy_mmu_mode();\n\tdo {\n\t\tBUG_ON(!pte_none(*pte));\n\t\tif (!pfn_modify_allowed(pfn, prot)) {\n\t\t\terr = -EACCES;\n\t\t\tbreak;\n\t\t}\n\t\tset_pte_at(mm, addr, pte, pte_mkspecial(pfn_pte(pfn, prot)));\n\t\tpfn++;\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\tarch_leave_lazy_mmu_mode();\n\tpte_unmap_unlock(pte - 1, ptl);\n\treturn err;\n}\n\nstatic inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint err;\n\n\tpfn -= addr >> PAGE_SHIFT;\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn -ENOMEM;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\terr = remap_pte_range(mm, pmd, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (pmd++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint err;\n\n\tpfn -= addr >> PAGE_SHIFT;\n\tpud = pud_alloc(mm, p4d, addr);\n\tif (!pud)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\terr = remap_pmd_range(mm, pud, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (pud++, addr = next, addr != end);\n\treturn 0;\n}\n\nstatic inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned long pfn, pgprot_t prot)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint err;\n\n\tpfn -= addr >> PAGE_SHIFT;\n\tp4d = p4d_alloc(mm, pgd, addr);\n\tif (!p4d)\n\t\treturn -ENOMEM;\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\terr = remap_pud_range(mm, p4d, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\treturn err;\n\t} while (p4d++, addr = next, addr != end);\n\treturn 0;\n}\n\n/**\n * remap_pfn_range - remap kernel memory to userspace\n * @vma: user vma to map to\n * @addr: target page aligned user address to start at\n * @pfn: page frame number of kernel physical memory address\n * @size: size of mapping area\n * @prot: page protection flags for this mapping\n *\n * Note: this is only safe if the mm semaphore is held when called.\n *\n * Return: %0 on success, negative error code otherwise.\n */\nint remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,\n\t\t    unsigned long pfn, unsigned long size, pgprot_t prot)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\tunsigned long end = addr + PAGE_ALIGN(size);\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long remap_pfn = pfn;\n\tint err;\n\n\tif (WARN_ON_ONCE(!PAGE_ALIGNED(addr)))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Physically remapped pages are special. Tell the\n\t * rest of the world about it:\n\t *   VM_IO tells people not to look at these pages\n\t *\t(accesses can have side effects).\n\t *   VM_PFNMAP tells the core MM that the base pages are just\n\t *\traw PFN mappings, and do not have a \"struct page\" associated\n\t *\twith them.\n\t *   VM_DONTEXPAND\n\t *      Disable vma merging and expanding with mremap().\n\t *   VM_DONTDUMP\n\t *      Omit vma from core dump, even when VM_IO turned off.\n\t *\n\t * There's a horrible special case to handle copy-on-write\n\t * behaviour that some programs depend on. We mark the \"original\"\n\t * un-COW'ed pages by matching them up with \"vma->vm_pgoff\".\n\t * See vm_normal_page() for details.\n\t */\n\tif (is_cow_mapping(vma->vm_flags)) {\n\t\tif (addr != vma->vm_start || end != vma->vm_end)\n\t\t\treturn -EINVAL;\n\t\tvma->vm_pgoff = pfn;\n\t}\n\n\terr = track_pfn_remap(vma, &prot, remap_pfn, addr, PAGE_ALIGN(size));\n\tif (err)\n\t\treturn -EINVAL;\n\n\tvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;\n\n\tBUG_ON(addr >= end);\n\tpfn -= addr >> PAGE_SHIFT;\n\tpgd = pgd_offset(mm, addr);\n\tflush_cache_range(vma, addr, end);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\terr = remap_p4d_range(mm, pgd, addr, next,\n\t\t\t\tpfn + (addr >> PAGE_SHIFT), prot);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, addr = next, addr != end);\n\n\tif (err)\n\t\tuntrack_pfn(vma, remap_pfn, PAGE_ALIGN(size));\n\n\treturn err;\n}\nEXPORT_SYMBOL(remap_pfn_range);\n\n/**\n * vm_iomap_memory - remap memory to userspace\n * @vma: user vma to map to\n * @start: start of the physical memory to be mapped\n * @len: size of area\n *\n * This is a simplified io_remap_pfn_range() for common driver use. The\n * driver just needs to give us the physical memory range to be mapped,\n * we'll figure out the rest from the vma information.\n *\n * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get\n * whatever write-combining details or similar.\n *\n * Return: %0 on success, negative error code otherwise.\n */\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)\n{\n\tunsigned long vm_len, pfn, pages;\n\n\t/* Check that the physical memory area passed in looks valid */\n\tif (start + len < start)\n\t\treturn -EINVAL;\n\t/*\n\t * You *really* shouldn't map things that aren't page-aligned,\n\t * but we've historically allowed it because IO memory might\n\t * just have smaller alignment.\n\t */\n\tlen += start & ~PAGE_MASK;\n\tpfn = start >> PAGE_SHIFT;\n\tpages = (len + ~PAGE_MASK) >> PAGE_SHIFT;\n\tif (pfn + pages < pfn)\n\t\treturn -EINVAL;\n\n\t/* We start the mapping 'vm_pgoff' pages into the area */\n\tif (vma->vm_pgoff > pages)\n\t\treturn -EINVAL;\n\tpfn += vma->vm_pgoff;\n\tpages -= vma->vm_pgoff;\n\n\t/* Can we fit all of the mapping? */\n\tvm_len = vma->vm_end - vma->vm_start;\n\tif (vm_len >> PAGE_SHIFT > pages)\n\t\treturn -EINVAL;\n\n\t/* Ok, let it rip */\n\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);\n}\nEXPORT_SYMBOL(vm_iomap_memory);\n\nstatic int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpte_t *pte;\n\tint err = 0;\n\tspinlock_t *ptl;\n\n\tif (create) {\n\t\tpte = (mm == &init_mm) ?\n\t\t\tpte_alloc_kernel_track(pmd, addr, mask) :\n\t\t\tpte_alloc_map_lock(mm, pmd, addr, &ptl);\n\t\tif (!pte)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tpte = (mm == &init_mm) ?\n\t\t\tpte_offset_kernel(pmd, addr) :\n\t\t\tpte_offset_map_lock(mm, pmd, addr, &ptl);\n\t}\n\n\tBUG_ON(pmd_huge(*pmd));\n\n\tarch_enter_lazy_mmu_mode();\n\n\tif (fn) {\n\t\tdo {\n\t\t\tif (create || !pte_none(*pte)) {\n\t\t\t\terr = fn(pte++, addr, data);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (addr += PAGE_SIZE, addr != end);\n\t}\n\t*mask |= PGTBL_PTE_MODIFIED;\n\n\tarch_leave_lazy_mmu_mode();\n\n\tif (mm != &init_mm)\n\t\tpte_unmap_unlock(pte-1, ptl);\n\treturn err;\n}\n\nstatic int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint err = 0;\n\n\tBUG_ON(pud_huge(*pud));\n\n\tif (create) {\n\t\tpmd = pmd_alloc_track(mm, pud, addr, mask);\n\t\tif (!pmd)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tpmd = pmd_offset(pud, addr);\n\t}\n\tdo {\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (create || !pmd_none_or_clear_bad(pmd)) {\n\t\t\terr = apply_to_pte_range(mm, pmd, addr, next, fn, data,\n\t\t\t\t\t\t create, mask);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t} while (pmd++, addr = next, addr != end);\n\treturn err;\n}\n\nstatic int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint err = 0;\n\n\tif (create) {\n\t\tpud = pud_alloc_track(mm, p4d, addr, mask);\n\t\tif (!pud)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tpud = pud_offset(p4d, addr);\n\t}\n\tdo {\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (create || !pud_none_or_clear_bad(pud)) {\n\t\t\terr = apply_to_pmd_range(mm, pud, addr, next, fn, data,\n\t\t\t\t\t\t create, mask);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t} while (pud++, addr = next, addr != end);\n\treturn err;\n}\n\nstatic int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t     unsigned long addr, unsigned long end,\n\t\t\t\t     pte_fn_t fn, void *data, bool create,\n\t\t\t\t     pgtbl_mod_mask *mask)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint err = 0;\n\n\tif (create) {\n\t\tp4d = p4d_alloc_track(mm, pgd, addr, mask);\n\t\tif (!p4d)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\tp4d = p4d_offset(pgd, addr);\n\t}\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (create || !p4d_none_or_clear_bad(p4d)) {\n\t\t\terr = apply_to_pud_range(mm, p4d, addr, next, fn, data,\n\t\t\t\t\t\t create, mask);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t} while (p4d++, addr = next, addr != end);\n\treturn err;\n}\n\nstatic int __apply_to_page_range(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t unsigned long size, pte_fn_t fn,\n\t\t\t\t void *data, bool create)\n{\n\tpgd_t *pgd;\n\tunsigned long start = addr, next;\n\tunsigned long end = addr + size;\n\tpgtbl_mod_mask mask = 0;\n\tint err = 0;\n\n\tif (WARN_ON(addr >= end))\n\t\treturn -EINVAL;\n\n\tpgd = pgd_offset(mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (!create && pgd_none_or_clear_bad(pgd))\n\t\t\tcontinue;\n\t\terr = apply_to_p4d_range(mm, pgd, addr, next, fn, data, create, &mask);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, addr = next, addr != end);\n\n\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)\n\t\tarch_sync_kernel_mappings(start, start + size);\n\n\treturn err;\n}\n\n/*\n * Scan a region of virtual memory, filling in page tables as necessary\n * and calling a provided function on each leaf page table.\n */\nint apply_to_page_range(struct mm_struct *mm, unsigned long addr,\n\t\t\tunsigned long size, pte_fn_t fn, void *data)\n{\n\treturn __apply_to_page_range(mm, addr, size, fn, data, true);\n}\nEXPORT_SYMBOL_GPL(apply_to_page_range);\n\n/*\n * Scan a region of virtual memory, calling a provided function on\n * each leaf page table where it exists.\n *\n * Unlike apply_to_page_range, this does _not_ fill in page tables\n * where they are absent.\n */\nint apply_to_existing_page_range(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t unsigned long size, pte_fn_t fn, void *data)\n{\n\treturn __apply_to_page_range(mm, addr, size, fn, data, false);\n}\nEXPORT_SYMBOL_GPL(apply_to_existing_page_range);\n\n/*\n * handle_pte_fault chooses page fault handler according to an entry which was\n * read non-atomically.  Before making any commitment, on those architectures\n * or configurations (e.g. i386 with PAE) which might give a mix of unmatched\n * parts, do_swap_page must check under lock before unmapping the pte and\n * proceeding (but do_wp_page is only called after already making such a check;\n * and do_anonymous_page can safely check later on).\n */\nstatic inline int pte_unmap_same(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\tpte_t *page_table, pte_t orig_pte)\n{\n\tint same = 1;\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPTION)\n\tif (sizeof(pte_t) > sizeof(unsigned long)) {\n\t\tspinlock_t *ptl = pte_lockptr(mm, pmd);\n\t\tspin_lock(ptl);\n\t\tsame = pte_same(*page_table, orig_pte);\n\t\tspin_unlock(ptl);\n\t}\n#endif\n\tpte_unmap(page_table);\n\treturn same;\n}\n\nstatic inline bool cow_user_page(struct page *dst, struct page *src,\n\t\t\t\t struct vm_fault *vmf)\n{\n\tbool ret;\n\tvoid *kaddr;\n\tvoid __user *uaddr;\n\tbool locked = false;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long addr = vmf->address;\n\n\tif (likely(src)) {\n\t\tcopy_user_highpage(dst, src, addr, vma);\n\t\treturn true;\n\t}\n\n\t/*\n\t * If the source page was a PFN mapping, we don't have\n\t * a \"struct page\" for it. We do a best-effort copy by\n\t * just copying from the original user address. If that\n\t * fails, we just zero-fill it. Live with it.\n\t */\n\tkaddr = kmap_atomic(dst);\n\tuaddr = (void __user *)(addr & PAGE_MASK);\n\n\t/*\n\t * On architectures with software \"accessed\" bits, we would\n\t * take a double page fault, so mark it accessed here.\n\t */\n\tif (arch_faults_on_old_pte() && !pte_young(vmf->orig_pte)) {\n\t\tpte_t entry;\n\n\t\tvmf->pte = pte_offset_map_lock(mm, vmf->pmd, addr, &vmf->ptl);\n\t\tlocked = true;\n\t\tif (!likely(pte_same(*vmf->pte, vmf->orig_pte))) {\n\t\t\t/*\n\t\t\t * Other thread has already handled the fault\n\t\t\t * and update local tlb only\n\t\t\t */\n\t\t\tupdate_mmu_tlb(vma, addr, vmf->pte);\n\t\t\tret = false;\n\t\t\tgoto pte_unlock;\n\t\t}\n\n\t\tentry = pte_mkyoung(vmf->orig_pte);\n\t\tif (ptep_set_access_flags(vma, addr, vmf->pte, entry, 0))\n\t\t\tupdate_mmu_cache(vma, addr, vmf->pte);\n\t}\n\n\t/*\n\t * This really shouldn't fail, because the page is there\n\t * in the page tables. But it might just be unreadable,\n\t * in which case we just give up and fill the result with\n\t * zeroes.\n\t */\n\tif (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE)) {\n\t\tif (locked)\n\t\t\tgoto warn;\n\n\t\t/* Re-validate under PTL if the page is still mapped */\n\t\tvmf->pte = pte_offset_map_lock(mm, vmf->pmd, addr, &vmf->ptl);\n\t\tlocked = true;\n\t\tif (!likely(pte_same(*vmf->pte, vmf->orig_pte))) {\n\t\t\t/* The PTE changed under us, update local tlb */\n\t\t\tupdate_mmu_tlb(vma, addr, vmf->pte);\n\t\t\tret = false;\n\t\t\tgoto pte_unlock;\n\t\t}\n\n\t\t/*\n\t\t * The same page can be mapped back since last copy attempt.\n\t\t * Try to copy again under PTL.\n\t\t */\n\t\tif (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE)) {\n\t\t\t/*\n\t\t\t * Give a warn in case there can be some obscure\n\t\t\t * use-case\n\t\t\t */\nwarn:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tclear_page(kaddr);\n\t\t}\n\t}\n\n\tret = true;\n\npte_unlock:\n\tif (locked)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tkunmap_atomic(kaddr);\n\tflush_dcache_page(dst);\n\n\treturn ret;\n}\n\nstatic gfp_t __get_fault_gfp_mask(struct vm_area_struct *vma)\n{\n\tstruct file *vm_file = vma->vm_file;\n\n\tif (vm_file)\n\t\treturn mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;\n\n\t/*\n\t * Special mappings (e.g. VDSO) do not have any file so fake\n\t * a default GFP_KERNEL for them.\n\t */\n\treturn GFP_KERNEL;\n}\n\n/*\n * Notify the address space that the page is about to become writable so that\n * it can prohibit this or wait for the page to get into an appropriate state.\n *\n * We do this without the lock held, so that it can sleep if it needs to.\n */\nstatic vm_fault_t do_page_mkwrite(struct vm_fault *vmf)\n{\n\tvm_fault_t ret;\n\tstruct page *page = vmf->page;\n\tunsigned int old_flags = vmf->flags;\n\n\tvmf->flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;\n\n\tif (vmf->vma->vm_file &&\n\t    IS_SWAPFILE(vmf->vma->vm_file->f_mapping->host))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tret = vmf->vma->vm_ops->page_mkwrite(vmf);\n\t/* Restore original flags so that caller is not surprised */\n\tvmf->flags = old_flags;\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))\n\t\treturn ret;\n\tif (unlikely(!(ret & VM_FAULT_LOCKED))) {\n\t\tlock_page(page);\n\t\tif (!page->mapping) {\n\t\t\tunlock_page(page);\n\t\t\treturn 0; /* retry */\n\t\t}\n\t\tret |= VM_FAULT_LOCKED;\n\t} else\n\t\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\treturn ret;\n}\n\n/*\n * Handle dirtying of a page in shared file mapping on a write fault.\n *\n * The function expects the page to be locked and unlocks it.\n */\nstatic vm_fault_t fault_dirty_shared_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct address_space *mapping;\n\tstruct page *page = vmf->page;\n\tbool dirtied;\n\tbool page_mkwrite = vma->vm_ops && vma->vm_ops->page_mkwrite;\n\n\tdirtied = set_page_dirty(page);\n\tVM_BUG_ON_PAGE(PageAnon(page), page);\n\t/*\n\t * Take a local copy of the address_space - page.mapping may be zeroed\n\t * by truncate after unlock_page().   The address_space itself remains\n\t * pinned by vma->vm_file's reference.  We rely on unlock_page()'s\n\t * release semantics to prevent the compiler from undoing this copying.\n\t */\n\tmapping = page_rmapping(page);\n\tunlock_page(page);\n\n\tif (!page_mkwrite)\n\t\tfile_update_time(vma->vm_file);\n\n\t/*\n\t * Throttle page dirtying rate down to writeback speed.\n\t *\n\t * mapping may be NULL here because some device drivers do not\n\t * set page.mapping but still dirty their pages\n\t *\n\t * Drop the mmap_lock before waiting on IO, if we can. The file\n\t * is pinning the mapping, as per above.\n\t */\n\tif ((dirtied || page_mkwrite) && mapping) {\n\t\tstruct file *fpin;\n\n\t\tfpin = maybe_unlock_mmap_for_io(vmf, NULL);\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\t\tif (fpin) {\n\t\t\tfput(fpin);\n\t\t\treturn VM_FAULT_RETRY;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Handle write page faults for pages that can be reused in the current vma\n *\n * This can happen either due to the mapping being with the VM_SHARED flag,\n * or due to us being the last reference standing to the page. In either\n * case, all we need to do here is to mark the page as writable and update\n * any related book-keeping.\n */\nstatic inline void wp_page_reuse(struct vm_fault *vmf)\n\t__releases(vmf->ptl)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = vmf->page;\n\tpte_t entry;\n\t/*\n\t * Clear the pages cpupid information as the existing\n\t * information potentially belongs to a now completely\n\t * unrelated process.\n\t */\n\tif (page)\n\t\tpage_cpupid_xchg_last(page, (1 << LAST_CPUPID_SHIFT) - 1);\n\n\tflush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));\n\tentry = pte_mkyoung(vmf->orig_pte);\n\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\tif (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))\n\t\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tcount_vm_event(PGREUSE);\n}\n\n/*\n * Handle the case of a page which we actually need to copy to a new page.\n *\n * Called with mmap_lock locked and the old page referenced, but\n * without the ptl held.\n *\n * High level logic flow:\n *\n * - Allocate a page, copy the content of the old page to the new one.\n * - Handle book keeping and accounting - cgroups, mmu-notifiers, etc.\n * - Take the PTL. If the pte changed, bail out and release the allocated page\n * - If the pte is still the way we remember it, update the page table and all\n *   relevant references. This includes dropping the reference the page-table\n *   held to the old page, as well as updating the rmap.\n * - In any case, unlock the PTL and drop the reference we took to the old page.\n */\nstatic vm_fault_t wp_page_copy(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct page *old_page = vmf->page;\n\tstruct page *new_page = NULL;\n\tpte_t entry;\n\tint page_copied = 0;\n\tstruct mmu_notifier_range range;\n\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\n\tif (is_zero_pfn(pte_pfn(vmf->orig_pte))) {\n\t\tnew_page = alloc_zeroed_user_highpage_movable(vma,\n\t\t\t\t\t\t\t      vmf->address);\n\t\tif (!new_page)\n\t\t\tgoto oom;\n\t} else {\n\t\tnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,\n\t\t\t\tvmf->address);\n\t\tif (!new_page)\n\t\t\tgoto oom;\n\n\t\tif (!cow_user_page(new_page, old_page, vmf)) {\n\t\t\t/*\n\t\t\t * COW failed, if the fault was solved by other,\n\t\t\t * it's fine. If not, userspace would re-fault on\n\t\t\t * the same address and we will handle the fault\n\t\t\t * from the second attempt.\n\t\t\t */\n\t\t\tput_page(new_page);\n\t\t\tif (old_page)\n\t\t\t\tput_page(old_page);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (mem_cgroup_charge(new_page, mm, GFP_KERNEL))\n\t\tgoto oom_free_new;\n\tcgroup_throttle_swaprate(new_page, GFP_KERNEL);\n\n\t__SetPageUptodate(new_page);\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, mm,\n\t\t\t\tvmf->address & PAGE_MASK,\n\t\t\t\t(vmf->address & PAGE_MASK) + PAGE_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\n\t/*\n\t * Re-check the pte - we dropped the lock\n\t */\n\tvmf->pte = pte_offset_map_lock(mm, vmf->pmd, vmf->address, &vmf->ptl);\n\tif (likely(pte_same(*vmf->pte, vmf->orig_pte))) {\n\t\tif (old_page) {\n\t\t\tif (!PageAnon(old_page)) {\n\t\t\t\tdec_mm_counter_fast(mm,\n\t\t\t\t\t\tmm_counter_file(old_page));\n\t\t\t\tinc_mm_counter_fast(mm, MM_ANONPAGES);\n\t\t\t}\n\t\t} else {\n\t\t\tinc_mm_counter_fast(mm, MM_ANONPAGES);\n\t\t}\n\t\tflush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));\n\t\tentry = mk_pte(new_page, vma->vm_page_prot);\n\t\tentry = pte_sw_mkyoung(entry);\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\t/*\n\t\t * Clear the pte entry and flush it first, before updating the\n\t\t * pte with the new entry. This will avoid a race condition\n\t\t * seen in the presence of one thread doing SMC and another\n\t\t * thread doing COW.\n\t\t */\n\t\tptep_clear_flush_notify(vma, vmf->address, vmf->pte);\n\t\tpage_add_new_anon_rmap(new_page, vma, vmf->address, false);\n\t\tlru_cache_add_inactive_or_unevictable(new_page, vma);\n\t\t/*\n\t\t * We call the notify macro here because, when using secondary\n\t\t * mmu page tables (such as kvm shadow page tables), we want the\n\t\t * new page to be mapped directly into the secondary page table.\n\t\t */\n\t\tset_pte_at_notify(mm, vmf->address, vmf->pte, entry);\n\t\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\n\t\tif (old_page) {\n\t\t\t/*\n\t\t\t * Only after switching the pte to the new page may\n\t\t\t * we remove the mapcount here. Otherwise another\n\t\t\t * process may come and find the rmap count decremented\n\t\t\t * before the pte is switched to the new page, and\n\t\t\t * \"reuse\" the old page writing into it while our pte\n\t\t\t * here still points into it and can be read by other\n\t\t\t * threads.\n\t\t\t *\n\t\t\t * The critical issue is to order this\n\t\t\t * page_remove_rmap with the ptp_clear_flush above.\n\t\t\t * Those stores are ordered by (if nothing else,)\n\t\t\t * the barrier present in the atomic_add_negative\n\t\t\t * in page_remove_rmap.\n\t\t\t *\n\t\t\t * Then the TLB flush in ptep_clear_flush ensures that\n\t\t\t * no process can access the old page before the\n\t\t\t * decremented mapcount is visible. And the old page\n\t\t\t * cannot be reused until after the decremented\n\t\t\t * mapcount is visible. So transitively, TLBs to\n\t\t\t * old page will be flushed before it can be reused.\n\t\t\t */\n\t\t\tpage_remove_rmap(old_page, false);\n\t\t}\n\n\t\t/* Free the old page.. */\n\t\tnew_page = old_page;\n\t\tpage_copied = 1;\n\t} else {\n\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t}\n\n\tif (new_page)\n\t\tput_page(new_page);\n\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback as\n\t * the above ptep_clear_flush_notify() did already call it.\n\t */\n\tmmu_notifier_invalidate_range_only_end(&range);\n\tif (old_page) {\n\t\t/*\n\t\t * Don't let another task, with possibly unlocked vma,\n\t\t * keep the mlocked page.\n\t\t */\n\t\tif (page_copied && (vma->vm_flags & VM_LOCKED)) {\n\t\t\tlock_page(old_page);\t/* LRU manipulation */\n\t\t\tif (PageMlocked(old_page))\n\t\t\t\tmunlock_vma_page(old_page);\n\t\t\tunlock_page(old_page);\n\t\t}\n\t\tput_page(old_page);\n\t}\n\treturn page_copied ? VM_FAULT_WRITE : 0;\noom_free_new:\n\tput_page(new_page);\noom:\n\tif (old_page)\n\t\tput_page(old_page);\n\treturn VM_FAULT_OOM;\n}\n\n/**\n * finish_mkwrite_fault - finish page fault for a shared mapping, making PTE\n *\t\t\t  writeable once the page is prepared\n *\n * @vmf: structure describing the fault\n *\n * This function handles all that is needed to finish a write page fault in a\n * shared mapping due to PTE being read-only once the mapped page is prepared.\n * It handles locking of PTE and modifying it.\n *\n * The function expects the page to be locked or other protection against\n * concurrent faults / writeback (such as DAX radix tree locks).\n *\n * Return: %VM_FAULT_WRITE on success, %0 when PTE got changed before\n * we acquired PTE lock.\n */\nvm_fault_t finish_mkwrite_fault(struct vm_fault *vmf)\n{\n\tWARN_ON_ONCE(!(vmf->vma->vm_flags & VM_SHARED));\n\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t\t       &vmf->ptl);\n\t/*\n\t * We might have raced with another page fault while we released the\n\t * pte_offset_map_lock.\n\t */\n\tif (!pte_same(*vmf->pte, vmf->orig_pte)) {\n\t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\twp_page_reuse(vmf);\n\treturn 0;\n}\n\n/*\n * Handle write page faults for VM_MIXEDMAP or VM_PFNMAP for a VM_SHARED\n * mapping\n */\nstatic vm_fault_t wp_pfn_shared(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\n\tif (vma->vm_ops && vma->vm_ops->pfn_mkwrite) {\n\t\tvm_fault_t ret;\n\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tvmf->flags |= FAULT_FLAG_MKWRITE;\n\t\tret = vma->vm_ops->pfn_mkwrite(vmf);\n\t\tif (ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))\n\t\t\treturn ret;\n\t\treturn finish_mkwrite_fault(vmf);\n\t}\n\twp_page_reuse(vmf);\n\treturn VM_FAULT_WRITE;\n}\n\nstatic vm_fault_t wp_page_shared(struct vm_fault *vmf)\n\t__releases(vmf->ptl)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret = VM_FAULT_WRITE;\n\n\tget_page(vmf->page);\n\n\tif (vma->vm_ops && vma->vm_ops->page_mkwrite) {\n\t\tvm_fault_t tmp;\n\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\ttmp = do_page_mkwrite(vmf);\n\t\tif (unlikely(!tmp || (tmp &\n\t\t\t\t      (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {\n\t\t\tput_page(vmf->page);\n\t\t\treturn tmp;\n\t\t}\n\t\ttmp = finish_mkwrite_fault(vmf);\n\t\tif (unlikely(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {\n\t\t\tunlock_page(vmf->page);\n\t\t\tput_page(vmf->page);\n\t\t\treturn tmp;\n\t\t}\n\t} else {\n\t\twp_page_reuse(vmf);\n\t\tlock_page(vmf->page);\n\t}\n\tret |= fault_dirty_shared_page(vmf);\n\tput_page(vmf->page);\n\n\treturn ret;\n}\n\n/*\n * This routine handles present pages, when users try to write\n * to a shared page. It is done by copying the page to a new address\n * and decrementing the shared-page counter for the old page.\n *\n * Note that this routine assumes that the protection checks have been\n * done by the caller (the low-level page fault routine in most cases).\n * Thus we can safely just mark it writable once we've done any necessary\n * COW.\n *\n * We also mark the page dirty at this point even though the page will\n * change only once the write actually happens. This avoids a few races,\n * and potentially makes it more efficient.\n *\n * We enter with non-exclusive mmap_lock (to exclude vma changes,\n * but allow concurrent faults), with pte both mapped and locked.\n * We return with mmap_lock still held, but pte unmapped and unlocked.\n */\nstatic vm_fault_t do_wp_page(struct vm_fault *vmf)\n\t__releases(vmf->ptl)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\n\tif (userfaultfd_pte_wp(vma, *vmf->pte)) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\treturn handle_userfault(vmf, VM_UFFD_WP);\n\t}\n\n\tvmf->page = vm_normal_page(vma, vmf->address, vmf->orig_pte);\n\tif (!vmf->page) {\n\t\t/*\n\t\t * VM_MIXEDMAP !pfn_valid() case, or VM_SOFTDIRTY clear on a\n\t\t * VM_PFNMAP VMA.\n\t\t *\n\t\t * We should not cow pages in a shared writeable mapping.\n\t\t * Just mark the pages writable and/or call ops->pfn_mkwrite.\n\t\t */\n\t\tif ((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==\n\t\t\t\t     (VM_WRITE|VM_SHARED))\n\t\t\treturn wp_pfn_shared(vmf);\n\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\treturn wp_page_copy(vmf);\n\t}\n\n\t/*\n\t * Take out anonymous pages first, anonymous shared vmas are\n\t * not dirty accountable.\n\t */\n\tif (PageAnon(vmf->page)) {\n\t\tstruct page *page = vmf->page;\n\n\t\t/* PageKsm() doesn't necessarily raise the page refcount */\n\t\tif (PageKsm(page) || page_count(page) != 1)\n\t\t\tgoto copy;\n\t\tif (!trylock_page(page))\n\t\t\tgoto copy;\n\t\tif (PageKsm(page) || page_mapcount(page) != 1 || page_count(page) != 1) {\n\t\t\tunlock_page(page);\n\t\t\tgoto copy;\n\t\t}\n\t\t/*\n\t\t * Ok, we've got the only map reference, and the only\n\t\t * page count reference, and the page is locked,\n\t\t * it's dark out, and we're wearing sunglasses. Hit it.\n\t\t */\n\t\tunlock_page(page);\n\t\twp_page_reuse(vmf);\n\t\treturn VM_FAULT_WRITE;\n\t} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==\n\t\t\t\t\t(VM_WRITE|VM_SHARED))) {\n\t\treturn wp_page_shared(vmf);\n\t}\ncopy:\n\t/*\n\t * Ok, we need to copy. Oh, well..\n\t */\n\tget_page(vmf->page);\n\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn wp_page_copy(vmf);\n}\n\nstatic void unmap_mapping_range_vma(struct vm_area_struct *vma,\n\t\tunsigned long start_addr, unsigned long end_addr,\n\t\tstruct zap_details *details)\n{\n\tzap_page_range_single(vma, start_addr, end_addr - start_addr, details);\n}\n\nstatic inline void unmap_mapping_range_tree(struct rb_root_cached *root,\n\t\t\t\t\t    struct zap_details *details)\n{\n\tstruct vm_area_struct *vma;\n\tpgoff_t vba, vea, zba, zea;\n\n\tvma_interval_tree_foreach(vma, root,\n\t\t\tdetails->first_index, details->last_index) {\n\n\t\tvba = vma->vm_pgoff;\n\t\tvea = vba + vma_pages(vma) - 1;\n\t\tzba = details->first_index;\n\t\tif (zba < vba)\n\t\t\tzba = vba;\n\t\tzea = details->last_index;\n\t\tif (zea > vea)\n\t\t\tzea = vea;\n\n\t\tunmap_mapping_range_vma(vma,\n\t\t\t((zba - vba) << PAGE_SHIFT) + vma->vm_start,\n\t\t\t((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,\n\t\t\t\tdetails);\n\t}\n}\n\n/**\n * unmap_mapping_pages() - Unmap pages from processes.\n * @mapping: The address space containing pages to be unmapped.\n * @start: Index of first page to be unmapped.\n * @nr: Number of pages to be unmapped.  0 to unmap to end of file.\n * @even_cows: Whether to unmap even private COWed pages.\n *\n * Unmap the pages in this address space from any userspace process which\n * has them mmaped.  Generally, you want to remove COWed pages as well when\n * a file is being truncated, but not when invalidating pages from the page\n * cache.\n */\nvoid unmap_mapping_pages(struct address_space *mapping, pgoff_t start,\n\t\tpgoff_t nr, bool even_cows)\n{\n\tstruct zap_details details = { };\n\n\tdetails.check_mapping = even_cows ? NULL : mapping;\n\tdetails.first_index = start;\n\tdetails.last_index = start + nr - 1;\n\tif (details.last_index < details.first_index)\n\t\tdetails.last_index = ULONG_MAX;\n\n\ti_mmap_lock_write(mapping);\n\tif (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root)))\n\t\tunmap_mapping_range_tree(&mapping->i_mmap, &details);\n\ti_mmap_unlock_write(mapping);\n}\n\n/**\n * unmap_mapping_range - unmap the portion of all mmaps in the specified\n * address_space corresponding to the specified byte range in the underlying\n * file.\n *\n * @mapping: the address space containing mmaps to be unmapped.\n * @holebegin: byte in first page to unmap, relative to the start of\n * the underlying file.  This will be rounded down to a PAGE_SIZE\n * boundary.  Note that this is different from truncate_pagecache(), which\n * must keep the partial page.  In contrast, we must get rid of\n * partial pages.\n * @holelen: size of prospective hole in bytes.  This will be rounded\n * up to a PAGE_SIZE boundary.  A holelen of zero truncates to the\n * end of the file.\n * @even_cows: 1 when truncating a file, unmap even private COWed pages;\n * but 0 when invalidating pagecache, don't throw away private data.\n */\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows)\n{\n\tpgoff_t hba = holebegin >> PAGE_SHIFT;\n\tpgoff_t hlen = (holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\n\t/* Check for overflow. */\n\tif (sizeof(holelen) > sizeof(hlen)) {\n\t\tlong long holeend =\n\t\t\t(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tif (holeend & ~(long long)ULONG_MAX)\n\t\t\thlen = ULONG_MAX - hba + 1;\n\t}\n\n\tunmap_mapping_pages(mapping, hba, hlen, even_cows);\n}\nEXPORT_SYMBOL(unmap_mapping_range);\n\n/*\n * We enter with non-exclusive mmap_lock (to exclude vma changes,\n * but allow concurrent faults), and pte mapped but not yet locked.\n * We return with pte unmapped and unlocked.\n *\n * We return with the mmap_lock locked or unlocked in the same cases\n * as does filemap_fault().\n */\nvm_fault_t do_swap_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = NULL, *swapcache;\n\tswp_entry_t entry;\n\tpte_t pte;\n\tint locked;\n\tint exclusive = 0;\n\tvm_fault_t ret = 0;\n\tvoid *shadow = NULL;\n\n\tif (!pte_unmap_same(vma->vm_mm, vmf->pmd, vmf->pte, vmf->orig_pte))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(vmf->orig_pte);\n\tif (unlikely(non_swap_entry(entry))) {\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,\n\t\t\t\t\t     vmf->address);\n\t\t} else if (is_device_private_entry(entry)) {\n\t\t\tvmf->page = device_private_entry_to_page(entry);\n\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);\n\t\t} else if (is_hwpoison_entry(entry)) {\n\t\t\tret = VM_FAULT_HWPOISON;\n\t\t} else {\n\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t}\n\t\tgoto out;\n\t}\n\n\n\tdelayacct_set_flag(DELAYACCT_PF_SWAPIN);\n\tpage = lookup_swap_cache(entry, vma, vmf->address);\n\tswapcache = page;\n\n\tif (!page) {\n\t\tstruct swap_info_struct *si = swp_swap_info(entry);\n\n\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&\n\t\t    __swap_count(entry) == 1) {\n\t\t\t/* skip swapcache */\n\t\t\tpage = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,\n\t\t\t\t\t\t\tvmf->address);\n\t\t\tif (page) {\n\t\t\t\tint err;\n\n\t\t\t\t__SetPageLocked(page);\n\t\t\t\t__SetPageSwapBacked(page);\n\t\t\t\tset_page_private(page, entry.val);\n\n\t\t\t\t/* Tell memcg to use swap ownership records */\n\t\t\t\tSetPageSwapCache(page);\n\t\t\t\terr = mem_cgroup_charge(page, vma->vm_mm,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\tClearPageSwapCache(page);\n\t\t\t\tif (err) {\n\t\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\t\tgoto out_page;\n\t\t\t\t}\n\n\t\t\t\tshadow = get_shadow_from_swap_cache(entry);\n\t\t\t\tif (shadow)\n\t\t\t\t\tworkingset_refault(page, shadow);\n\n\t\t\t\tlru_cache_add(page);\n\t\t\t\tswap_readpage(page, true);\n\t\t\t}\n\t\t} else {\n\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,\n\t\t\t\t\t\tvmf);\n\t\t\tswapcache = page;\n\t\t}\n\n\t\tif (!page) {\n\t\t\t/*\n\t\t\t * Back out if somebody else faulted in this pte\n\t\t\t * while we released the pte lock.\n\t\t\t */\n\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\t\tvmf->address, &vmf->ptl);\n\t\t\tif (likely(pte_same(*vmf->pte, vmf->orig_pte)))\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\tdelayacct_clear_flag(DELAYACCT_PF_SWAPIN);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t/* Had to read the page from swap area: Major fault */\n\t\tret = VM_FAULT_MAJOR;\n\t\tcount_vm_event(PGMAJFAULT);\n\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);\n\t} else if (PageHWPoison(page)) {\n\t\t/*\n\t\t * hwpoisoned dirty swapcache pages are kept for killing\n\t\t * owner processes (which may be unknown at hwpoison time)\n\t\t */\n\t\tret = VM_FAULT_HWPOISON;\n\t\tdelayacct_clear_flag(DELAYACCT_PF_SWAPIN);\n\t\tgoto out_release;\n\t}\n\n\tlocked = lock_page_or_retry(page, vma->vm_mm, vmf->flags);\n\n\tdelayacct_clear_flag(DELAYACCT_PF_SWAPIN);\n\tif (!locked) {\n\t\tret |= VM_FAULT_RETRY;\n\t\tgoto out_release;\n\t}\n\n\t/*\n\t * Make sure try_to_free_swap or reuse_swap_page or swapoff did not\n\t * release the swapcache from under us.  The page pin, and pte_same\n\t * test below, are not enough to exclude that.  Even if it is still\n\t * swapcache, we need to check that the page's swap has not changed.\n\t */\n\tif (unlikely((!PageSwapCache(page) ||\n\t\t\tpage_private(page) != entry.val)) && swapcache)\n\t\tgoto out_page;\n\n\tpage = ksm_might_need_to_copy(page, vma, vmf->address);\n\tif (unlikely(!page)) {\n\t\tret = VM_FAULT_OOM;\n\t\tpage = swapcache;\n\t\tgoto out_page;\n\t}\n\n\tcgroup_throttle_swaprate(page, GFP_KERNEL);\n\n\t/*\n\t * Back out if somebody else already faulted in this pte.\n\t */\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte)))\n\t\tgoto out_nomap;\n\n\tif (unlikely(!PageUptodate(page))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_nomap;\n\t}\n\n\t/*\n\t * The page isn't present yet, go ahead with the fault.\n\t *\n\t * Be careful about the sequence of operations here.\n\t * To get its accounting right, reuse_swap_page() must be called\n\t * while the page is counted on swap but not yet in mapcount i.e.\n\t * before page_add_anon_rmap() and swap_free(); try_to_free_swap()\n\t * must be called after the swap_free(), or it will never succeed.\n\t */\n\n\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\tdec_mm_counter_fast(vma->vm_mm, MM_SWAPENTS);\n\tpte = mk_pte(page, vma->vm_page_prot);\n\tif ((vmf->flags & FAULT_FLAG_WRITE) && reuse_swap_page(page, NULL)) {\n\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);\n\t\tvmf->flags &= ~FAULT_FLAG_WRITE;\n\t\tret |= VM_FAULT_WRITE;\n\t\texclusive = RMAP_EXCLUSIVE;\n\t}\n\tflush_icache_page(vma, page);\n\tif (pte_swp_soft_dirty(vmf->orig_pte))\n\t\tpte = pte_mksoft_dirty(pte);\n\tif (pte_swp_uffd_wp(vmf->orig_pte)) {\n\t\tpte = pte_mkuffd_wp(pte);\n\t\tpte = pte_wrprotect(pte);\n\t}\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);\n\tarch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);\n\tvmf->orig_pte = pte;\n\n\t/* ksm created a completely new copy */\n\tif (unlikely(page != swapcache && swapcache)) {\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address, false);\n\t\tlru_cache_add_inactive_or_unevictable(page, vma);\n\t} else {\n\t\tdo_page_add_anon_rmap(page, vma, vmf->address, exclusive);\n\t}\n\n\tswap_free(entry);\n\tif (mem_cgroup_swap_full(page) ||\n\t    (vma->vm_flags & VM_LOCKED) || PageMlocked(page))\n\t\ttry_to_free_swap(page);\n\tunlock_page(page);\n\tif (page != swapcache && swapcache) {\n\t\t/*\n\t\t * Hold the lock to avoid the swap entry to be reused\n\t\t * until we take the PT lock for the pte_same() check\n\t\t * (to avoid false positives from pte_same). For\n\t\t * further safety release the lock after the swap_free\n\t\t * so that the swap count won't change under a\n\t\t * parallel locked swapcache.\n\t\t */\n\t\tunlock_page(swapcache);\n\t\tput_page(swapcache);\n\t}\n\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tret |= do_wp_page(vmf);\n\t\tif (ret & VM_FAULT_ERROR)\n\t\t\tret &= VM_FAULT_ERROR;\n\t\tgoto out;\n\t}\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\treturn ret;\nout_nomap:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout_page:\n\tunlock_page(page);\nout_release:\n\tput_page(page);\n\tif (page != swapcache && swapcache) {\n\t\tunlock_page(swapcache);\n\t\tput_page(swapcache);\n\t}\n\treturn ret;\n}\n\n/*\n * We enter with non-exclusive mmap_lock (to exclude vma changes,\n * but allow concurrent faults), and pte mapped but not yet locked.\n * We return with mmap_lock still held, but pte unmapped and unlocked.\n */\nstatic vm_fault_t do_anonymous_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page;\n\tvm_fault_t ret = 0;\n\tpte_t entry;\n\n\t/* File mapping without ->vm_ops ? */\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t/*\n\t * Use pte_alloc() instead of pte_alloc_map().  We can't run\n\t * pte_offset_map() on pmds where a huge pmd might be created\n\t * from a different thread.\n\t *\n\t * pte_alloc_map() is safe to use under mmap_write_lock(mm) or when\n\t * parallel threads are excluded by other means.\n\t *\n\t * Here we only have mmap_read_lock(mm).\n\t */\n\tif (pte_alloc(vma->vm_mm, vmf->pmd))\n\t\treturn VM_FAULT_OOM;\n\n\t/* See the comment in pte_alloc_one_map() */\n\tif (unlikely(pmd_trans_unstable(vmf->pmd)))\n\t\treturn 0;\n\n\t/* Use the zero-page for reads */\n\tif (!(vmf->flags & FAULT_FLAG_WRITE) &&\n\t\t\t!mm_forbids_zeropage(vma->vm_mm)) {\n\t\tentry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),\n\t\t\t\t\t\tvma->vm_page_prot));\n\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n\t\t\t\tvmf->address, &vmf->ptl);\n\t\tif (!pte_none(*vmf->pte)) {\n\t\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t\t\tgoto unlock;\n\t\t}\n\t\tret = check_stable_address_space(vma->vm_mm);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t\t/* Deliver the page fault to userland, check inside PT lock */\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t\treturn handle_userfault(vmf, VM_UFFD_MISSING);\n\t\t}\n\t\tgoto setpte;\n\t}\n\n\t/* Allocate our own private page. */\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\tgoto oom;\n\tpage = alloc_zeroed_user_highpage_movable(vma, vmf->address);\n\tif (!page)\n\t\tgoto oom;\n\n\tif (mem_cgroup_charge(page, vma->vm_mm, GFP_KERNEL))\n\t\tgoto oom_free_page;\n\tcgroup_throttle_swaprate(page, GFP_KERNEL);\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\n\tentry = mk_pte(page, vma->vm_page_prot);\n\tentry = pte_sw_mkyoung(entry);\n\tif (vma->vm_flags & VM_WRITE)\n\t\tentry = pte_mkwrite(pte_mkdirty(entry));\n\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\tif (!pte_none(*vmf->pte)) {\n\t\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\n\t\tgoto release;\n\t}\n\n\tret = check_stable_address_space(vma->vm_mm);\n\tif (ret)\n\t\tgoto release;\n\n\t/* Deliver the page fault to userland, check inside PT lock */\n\tif (userfaultfd_missing(vma)) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tput_page(page);\n\t\treturn handle_userfault(vmf, VM_UFFD_MISSING);\n\t}\n\n\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\tpage_add_new_anon_rmap(page, vma, vmf->address, false);\n\tlru_cache_add_inactive_or_unevictable(page, vma);\nsetpte:\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn ret;\nrelease:\n\tput_page(page);\n\tgoto unlock;\noom_free_page:\n\tput_page(page);\noom:\n\treturn VM_FAULT_OOM;\n}\n\n/*\n * The mmap_lock must have been held on entry, and may have been\n * released depending on flags and vma->vm_ops->fault() return value.\n * See filemap_fault() and __lock_page_retry().\n */\nstatic vm_fault_t __do_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret;\n\n\t/*\n\t * Preallocate pte before we take page_lock because this might lead to\n\t * deadlocks for memcg reclaim which waits for pages under writeback:\n\t *\t\t\t\tlock_page(A)\n\t *\t\t\t\tSetPageWriteback(A)\n\t *\t\t\t\tunlock_page(A)\n\t * lock_page(B)\n\t *\t\t\t\tlock_page(B)\n\t * pte_alloc_one\n\t *   shrink_page_list\n\t *     wait_on_page_writeback(A)\n\t *\t\t\t\tSetPageWriteback(B)\n\t *\t\t\t\tunlock_page(B)\n\t *\t\t\t\t# flush A, B to clear the writeback\n\t */\n\tif (pmd_none(*vmf->pmd) && !vmf->prealloc_pte) {\n\t\tvmf->prealloc_pte = pte_alloc_one(vma->vm_mm);\n\t\tif (!vmf->prealloc_pte)\n\t\t\treturn VM_FAULT_OOM;\n\t\tsmp_wmb(); /* See comment in __pte_alloc() */\n\t}\n\n\tret = vma->vm_ops->fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY |\n\t\t\t    VM_FAULT_DONE_COW)))\n\t\treturn ret;\n\n\tif (unlikely(PageHWPoison(vmf->page))) {\n\t\tif (ret & VM_FAULT_LOCKED)\n\t\t\tunlock_page(vmf->page);\n\t\tput_page(vmf->page);\n\t\tvmf->page = NULL;\n\t\treturn VM_FAULT_HWPOISON;\n\t}\n\n\tif (unlikely(!(ret & VM_FAULT_LOCKED)))\n\t\tlock_page(vmf->page);\n\telse\n\t\tVM_BUG_ON_PAGE(!PageLocked(vmf->page), vmf->page);\n\n\treturn ret;\n}\n\n/*\n * The ordering of these checks is important for pmds with _PAGE_DEVMAP set.\n * If we check pmd_trans_unstable() first we will trip the bad_pmd() check\n * inside of pmd_none_or_trans_huge_or_clear_bad(). This will end up correctly\n * returning 1 but not before it spams dmesg with the pmd_clear_bad() output.\n */\nstatic int pmd_devmap_trans_unstable(pmd_t *pmd)\n{\n\treturn pmd_devmap(*pmd) || pmd_trans_unstable(pmd);\n}\n\nstatic vm_fault_t pte_alloc_one_map(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\n\tif (!pmd_none(*vmf->pmd))\n\t\tgoto map_pte;\n\tif (vmf->prealloc_pte) {\n\t\tvmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);\n\t\tif (unlikely(!pmd_none(*vmf->pmd))) {\n\t\t\tspin_unlock(vmf->ptl);\n\t\t\tgoto map_pte;\n\t\t}\n\n\t\tmm_inc_nr_ptes(vma->vm_mm);\n\t\tpmd_populate(vma->vm_mm, vmf->pmd, vmf->prealloc_pte);\n\t\tspin_unlock(vmf->ptl);\n\t\tvmf->prealloc_pte = NULL;\n\t} else if (unlikely(pte_alloc(vma->vm_mm, vmf->pmd))) {\n\t\treturn VM_FAULT_OOM;\n\t}\nmap_pte:\n\t/*\n\t * If a huge pmd materialized under us just retry later.  Use\n\t * pmd_trans_unstable() via pmd_devmap_trans_unstable() instead of\n\t * pmd_trans_huge() to ensure the pmd didn't become pmd_trans_huge\n\t * under us and then back to pmd_none, as a result of MADV_DONTNEED\n\t * running immediately after a huge pmd fault in a different thread of\n\t * this mm, in turn leading to a misleading pmd_trans_huge() retval.\n\t * All we have to ensure is that it is a regular pmd that we can walk\n\t * with pte_offset_map() and we can do that through an atomic read in\n\t * C, which is what pmd_trans_unstable() provides.\n\t */\n\tif (pmd_devmap_trans_unstable(vmf->pmd))\n\t\treturn VM_FAULT_NOPAGE;\n\n\t/*\n\t * At this point we know that our vmf->pmd points to a page of ptes\n\t * and it cannot become pmd_none(), pmd_devmap() or pmd_trans_huge()\n\t * for the duration of the fault.  If a racing MADV_DONTNEED runs and\n\t * we zap the ptes pointed to by our vmf->pmd, the vmf->ptl will still\n\t * be valid and we will re-check to make sure the vmf->pte isn't\n\t * pte_none() under vmf->ptl protection when we return to\n\t * alloc_set_pte().\n\t */\n\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,\n\t\t\t&vmf->ptl);\n\treturn 0;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void deposit_prealloc_pte(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\n\tpgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, vmf->prealloc_pte);\n\t/*\n\t * We are going to consume the prealloc table,\n\t * count that as nr_ptes.\n\t */\n\tmm_inc_nr_ptes(vma->vm_mm);\n\tvmf->prealloc_pte = NULL;\n}\n\nstatic vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n\tpmd_t entry;\n\tint i;\n\tvm_fault_t ret = VM_FAULT_FALLBACK;\n\n\tif (!transhuge_vma_suitable(vma, haddr))\n\t\treturn ret;\n\n\tpage = compound_head(page);\n\tif (compound_order(page) != HPAGE_PMD_ORDER)\n\t\treturn ret;\n\n\t/*\n\t * Archs like ppc64 need additonal space to store information\n\t * related to pte entry. Use the preallocated table for that.\n\t */\n\tif (arch_needs_pgtable_deposit() && !vmf->prealloc_pte) {\n\t\tvmf->prealloc_pte = pte_alloc_one(vma->vm_mm);\n\t\tif (!vmf->prealloc_pte)\n\t\t\treturn VM_FAULT_OOM;\n\t\tsmp_wmb(); /* See comment in __pte_alloc() */\n\t}\n\n\tvmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);\n\tif (unlikely(!pmd_none(*vmf->pmd)))\n\t\tgoto out;\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++)\n\t\tflush_icache_page(vma, page + i);\n\n\tentry = mk_huge_pmd(page, vma->vm_page_prot);\n\tif (write)\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\n\tadd_mm_counter(vma->vm_mm, mm_counter_file(page), HPAGE_PMD_NR);\n\tpage_add_file_rmap(page, true);\n\t/*\n\t * deposit and withdraw with pmd lock held\n\t */\n\tif (arch_needs_pgtable_deposit())\n\t\tdeposit_prealloc_pte(vmf);\n\n\tset_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);\n\n\tupdate_mmu_cache_pmd(vma, haddr, vmf->pmd);\n\n\t/* fault is handled */\n\tret = 0;\n\tcount_vm_event(THP_FILE_MAPPED);\nout:\n\tspin_unlock(vmf->ptl);\n\treturn ret;\n}\n#else\nstatic vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif\n\n/**\n * alloc_set_pte - setup new PTE entry for given page and add reverse page\n * mapping. If needed, the function allocates page table or use pre-allocated.\n *\n * @vmf: fault environment\n * @page: page to map\n *\n * Caller must take care of unlocking vmf->ptl, if vmf->pte is non-NULL on\n * return.\n *\n * Target users are page handler itself and implementations of\n * vm_ops->map_pages.\n *\n * Return: %0 on success, %VM_FAULT_ code in case of error.\n */\nvm_fault_t alloc_set_pte(struct vm_fault *vmf, struct page *page)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\tpte_t entry;\n\tvm_fault_t ret;\n\n\tif (pmd_none(*vmf->pmd) && PageTransCompound(page)) {\n\t\tret = do_set_pmd(vmf, page);\n\t\tif (ret != VM_FAULT_FALLBACK)\n\t\t\treturn ret;\n\t}\n\n\tif (!vmf->pte) {\n\t\tret = pte_alloc_one_map(vmf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t/* Re-check under ptl */\n\tif (unlikely(!pte_none(*vmf->pte))) {\n\t\tupdate_mmu_tlb(vma, vmf->address, vmf->pte);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\n\tflush_icache_page(vma, page);\n\tentry = mk_pte(page, vma->vm_page_prot);\n\tentry = pte_sw_mkyoung(entry);\n\tif (write)\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t/* copy-on-write page */\n\tif (write && !(vma->vm_flags & VM_SHARED)) {\n\t\tinc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);\n\t\tpage_add_new_anon_rmap(page, vma, vmf->address, false);\n\t\tlru_cache_add_inactive_or_unevictable(page, vma);\n\t} else {\n\t\tinc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));\n\t\tpage_add_file_rmap(page, false);\n\t}\n\tset_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);\n\n\t/* no need to invalidate: a not-present page won't be cached */\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\n\n\treturn 0;\n}\n\n\n/**\n * finish_fault - finish page fault once we have prepared the page to fault\n *\n * @vmf: structure describing the fault\n *\n * This function handles all that is needed to finish a page fault once the\n * page to fault in is prepared. It handles locking of PTEs, inserts PTE for\n * given page, adds reverse page mapping, handles memcg charges and LRU\n * addition.\n *\n * The function expects the page to be locked and on success it consumes a\n * reference of a page being mapped (for the PTE which maps it).\n *\n * Return: %0 on success, %VM_FAULT_ code in case of error.\n */\nvm_fault_t finish_fault(struct vm_fault *vmf)\n{\n\tstruct page *page;\n\tvm_fault_t ret = 0;\n\n\t/* Did we COW the page? */\n\tif ((vmf->flags & FAULT_FLAG_WRITE) &&\n\t    !(vmf->vma->vm_flags & VM_SHARED))\n\t\tpage = vmf->cow_page;\n\telse\n\t\tpage = vmf->page;\n\n\t/*\n\t * check even for read faults because we might have lost our CoWed\n\t * page\n\t */\n\tif (!(vmf->vma->vm_flags & VM_SHARED))\n\t\tret = check_stable_address_space(vmf->vma->vm_mm);\n\tif (!ret)\n\t\tret = alloc_set_pte(vmf, page);\n\tif (vmf->pte)\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn ret;\n}\n\nstatic unsigned long fault_around_bytes __read_mostly =\n\trounddown_pow_of_two(65536);\n\n#ifdef CONFIG_DEBUG_FS\nstatic int fault_around_bytes_get(void *data, u64 *val)\n{\n\t*val = fault_around_bytes;\n\treturn 0;\n}\n\n/*\n * fault_around_bytes must be rounded down to the nearest page order as it's\n * what do_fault_around() expects to see.\n */\nstatic int fault_around_bytes_set(void *data, u64 val)\n{\n\tif (val / PAGE_SIZE > PTRS_PER_PTE)\n\t\treturn -EINVAL;\n\tif (val > PAGE_SIZE)\n\t\tfault_around_bytes = rounddown_pow_of_two(val);\n\telse\n\t\tfault_around_bytes = PAGE_SIZE; /* rounddown_pow_of_two(0) is undefined */\n\treturn 0;\n}\nDEFINE_DEBUGFS_ATTRIBUTE(fault_around_bytes_fops,\n\t\tfault_around_bytes_get, fault_around_bytes_set, \"%llu\\n\");\n\nstatic int __init fault_around_debugfs(void)\n{\n\tdebugfs_create_file_unsafe(\"fault_around_bytes\", 0644, NULL, NULL,\n\t\t\t\t   &fault_around_bytes_fops);\n\treturn 0;\n}\nlate_initcall(fault_around_debugfs);\n#endif\n\n/*\n * do_fault_around() tries to map few pages around the fault address. The hope\n * is that the pages will be needed soon and this will lower the number of\n * faults to handle.\n *\n * It uses vm_ops->map_pages() to map the pages, which skips the page if it's\n * not ready to be mapped: not up-to-date, locked, etc.\n *\n * This function is called with the page table lock taken. In the split ptlock\n * case the page table lock only protects only those entries which belong to\n * the page table corresponding to the fault address.\n *\n * This function doesn't cross the VMA boundaries, in order to call map_pages()\n * only once.\n *\n * fault_around_bytes defines how many bytes we'll try to map.\n * do_fault_around() expects it to be set to a power of two less than or equal\n * to PTRS_PER_PTE.\n *\n * The virtual address of the area that we map is naturally aligned to\n * fault_around_bytes rounded down to the machine page size\n * (and therefore to page order).  This way it's easier to guarantee\n * that we don't cross page table boundaries.\n */\nstatic vm_fault_t do_fault_around(struct vm_fault *vmf)\n{\n\tunsigned long address = vmf->address, nr_pages, mask;\n\tpgoff_t start_pgoff = vmf->pgoff;\n\tpgoff_t end_pgoff;\n\tint off;\n\tvm_fault_t ret = 0;\n\n\tnr_pages = READ_ONCE(fault_around_bytes) >> PAGE_SHIFT;\n\tmask = ~(nr_pages * PAGE_SIZE - 1) & PAGE_MASK;\n\n\tvmf->address = max(address & mask, vmf->vma->vm_start);\n\toff = ((address - vmf->address) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);\n\tstart_pgoff -= off;\n\n\t/*\n\t *  end_pgoff is either the end of the page table, the end of\n\t *  the vma or nr_pages from start_pgoff, depending what is nearest.\n\t */\n\tend_pgoff = start_pgoff -\n\t\t((vmf->address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1)) +\n\t\tPTRS_PER_PTE - 1;\n\tend_pgoff = min3(end_pgoff, vma_pages(vmf->vma) + vmf->vma->vm_pgoff - 1,\n\t\t\tstart_pgoff + nr_pages - 1);\n\n\tif (pmd_none(*vmf->pmd)) {\n\t\tvmf->prealloc_pte = pte_alloc_one(vmf->vma->vm_mm);\n\t\tif (!vmf->prealloc_pte)\n\t\t\tgoto out;\n\t\tsmp_wmb(); /* See comment in __pte_alloc() */\n\t}\n\n\tvmf->vma->vm_ops->map_pages(vmf, start_pgoff, end_pgoff);\n\n\t/* Huge page is mapped? Page fault is solved */\n\tif (pmd_trans_huge(*vmf->pmd)) {\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\t/* ->map_pages() haven't done anything useful. Cold page cache? */\n\tif (!vmf->pte)\n\t\tgoto out;\n\n\t/* check if the page fault is solved */\n\tvmf->pte -= (vmf->address >> PAGE_SHIFT) - (address >> PAGE_SHIFT);\n\tif (!pte_none(*vmf->pte))\n\t\tret = VM_FAULT_NOPAGE;\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\nout:\n\tvmf->address = address;\n\tvmf->pte = NULL;\n\treturn ret;\n}\n\nstatic vm_fault_t do_read_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret = 0;\n\n\t/*\n\t * Let's call ->map_pages() first and use ->fault() as fallback\n\t * if page by the offset is not ready to be mapped (cold cache or\n\t * something).\n\t */\n\tif (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {\n\t\tret = do_fault_around(vmf);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = __do_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\treturn ret;\n\n\tret |= finish_fault(vmf);\n\tunlock_page(vmf->page);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\tput_page(vmf->page);\n\treturn ret;\n}\n\nstatic vm_fault_t do_cow_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret;\n\n\tif (unlikely(anon_vma_prepare(vma)))\n\t\treturn VM_FAULT_OOM;\n\n\tvmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);\n\tif (!vmf->cow_page)\n\t\treturn VM_FAULT_OOM;\n\n\tif (mem_cgroup_charge(vmf->cow_page, vma->vm_mm, GFP_KERNEL)) {\n\t\tput_page(vmf->cow_page);\n\t\treturn VM_FAULT_OOM;\n\t}\n\tcgroup_throttle_swaprate(vmf->cow_page, GFP_KERNEL);\n\n\tret = __do_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\tgoto uncharge_out;\n\tif (ret & VM_FAULT_DONE_COW)\n\t\treturn ret;\n\n\tcopy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);\n\t__SetPageUptodate(vmf->cow_page);\n\n\tret |= finish_fault(vmf);\n\tunlock_page(vmf->page);\n\tput_page(vmf->page);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\tgoto uncharge_out;\n\treturn ret;\nuncharge_out:\n\tput_page(vmf->cow_page);\n\treturn ret;\n}\n\nstatic vm_fault_t do_shared_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tvm_fault_t ret, tmp;\n\n\tret = __do_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))\n\t\treturn ret;\n\n\t/*\n\t * Check if the backing address space wants to know that the page is\n\t * about to become writable\n\t */\n\tif (vma->vm_ops->page_mkwrite) {\n\t\tunlock_page(vmf->page);\n\t\ttmp = do_page_mkwrite(vmf);\n\t\tif (unlikely(!tmp ||\n\t\t\t\t(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {\n\t\t\tput_page(vmf->page);\n\t\t\treturn tmp;\n\t\t}\n\t}\n\n\tret |= finish_fault(vmf);\n\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |\n\t\t\t\t\tVM_FAULT_RETRY))) {\n\t\tunlock_page(vmf->page);\n\t\tput_page(vmf->page);\n\t\treturn ret;\n\t}\n\n\tret |= fault_dirty_shared_page(vmf);\n\treturn ret;\n}\n\n/*\n * We enter with non-exclusive mmap_lock (to exclude vma changes,\n * but allow concurrent faults).\n * The mmap_lock may have been released depending on flags and our\n * return value.  See filemap_fault() and __lock_page_or_retry().\n * If mmap_lock is released, vma may become invalid (for example\n * by other thread calling munmap()).\n */\nstatic vm_fault_t do_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *vm_mm = vma->vm_mm;\n\tvm_fault_t ret;\n\n\t/*\n\t * The VMA was not fully populated on mmap() or missing VM_DONTEXPAND\n\t */\n\tif (!vma->vm_ops->fault) {\n\t\t/*\n\t\t * If we find a migration pmd entry or a none pmd entry, which\n\t\t * should never happen, return SIGBUS\n\t\t */\n\t\tif (unlikely(!pmd_present(*vmf->pmd)))\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\telse {\n\t\t\tvmf->pte = pte_offset_map_lock(vmf->vma->vm_mm,\n\t\t\t\t\t\t       vmf->pmd,\n\t\t\t\t\t\t       vmf->address,\n\t\t\t\t\t\t       &vmf->ptl);\n\t\t\t/*\n\t\t\t * Make sure this is not a temporary clearing of pte\n\t\t\t * by holding ptl and checking again. A R/M/W update\n\t\t\t * of pte involves: take ptl, clearing the pte so that\n\t\t\t * we don't have concurrent modification by hardware\n\t\t\t * followed by an update.\n\t\t\t */\n\t\t\tif (unlikely(pte_none(*vmf->pte)))\n\t\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\telse\n\t\t\t\tret = VM_FAULT_NOPAGE;\n\n\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\t}\n\t} else if (!(vmf->flags & FAULT_FLAG_WRITE))\n\t\tret = do_read_fault(vmf);\n\telse if (!(vma->vm_flags & VM_SHARED))\n\t\tret = do_cow_fault(vmf);\n\telse\n\t\tret = do_shared_fault(vmf);\n\n\t/* preallocated pagetable is unused: free it */\n\tif (vmf->prealloc_pte) {\n\t\tpte_free(vm_mm, vmf->prealloc_pte);\n\t\tvmf->prealloc_pte = NULL;\n\t}\n\treturn ret;\n}\n\nstatic int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, int page_nid,\n\t\t\t\tint *flags)\n{\n\tget_page(page);\n\n\tcount_vm_numa_event(NUMA_HINT_FAULTS);\n\tif (page_nid == numa_node_id()) {\n\t\tcount_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);\n\t\t*flags |= TNF_FAULT_LOCAL;\n\t}\n\n\treturn mpol_misplaced(page, vma, addr);\n}\n\nstatic vm_fault_t do_numa_page(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct page *page = NULL;\n\tint page_nid = NUMA_NO_NODE;\n\tint last_cpupid;\n\tint target_nid;\n\tbool migrated = false;\n\tpte_t pte, old_pte;\n\tbool was_writable = pte_savedwrite(vmf->orig_pte);\n\tint flags = 0;\n\n\t/*\n\t * The \"pte\" at this point cannot be used safely without\n\t * validation through pte_unmap_same(). It's of NUMA type but\n\t * the pfn may be screwed if the read is non atomic.\n\t */\n\tvmf->ptl = pte_lockptr(vma->vm_mm, vmf->pmd);\n\tspin_lock(vmf->ptl);\n\tif (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Make it present again, Depending on how arch implementes non\n\t * accessible ptes, some can allow access by kernel mode.\n\t */\n\told_pte = ptep_modify_prot_start(vma, vmf->address, vmf->pte);\n\tpte = pte_modify(old_pte, vma->vm_page_prot);\n\tpte = pte_mkyoung(pte);\n\tif (was_writable)\n\t\tpte = pte_mkwrite(pte);\n\tptep_modify_prot_commit(vma, vmf->address, vmf->pte, old_pte, pte);\n\tupdate_mmu_cache(vma, vmf->address, vmf->pte);\n\n\tpage = vm_normal_page(vma, vmf->address, pte);\n\tif (!page) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\treturn 0;\n\t}\n\n\t/* TODO: handle PTE-mapped THP */\n\tif (PageCompound(page)) {\n\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Avoid grouping on RO pages in general. RO pages shouldn't hurt as\n\t * much anyway since they can be in shared cache state. This misses\n\t * the case where a mapping is writable but the process never writes\n\t * to it but pte_write gets cleared during protection updates and\n\t * pte_dirty has unpredictable behaviour between PTE scan updates,\n\t * background writeback, dirty balancing and application behaviour.\n\t */\n\tif (!pte_write(pte))\n\t\tflags |= TNF_NO_GROUP;\n\n\t/*\n\t * Flag if the page is shared between multiple address spaces. This\n\t * is later used when determining whether to group tasks together\n\t */\n\tif (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))\n\t\tflags |= TNF_SHARED;\n\n\tlast_cpupid = page_cpupid_last(page);\n\tpage_nid = page_to_nid(page);\n\ttarget_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,\n\t\t\t&flags);\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\tif (target_nid == NUMA_NO_NODE) {\n\t\tput_page(page);\n\t\tgoto out;\n\t}\n\n\t/* Migrate to the requested node */\n\tmigrated = migrate_misplaced_page(page, vma, target_nid);\n\tif (migrated) {\n\t\tpage_nid = target_nid;\n\t\tflags |= TNF_MIGRATED;\n\t} else\n\t\tflags |= TNF_MIGRATE_FAIL;\n\nout:\n\tif (page_nid != NUMA_NO_NODE)\n\t\ttask_numa_fault(last_cpupid, page_nid, 1, flags);\n\treturn 0;\n}\n\nstatic inline vm_fault_t create_huge_pmd(struct vm_fault *vmf)\n{\n\tif (vma_is_anonymous(vmf->vma))\n\t\treturn do_huge_pmd_anonymous_page(vmf);\n\tif (vmf->vma->vm_ops->huge_fault)\n\t\treturn vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);\n\treturn VM_FAULT_FALLBACK;\n}\n\n/* `inline' is required to avoid gcc 4.1.2 build error */\nstatic inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)\n{\n\tif (vma_is_anonymous(vmf->vma)) {\n\t\tif (userfaultfd_huge_pmd_wp(vmf->vma, orig_pmd))\n\t\t\treturn handle_userfault(vmf, VM_UFFD_WP);\n\t\treturn do_huge_pmd_wp_page(vmf, orig_pmd);\n\t}\n\tif (vmf->vma->vm_ops->huge_fault) {\n\t\tvm_fault_t ret = vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);\n\n\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\treturn ret;\n\t}\n\n\t/* COW or write-notify handled on pte level: split pmd. */\n\t__split_huge_pmd(vmf->vma, vmf->pmd, vmf->address, false, NULL);\n\n\treturn VM_FAULT_FALLBACK;\n}\n\nstatic vm_fault_t create_huge_pud(struct vm_fault *vmf)\n{\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&\t\t\t\\\n\tdefined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\n\t/* No support for anonymous transparent PUD pages yet */\n\tif (vma_is_anonymous(vmf->vma))\n\t\tgoto split;\n\tif (vmf->vma->vm_ops->huge_fault) {\n\t\tvm_fault_t ret = vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);\n\n\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\treturn ret;\n\t}\nsplit:\n\t/* COW or write-notify not handled on PUD level: split pud.*/\n\t__split_huge_pud(vmf->vma, vmf->pud, vmf->address);\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\treturn VM_FAULT_FALLBACK;\n}\n\nstatic vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t/* No support for anonymous transparent PUD pages yet */\n\tif (vma_is_anonymous(vmf->vma))\n\t\treturn VM_FAULT_FALLBACK;\n\tif (vmf->vma->vm_ops->huge_fault)\n\t\treturn vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PUD);\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\treturn VM_FAULT_FALLBACK;\n}\n\n/*\n * These routines also need to handle stuff like marking pages dirty\n * and/or accessed for architectures that don't do it in hardware (most\n * RISC architectures).  The early dirtying is also good on the i386.\n *\n * There is also a hook called \"update_mmu_cache()\" that architectures\n * with external mmu caches can use to update those (ie the Sparc or\n * PowerPC hashed page tables that act as extended TLBs).\n *\n * We enter with non-exclusive mmap_lock (to exclude vma changes, but allow\n * concurrent faults).\n *\n * The mmap_lock may have been released depending on flags and our return value.\n * See filemap_fault() and __lock_page_or_retry().\n */\nstatic vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n{\n\tpte_t entry;\n\n\tif (unlikely(pmd_none(*vmf->pmd))) {\n\t\t/*\n\t\t * Leave __pte_alloc() until later: because vm_ops->fault may\n\t\t * want to allocate huge page, and if we expose page table\n\t\t * for an instant, it will be difficult to retract from\n\t\t * concurrent faults and from rmap lookups.\n\t\t */\n\t\tvmf->pte = NULL;\n\t} else {\n\t\t/* See comment in pte_alloc_one_map() */\n\t\tif (pmd_devmap_trans_unstable(vmf->pmd))\n\t\t\treturn 0;\n\t\t/*\n\t\t * A regular pmd is established and it can't morph into a huge\n\t\t * pmd from under us anymore at this point because we hold the\n\t\t * mmap_lock read mode and khugepaged takes it in write mode.\n\t\t * So now it's safe to run pte_offset_map().\n\t\t */\n\t\tvmf->pte = pte_offset_map(vmf->pmd, vmf->address);\n\t\tvmf->orig_pte = *vmf->pte;\n\n\t\t/*\n\t\t * some architectures can have larger ptes than wordsize,\n\t\t * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and\n\t\t * CONFIG_32BIT=y, so READ_ONCE cannot guarantee atomic\n\t\t * accesses.  The code below just needs a consistent view\n\t\t * for the ifs and we later double check anyway with the\n\t\t * ptl lock held. So here a barrier will do.\n\t\t */\n\t\tbarrier();\n\t\tif (pte_none(vmf->orig_pte)) {\n\t\t\tpte_unmap(vmf->pte);\n\t\t\tvmf->pte = NULL;\n\t\t}\n\t}\n\n\tif (!vmf->pte) {\n\t\tif (vma_is_anonymous(vmf->vma))\n\t\t\treturn do_anonymous_page(vmf);\n\t\telse\n\t\t\treturn do_fault(vmf);\n\t}\n\n\tif (!pte_present(vmf->orig_pte))\n\t\treturn do_swap_page(vmf);\n\n\tif (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))\n\t\treturn do_numa_page(vmf);\n\n\tvmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);\n\tspin_lock(vmf->ptl);\n\tentry = vmf->orig_pte;\n\tif (unlikely(!pte_same(*vmf->pte, entry))) {\n\t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n\t\tgoto unlock;\n\t}\n\tif (vmf->flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry))\n\t\t\treturn do_wp_page(vmf);\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,\n\t\t\t\tvmf->flags & FAULT_FLAG_WRITE)) {\n\t\tupdate_mmu_cache(vmf->vma, vmf->address, vmf->pte);\n\t} else {\n\t\t/* Skip spurious TLB flush for retried page fault */\n\t\tif (vmf->flags & FAULT_FLAG_TRIED)\n\t\t\tgoto unlock;\n\t\t/*\n\t\t * This is needed only for protection faults but the arch code\n\t\t * is not yet telling us if this is a protection fault or not.\n\t\t * This still avoids useless tlb flushes for .text page faults\n\t\t * with threads.\n\t\t */\n\t\tif (vmf->flags & FAULT_FLAG_WRITE)\n\t\t\tflush_tlb_fix_spurious_fault(vmf->vma, vmf->address);\n\t}\nunlock:\n\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n\treturn 0;\n}\n\n/*\n * By the time we get here, we already hold the mm semaphore\n *\n * The mmap_lock may have been released depending on flags and our\n * return value.  See filemap_fault() and __lock_page_or_retry().\n */\nstatic vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int flags)\n{\n\tstruct vm_fault vmf = {\n\t\t.vma = vma,\n\t\t.address = address & PAGE_MASK,\n\t\t.flags = flags,\n\t\t.pgoff = linear_page_index(vma, address),\n\t\t.gfp_mask = __get_fault_gfp_mask(vma),\n\t};\n\tunsigned int dirty = flags & FAULT_FLAG_WRITE;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tvm_fault_t ret;\n\n\tpgd = pgd_offset(mm, address);\n\tp4d = p4d_alloc(mm, pgd, address);\n\tif (!p4d)\n\t\treturn VM_FAULT_OOM;\n\n\tvmf.pud = pud_alloc(mm, p4d, address);\n\tif (!vmf.pud)\n\t\treturn VM_FAULT_OOM;\nretry_pud:\n\tif (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {\n\t\tret = create_huge_pud(&vmf);\n\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\treturn ret;\n\t} else {\n\t\tpud_t orig_pud = *vmf.pud;\n\n\t\tbarrier();\n\t\tif (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {\n\n\t\t\t/* NUMA case for anonymous PUDs would go here */\n\n\t\t\tif (dirty && !pud_write(orig_pud)) {\n\t\t\t\tret = wp_huge_pud(&vmf, orig_pud);\n\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\t\t\treturn ret;\n\t\t\t} else {\n\t\t\t\thuge_pud_set_accessed(&vmf, orig_pud);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tvmf.pmd = pmd_alloc(mm, vmf.pud, address);\n\tif (!vmf.pmd)\n\t\treturn VM_FAULT_OOM;\n\n\t/* Huge pud page fault raced with pmd_alloc? */\n\tif (pud_trans_unstable(vmf.pud))\n\t\tgoto retry_pud;\n\n\tif (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {\n\t\tret = create_huge_pmd(&vmf);\n\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\treturn ret;\n\t} else {\n\t\tpmd_t orig_pmd = *vmf.pmd;\n\n\t\tbarrier();\n\t\tif (unlikely(is_swap_pmd(orig_pmd))) {\n\t\t\tVM_BUG_ON(thp_migration_supported() &&\n\t\t\t\t\t  !is_pmd_migration_entry(orig_pmd));\n\t\t\tif (is_pmd_migration_entry(orig_pmd))\n\t\t\t\tpmd_migration_entry_wait(mm, vmf.pmd);\n\t\t\treturn 0;\n\t\t}\n\t\tif (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {\n\t\t\tif (pmd_protnone(orig_pmd) && vma_is_accessible(vma))\n\t\t\t\treturn do_huge_pmd_numa_page(&vmf, orig_pmd);\n\n\t\t\tif (dirty && !pmd_write(orig_pmd)) {\n\t\t\t\tret = wp_huge_pmd(&vmf, orig_pmd);\n\t\t\t\tif (!(ret & VM_FAULT_FALLBACK))\n\t\t\t\t\treturn ret;\n\t\t\t} else {\n\t\t\t\thuge_pmd_set_accessed(&vmf, orig_pmd);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn handle_pte_fault(&vmf);\n}\n\n/**\n * mm_account_fault - Do page fault accountings\n *\n * @regs: the pt_regs struct pointer.  When set to NULL, will skip accounting\n *        of perf event counters, but we'll still do the per-task accounting to\n *        the task who triggered this page fault.\n * @address: the faulted address.\n * @flags: the fault flags.\n * @ret: the fault retcode.\n *\n * This will take care of most of the page fault accountings.  Meanwhile, it\n * will also include the PERF_COUNT_SW_PAGE_FAULTS_[MAJ|MIN] perf counter\n * updates.  However note that the handling of PERF_COUNT_SW_PAGE_FAULTS should\n * still be in per-arch page fault handlers at the entry of page fault.\n */\nstatic inline void mm_account_fault(struct pt_regs *regs,\n\t\t\t\t    unsigned long address, unsigned int flags,\n\t\t\t\t    vm_fault_t ret)\n{\n\tbool major;\n\n\t/*\n\t * We don't do accounting for some specific faults:\n\t *\n\t * - Unsuccessful faults (e.g. when the address wasn't valid).  That\n\t *   includes arch_vma_access_permitted() failing before reaching here.\n\t *   So this is not a \"this many hardware page faults\" counter.  We\n\t *   should use the hw profiling for that.\n\t *\n\t * - Incomplete faults (VM_FAULT_RETRY).  They will only be counted\n\t *   once they're completed.\n\t */\n\tif (ret & (VM_FAULT_ERROR | VM_FAULT_RETRY))\n\t\treturn;\n\n\t/*\n\t * We define the fault as a major fault when the final successful fault\n\t * is VM_FAULT_MAJOR, or if it retried (which implies that we couldn't\n\t * handle it immediately previously).\n\t */\n\tmajor = (ret & VM_FAULT_MAJOR) || (flags & FAULT_FLAG_TRIED);\n\n\tif (major)\n\t\tcurrent->maj_flt++;\n\telse\n\t\tcurrent->min_flt++;\n\n\t/*\n\t * If the fault is done for GUP, regs will be NULL.  We only do the\n\t * accounting for the per thread fault counters who triggered the\n\t * fault, and we skip the perf event updates.\n\t */\n\tif (!regs)\n\t\treturn;\n\n\tif (major)\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);\n\telse\n\t\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);\n}\n\n/*\n * By the time we get here, we already hold the mm semaphore\n *\n * The mmap_lock may have been released depending on flags and our\n * return value.  See filemap_fault() and __lock_page_or_retry().\n */\nvm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,\n\t\t\t   unsigned int flags, struct pt_regs *regs)\n{\n\tvm_fault_t ret;\n\n\t__set_current_state(TASK_RUNNING);\n\n\tcount_vm_event(PGFAULT);\n\tcount_memcg_event_mm(vma->vm_mm, PGFAULT);\n\n\t/* do counter updates before entering really critical section. */\n\tcheck_sync_rss_stat(current);\n\n\tif (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,\n\t\t\t\t\t    flags & FAULT_FLAG_INSTRUCTION,\n\t\t\t\t\t    flags & FAULT_FLAG_REMOTE))\n\t\treturn VM_FAULT_SIGSEGV;\n\n\t/*\n\t * Enable the memcg OOM handling for faults triggered in user\n\t * space.  Kernel faults are handled more gracefully.\n\t */\n\tif (flags & FAULT_FLAG_USER)\n\t\tmem_cgroup_enter_user_fault();\n\n\tif (unlikely(is_vm_hugetlb_page(vma)))\n\t\tret = hugetlb_fault(vma->vm_mm, vma, address, flags);\n\telse\n\t\tret = __handle_mm_fault(vma, address, flags);\n\n\tif (flags & FAULT_FLAG_USER) {\n\t\tmem_cgroup_exit_user_fault();\n\t\t/*\n\t\t * The task may have entered a memcg OOM situation but\n\t\t * if the allocation error was handled gracefully (no\n\t\t * VM_FAULT_OOM), there is no need to kill anything.\n\t\t * Just clean up the OOM state peacefully.\n\t\t */\n\t\tif (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))\n\t\t\tmem_cgroup_oom_synchronize(false);\n\t}\n\n\tmm_account_fault(regs, address, flags, ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(handle_mm_fault);\n\n#ifndef __PAGETABLE_P4D_FOLDED\n/*\n * Allocate p4d page table.\n * We've already handled the fast-path in-line.\n */\nint __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)\n{\n\tp4d_t *new = p4d_alloc_one(mm, address);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tsmp_wmb(); /* See comment in __pte_alloc */\n\n\tspin_lock(&mm->page_table_lock);\n\tif (pgd_present(*pgd))\t\t/* Another has populated it */\n\t\tp4d_free(mm, new);\n\telse\n\t\tpgd_populate(mm, pgd, new);\n\tspin_unlock(&mm->page_table_lock);\n\treturn 0;\n}\n#endif /* __PAGETABLE_P4D_FOLDED */\n\n#ifndef __PAGETABLE_PUD_FOLDED\n/*\n * Allocate page upper directory.\n * We've already handled the fast-path in-line.\n */\nint __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)\n{\n\tpud_t *new = pud_alloc_one(mm, address);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tsmp_wmb(); /* See comment in __pte_alloc */\n\n\tspin_lock(&mm->page_table_lock);\n\tif (!p4d_present(*p4d)) {\n\t\tmm_inc_nr_puds(mm);\n\t\tp4d_populate(mm, p4d, new);\n\t} else\t/* Another has populated it */\n\t\tpud_free(mm, new);\n\tspin_unlock(&mm->page_table_lock);\n\treturn 0;\n}\n#endif /* __PAGETABLE_PUD_FOLDED */\n\n#ifndef __PAGETABLE_PMD_FOLDED\n/*\n * Allocate page middle directory.\n * We've already handled the fast-path in-line.\n */\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\tspinlock_t *ptl;\n\tpmd_t *new = pmd_alloc_one(mm, address);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tsmp_wmb(); /* See comment in __pte_alloc */\n\n\tptl = pud_lock(mm, pud);\n\tif (!pud_present(*pud)) {\n\t\tmm_inc_nr_pmds(mm);\n\t\tpud_populate(mm, pud, new);\n\t} else\t/* Another has populated it */\n\t\tpmd_free(mm, new);\n\tspin_unlock(ptl);\n\treturn 0;\n}\n#endif /* __PAGETABLE_PMD_FOLDED */\n\nint follow_pte(struct mm_struct *mm, unsigned long address,\n\t       struct mmu_notifier_range *range, pte_t **ptepp, pmd_t **pmdpp,\n\t       spinlock_t **ptlp)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *ptep;\n\n\tpgd = pgd_offset(mm, address);\n\tif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\n\t\tgoto out;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))\n\t\tgoto out;\n\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud) || unlikely(pud_bad(*pud)))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\n\tif (pmd_huge(*pmd)) {\n\t\tif (!pmdpp)\n\t\t\tgoto out;\n\n\t\tif (range) {\n\t\t\tmmu_notifier_range_init(range, MMU_NOTIFY_CLEAR, 0,\n\t\t\t\t\t\tNULL, mm, address & PMD_MASK,\n\t\t\t\t\t\t(address & PMD_MASK) + PMD_SIZE);\n\t\t\tmmu_notifier_invalidate_range_start(range);\n\t\t}\n\t\t*ptlp = pmd_lock(mm, pmd);\n\t\tif (pmd_huge(*pmd)) {\n\t\t\t*pmdpp = pmd;\n\t\t\treturn 0;\n\t\t}\n\t\tspin_unlock(*ptlp);\n\t\tif (range)\n\t\t\tmmu_notifier_invalidate_range_end(range);\n\t}\n\n\tif (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))\n\t\tgoto out;\n\n\tif (range) {\n\t\tmmu_notifier_range_init(range, MMU_NOTIFY_CLEAR, 0, NULL, mm,\n\t\t\t\t\taddress & PAGE_MASK,\n\t\t\t\t\t(address & PAGE_MASK) + PAGE_SIZE);\n\t\tmmu_notifier_invalidate_range_start(range);\n\t}\n\tptep = pte_offset_map_lock(mm, pmd, address, ptlp);\n\tif (!pte_present(*ptep))\n\t\tgoto unlock;\n\t*ptepp = ptep;\n\treturn 0;\nunlock:\n\tpte_unmap_unlock(ptep, *ptlp);\n\tif (range)\n\t\tmmu_notifier_invalidate_range_end(range);\nout:\n\treturn -EINVAL;\n}\n\n/**\n * follow_pfn - look up PFN at a user virtual address\n * @vma: memory mapping\n * @address: user virtual address\n * @pfn: location to store found PFN\n *\n * Only IO mappings and raw PFN mappings are allowed.\n *\n * Return: zero and the pfn at @pfn on success, -ve otherwise.\n */\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn)\n{\n\tint ret = -EINVAL;\n\tspinlock_t *ptl;\n\tpte_t *ptep;\n\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\treturn ret;\n\n\tret = follow_pte(vma->vm_mm, address, NULL, &ptep, NULL, &ptl);\n\tif (ret)\n\t\treturn ret;\n\t*pfn = pte_pfn(*ptep);\n\tpte_unmap_unlock(ptep, ptl);\n\treturn 0;\n}\nEXPORT_SYMBOL(follow_pfn);\n\n#ifdef CONFIG_HAVE_IOREMAP_PROT\nint follow_phys(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int flags,\n\t\tunsigned long *prot, resource_size_t *phys)\n{\n\tint ret = -EINVAL;\n\tpte_t *ptep, pte;\n\tspinlock_t *ptl;\n\n\tif (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))\n\t\tgoto out;\n\n\tif (follow_pte(vma->vm_mm, address, NULL, &ptep, NULL, &ptl))\n\t\tgoto out;\n\tpte = *ptep;\n\n\tif ((flags & FOLL_WRITE) && !pte_write(pte))\n\t\tgoto unlock;\n\n\t*prot = pgprot_val(pte_pgprot(pte));\n\t*phys = (resource_size_t)pte_pfn(pte) << PAGE_SHIFT;\n\n\tret = 0;\nunlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn ret;\n}\n\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write)\n{\n\tresource_size_t phys_addr;\n\tunsigned long prot = 0;\n\tvoid __iomem *maddr;\n\tint offset = addr & (PAGE_SIZE-1);\n\n\tif (follow_phys(vma, addr, write, &prot, &phys_addr))\n\t\treturn -EINVAL;\n\n\tmaddr = ioremap_prot(phys_addr, PAGE_ALIGN(len + offset), prot);\n\tif (!maddr)\n\t\treturn -ENOMEM;\n\n\tif (write)\n\t\tmemcpy_toio(maddr + offset, buf, len);\n\telse\n\t\tmemcpy_fromio(buf, maddr + offset, len);\n\tiounmap(maddr);\n\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(generic_access_phys);\n#endif\n\n/*\n * Access another process' address space as given in mm.\n */\nint __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,\n\t\t       int len, unsigned int gup_flags)\n{\n\tstruct vm_area_struct *vma;\n\tvoid *old_buf = buf;\n\tint write = gup_flags & FOLL_WRITE;\n\n\tif (mmap_read_lock_killable(mm))\n\t\treturn 0;\n\n\t/* ignore errors, just check how much was successfully transferred */\n\twhile (len) {\n\t\tint bytes, ret, offset;\n\t\tvoid *maddr;\n\t\tstruct page *page = NULL;\n\n\t\tret = get_user_pages_remote(mm, addr, 1,\n\t\t\t\tgup_flags, &page, &vma, NULL);\n\t\tif (ret <= 0) {\n#ifndef CONFIG_HAVE_IOREMAP_PROT\n\t\t\tbreak;\n#else\n\t\t\t/*\n\t\t\t * Check if this is a VM_IO | VM_PFNMAP VMA, which\n\t\t\t * we can access using slightly different code.\n\t\t\t */\n\t\t\tvma = find_vma(mm, addr);\n\t\t\tif (!vma || vma->vm_start > addr)\n\t\t\t\tbreak;\n\t\t\tif (vma->vm_ops && vma->vm_ops->access)\n\t\t\t\tret = vma->vm_ops->access(vma, addr, buf,\n\t\t\t\t\t\t\t  len, write);\n\t\t\tif (ret <= 0)\n\t\t\t\tbreak;\n\t\t\tbytes = ret;\n#endif\n\t\t} else {\n\t\t\tbytes = len;\n\t\t\toffset = addr & (PAGE_SIZE-1);\n\t\t\tif (bytes > PAGE_SIZE-offset)\n\t\t\t\tbytes = PAGE_SIZE-offset;\n\n\t\t\tmaddr = kmap(page);\n\t\t\tif (write) {\n\t\t\t\tcopy_to_user_page(vma, page, addr,\n\t\t\t\t\t\t  maddr + offset, buf, bytes);\n\t\t\t\tset_page_dirty_lock(page);\n\t\t\t} else {\n\t\t\t\tcopy_from_user_page(vma, page, addr,\n\t\t\t\t\t\t    buf, maddr + offset, bytes);\n\t\t\t}\n\t\t\tkunmap(page);\n\t\t\tput_page(page);\n\t\t}\n\t\tlen -= bytes;\n\t\tbuf += bytes;\n\t\taddr += bytes;\n\t}\n\tmmap_read_unlock(mm);\n\n\treturn buf - old_buf;\n}\n\n/**\n * access_remote_vm - access another process' address space\n * @mm:\t\tthe mm_struct of the target address space\n * @addr:\tstart address to access\n * @buf:\tsource or destination buffer\n * @len:\tnumber of bytes to transfer\n * @gup_flags:\tflags modifying lookup behaviour\n *\n * The caller must hold a reference on @mm.\n *\n * Return: number of bytes copied from source to destination.\n */\nint access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\treturn __access_remote_vm(mm, addr, buf, len, gup_flags);\n}\n\n/*\n * Access another process' address space.\n * Source/target buffer must be kernel space,\n * Do not walk the page table directly, use get_user_pages\n */\nint access_process_vm(struct task_struct *tsk, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags)\n{\n\tstruct mm_struct *mm;\n\tint ret;\n\n\tmm = get_task_mm(tsk);\n\tif (!mm)\n\t\treturn 0;\n\n\tret = __access_remote_vm(mm, addr, buf, len, gup_flags);\n\n\tmmput(mm);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(access_process_vm);\n\n/*\n * Print the name of a VMA.\n */\nvoid print_vma_addr(char *prefix, unsigned long ip)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\n\t/*\n\t * we might be running from an atomic context so we cannot sleep\n\t */\n\tif (!mmap_read_trylock(mm))\n\t\treturn;\n\n\tvma = find_vma(mm, ip);\n\tif (vma && vma->vm_file) {\n\t\tstruct file *f = vma->vm_file;\n\t\tchar *buf = (char *)__get_free_page(GFP_NOWAIT);\n\t\tif (buf) {\n\t\t\tchar *p;\n\n\t\t\tp = file_path(f, buf, PAGE_SIZE);\n\t\t\tif (IS_ERR(p))\n\t\t\t\tp = \"?\";\n\t\t\tprintk(\"%s%s[%lx+%lx]\", prefix, kbasename(p),\n\t\t\t\t\tvma->vm_start,\n\t\t\t\t\tvma->vm_end - vma->vm_start);\n\t\t\tfree_page((unsigned long)buf);\n\t\t}\n\t}\n\tmmap_read_unlock(mm);\n}\n\n#if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)\nvoid __might_fault(const char *file, int line)\n{\n\t/*\n\t * Some code (nfs/sunrpc) uses socket ops on kernel memory while\n\t * holding the mmap_lock, this is safe because kernel memory doesn't\n\t * get paged out, therefore we'll never actually fault, and the\n\t * below annotations will generate false positives.\n\t */\n\tif (uaccess_kernel())\n\t\treturn;\n\tif (pagefault_disabled())\n\t\treturn;\n\t__might_sleep(file, line, 0);\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP)\n\tif (current->mm)\n\t\tmight_lock_read(&current->mm->mmap_lock);\n#endif\n}\nEXPORT_SYMBOL(__might_fault);\n#endif\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\n/*\n * Process all subpages of the specified huge page with the specified\n * operation.  The target subpage will be processed last to keep its\n * cache lines hot.\n */\nstatic inline void process_huge_page(\n\tunsigned long addr_hint, unsigned int pages_per_huge_page,\n\tvoid (*process_subpage)(unsigned long addr, int idx, void *arg),\n\tvoid *arg)\n{\n\tint i, n, base, l;\n\tunsigned long addr = addr_hint &\n\t\t~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);\n\n\t/* Process target subpage last to keep its cache lines hot */\n\tmight_sleep();\n\tn = (addr_hint - addr) / PAGE_SIZE;\n\tif (2 * n <= pages_per_huge_page) {\n\t\t/* If target subpage in first half of huge page */\n\t\tbase = 0;\n\t\tl = n;\n\t\t/* Process subpages at the end of huge page */\n\t\tfor (i = pages_per_huge_page - 1; i >= 2 * n; i--) {\n\t\t\tcond_resched();\n\t\t\tprocess_subpage(addr + i * PAGE_SIZE, i, arg);\n\t\t}\n\t} else {\n\t\t/* If target subpage in second half of huge page */\n\t\tbase = pages_per_huge_page - 2 * (pages_per_huge_page - n);\n\t\tl = pages_per_huge_page - n;\n\t\t/* Process subpages at the begin of huge page */\n\t\tfor (i = 0; i < base; i++) {\n\t\t\tcond_resched();\n\t\t\tprocess_subpage(addr + i * PAGE_SIZE, i, arg);\n\t\t}\n\t}\n\t/*\n\t * Process remaining subpages in left-right-left-right pattern\n\t * towards the target subpage\n\t */\n\tfor (i = 0; i < l; i++) {\n\t\tint left_idx = base + i;\n\t\tint right_idx = base + 2 * l - 1 - i;\n\n\t\tcond_resched();\n\t\tprocess_subpage(addr + left_idx * PAGE_SIZE, left_idx, arg);\n\t\tcond_resched();\n\t\tprocess_subpage(addr + right_idx * PAGE_SIZE, right_idx, arg);\n\t}\n}\n\nstatic void clear_gigantic_page(struct page *page,\n\t\t\t\tunsigned long addr,\n\t\t\t\tunsigned int pages_per_huge_page)\n{\n\tint i;\n\tstruct page *p = page;\n\n\tmight_sleep();\n\tfor (i = 0; i < pages_per_huge_page;\n\t     i++, p = mem_map_next(p, page, i)) {\n\t\tcond_resched();\n\t\tclear_user_highpage(p, addr + i * PAGE_SIZE);\n\t}\n}\n\nstatic void clear_subpage(unsigned long addr, int idx, void *arg)\n{\n\tstruct page *page = arg;\n\n\tclear_user_highpage(page + idx, addr);\n}\n\nvoid clear_huge_page(struct page *page,\n\t\t     unsigned long addr_hint, unsigned int pages_per_huge_page)\n{\n\tunsigned long addr = addr_hint &\n\t\t~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);\n\n\tif (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {\n\t\tclear_gigantic_page(page, addr, pages_per_huge_page);\n\t\treturn;\n\t}\n\n\tprocess_huge_page(addr_hint, pages_per_huge_page, clear_subpage, page);\n}\n\nstatic void copy_user_gigantic_page(struct page *dst, struct page *src,\n\t\t\t\t    unsigned long addr,\n\t\t\t\t    struct vm_area_struct *vma,\n\t\t\t\t    unsigned int pages_per_huge_page)\n{\n\tint i;\n\tstruct page *dst_base = dst;\n\tstruct page *src_base = src;\n\n\tfor (i = 0; i < pages_per_huge_page; ) {\n\t\tcond_resched();\n\t\tcopy_user_highpage(dst, src, addr + i*PAGE_SIZE, vma);\n\n\t\ti++;\n\t\tdst = mem_map_next(dst, dst_base, i);\n\t\tsrc = mem_map_next(src, src_base, i);\n\t}\n}\n\nstruct copy_subpage_arg {\n\tstruct page *dst;\n\tstruct page *src;\n\tstruct vm_area_struct *vma;\n};\n\nstatic void copy_subpage(unsigned long addr, int idx, void *arg)\n{\n\tstruct copy_subpage_arg *copy_arg = arg;\n\n\tcopy_user_highpage(copy_arg->dst + idx, copy_arg->src + idx,\n\t\t\t   addr, copy_arg->vma);\n}\n\nvoid copy_user_huge_page(struct page *dst, struct page *src,\n\t\t\t unsigned long addr_hint, struct vm_area_struct *vma,\n\t\t\t unsigned int pages_per_huge_page)\n{\n\tunsigned long addr = addr_hint &\n\t\t~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);\n\tstruct copy_subpage_arg arg = {\n\t\t.dst = dst,\n\t\t.src = src,\n\t\t.vma = vma,\n\t};\n\n\tif (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {\n\t\tcopy_user_gigantic_page(dst, src, addr, vma,\n\t\t\t\t\tpages_per_huge_page);\n\t\treturn;\n\t}\n\n\tprocess_huge_page(addr_hint, pages_per_huge_page, copy_subpage, &arg);\n}\n\nlong copy_huge_page_from_user(struct page *dst_page,\n\t\t\t\tconst void __user *usr_src,\n\t\t\t\tunsigned int pages_per_huge_page,\n\t\t\t\tbool allow_pagefault)\n{\n\tvoid *src = (void *)usr_src;\n\tvoid *page_kaddr;\n\tunsigned long i, rc = 0;\n\tunsigned long ret_val = pages_per_huge_page * PAGE_SIZE;\n\n\tfor (i = 0; i < pages_per_huge_page; i++) {\n\t\tif (allow_pagefault)\n\t\t\tpage_kaddr = kmap(dst_page + i);\n\t\telse\n\t\t\tpage_kaddr = kmap_atomic(dst_page + i);\n\t\trc = copy_from_user(page_kaddr,\n\t\t\t\t(const void __user *)(src + i * PAGE_SIZE),\n\t\t\t\tPAGE_SIZE);\n\t\tif (allow_pagefault)\n\t\t\tkunmap(dst_page + i);\n\t\telse\n\t\t\tkunmap_atomic(page_kaddr);\n\n\t\tret_val -= (PAGE_SIZE - rc);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n\treturn ret_val;\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\n\n#if USE_SPLIT_PTE_PTLOCKS && ALLOC_SPLIT_PTLOCKS\n\nstatic struct kmem_cache *page_ptl_cachep;\n\nvoid __init ptlock_cache_init(void)\n{\n\tpage_ptl_cachep = kmem_cache_create(\"page->ptl\", sizeof(spinlock_t), 0,\n\t\t\tSLAB_PANIC, NULL);\n}\n\nbool ptlock_alloc(struct page *page)\n{\n\tspinlock_t *ptl;\n\n\tptl = kmem_cache_alloc(page_ptl_cachep, GFP_KERNEL);\n\tif (!ptl)\n\t\treturn false;\n\tpage->ptl = ptl;\n\treturn true;\n}\n\nvoid ptlock_free(struct page *page)\n{\n\tkmem_cache_free(page_ptl_cachep, page->ptl);\n}\n#endif\n"}, "2": {"id": 2, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}, "3": {"id": 3, "path": "/src/include/linux/pgtable.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_PGTABLE_H\n#define _LINUX_PGTABLE_H\n\n#include <linux/pfn.h>\n#include <asm/pgtable.h>\n\n#ifndef __ASSEMBLY__\n#ifdef CONFIG_MMU\n\n#include <linux/mm_types.h>\n#include <linux/bug.h>\n#include <linux/errno.h>\n#include <asm-generic/pgtable_uffd.h>\n\n#if 5 - defined(__PAGETABLE_P4D_FOLDED) - defined(__PAGETABLE_PUD_FOLDED) - \\\n\tdefined(__PAGETABLE_PMD_FOLDED) != CONFIG_PGTABLE_LEVELS\n#error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{P4D,PUD,PMD}_FOLDED\n#endif\n\n/*\n * On almost all architectures and configurations, 0 can be used as the\n * upper ceiling to free_pgtables(): on many architectures it has the same\n * effect as using TASK_SIZE.  However, there is one configuration which\n * must impose a more careful limit, to avoid freeing kernel pgtables.\n */\n#ifndef USER_PGTABLES_CEILING\n#define USER_PGTABLES_CEILING\t0UL\n#endif\n\n/* Number of base pages in a second level leaf page */\n#define PMD_PAGE_ORDER\t(PMD_SHIFT - PAGE_SHIFT)\n\n/*\n * A page table page can be thought of an array like this: pXd_t[PTRS_PER_PxD]\n *\n * The pXx_index() functions return the index of the entry in the page\n * table page which would control the given virtual address\n *\n * As these functions may be used by the same code for different levels of\n * the page table folding, they are always available, regardless of\n * CONFIG_PGTABLE_LEVELS value. For the folded levels they simply return 0\n * because in such cases PTRS_PER_PxD equals 1.\n */\n\nstatic inline unsigned long pte_index(unsigned long address)\n{\n\treturn (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);\n}\n\n#ifndef pmd_index\nstatic inline unsigned long pmd_index(unsigned long address)\n{\n\treturn (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);\n}\n#define pmd_index pmd_index\n#endif\n\n#ifndef pud_index\nstatic inline unsigned long pud_index(unsigned long address)\n{\n\treturn (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);\n}\n#define pud_index pud_index\n#endif\n\n#ifndef pgd_index\n/* Must be a compile-time constant, so implement it as a macro */\n#define pgd_index(a)  (((a) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))\n#endif\n\n#ifndef pte_offset_kernel\nstatic inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)\n{\n\treturn (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);\n}\n#define pte_offset_kernel pte_offset_kernel\n#endif\n\n#if defined(CONFIG_HIGHPTE)\n#define pte_offset_map(dir, address)\t\t\t\t\\\n\t((pte_t *)kmap_atomic(pmd_page(*(dir))) +\t\t\\\n\t pte_index((address)))\n#define pte_unmap(pte) kunmap_atomic((pte))\n#else\n#define pte_offset_map(dir, address)\tpte_offset_kernel((dir), (address))\n#define pte_unmap(pte) ((void)(pte))\t/* NOP */\n#endif\n\n/* Find an entry in the second-level page table.. */\n#ifndef pmd_offset\nstatic inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)\n{\n\treturn (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);\n}\n#define pmd_offset pmd_offset\n#endif\n\n#ifndef pud_offset\nstatic inline pud_t *pud_offset(p4d_t *p4d, unsigned long address)\n{\n\treturn (pud_t *)p4d_page_vaddr(*p4d) + pud_index(address);\n}\n#define pud_offset pud_offset\n#endif\n\nstatic inline pgd_t *pgd_offset_pgd(pgd_t *pgd, unsigned long address)\n{\n\treturn (pgd + pgd_index(address));\n};\n\n/*\n * a shortcut to get a pgd_t in a given mm\n */\n#ifndef pgd_offset\n#define pgd_offset(mm, address)\t\tpgd_offset_pgd((mm)->pgd, (address))\n#endif\n\n/*\n * a shortcut which implies the use of the kernel's pgd, instead\n * of a process's\n */\n#ifndef pgd_offset_k\n#define pgd_offset_k(address)\t\tpgd_offset(&init_mm, (address))\n#endif\n\n/*\n * In many cases it is known that a virtual address is mapped at PMD or PTE\n * level, so instead of traversing all the page table levels, we can get a\n * pointer to the PMD entry in user or kernel page table or translate a virtual\n * address to the pointer in the PTE in the kernel page tables with simple\n * helpers.\n */\nstatic inline pmd_t *pmd_off(struct mm_struct *mm, unsigned long va)\n{\n\treturn pmd_offset(pud_offset(p4d_offset(pgd_offset(mm, va), va), va), va);\n}\n\nstatic inline pmd_t *pmd_off_k(unsigned long va)\n{\n\treturn pmd_offset(pud_offset(p4d_offset(pgd_offset_k(va), va), va), va);\n}\n\nstatic inline pte_t *virt_to_kpte(unsigned long vaddr)\n{\n\tpmd_t *pmd = pmd_off_k(vaddr);\n\n\treturn pmd_none(*pmd) ? NULL : pte_offset_kernel(pmd, vaddr);\n}\n\n#ifndef __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS\nextern int ptep_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pte_t *ptep,\n\t\t\t\t pte_t entry, int dirty);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nextern int pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pmd_t *pmdp,\n\t\t\t\t pmd_t entry, int dirty);\nextern int pudp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pud_t *pudp,\n\t\t\t\t pud_t entry, int dirty);\n#else\nstatic inline int pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address, pmd_t *pmdp,\n\t\t\t\t\tpmd_t entry, int dirty)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\nstatic inline int pudp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address, pud_t *pudp,\n\t\t\t\t\tpud_t entry, int dirty)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG\nstatic inline int ptep_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pte_t *ptep)\n{\n\tpte_t pte = *ptep;\n\tint r = 1;\n\tif (!pte_young(pte))\n\t\tr = 0;\n\telse\n\t\tset_pte_at(vma->vm_mm, address, ptep, pte_mkold(pte));\n\treturn r;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pmd_t *pmdp)\n{\n\tpmd_t pmd = *pmdp;\n\tint r = 1;\n\tif (!pmd_young(pmd))\n\t\tr = 0;\n\telse\n\t\tset_pmd_at(vma->vm_mm, address, pmdp, pmd_mkold(pmd));\n\treturn r;\n}\n#else\nstatic inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pmd_t *pmdp)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH\nint ptep_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t   unsigned long address, pte_t *ptep);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nextern int pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, pmd_t *pmdp);\n#else\n/*\n * Despite relevant to THP only, this API is called from generic rmap code\n * under PageTransHuge(), hence needs a dummy implementation for !THP\n */\nstatic inline int pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address, pmd_t *pmdp)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR\nstatic inline pte_t ptep_get_and_clear(struct mm_struct *mm,\n\t\t\t\t       unsigned long address,\n\t\t\t\t       pte_t *ptep)\n{\n\tpte_t pte = *ptep;\n\tpte_clear(mm, address, ptep);\n\treturn pte;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_GET\nstatic inline pte_t ptep_get(pte_t *ptep)\n{\n\treturn READ_ONCE(*ptep);\n}\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR\nstatic inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pmd_t *pmdp)\n{\n\tpmd_t pmd = *pmdp;\n\tpmd_clear(pmdp);\n\treturn pmd;\n}\n#endif /* __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR */\n#ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR\nstatic inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pud_t *pudp)\n{\n\tpud_t pud = *pudp;\n\n\tpud_clear(pudp);\n\treturn pud;\n}\n#endif /* __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR */\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR_FULL\nstatic inline pmd_t pmdp_huge_get_and_clear_full(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address, pmd_t *pmdp,\n\t\t\t\t\t    int full)\n{\n\treturn pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR_FULL\nstatic inline pud_t pudp_huge_get_and_clear_full(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t\t    int full)\n{\n\treturn pudp_huge_get_and_clear(mm, address, pudp);\n}\n#endif\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n#ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL\nstatic inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address, pte_t *ptep,\n\t\t\t\t\t    int full)\n{\n\tpte_t pte;\n\tpte = ptep_get_and_clear(mm, address, ptep);\n\treturn pte;\n}\n#endif\n\n\n/*\n * If two threads concurrently fault at the same page, the thread that\n * won the race updates the PTE and its local TLB/Cache. The other thread\n * gives up, simply does nothing, and continues; on architectures where\n * software can update TLB,  local TLB can be updated here to avoid next page\n * fault. This function updates TLB only, do nothing with cache or others.\n * It is the difference with function update_mmu_cache.\n */\n#ifndef __HAVE_ARCH_UPDATE_MMU_TLB\nstatic inline void update_mmu_tlb(struct vm_area_struct *vma,\n\t\t\t\tunsigned long address, pte_t *ptep)\n{\n}\n#define __HAVE_ARCH_UPDATE_MMU_TLB\n#endif\n\n/*\n * Some architectures may be able to avoid expensive synchronization\n * primitives when modifications are made to PTE's which are already\n * not present, or in the process of an address space destruction.\n */\n#ifndef __HAVE_ARCH_PTE_CLEAR_NOT_PRESENT_FULL\nstatic inline void pte_clear_not_present_full(struct mm_struct *mm,\n\t\t\t\t\t      unsigned long address,\n\t\t\t\t\t      pte_t *ptep,\n\t\t\t\t\t      int full)\n{\n\tpte_clear(mm, address, ptep);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_CLEAR_FLUSH\nextern pte_t ptep_clear_flush(struct vm_area_struct *vma,\n\t\t\t      unsigned long address,\n\t\t\t      pte_t *ptep);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_HUGE_CLEAR_FLUSH\nextern pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma,\n\t\t\t      unsigned long address,\n\t\t\t      pmd_t *pmdp);\nextern pud_t pudp_huge_clear_flush(struct vm_area_struct *vma,\n\t\t\t      unsigned long address,\n\t\t\t      pud_t *pudp);\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_SET_WRPROTECT\nstruct mm_struct;\nstatic inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)\n{\n\tpte_t old_pte = *ptep;\n\tset_pte_at(mm, address, ptep, pte_wrprotect(old_pte));\n}\n#endif\n\n/*\n * On some architectures hardware does not set page access bit when accessing\n * memory page, it is responsibilty of software setting this bit. It brings\n * out extra page fault penalty to track page access bit. For optimization page\n * access bit can be set during all page fault flow on these arches.\n * To be differentiate with macro pte_mkyoung, this macro is used on platforms\n * where software maintains page access bit.\n */\n#ifndef pte_sw_mkyoung\nstatic inline pte_t pte_sw_mkyoung(pte_t pte)\n{\n\treturn pte;\n}\n#define pte_sw_mkyoung\tpte_sw_mkyoung\n#endif\n\n#ifndef pte_savedwrite\n#define pte_savedwrite pte_write\n#endif\n\n#ifndef pte_mk_savedwrite\n#define pte_mk_savedwrite pte_mkwrite\n#endif\n\n#ifndef pte_clear_savedwrite\n#define pte_clear_savedwrite pte_wrprotect\n#endif\n\n#ifndef pmd_savedwrite\n#define pmd_savedwrite pmd_write\n#endif\n\n#ifndef pmd_mk_savedwrite\n#define pmd_mk_savedwrite pmd_mkwrite\n#endif\n\n#ifndef pmd_clear_savedwrite\n#define pmd_clear_savedwrite pmd_wrprotect\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_SET_WRPROTECT\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic inline void pmdp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pmd_t *pmdp)\n{\n\tpmd_t old_pmd = *pmdp;\n\tset_pmd_at(mm, address, pmdp, pmd_wrprotect(old_pmd));\n}\n#else\nstatic inline void pmdp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pmd_t *pmdp)\n{\n\tBUILD_BUG();\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n#endif\n#ifndef __HAVE_ARCH_PUDP_SET_WRPROTECT\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\nstatic inline void pudp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pud_t *pudp)\n{\n\tpud_t old_pud = *pudp;\n\n\tset_pud_at(mm, address, pudp, pud_wrprotect(old_pud));\n}\n#else\nstatic inline void pudp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pud_t *pudp)\n{\n\tBUILD_BUG();\n}\n#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */\n#endif\n\n#ifndef pmdp_collapse_flush\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nextern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pmd_t *pmdp);\n#else\nstatic inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpmd_t *pmdp)\n{\n\tBUILD_BUG();\n\treturn *pmdp;\n}\n#define pmdp_collapse_flush pmdp_collapse_flush\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n#endif\n\n#ifndef __HAVE_ARCH_PGTABLE_DEPOSIT\nextern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,\n\t\t\t\t       pgtable_t pgtable);\n#endif\n\n#ifndef __HAVE_ARCH_PGTABLE_WITHDRAW\nextern pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n/*\n * This is an implementation of pmdp_establish() that is only suitable for an\n * architecture that doesn't have hardware dirty/accessed bits. In this case we\n * can't race with CPU which sets these bits and non-atomic aproach is fine.\n */\nstatic inline pmd_t generic_pmdp_establish(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmdp, pmd_t pmd)\n{\n\tpmd_t old_pmd = *pmdp;\n\tset_pmd_at(vma->vm_mm, address, pmdp, pmd);\n\treturn old_pmd;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_INVALIDATE\nextern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,\n\t\t\t    pmd_t *pmdp);\n#endif\n\n#ifndef __HAVE_ARCH_PTE_SAME\nstatic inline int pte_same(pte_t pte_a, pte_t pte_b)\n{\n\treturn pte_val(pte_a) == pte_val(pte_b);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTE_UNUSED\n/*\n * Some architectures provide facilities to virtualization guests\n * so that they can flag allocated pages as unused. This allows the\n * host to transparently reclaim unused pages. This function returns\n * whether the pte's page is unused.\n */\nstatic inline int pte_unused(pte_t pte)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef pte_access_permitted\n#define pte_access_permitted(pte, write) \\\n\t(pte_present(pte) && (!(write) || pte_write(pte)))\n#endif\n\n#ifndef pmd_access_permitted\n#define pmd_access_permitted(pmd, write) \\\n\t(pmd_present(pmd) && (!(write) || pmd_write(pmd)))\n#endif\n\n#ifndef pud_access_permitted\n#define pud_access_permitted(pud, write) \\\n\t(pud_present(pud) && (!(write) || pud_write(pud)))\n#endif\n\n#ifndef p4d_access_permitted\n#define p4d_access_permitted(p4d, write) \\\n\t(p4d_present(p4d) && (!(write) || p4d_write(p4d)))\n#endif\n\n#ifndef pgd_access_permitted\n#define pgd_access_permitted(pgd, write) \\\n\t(pgd_present(pgd) && (!(write) || pgd_write(pgd)))\n#endif\n\n#ifndef __HAVE_ARCH_PMD_SAME\nstatic inline int pmd_same(pmd_t pmd_a, pmd_t pmd_b)\n{\n\treturn pmd_val(pmd_a) == pmd_val(pmd_b);\n}\n\nstatic inline int pud_same(pud_t pud_a, pud_t pud_b)\n{\n\treturn pud_val(pud_a) == pud_val(pud_b);\n}\n#endif\n\n#ifndef __HAVE_ARCH_P4D_SAME\nstatic inline int p4d_same(p4d_t p4d_a, p4d_t p4d_b)\n{\n\treturn p4d_val(p4d_a) == p4d_val(p4d_b);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PGD_SAME\nstatic inline int pgd_same(pgd_t pgd_a, pgd_t pgd_b)\n{\n\treturn pgd_val(pgd_a) == pgd_val(pgd_b);\n}\n#endif\n\n/*\n * Use set_p*_safe(), and elide TLB flushing, when confident that *no*\n * TLB flush will be required as a result of the \"set\". For example, use\n * in scenarios where it is known ahead of time that the routine is\n * setting non-present entries, or re-setting an existing entry to the\n * same value. Otherwise, use the typical \"set\" helpers and flush the\n * TLB.\n */\n#define set_pte_safe(ptep, pte) \\\n({ \\\n\tWARN_ON_ONCE(pte_present(*ptep) && !pte_same(*ptep, pte)); \\\n\tset_pte(ptep, pte); \\\n})\n\n#define set_pmd_safe(pmdp, pmd) \\\n({ \\\n\tWARN_ON_ONCE(pmd_present(*pmdp) && !pmd_same(*pmdp, pmd)); \\\n\tset_pmd(pmdp, pmd); \\\n})\n\n#define set_pud_safe(pudp, pud) \\\n({ \\\n\tWARN_ON_ONCE(pud_present(*pudp) && !pud_same(*pudp, pud)); \\\n\tset_pud(pudp, pud); \\\n})\n\n#define set_p4d_safe(p4dp, p4d) \\\n({ \\\n\tWARN_ON_ONCE(p4d_present(*p4dp) && !p4d_same(*p4dp, p4d)); \\\n\tset_p4d(p4dp, p4d); \\\n})\n\n#define set_pgd_safe(pgdp, pgd) \\\n({ \\\n\tWARN_ON_ONCE(pgd_present(*pgdp) && !pgd_same(*pgdp, pgd)); \\\n\tset_pgd(pgdp, pgd); \\\n})\n\n#ifndef __HAVE_ARCH_DO_SWAP_PAGE\n/*\n * Some architectures support metadata associated with a page. When a\n * page is being swapped out, this metadata must be saved so it can be\n * restored when the page is swapped back in. SPARC M7 and newer\n * processors support an ADI (Application Data Integrity) tag for the\n * page as metadata for the page. arch_do_swap_page() can restore this\n * metadata when a page is swapped back in.\n */\nstatic inline void arch_do_swap_page(struct mm_struct *mm,\n\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr,\n\t\t\t\t     pte_t pte, pte_t oldpte)\n{\n\n}\n#endif\n\n#ifndef __HAVE_ARCH_UNMAP_ONE\n/*\n * Some architectures support metadata associated with a page. When a\n * page is being swapped out, this metadata must be saved so it can be\n * restored when the page is swapped back in. SPARC M7 and newer\n * processors support an ADI (Application Data Integrity) tag for the\n * page as metadata for the page. arch_unmap_one() can save this\n * metadata on a swap-out of a page.\n */\nstatic inline int arch_unmap_one(struct mm_struct *mm,\n\t\t\t\t  struct vm_area_struct *vma,\n\t\t\t\t  unsigned long addr,\n\t\t\t\t  pte_t orig_pte)\n{\n\treturn 0;\n}\n#endif\n\n/*\n * Allow architectures to preserve additional metadata associated with\n * swapped-out pages. The corresponding __HAVE_ARCH_SWAP_* macros and function\n * prototypes must be defined in the arch-specific asm/pgtable.h file.\n */\n#ifndef __HAVE_ARCH_PREPARE_TO_SWAP\nstatic inline int arch_prepare_to_swap(struct page *page)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef __HAVE_ARCH_SWAP_INVALIDATE\nstatic inline void arch_swap_invalidate_page(int type, pgoff_t offset)\n{\n}\n\nstatic inline void arch_swap_invalidate_area(int type)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_SWAP_RESTORE\nstatic inline void arch_swap_restore(swp_entry_t entry, struct page *page)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_PGD_OFFSET_GATE\n#define pgd_offset_gate(mm, addr)\tpgd_offset(mm, addr)\n#endif\n\n#ifndef __HAVE_ARCH_MOVE_PTE\n#define move_pte(pte, prot, old_addr, new_addr)\t(pte)\n#endif\n\n#ifndef pte_accessible\n# define pte_accessible(mm, pte)\t((void)(pte), 1)\n#endif\n\n#ifndef flush_tlb_fix_spurious_fault\n#define flush_tlb_fix_spurious_fault(vma, address) flush_tlb_page(vma, address)\n#endif\n\n/*\n * When walking page tables, get the address of the next boundary,\n * or the end address of the range if that comes earlier.  Although no\n * vma end wraps to 0, rounded up __boundary may wrap to 0 throughout.\n */\n\n#define pgd_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + PGDIR_SIZE) & PGDIR_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n\n#ifndef p4d_addr_end\n#define p4d_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + P4D_SIZE) & P4D_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n#endif\n\n#ifndef pud_addr_end\n#define pud_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + PUD_SIZE) & PUD_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n#endif\n\n#ifndef pmd_addr_end\n#define pmd_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + PMD_SIZE) & PMD_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n#endif\n\n/*\n * When walking page tables, we usually want to skip any p?d_none entries;\n * and any p?d_bad entries - reporting the error before resetting to none.\n * Do the tests inline, but report and clear the bad entry in mm/memory.c.\n */\nvoid pgd_clear_bad(pgd_t *);\n\n#ifndef __PAGETABLE_P4D_FOLDED\nvoid p4d_clear_bad(p4d_t *);\n#else\n#define p4d_clear_bad(p4d)        do { } while (0)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\nvoid pud_clear_bad(pud_t *);\n#else\n#define pud_clear_bad(p4d)        do { } while (0)\n#endif\n\nvoid pmd_clear_bad(pmd_t *);\n\nstatic inline int pgd_none_or_clear_bad(pgd_t *pgd)\n{\n\tif (pgd_none(*pgd))\n\t\treturn 1;\n\tif (unlikely(pgd_bad(*pgd))) {\n\t\tpgd_clear_bad(pgd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline int p4d_none_or_clear_bad(p4d_t *p4d)\n{\n\tif (p4d_none(*p4d))\n\t\treturn 1;\n\tif (unlikely(p4d_bad(*p4d))) {\n\t\tp4d_clear_bad(p4d);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline int pud_none_or_clear_bad(pud_t *pud)\n{\n\tif (pud_none(*pud))\n\t\treturn 1;\n\tif (unlikely(pud_bad(*pud))) {\n\t\tpud_clear_bad(pud);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline int pmd_none_or_clear_bad(pmd_t *pmd)\n{\n\tif (pmd_none(*pmd))\n\t\treturn 1;\n\tif (unlikely(pmd_bad(*pmd))) {\n\t\tpmd_clear_bad(pmd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline pte_t __ptep_modify_prot_start(struct vm_area_struct *vma,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     pte_t *ptep)\n{\n\t/*\n\t * Get the current pte state, but zero it out to make it\n\t * non-present, preventing the hardware from asynchronously\n\t * updating it.\n\t */\n\treturn ptep_get_and_clear(vma->vm_mm, addr, ptep);\n}\n\nstatic inline void __ptep_modify_prot_commit(struct vm_area_struct *vma,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     pte_t *ptep, pte_t pte)\n{\n\t/*\n\t * The pte is non-present, so there's no hardware state to\n\t * preserve.\n\t */\n\tset_pte_at(vma->vm_mm, addr, ptep, pte);\n}\n\n#ifndef __HAVE_ARCH_PTEP_MODIFY_PROT_TRANSACTION\n/*\n * Start a pte protection read-modify-write transaction, which\n * protects against asynchronous hardware modifications to the pte.\n * The intention is not to prevent the hardware from making pte\n * updates, but to prevent any updates it may make from being lost.\n *\n * This does not protect against other software modifications of the\n * pte; the appropriate pte lock must be held over the transation.\n *\n * Note that this interface is intended to be batchable, meaning that\n * ptep_modify_prot_commit may not actually update the pte, but merely\n * queue the update to be done at some later time.  The update must be\n * actually committed before the pte lock is released, however.\n */\nstatic inline pte_t ptep_modify_prot_start(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   pte_t *ptep)\n{\n\treturn __ptep_modify_prot_start(vma, addr, ptep);\n}\n\n/*\n * Commit an update to a pte, leaving any hardware-controlled bits in\n * the PTE unmodified.\n */\nstatic inline void ptep_modify_prot_commit(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   pte_t *ptep, pte_t old_pte, pte_t pte)\n{\n\t__ptep_modify_prot_commit(vma, addr, ptep, pte);\n}\n#endif /* __HAVE_ARCH_PTEP_MODIFY_PROT_TRANSACTION */\n#endif /* CONFIG_MMU */\n\n/*\n * No-op macros that just return the current protection value. Defined here\n * because these macros can be used even if CONFIG_MMU is not defined.\n */\n\n#ifndef pgprot_nx\n#define pgprot_nx(prot)\t(prot)\n#endif\n\n#ifndef pgprot_noncached\n#define pgprot_noncached(prot)\t(prot)\n#endif\n\n#ifndef pgprot_writecombine\n#define pgprot_writecombine pgprot_noncached\n#endif\n\n#ifndef pgprot_writethrough\n#define pgprot_writethrough pgprot_noncached\n#endif\n\n#ifndef pgprot_device\n#define pgprot_device pgprot_noncached\n#endif\n\n#ifdef CONFIG_MMU\n#ifndef pgprot_modify\n#define pgprot_modify pgprot_modify\nstatic inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)\n{\n\tif (pgprot_val(oldprot) == pgprot_val(pgprot_noncached(oldprot)))\n\t\tnewprot = pgprot_noncached(newprot);\n\tif (pgprot_val(oldprot) == pgprot_val(pgprot_writecombine(oldprot)))\n\t\tnewprot = pgprot_writecombine(newprot);\n\tif (pgprot_val(oldprot) == pgprot_val(pgprot_device(oldprot)))\n\t\tnewprot = pgprot_device(newprot);\n\treturn newprot;\n}\n#endif\n#endif /* CONFIG_MMU */\n\n#ifndef pgprot_encrypted\n#define pgprot_encrypted(prot)\t(prot)\n#endif\n\n#ifndef pgprot_decrypted\n#define pgprot_decrypted(prot)\t(prot)\n#endif\n\n/*\n * A facility to provide lazy MMU batching.  This allows PTE updates and\n * page invalidations to be delayed until a call to leave lazy MMU mode\n * is issued.  Some architectures may benefit from doing this, and it is\n * beneficial for both shadow and direct mode hypervisors, which may batch\n * the PTE updates which happen during this window.  Note that using this\n * interface requires that read hazards be removed from the code.  A read\n * hazard could result in the direct mode hypervisor case, since the actual\n * write to the page tables may not yet have taken place, so reads though\n * a raw PTE pointer after it has been modified are not guaranteed to be\n * up to date.  This mode can only be entered and left under the protection of\n * the page table locks for all page tables which may be modified.  In the UP\n * case, this is required so that preemption is disabled, and in the SMP case,\n * it must synchronize the delayed page table writes properly on other CPUs.\n */\n#ifndef __HAVE_ARCH_ENTER_LAZY_MMU_MODE\n#define arch_enter_lazy_mmu_mode()\tdo {} while (0)\n#define arch_leave_lazy_mmu_mode()\tdo {} while (0)\n#define arch_flush_lazy_mmu_mode()\tdo {} while (0)\n#endif\n\n/*\n * A facility to provide batching of the reload of page tables and\n * other process state with the actual context switch code for\n * paravirtualized guests.  By convention, only one of the batched\n * update (lazy) modes (CPU, MMU) should be active at any given time,\n * entry should never be nested, and entry and exits should always be\n * paired.  This is for sanity of maintaining and reasoning about the\n * kernel code.  In this case, the exit (end of the context switch) is\n * in architecture-specific code, and so doesn't need a generic\n * definition.\n */\n#ifndef __HAVE_ARCH_START_CONTEXT_SWITCH\n#define arch_start_context_switch(prev)\tdo {} while (0)\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY\n#ifndef CONFIG_ARCH_ENABLE_THP_MIGRATION\nstatic inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline int pmd_swp_soft_dirty(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n#endif\n#else /* !CONFIG_HAVE_ARCH_SOFT_DIRTY */\nstatic inline int pte_soft_dirty(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline int pmd_soft_dirty(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline pte_t pte_mksoft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline pmd_t pmd_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline pte_t pte_clear_soft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline pte_t pte_swp_mksoft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline int pte_swp_soft_dirty(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline pte_t pte_swp_clear_soft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline int pmd_swp_soft_dirty(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n#endif\n\n#ifndef __HAVE_PFNMAP_TRACKING\n/*\n * Interfaces that can be used by architecture code to keep track of\n * memory type of pfn mappings specified by the remap_pfn_range,\n * vmf_insert_pfn.\n */\n\n/*\n * track_pfn_remap is called when a _new_ pfn mapping is being established\n * by remap_pfn_range() for physical range indicated by pfn and size.\n */\nstatic inline int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t\t  unsigned long pfn, unsigned long addr,\n\t\t\t\t  unsigned long size)\n{\n\treturn 0;\n}\n\n/*\n * track_pfn_insert is called when a _new_ single pfn is established\n * by vmf_insert_pfn().\n */\nstatic inline void track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t\t    pfn_t pfn)\n{\n}\n\n/*\n * track_pfn_copy is called when vma that is covering the pfnmap gets\n * copied through copy_page_range().\n */\nstatic inline int track_pfn_copy(struct vm_area_struct *vma)\n{\n\treturn 0;\n}\n\n/*\n * untrack_pfn is called while unmapping a pfnmap for a region.\n * untrack can be called for a specific region indicated by pfn and size or\n * can be for the entire vma (in which case pfn, size are zero).\n */\nstatic inline void untrack_pfn(struct vm_area_struct *vma,\n\t\t\t       unsigned long pfn, unsigned long size)\n{\n}\n\n/*\n * untrack_pfn_moved is called while mremapping a pfnmap for a new region.\n */\nstatic inline void untrack_pfn_moved(struct vm_area_struct *vma)\n{\n}\n#else\nextern int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t   unsigned long pfn, unsigned long addr,\n\t\t\t   unsigned long size);\nextern void track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t     pfn_t pfn);\nextern int track_pfn_copy(struct vm_area_struct *vma);\nextern void untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,\n\t\t\tunsigned long size);\nextern void untrack_pfn_moved(struct vm_area_struct *vma);\n#endif\n\n#ifdef __HAVE_COLOR_ZERO_PAGE\nstatic inline int is_zero_pfn(unsigned long pfn)\n{\n\textern unsigned long zero_pfn;\n\tunsigned long offset_from_zero_pfn = pfn - zero_pfn;\n\treturn offset_from_zero_pfn <= (zero_page_mask >> PAGE_SHIFT);\n}\n\n#define my_zero_pfn(addr)\tpage_to_pfn(ZERO_PAGE(addr))\n\n#else\nstatic inline int is_zero_pfn(unsigned long pfn)\n{\n\textern unsigned long zero_pfn;\n\treturn pfn == zero_pfn;\n}\n\nstatic inline unsigned long my_zero_pfn(unsigned long addr)\n{\n\textern unsigned long zero_pfn;\n\treturn zero_pfn;\n}\n#endif\n\n#ifdef CONFIG_MMU\n\n#ifndef CONFIG_TRANSPARENT_HUGEPAGE\nstatic inline int pmd_trans_huge(pmd_t pmd)\n{\n\treturn 0;\n}\n#ifndef pmd_write\nstatic inline int pmd_write(pmd_t pmd)\n{\n\tBUG();\n\treturn 0;\n}\n#endif /* pmd_write */\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n#ifndef pud_write\nstatic inline int pud_write(pud_t pud)\n{\n\tBUG();\n\treturn 0;\n}\n#endif /* pud_write */\n\n#if !defined(CONFIG_ARCH_HAS_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline int pmd_devmap(pmd_t pmd)\n{\n\treturn 0;\n}\nstatic inline int pud_devmap(pud_t pud)\n{\n\treturn 0;\n}\nstatic inline int pgd_devmap(pgd_t pgd)\n{\n\treturn 0;\n}\n#endif\n\n#if !defined(CONFIG_TRANSPARENT_HUGEPAGE) || \\\n\t(defined(CONFIG_TRANSPARENT_HUGEPAGE) && \\\n\t !defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD))\nstatic inline int pud_trans_huge(pud_t pud)\n{\n\treturn 0;\n}\n#endif\n\n/* See pmd_none_or_trans_huge_or_clear_bad for discussion. */\nstatic inline int pud_none_or_trans_huge_or_dev_or_clear_bad(pud_t *pud)\n{\n\tpud_t pudval = READ_ONCE(*pud);\n\n\tif (pud_none(pudval) || pud_trans_huge(pudval) || pud_devmap(pudval))\n\t\treturn 1;\n\tif (unlikely(pud_bad(pudval))) {\n\t\tpud_clear_bad(pud);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/* See pmd_trans_unstable for discussion. */\nstatic inline int pud_trans_unstable(pud_t *pud)\n{\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&\t\t\t\\\n\tdefined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\n\treturn pud_none_or_trans_huge_or_dev_or_clear_bad(pud);\n#else\n\treturn 0;\n#endif\n}\n\n#ifndef pmd_read_atomic\nstatic inline pmd_t pmd_read_atomic(pmd_t *pmdp)\n{\n\t/*\n\t * Depend on compiler for an atomic pmd read. NOTE: this is\n\t * only going to work, if the pmdval_t isn't larger than\n\t * an unsigned long.\n\t */\n\treturn *pmdp;\n}\n#endif\n\n#ifndef arch_needs_pgtable_deposit\n#define arch_needs_pgtable_deposit() (false)\n#endif\n/*\n * This function is meant to be used by sites walking pagetables with\n * the mmap_lock held in read mode to protect against MADV_DONTNEED and\n * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd\n * into a null pmd and the transhuge page fault can convert a null pmd\n * into an hugepmd or into a regular pmd (if the hugepage allocation\n * fails). While holding the mmap_lock in read mode the pmd becomes\n * stable and stops changing under us only if it's not null and not a\n * transhuge pmd. When those races occurs and this function makes a\n * difference vs the standard pmd_none_or_clear_bad, the result is\n * undefined so behaving like if the pmd was none is safe (because it\n * can return none anyway). The compiler level barrier() is critically\n * important to compute the two checks atomically on the same pmdval.\n *\n * For 32bit kernels with a 64bit large pmd_t this automatically takes\n * care of reading the pmd atomically to avoid SMP race conditions\n * against pmd_populate() when the mmap_lock is hold for reading by the\n * caller (a special atomic read not done by \"gcc\" as in the generic\n * version above, is also needed when THP is disabled because the page\n * fault can populate the pmd from under us).\n */\nstatic inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)\n{\n\tpmd_t pmdval = pmd_read_atomic(pmd);\n\t/*\n\t * The barrier will stabilize the pmdval in a register or on\n\t * the stack so that it will stop changing under the code.\n\t *\n\t * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,\n\t * pmd_read_atomic is allowed to return a not atomic pmdval\n\t * (for example pointing to an hugepage that has never been\n\t * mapped in the pmd). The below checks will only care about\n\t * the low part of the pmd with 32bit PAE x86 anyway, with the\n\t * exception of pmd_none(). So the important thing is that if\n\t * the low part of the pmd is found null, the high part will\n\t * be also null or the pmd_none() check below would be\n\t * confused.\n\t */\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbarrier();\n#endif\n\t/*\n\t * !pmd_present() checks for pmd migration entries\n\t *\n\t * The complete check uses is_pmd_migration_entry() in linux/swapops.h\n\t * But using that requires moving current function and pmd_trans_unstable()\n\t * to linux/swapops.h to resovle dependency, which is too much code move.\n\t *\n\t * !pmd_present() is equivalent to is_pmd_migration_entry() currently,\n\t * because !pmd_present() pages can only be under migration not swapped\n\t * out.\n\t *\n\t * pmd_none() is preseved for future condition checks on pmd migration\n\t * entries and not confusing with this function name, although it is\n\t * redundant with !pmd_present().\n\t */\n\tif (pmd_none(pmdval) || pmd_trans_huge(pmdval) ||\n\t\t(IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION) && !pmd_present(pmdval)))\n\t\treturn 1;\n\tif (unlikely(pmd_bad(pmdval))) {\n\t\tpmd_clear_bad(pmd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * This is a noop if Transparent Hugepage Support is not built into\n * the kernel. Otherwise it is equivalent to\n * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in\n * places that already verified the pmd is not none and they want to\n * walk ptes while holding the mmap sem in read mode (write mode don't\n * need this). If THP is not enabled, the pmd can't go away under the\n * code even if MADV_DONTNEED runs, but if THP is enabled we need to\n * run a pmd_trans_unstable before walking the ptes after\n * split_huge_pmd returns (because it may have run when the pmd become\n * null, but then a page fault can map in a THP and not a regular page).\n */\nstatic inline int pmd_trans_unstable(pmd_t *pmd)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\treturn pmd_none_or_trans_huge_or_clear_bad(pmd);\n#else\n\treturn 0;\n#endif\n}\n\n#ifndef CONFIG_NUMA_BALANCING\n/*\n * Technically a PTE can be PROTNONE even when not doing NUMA balancing but\n * the only case the kernel cares is for NUMA balancing and is only ever set\n * when the VMA is accessible. For PROT_NONE VMAs, the PTEs are not marked\n * _PAGE_PROTNONE so by default, implement the helper as \"always no\". It\n * is the responsibility of the caller to distinguish between PROT_NONE\n * protections and NUMA hinting fault protections.\n */\nstatic inline int pte_protnone(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline int pmd_protnone(pmd_t pmd)\n{\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#endif /* CONFIG_MMU */\n\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP\n\n#ifndef __PAGETABLE_P4D_FOLDED\nint p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot);\nint p4d_clear_huge(p4d_t *p4d);\n#else\nstatic inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline int p4d_clear_huge(p4d_t *p4d)\n{\n\treturn 0;\n}\n#endif /* !__PAGETABLE_P4D_FOLDED */\n\nint pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot);\nint pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot);\nint pud_clear_huge(pud_t *pud);\nint pmd_clear_huge(pmd_t *pmd);\nint p4d_free_pud_page(p4d_t *p4d, unsigned long addr);\nint pud_free_pmd_page(pud_t *pud, unsigned long addr);\nint pmd_free_pte_page(pmd_t *pmd, unsigned long addr);\n#else\t/* !CONFIG_HAVE_ARCH_HUGE_VMAP */\nstatic inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline int p4d_clear_huge(p4d_t *p4d)\n{\n\treturn 0;\n}\nstatic inline int pud_clear_huge(pud_t *pud)\n{\n\treturn 0;\n}\nstatic inline int pmd_clear_huge(pmd_t *pmd)\n{\n\treturn 0;\n}\nstatic inline int p4d_free_pud_page(p4d_t *p4d, unsigned long addr)\n{\n\treturn 0;\n}\nstatic inline int pud_free_pmd_page(pud_t *pud, unsigned long addr)\n{\n\treturn 0;\n}\nstatic inline int pmd_free_pte_page(pmd_t *pmd, unsigned long addr)\n{\n\treturn 0;\n}\n#endif\t/* CONFIG_HAVE_ARCH_HUGE_VMAP */\n\n#ifndef __HAVE_ARCH_FLUSH_PMD_TLB_RANGE\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n/*\n * ARCHes with special requirements for evicting THP backing TLB entries can\n * implement this. Otherwise also, it can help optimize normal TLB flush in\n * THP regime. Stock flush_tlb_range() typically has optimization to nuke the\n * entire TLB if flush span is greater than a threshold, which will\n * likely be true for a single huge page. Thus a single THP flush will\n * invalidate the entire TLB which is not desirable.\n * e.g. see arch/arc: flush_pmd_tlb_range\n */\n#define flush_pmd_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#define flush_pud_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#else\n#define flush_pmd_tlb_range(vma, addr, end)\tBUILD_BUG()\n#define flush_pud_tlb_range(vma, addr, end)\tBUILD_BUG()\n#endif\n#endif\n\nstruct file;\nint phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,\n\t\t\tunsigned long size, pgprot_t *vma_prot);\n\n#ifndef CONFIG_X86_ESPFIX64\nstatic inline void init_espfix_bsp(void) { }\n#endif\n\nextern void __init pgtable_cache_init(void);\n\n#ifndef __HAVE_ARCH_PFN_MODIFY_ALLOWED\nstatic inline bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)\n{\n\treturn true;\n}\n\nstatic inline bool arch_has_pfn_modify_check(void)\n{\n\treturn false;\n}\n#endif /* !_HAVE_ARCH_PFN_MODIFY_ALLOWED */\n\n/*\n * Architecture PAGE_KERNEL_* fallbacks\n *\n * Some architectures don't define certain PAGE_KERNEL_* flags. This is either\n * because they really don't support them, or the port needs to be updated to\n * reflect the required functionality. Below are a set of relatively safe\n * fallbacks, as best effort, which we can count on in lieu of the architectures\n * not defining them on their own yet.\n */\n\n#ifndef PAGE_KERNEL_RO\n# define PAGE_KERNEL_RO PAGE_KERNEL\n#endif\n\n#ifndef PAGE_KERNEL_EXEC\n# define PAGE_KERNEL_EXEC PAGE_KERNEL\n#endif\n\n/*\n * Page Table Modification bits for pgtbl_mod_mask.\n *\n * These are used by the p?d_alloc_track*() set of functions an in the generic\n * vmalloc/ioremap code to track at which page-table levels entries have been\n * modified. Based on that the code can better decide when vmalloc and ioremap\n * mapping changes need to be synchronized to other page-tables in the system.\n */\n#define\t\t__PGTBL_PGD_MODIFIED\t0\n#define\t\t__PGTBL_P4D_MODIFIED\t1\n#define\t\t__PGTBL_PUD_MODIFIED\t2\n#define\t\t__PGTBL_PMD_MODIFIED\t3\n#define\t\t__PGTBL_PTE_MODIFIED\t4\n\n#define\t\tPGTBL_PGD_MODIFIED\tBIT(__PGTBL_PGD_MODIFIED)\n#define\t\tPGTBL_P4D_MODIFIED\tBIT(__PGTBL_P4D_MODIFIED)\n#define\t\tPGTBL_PUD_MODIFIED\tBIT(__PGTBL_PUD_MODIFIED)\n#define\t\tPGTBL_PMD_MODIFIED\tBIT(__PGTBL_PMD_MODIFIED)\n#define\t\tPGTBL_PTE_MODIFIED\tBIT(__PGTBL_PTE_MODIFIED)\n\n/* Page-Table Modification Mask */\ntypedef unsigned int pgtbl_mod_mask;\n\n#endif /* !__ASSEMBLY__ */\n\n#ifndef has_transparent_hugepage\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define has_transparent_hugepage() 1\n#else\n#define has_transparent_hugepage() 0\n#endif\n#endif\n\n/*\n * On some architectures it depends on the mm if the p4d/pud or pmd\n * layer of the page table hierarchy is folded or not.\n */\n#ifndef mm_p4d_folded\n#define mm_p4d_folded(mm)\t__is_defined(__PAGETABLE_P4D_FOLDED)\n#endif\n\n#ifndef mm_pud_folded\n#define mm_pud_folded(mm)\t__is_defined(__PAGETABLE_PUD_FOLDED)\n#endif\n\n#ifndef mm_pmd_folded\n#define mm_pmd_folded(mm)\t__is_defined(__PAGETABLE_PMD_FOLDED)\n#endif\n\n#ifndef p4d_offset_lockless\n#define p4d_offset_lockless(pgdp, pgd, address) p4d_offset(&(pgd), address)\n#endif\n#ifndef pud_offset_lockless\n#define pud_offset_lockless(p4dp, p4d, address) pud_offset(&(p4d), address)\n#endif\n#ifndef pmd_offset_lockless\n#define pmd_offset_lockless(pudp, pud, address) pmd_offset(&(pud), address)\n#endif\n\n/*\n * p?d_leaf() - true if this entry is a final mapping to a physical address.\n * This differs from p?d_huge() by the fact that they are always available (if\n * the architecture supports large pages at the appropriate level) even\n * if CONFIG_HUGETLB_PAGE is not defined.\n * Only meaningful when called on a valid entry.\n */\n#ifndef pgd_leaf\n#define pgd_leaf(x)\t0\n#endif\n#ifndef p4d_leaf\n#define p4d_leaf(x)\t0\n#endif\n#ifndef pud_leaf\n#define pud_leaf(x)\t0\n#endif\n#ifndef pmd_leaf\n#define pmd_leaf(x)\t0\n#endif\n\n#endif /* _LINUX_PGTABLE_H */\n"}, "4": {"id": 4, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "5": {"id": 5, "path": "/src/arch/x86/include/asm/pgtable.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_PGTABLE_H\n#define _ASM_X86_PGTABLE_H\n\n#include <linux/mem_encrypt.h>\n#include <asm/page.h>\n#include <asm/pgtable_types.h>\n\n/*\n * Macro to mark a page protection value as UC-\n */\n#define pgprot_noncached(prot)\t\t\t\t\t\t\\\n\t((boot_cpu_data.x86 > 3)\t\t\t\t\t\\\n\t ? (__pgprot(pgprot_val(prot) |\t\t\t\t\t\\\n\t\t     cachemode2protval(_PAGE_CACHE_MODE_UC_MINUS)))\t\\\n\t : (prot))\n\n/*\n * Macros to add or remove encryption attribute\n */\n#define pgprot_encrypted(prot)\t__pgprot(__sme_set(pgprot_val(prot)))\n#define pgprot_decrypted(prot)\t__pgprot(__sme_clr(pgprot_val(prot)))\n\n#ifndef __ASSEMBLY__\n#include <asm/x86_init.h>\n#include <asm/fpu/xstate.h>\n#include <asm/fpu/api.h>\n#include <asm-generic/pgtable_uffd.h>\n\nextern pgd_t early_top_pgt[PTRS_PER_PGD];\nbool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);\n\nvoid ptdump_walk_pgd_level(struct seq_file *m, struct mm_struct *mm);\nvoid ptdump_walk_pgd_level_debugfs(struct seq_file *m, struct mm_struct *mm,\n\t\t\t\t   bool user);\nvoid ptdump_walk_pgd_level_checkwx(void);\nvoid ptdump_walk_user_pgd_level_checkwx(void);\n\n#ifdef CONFIG_DEBUG_WX\n#define debug_checkwx()\t\tptdump_walk_pgd_level_checkwx()\n#define debug_checkwx_user()\tptdump_walk_user_pgd_level_checkwx()\n#else\n#define debug_checkwx()\t\tdo { } while (0)\n#define debug_checkwx_user()\tdo { } while (0)\n#endif\n\n/*\n * ZERO_PAGE is a global shared page that is always zero: used\n * for zero-mapped memory areas etc..\n */\nextern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]\n\t__visible;\n#define ZERO_PAGE(vaddr) ((void)(vaddr),virt_to_page(empty_zero_page))\n\nextern spinlock_t pgd_lock;\nextern struct list_head pgd_list;\n\nextern struct mm_struct *pgd_page_get_mm(struct page *page);\n\nextern pmdval_t early_pmd_flags;\n\n#ifdef CONFIG_PARAVIRT_XXL\n#include <asm/paravirt.h>\n#else  /* !CONFIG_PARAVIRT_XXL */\n#define set_pte(ptep, pte)\t\tnative_set_pte(ptep, pte)\n\n#define set_pte_atomic(ptep, pte)\t\t\t\t\t\\\n\tnative_set_pte_atomic(ptep, pte)\n\n#define set_pmd(pmdp, pmd)\t\tnative_set_pmd(pmdp, pmd)\n\n#ifndef __PAGETABLE_P4D_FOLDED\n#define set_pgd(pgdp, pgd)\t\tnative_set_pgd(pgdp, pgd)\n#define pgd_clear(pgd)\t\t\t(pgtable_l5_enabled() ? native_pgd_clear(pgd) : 0)\n#endif\n\n#ifndef set_p4d\n# define set_p4d(p4dp, p4d)\t\tnative_set_p4d(p4dp, p4d)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\n#define p4d_clear(p4d)\t\t\tnative_p4d_clear(p4d)\n#endif\n\n#ifndef set_pud\n# define set_pud(pudp, pud)\t\tnative_set_pud(pudp, pud)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\n#define pud_clear(pud)\t\t\tnative_pud_clear(pud)\n#endif\n\n#define pte_clear(mm, addr, ptep)\tnative_pte_clear(mm, addr, ptep)\n#define pmd_clear(pmd)\t\t\tnative_pmd_clear(pmd)\n\n#define pgd_val(x)\tnative_pgd_val(x)\n#define __pgd(x)\tnative_make_pgd(x)\n\n#ifndef __PAGETABLE_P4D_FOLDED\n#define p4d_val(x)\tnative_p4d_val(x)\n#define __p4d(x)\tnative_make_p4d(x)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\n#define pud_val(x)\tnative_pud_val(x)\n#define __pud(x)\tnative_make_pud(x)\n#endif\n\n#ifndef __PAGETABLE_PMD_FOLDED\n#define pmd_val(x)\tnative_pmd_val(x)\n#define __pmd(x)\tnative_make_pmd(x)\n#endif\n\n#define pte_val(x)\tnative_pte_val(x)\n#define __pte(x)\tnative_make_pte(x)\n\n#define arch_end_context_switch(prev)\tdo {} while(0)\n#endif\t/* CONFIG_PARAVIRT_XXL */\n\n/*\n * The following only work if pte_present() is true.\n * Undefined behaviour if not..\n */\nstatic inline int pte_dirty(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_DIRTY;\n}\n\n\nstatic inline u32 read_pkru(void)\n{\n\tif (boot_cpu_has(X86_FEATURE_OSPKE))\n\t\treturn rdpkru();\n\treturn 0;\n}\n\nstatic inline void write_pkru(u32 pkru)\n{\n\tstruct pkru_state *pk;\n\n\tif (!boot_cpu_has(X86_FEATURE_OSPKE))\n\t\treturn;\n\n\tpk = get_xsave_addr(&current->thread.fpu.state.xsave, XFEATURE_PKRU);\n\n\t/*\n\t * The PKRU value in xstate needs to be in sync with the value that is\n\t * written to the CPU. The FPU restore on return to userland would\n\t * otherwise load the previous value again.\n\t */\n\tfpregs_lock();\n\tif (pk)\n\t\tpk->pkru = pkru;\n\t__write_pkru(pkru);\n\tfpregs_unlock();\n}\n\nstatic inline int pte_young(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_ACCESSED;\n}\n\nstatic inline int pmd_dirty(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_DIRTY;\n}\n\nstatic inline int pmd_young(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_ACCESSED;\n}\n\nstatic inline int pud_dirty(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_DIRTY;\n}\n\nstatic inline int pud_young(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_ACCESSED;\n}\n\nstatic inline int pte_write(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_RW;\n}\n\nstatic inline int pte_huge(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_PSE;\n}\n\nstatic inline int pte_global(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_GLOBAL;\n}\n\nstatic inline int pte_exec(pte_t pte)\n{\n\treturn !(pte_flags(pte) & _PAGE_NX);\n}\n\nstatic inline int pte_special(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SPECIAL;\n}\n\n/* Entries that were set to PROT_NONE are inverted */\n\nstatic inline u64 protnone_mask(u64 val);\n\nstatic inline unsigned long pte_pfn(pte_t pte)\n{\n\tphys_addr_t pfn = pte_val(pte);\n\tpfn ^= protnone_mask(pfn);\n\treturn (pfn & PTE_PFN_MASK) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long pmd_pfn(pmd_t pmd)\n{\n\tphys_addr_t pfn = pmd_val(pmd);\n\tpfn ^= protnone_mask(pfn);\n\treturn (pfn & pmd_pfn_mask(pmd)) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long pud_pfn(pud_t pud)\n{\n\tphys_addr_t pfn = pud_val(pud);\n\tpfn ^= protnone_mask(pfn);\n\treturn (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long p4d_pfn(p4d_t p4d)\n{\n\treturn (p4d_val(p4d) & p4d_pfn_mask(p4d)) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long pgd_pfn(pgd_t pgd)\n{\n\treturn (pgd_val(pgd) & PTE_PFN_MASK) >> PAGE_SHIFT;\n}\n\n#define p4d_leaf\tp4d_large\nstatic inline int p4d_large(p4d_t p4d)\n{\n\t/* No 512 GiB pages yet */\n\treturn 0;\n}\n\n#define pte_page(pte)\tpfn_to_page(pte_pfn(pte))\n\n#define pmd_leaf\tpmd_large\nstatic inline int pmd_large(pmd_t pte)\n{\n\treturn pmd_flags(pte) & _PAGE_PSE;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n/* NOTE: when predicate huge page, consider also pmd_devmap, or use pmd_large */\nstatic inline int pmd_trans_huge(pmd_t pmd)\n{\n\treturn (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\nstatic inline int pud_trans_huge(pud_t pud)\n{\n\treturn (pud_val(pud) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;\n}\n#endif\n\n#define has_transparent_hugepage has_transparent_hugepage\nstatic inline int has_transparent_hugepage(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_PSE);\n}\n\n#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP\nstatic inline int pmd_devmap(pmd_t pmd)\n{\n\treturn !!(pmd_val(pmd) & _PAGE_DEVMAP);\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\nstatic inline int pud_devmap(pud_t pud)\n{\n\treturn !!(pud_val(pud) & _PAGE_DEVMAP);\n}\n#else\nstatic inline int pud_devmap(pud_t pud)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline int pgd_devmap(pgd_t pgd)\n{\n\treturn 0;\n}\n#endif\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\nstatic inline pte_t pte_set_flags(pte_t pte, pteval_t set)\n{\n\tpteval_t v = native_pte_val(pte);\n\n\treturn native_make_pte(v | set);\n}\n\nstatic inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)\n{\n\tpteval_t v = native_pte_val(pte);\n\n\treturn native_make_pte(v & ~clear);\n}\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP\nstatic inline int pte_uffd_wp(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_UFFD_WP;\n}\n\nstatic inline pte_t pte_mkuffd_wp(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_UFFD_WP);\n}\n\nstatic inline pte_t pte_clear_uffd_wp(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_UFFD_WP);\n}\n#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */\n\nstatic inline pte_t pte_mkclean(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_DIRTY);\n}\n\nstatic inline pte_t pte_mkold(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_ACCESSED);\n}\n\nstatic inline pte_t pte_wrprotect(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_RW);\n}\n\nstatic inline pte_t pte_mkexec(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_NX);\n}\n\nstatic inline pte_t pte_mkdirty(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pte_t pte_mkyoung(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_ACCESSED);\n}\n\nstatic inline pte_t pte_mkwrite(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_RW);\n}\n\nstatic inline pte_t pte_mkhuge(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_PSE);\n}\n\nstatic inline pte_t pte_clrhuge(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_PSE);\n}\n\nstatic inline pte_t pte_mkglobal(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_GLOBAL);\n}\n\nstatic inline pte_t pte_clrglobal(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_GLOBAL);\n}\n\nstatic inline pte_t pte_mkspecial(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SPECIAL);\n}\n\nstatic inline pte_t pte_mkdevmap(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SPECIAL|_PAGE_DEVMAP);\n}\n\nstatic inline pmd_t pmd_set_flags(pmd_t pmd, pmdval_t set)\n{\n\tpmdval_t v = native_pmd_val(pmd);\n\n\treturn native_make_pmd(v | set);\n}\n\nstatic inline pmd_t pmd_clear_flags(pmd_t pmd, pmdval_t clear)\n{\n\tpmdval_t v = native_pmd_val(pmd);\n\n\treturn native_make_pmd(v & ~clear);\n}\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP\nstatic inline int pmd_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_UFFD_WP;\n}\n\nstatic inline pmd_t pmd_mkuffd_wp(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_UFFD_WP);\n}\n\nstatic inline pmd_t pmd_clear_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_UFFD_WP);\n}\n#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */\n\nstatic inline pmd_t pmd_mkold(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_ACCESSED);\n}\n\nstatic inline pmd_t pmd_mkclean(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_DIRTY);\n}\n\nstatic inline pmd_t pmd_wrprotect(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_RW);\n}\n\nstatic inline pmd_t pmd_mkdirty(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pmd_t pmd_mkdevmap(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_DEVMAP);\n}\n\nstatic inline pmd_t pmd_mkhuge(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_PSE);\n}\n\nstatic inline pmd_t pmd_mkyoung(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_ACCESSED);\n}\n\nstatic inline pmd_t pmd_mkwrite(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_RW);\n}\n\nstatic inline pud_t pud_set_flags(pud_t pud, pudval_t set)\n{\n\tpudval_t v = native_pud_val(pud);\n\n\treturn native_make_pud(v | set);\n}\n\nstatic inline pud_t pud_clear_flags(pud_t pud, pudval_t clear)\n{\n\tpudval_t v = native_pud_val(pud);\n\n\treturn native_make_pud(v & ~clear);\n}\n\nstatic inline pud_t pud_mkold(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_ACCESSED);\n}\n\nstatic inline pud_t pud_mkclean(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_DIRTY);\n}\n\nstatic inline pud_t pud_wrprotect(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_RW);\n}\n\nstatic inline pud_t pud_mkdirty(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pud_t pud_mkdevmap(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_DEVMAP);\n}\n\nstatic inline pud_t pud_mkhuge(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_PSE);\n}\n\nstatic inline pud_t pud_mkyoung(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_ACCESSED);\n}\n\nstatic inline pud_t pud_mkwrite(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_RW);\n}\n\n#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY\nstatic inline int pte_soft_dirty(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SOFT_DIRTY;\n}\n\nstatic inline int pmd_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_SOFT_DIRTY;\n}\n\nstatic inline int pud_soft_dirty(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_SOFT_DIRTY;\n}\n\nstatic inline pte_t pte_mksoft_dirty(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pmd_t pmd_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pud_t pud_mksoft_dirty(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pte_t pte_clear_soft_dirty(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pud_t pud_clear_soft_dirty(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_SOFT_DIRTY);\n}\n\n#endif /* CONFIG_HAVE_ARCH_SOFT_DIRTY */\n\n/*\n * Mask out unsupported bits in a present pgprot.  Non-present pgprots\n * can use those bits for other purposes, so leave them be.\n */\nstatic inline pgprotval_t massage_pgprot(pgprot_t pgprot)\n{\n\tpgprotval_t protval = pgprot_val(pgprot);\n\n\tif (protval & _PAGE_PRESENT)\n\t\tprotval &= __supported_pte_mask;\n\n\treturn protval;\n}\n\nstatic inline pgprotval_t check_pgprot(pgprot_t pgprot)\n{\n\tpgprotval_t massaged_val = massage_pgprot(pgprot);\n\n\t/* mmdebug.h can not be included here because of dependencies */\n#ifdef CONFIG_DEBUG_VM\n\tWARN_ONCE(pgprot_val(pgprot) != massaged_val,\n\t\t  \"attempted to set unsupported pgprot: %016llx \"\n\t\t  \"bits: %016llx supported: %016llx\\n\",\n\t\t  (u64)pgprot_val(pgprot),\n\t\t  (u64)pgprot_val(pgprot) ^ massaged_val,\n\t\t  (u64)__supported_pte_mask);\n#endif\n\n\treturn massaged_val;\n}\n\nstatic inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)\n{\n\tphys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;\n\tpfn ^= protnone_mask(pgprot_val(pgprot));\n\tpfn &= PTE_PFN_MASK;\n\treturn __pte(pfn | check_pgprot(pgprot));\n}\n\nstatic inline pmd_t pfn_pmd(unsigned long page_nr, pgprot_t pgprot)\n{\n\tphys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;\n\tpfn ^= protnone_mask(pgprot_val(pgprot));\n\tpfn &= PHYSICAL_PMD_PAGE_MASK;\n\treturn __pmd(pfn | check_pgprot(pgprot));\n}\n\nstatic inline pud_t pfn_pud(unsigned long page_nr, pgprot_t pgprot)\n{\n\tphys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;\n\tpfn ^= protnone_mask(pgprot_val(pgprot));\n\tpfn &= PHYSICAL_PUD_PAGE_MASK;\n\treturn __pud(pfn | check_pgprot(pgprot));\n}\n\nstatic inline pmd_t pmd_mkinvalid(pmd_t pmd)\n{\n\treturn pfn_pmd(pmd_pfn(pmd),\n\t\t      __pgprot(pmd_flags(pmd) & ~(_PAGE_PRESENT|_PAGE_PROTNONE)));\n}\n\nstatic inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);\n\nstatic inline pte_t pte_modify(pte_t pte, pgprot_t newprot)\n{\n\tpteval_t val = pte_val(pte), oldval = val;\n\n\t/*\n\t * Chop off the NX bit (if present), and add the NX portion of\n\t * the newprot (if present):\n\t */\n\tval &= _PAGE_CHG_MASK;\n\tval |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;\n\tval = flip_protnone_guard(oldval, val, PTE_PFN_MASK);\n\treturn __pte(val);\n}\n\nstatic inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)\n{\n\tpmdval_t val = pmd_val(pmd), oldval = val;\n\n\tval &= _HPAGE_CHG_MASK;\n\tval |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;\n\tval = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);\n\treturn __pmd(val);\n}\n\n/*\n * mprotect needs to preserve PAT and encryption bits when updating\n * vm_page_prot\n */\n#define pgprot_modify pgprot_modify\nstatic inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)\n{\n\tpgprotval_t preservebits = pgprot_val(oldprot) & _PAGE_CHG_MASK;\n\tpgprotval_t addbits = pgprot_val(newprot) & ~_PAGE_CHG_MASK;\n\treturn __pgprot(preservebits | addbits);\n}\n\n#define pte_pgprot(x) __pgprot(pte_flags(x))\n#define pmd_pgprot(x) __pgprot(pmd_flags(x))\n#define pud_pgprot(x) __pgprot(pud_flags(x))\n#define p4d_pgprot(x) __pgprot(p4d_flags(x))\n\n#define canon_pgprot(p) __pgprot(massage_pgprot(p))\n\nstatic inline pgprot_t arch_filter_pgprot(pgprot_t prot)\n{\n\treturn canon_pgprot(prot);\n}\n\nstatic inline int is_new_memtype_allowed(u64 paddr, unsigned long size,\n\t\t\t\t\t enum page_cache_mode pcm,\n\t\t\t\t\t enum page_cache_mode new_pcm)\n{\n\t/*\n\t * PAT type is always WB for untracked ranges, so no need to check.\n\t */\n\tif (x86_platform.is_untracked_pat_range(paddr, paddr + size))\n\t\treturn 1;\n\n\t/*\n\t * Certain new memtypes are not allowed with certain\n\t * requested memtype:\n\t * - request is uncached, return cannot be write-back\n\t * - request is write-combine, return cannot be write-back\n\t * - request is write-through, return cannot be write-back\n\t * - request is write-through, return cannot be write-combine\n\t */\n\tif ((pcm == _PAGE_CACHE_MODE_UC_MINUS &&\n\t     new_pcm == _PAGE_CACHE_MODE_WB) ||\n\t    (pcm == _PAGE_CACHE_MODE_WC &&\n\t     new_pcm == _PAGE_CACHE_MODE_WB) ||\n\t    (pcm == _PAGE_CACHE_MODE_WT &&\n\t     new_pcm == _PAGE_CACHE_MODE_WB) ||\n\t    (pcm == _PAGE_CACHE_MODE_WT &&\n\t     new_pcm == _PAGE_CACHE_MODE_WC)) {\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\npmd_t *populate_extra_pmd(unsigned long vaddr);\npte_t *populate_extra_pte(unsigned long vaddr);\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\npgd_t __pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd);\n\n/*\n * Take a PGD location (pgdp) and a pgd value that needs to be set there.\n * Populates the user and returns the resulting PGD that must be set in\n * the kernel copy of the page tables.\n */\nstatic inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)\n{\n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn pgd;\n\treturn __pti_set_user_pgtbl(pgdp, pgd);\n}\n#else   /* CONFIG_PAGE_TABLE_ISOLATION */\nstatic inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)\n{\n\treturn pgd;\n}\n#endif  /* CONFIG_PAGE_TABLE_ISOLATION */\n\n#endif\t/* __ASSEMBLY__ */\n\n\n#ifdef CONFIG_X86_32\n# include <asm/pgtable_32.h>\n#else\n# include <asm/pgtable_64.h>\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/mm_types.h>\n#include <linux/mmdebug.h>\n#include <linux/log2.h>\n#include <asm/fixmap.h>\n\nstatic inline int pte_none(pte_t pte)\n{\n\treturn !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));\n}\n\n#define __HAVE_ARCH_PTE_SAME\nstatic inline int pte_same(pte_t a, pte_t b)\n{\n\treturn a.pte == b.pte;\n}\n\nstatic inline int pte_present(pte_t a)\n{\n\treturn pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);\n}\n\n#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP\nstatic inline int pte_devmap(pte_t a)\n{\n\treturn (pte_flags(a) & _PAGE_DEVMAP) == _PAGE_DEVMAP;\n}\n#endif\n\n#define pte_accessible pte_accessible\nstatic inline bool pte_accessible(struct mm_struct *mm, pte_t a)\n{\n\tif (pte_flags(a) & _PAGE_PRESENT)\n\t\treturn true;\n\n\tif ((pte_flags(a) & _PAGE_PROTNONE) &&\n\t\t\tmm_tlb_flush_pending(mm))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline int pmd_present(pmd_t pmd)\n{\n\t/*\n\t * Checking for _PAGE_PSE is needed too because\n\t * split_huge_page will temporarily clear the present bit (but\n\t * the _PAGE_PSE flag will remain set at all times while the\n\t * _PAGE_PRESENT bit is clear).\n\t */\n\treturn pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * These work without NUMA balancing but the kernel does not care. See the\n * comment in include/linux/pgtable.h\n */\nstatic inline int pte_protnone(pte_t pte)\n{\n\treturn (pte_flags(pte) & (_PAGE_PROTNONE | _PAGE_PRESENT))\n\t\t== _PAGE_PROTNONE;\n}\n\nstatic inline int pmd_protnone(pmd_t pmd)\n{\n\treturn (pmd_flags(pmd) & (_PAGE_PROTNONE | _PAGE_PRESENT))\n\t\t== _PAGE_PROTNONE;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\nstatic inline int pmd_none(pmd_t pmd)\n{\n\t/* Only check low word on 32-bit platforms, since it might be\n\t   out of sync with upper half. */\n\tunsigned long val = native_pmd_val(pmd);\n\treturn (val & ~_PAGE_KNL_ERRATUM_MASK) == 0;\n}\n\nstatic inline unsigned long pmd_page_vaddr(pmd_t pmd)\n{\n\treturn (unsigned long)__va(pmd_val(pmd) & pmd_pfn_mask(pmd));\n}\n\n/*\n * Currently stuck as a macro due to indirect forward reference to\n * linux/mmzone.h's __section_mem_map_addr() definition:\n */\n#define pmd_page(pmd)\tpfn_to_page(pmd_pfn(pmd))\n\n/*\n * Conversion functions: convert a page and protection to a page entry,\n * and a page entry and page directory to the page they refer to.\n *\n * (Currently stuck as a macro because of indirect forward reference\n * to linux/mm.h:page_to_nid())\n */\n#define mk_pte(page, pgprot)   pfn_pte(page_to_pfn(page), (pgprot))\n\nstatic inline int pmd_bad(pmd_t pmd)\n{\n\treturn (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;\n}\n\nstatic inline unsigned long pages_to_mb(unsigned long npg)\n{\n\treturn npg >> (20 - PAGE_SHIFT);\n}\n\n#if CONFIG_PGTABLE_LEVELS > 2\nstatic inline int pud_none(pud_t pud)\n{\n\treturn (native_pud_val(pud) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;\n}\n\nstatic inline int pud_present(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_PRESENT;\n}\n\nstatic inline unsigned long pud_page_vaddr(pud_t pud)\n{\n\treturn (unsigned long)__va(pud_val(pud) & pud_pfn_mask(pud));\n}\n\n/*\n * Currently stuck as a macro due to indirect forward reference to\n * linux/mmzone.h's __section_mem_map_addr() definition:\n */\n#define pud_page(pud)\tpfn_to_page(pud_pfn(pud))\n\n#define pud_leaf\tpud_large\nstatic inline int pud_large(pud_t pud)\n{\n\treturn (pud_val(pud) & (_PAGE_PSE | _PAGE_PRESENT)) ==\n\t\t(_PAGE_PSE | _PAGE_PRESENT);\n}\n\nstatic inline int pud_bad(pud_t pud)\n{\n\treturn (pud_flags(pud) & ~(_KERNPG_TABLE | _PAGE_USER)) != 0;\n}\n#else\n#define pud_leaf\tpud_large\nstatic inline int pud_large(pud_t pud)\n{\n\treturn 0;\n}\n#endif\t/* CONFIG_PGTABLE_LEVELS > 2 */\n\n#if CONFIG_PGTABLE_LEVELS > 3\nstatic inline int p4d_none(p4d_t p4d)\n{\n\treturn (native_p4d_val(p4d) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;\n}\n\nstatic inline int p4d_present(p4d_t p4d)\n{\n\treturn p4d_flags(p4d) & _PAGE_PRESENT;\n}\n\nstatic inline unsigned long p4d_page_vaddr(p4d_t p4d)\n{\n\treturn (unsigned long)__va(p4d_val(p4d) & p4d_pfn_mask(p4d));\n}\n\n/*\n * Currently stuck as a macro due to indirect forward reference to\n * linux/mmzone.h's __section_mem_map_addr() definition:\n */\n#define p4d_page(p4d)\tpfn_to_page(p4d_pfn(p4d))\n\nstatic inline int p4d_bad(p4d_t p4d)\n{\n\tunsigned long ignore_flags = _KERNPG_TABLE | _PAGE_USER;\n\n\tif (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))\n\t\tignore_flags |= _PAGE_NX;\n\n\treturn (p4d_flags(p4d) & ~ignore_flags) != 0;\n}\n#endif  /* CONFIG_PGTABLE_LEVELS > 3 */\n\nstatic inline unsigned long p4d_index(unsigned long address)\n{\n\treturn (address >> P4D_SHIFT) & (PTRS_PER_P4D - 1);\n}\n\n#if CONFIG_PGTABLE_LEVELS > 4\nstatic inline int pgd_present(pgd_t pgd)\n{\n\tif (!pgtable_l5_enabled())\n\t\treturn 1;\n\treturn pgd_flags(pgd) & _PAGE_PRESENT;\n}\n\nstatic inline unsigned long pgd_page_vaddr(pgd_t pgd)\n{\n\treturn (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);\n}\n\n/*\n * Currently stuck as a macro due to indirect forward reference to\n * linux/mmzone.h's __section_mem_map_addr() definition:\n */\n#define pgd_page(pgd)\tpfn_to_page(pgd_pfn(pgd))\n\n/* to find an entry in a page-table-directory. */\nstatic inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)\n{\n\tif (!pgtable_l5_enabled())\n\t\treturn (p4d_t *)pgd;\n\treturn (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);\n}\n\nstatic inline int pgd_bad(pgd_t pgd)\n{\n\tunsigned long ignore_flags = _PAGE_USER;\n\n\tif (!pgtable_l5_enabled())\n\t\treturn 0;\n\n\tif (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))\n\t\tignore_flags |= _PAGE_NX;\n\n\treturn (pgd_flags(pgd) & ~ignore_flags) != _KERNPG_TABLE;\n}\n\nstatic inline int pgd_none(pgd_t pgd)\n{\n\tif (!pgtable_l5_enabled())\n\t\treturn 0;\n\t/*\n\t * There is no need to do a workaround for the KNL stray\n\t * A/D bit erratum here.  PGDs only point to page tables\n\t * except on 32-bit non-PAE which is not supported on\n\t * KNL.\n\t */\n\treturn !native_pgd_val(pgd);\n}\n#endif\t/* CONFIG_PGTABLE_LEVELS > 4 */\n\n#endif\t/* __ASSEMBLY__ */\n\n#define KERNEL_PGD_BOUNDARY\tpgd_index(PAGE_OFFSET)\n#define KERNEL_PGD_PTRS\t\t(PTRS_PER_PGD - KERNEL_PGD_BOUNDARY)\n\n#ifndef __ASSEMBLY__\n\nextern int direct_gbpages;\nvoid init_mem_mapping(void);\nvoid early_alloc_pgt_buf(void);\nextern void memblock_find_dma_reserve(void);\nvoid __init poking_init(void);\nunsigned long init_memory_mapping(unsigned long start,\n\t\t\t\t  unsigned long end, pgprot_t prot);\n\n#ifdef CONFIG_X86_64\nextern pgd_t trampoline_pgd_entry;\n#endif\n\n/* local pte updates need not use xchg for locking */\nstatic inline pte_t native_local_ptep_get_and_clear(pte_t *ptep)\n{\n\tpte_t res = *ptep;\n\n\t/* Pure native function needs no input for mm, addr */\n\tnative_pte_clear(NULL, 0, ptep);\n\treturn res;\n}\n\nstatic inline pmd_t native_local_pmdp_get_and_clear(pmd_t *pmdp)\n{\n\tpmd_t res = *pmdp;\n\n\tnative_pmd_clear(pmdp);\n\treturn res;\n}\n\nstatic inline pud_t native_local_pudp_get_and_clear(pud_t *pudp)\n{\n\tpud_t res = *pudp;\n\n\tnative_pud_clear(pudp);\n\treturn res;\n}\n\nstatic inline void set_pte_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pte_t *ptep, pte_t pte)\n{\n\tset_pte(ptep, pte);\n}\n\nstatic inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pmd_t *pmdp, pmd_t pmd)\n{\n\tset_pmd(pmdp, pmd);\n}\n\nstatic inline void set_pud_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pud_t *pudp, pud_t pud)\n{\n\tnative_set_pud(pudp, pud);\n}\n\n/*\n * We only update the dirty/accessed state if we set\n * the dirty bit by hand in the kernel, since the hardware\n * will do the accessed bit for us, and we don't want to\n * race with other CPU's that might be updating the dirty\n * bit at the same time.\n */\nstruct vm_area_struct;\n\n#define  __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS\nextern int ptep_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pte_t *ptep,\n\t\t\t\t pte_t entry, int dirty);\n\n#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG\nextern int ptep_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, pte_t *ptep);\n\n#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH\nextern int ptep_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, pte_t *ptep);\n\n#define __HAVE_ARCH_PTEP_GET_AND_CLEAR\nstatic inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t       pte_t *ptep)\n{\n\tpte_t pte = native_ptep_get_and_clear(ptep);\n\treturn pte;\n}\n\n#define __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL\nstatic inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long addr, pte_t *ptep,\n\t\t\t\t\t    int full)\n{\n\tpte_t pte;\n\tif (full) {\n\t\t/*\n\t\t * Full address destruction in progress; paravirt does not\n\t\t * care about updates and native needs no locking\n\t\t */\n\t\tpte = native_local_ptep_get_and_clear(ptep);\n\t} else {\n\t\tpte = ptep_get_and_clear(mm, addr, ptep);\n\t}\n\treturn pte;\n}\n\n#define __HAVE_ARCH_PTEP_SET_WRPROTECT\nstatic inline void ptep_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long addr, pte_t *ptep)\n{\n\tclear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);\n}\n\n#define flush_tlb_fix_spurious_fault(vma, address) do { } while (0)\n\n#define mk_pmd(page, pgprot)   pfn_pmd(page_to_pfn(page), (pgprot))\n\n#define  __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS\nextern int pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pmd_t *pmdp,\n\t\t\t\t pmd_t entry, int dirty);\nextern int pudp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pud_t *pudp,\n\t\t\t\t pud_t entry, int dirty);\n\n#define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG\nextern int pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, pmd_t *pmdp);\nextern int pudp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, pud_t *pudp);\n\n#define __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH\nextern int pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, pmd_t *pmdp);\n\n\n#define pmd_write pmd_write\nstatic inline int pmd_write(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_RW;\n}\n\n#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR\nstatic inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t       pmd_t *pmdp)\n{\n\treturn native_pmdp_get_and_clear(pmdp);\n}\n\n#define __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR\nstatic inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,\n\t\t\t\t\tunsigned long addr, pud_t *pudp)\n{\n\treturn native_pudp_get_and_clear(pudp);\n}\n\n#define __HAVE_ARCH_PMDP_SET_WRPROTECT\nstatic inline void pmdp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long addr, pmd_t *pmdp)\n{\n\tclear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);\n}\n\n#define pud_write pud_write\nstatic inline int pud_write(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_RW;\n}\n\n#ifndef pmdp_establish\n#define pmdp_establish pmdp_establish\nstatic inline pmd_t pmdp_establish(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmdp, pmd_t pmd)\n{\n\tif (IS_ENABLED(CONFIG_SMP)) {\n\t\treturn xchg(pmdp, pmd);\n\t} else {\n\t\tpmd_t old = *pmdp;\n\t\tWRITE_ONCE(*pmdp, pmd);\n\t\treturn old;\n\t}\n}\n#endif\n/*\n * Page table pages are page-aligned.  The lower half of the top\n * level is used for userspace and the top half for the kernel.\n *\n * Returns true for parts of the PGD that map userspace and\n * false for the parts that map the kernel.\n */\nstatic inline bool pgdp_maps_userspace(void *__ptr)\n{\n\tunsigned long ptr = (unsigned long)__ptr;\n\n\treturn (((ptr & ~PAGE_MASK) / sizeof(pgd_t)) < PGD_KERNEL_START);\n}\n\n#define pgd_leaf\tpgd_large\nstatic inline int pgd_large(pgd_t pgd) { return 0; }\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n/*\n * All top-level PAGE_TABLE_ISOLATION page tables are order-1 pages\n * (8k-aligned and 8k in size).  The kernel one is at the beginning 4k and\n * the user one is in the last 4k.  To switch between them, you\n * just need to flip the 12th bit in their addresses.\n */\n#define PTI_PGTABLE_SWITCH_BIT\tPAGE_SHIFT\n\n/*\n * This generates better code than the inline assembly in\n * __set_bit().\n */\nstatic inline void *ptr_set_bit(void *ptr, int bit)\n{\n\tunsigned long __ptr = (unsigned long)ptr;\n\n\t__ptr |= BIT(bit);\n\treturn (void *)__ptr;\n}\nstatic inline void *ptr_clear_bit(void *ptr, int bit)\n{\n\tunsigned long __ptr = (unsigned long)ptr;\n\n\t__ptr &= ~BIT(bit);\n\treturn (void *)__ptr;\n}\n\nstatic inline pgd_t *kernel_to_user_pgdp(pgd_t *pgdp)\n{\n\treturn ptr_set_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);\n}\n\nstatic inline pgd_t *user_to_kernel_pgdp(pgd_t *pgdp)\n{\n\treturn ptr_clear_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);\n}\n\nstatic inline p4d_t *kernel_to_user_p4dp(p4d_t *p4dp)\n{\n\treturn ptr_set_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);\n}\n\nstatic inline p4d_t *user_to_kernel_p4dp(p4d_t *p4dp)\n{\n\treturn ptr_clear_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);\n}\n#endif /* CONFIG_PAGE_TABLE_ISOLATION */\n\n/*\n * clone_pgd_range(pgd_t *dst, pgd_t *src, int count);\n *\n *  dst - pointer to pgd range anwhere on a pgd page\n *  src - \"\"\n *  count - the number of pgds to copy.\n *\n * dst and src can be on the same page, but the range must not overlap,\n * and must not cross a page boundary.\n */\nstatic inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)\n{\n\tmemcpy(dst, src, count * sizeof(pgd_t));\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\t/* Clone the user space pgd as well */\n\tmemcpy(kernel_to_user_pgdp(dst), kernel_to_user_pgdp(src),\n\t       count * sizeof(pgd_t));\n#endif\n}\n\n#define PTE_SHIFT ilog2(PTRS_PER_PTE)\nstatic inline int page_level_shift(enum pg_level level)\n{\n\treturn (PAGE_SHIFT - PTE_SHIFT) + level * PTE_SHIFT;\n}\nstatic inline unsigned long page_level_size(enum pg_level level)\n{\n\treturn 1UL << page_level_shift(level);\n}\nstatic inline unsigned long page_level_mask(enum pg_level level)\n{\n\treturn ~(page_level_size(level) - 1);\n}\n\n/*\n * The x86 doesn't have any external MMU info: the kernel page\n * tables contain all the necessary information.\n */\nstatic inline void update_mmu_cache(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *ptep)\n{\n}\nstatic inline void update_mmu_cache_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmd)\n{\n}\nstatic inline void update_mmu_cache_pud(struct vm_area_struct *vma,\n\t\tunsigned long addr, pud_t *pud)\n{\n}\n\n#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY\nstatic inline pte_t pte_swp_mksoft_dirty(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);\n}\n\nstatic inline int pte_swp_soft_dirty(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;\n}\n\nstatic inline pte_t pte_swp_clear_soft_dirty(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);\n}\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\nstatic inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_SWP_SOFT_DIRTY);\n}\n\nstatic inline int pmd_swp_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_SWP_SOFT_DIRTY;\n}\n\nstatic inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_SWP_SOFT_DIRTY);\n}\n#endif\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP\nstatic inline pte_t pte_swp_mkuffd_wp(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SWP_UFFD_WP);\n}\n\nstatic inline int pte_swp_uffd_wp(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SWP_UFFD_WP;\n}\n\nstatic inline pte_t pte_swp_clear_uffd_wp(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SWP_UFFD_WP);\n}\n\nstatic inline pmd_t pmd_swp_mkuffd_wp(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_SWP_UFFD_WP);\n}\n\nstatic inline int pmd_swp_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_SWP_UFFD_WP;\n}\n\nstatic inline pmd_t pmd_swp_clear_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_SWP_UFFD_WP);\n}\n#endif /* CONFIG_HAVE_ARCH_USERFAULTFD_WP */\n\n#define PKRU_AD_BIT 0x1\n#define PKRU_WD_BIT 0x2\n#define PKRU_BITS_PER_PKEY 2\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\nextern u32 init_pkru_value;\n#else\n#define init_pkru_value\t0\n#endif\n\nstatic inline bool __pkru_allows_read(u32 pkru, u16 pkey)\n{\n\tint pkru_pkey_bits = pkey * PKRU_BITS_PER_PKEY;\n\treturn !(pkru & (PKRU_AD_BIT << pkru_pkey_bits));\n}\n\nstatic inline bool __pkru_allows_write(u32 pkru, u16 pkey)\n{\n\tint pkru_pkey_bits = pkey * PKRU_BITS_PER_PKEY;\n\t/*\n\t * Access-disable disables writes too so we need to check\n\t * both bits here.\n\t */\n\treturn !(pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits));\n}\n\nstatic inline u16 pte_flags_pkey(unsigned long pte_flags)\n{\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\t/* ifdef to avoid doing 59-bit shift on 32-bit values */\n\treturn (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool __pkru_allows_pkey(u16 pkey, bool write)\n{\n\tu32 pkru = read_pkru();\n\n\tif (!__pkru_allows_read(pkru, pkey))\n\t\treturn false;\n\tif (write && !__pkru_allows_write(pkru, pkey))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * 'pteval' can come from a PTE, PMD or PUD.  We only check\n * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the\n * same value on all 3 types.\n */\nstatic inline bool __pte_access_permitted(unsigned long pteval, bool write)\n{\n\tunsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;\n\n\tif (write)\n\t\tneed_pte_bits |= _PAGE_RW;\n\n\tif ((pteval & need_pte_bits) != need_pte_bits)\n\t\treturn 0;\n\n\treturn __pkru_allows_pkey(pte_flags_pkey(pteval), write);\n}\n\n#define pte_access_permitted pte_access_permitted\nstatic inline bool pte_access_permitted(pte_t pte, bool write)\n{\n\treturn __pte_access_permitted(pte_val(pte), write);\n}\n\n#define pmd_access_permitted pmd_access_permitted\nstatic inline bool pmd_access_permitted(pmd_t pmd, bool write)\n{\n\treturn __pte_access_permitted(pmd_val(pmd), write);\n}\n\n#define pud_access_permitted pud_access_permitted\nstatic inline bool pud_access_permitted(pud_t pud, bool write)\n{\n\treturn __pte_access_permitted(pud_val(pud), write);\n}\n\n#define __HAVE_ARCH_PFN_MODIFY_ALLOWED 1\nextern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);\n\nstatic inline bool arch_has_pfn_modify_check(void)\n{\n\treturn boot_cpu_has_bug(X86_BUG_L1TF);\n}\n\n#define arch_faults_on_old_pte arch_faults_on_old_pte\nstatic inline bool arch_faults_on_old_pte(void)\n{\n\treturn false;\n}\n\n#endif\t/* __ASSEMBLY__ */\n\n#endif /* _ASM_X86_PGTABLE_H */\n"}}, "reports": [{"events": [{"location": {"col": 2, "file": 1, "line": 2216}, "message": "expanded from macro 'pte_unmap_unlock'"}, {"location": {"col": 9, "file": 0, "line": 2559}, "message": "Calling '__apply_to_page_range'"}, {"location": {"col": 14, "file": 0, "line": 2519}, "message": "Assuming 'addr' is < 'end'"}, {"location": {"col": 25, "file": 2, "line": 119}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 6, "file": 0, "line": 2519}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 2, "line": 120}, "message": "expanded from macro 'WARN_ON'"}, {"location": {"col": 2, "file": 0, "line": 2519}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 0, "line": 2524}, "message": "Assuming the condition is false"}, {"location": {"col": 3, "file": 3, "line": 691}, "message": "expanded from macro 'pgd_addr_end'"}, {"location": {"col": 10, "file": 0, "line": 2524}, "message": "'?' condition is false"}, {"location": {"col": 2, "file": 3, "line": 691}, "message": "expanded from macro 'pgd_addr_end'"}, {"location": {"col": 8, "file": 0, "line": 2525}, "message": "'create' is false"}, {"location": {"col": 7, "file": 0, "line": 2525}, "message": "Left side of '&&' is true"}, {"location": {"col": 18, "file": 0, "line": 2525}, "message": "Calling 'pgd_none_or_clear_bad'"}, {"location": {"col": 6, "file": 3, "line": 738}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 3, "line": 738}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 3, "line": 740}, "message": "Assuming the condition is true"}, {"location": {"col": 40, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 3, "line": 740}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 744}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 18, "file": 0, "line": 2525}, "message": "Returning from 'pgd_none_or_clear_bad'"}, {"location": {"col": 3, "file": 0, "line": 2525}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 0, "line": 2527}, "message": "Calling 'apply_to_p4d_range'"}, {"location": {"col": 6, "file": 0, "line": 2490}, "message": "'create' is false"}, {"location": {"col": 2, "file": 0, "line": 2490}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 0, "line": 2498}, "message": "Assuming the condition is false"}, {"location": {"col": 3, "file": 3, "line": 697}, "message": "expanded from macro 'p4d_addr_end'"}, {"location": {"col": 10, "file": 0, "line": 2498}, "message": "'?' condition is false"}, {"location": {"col": 2, "file": 3, "line": 697}, "message": "expanded from macro 'p4d_addr_end'"}, {"location": {"col": 7, "file": 0, "line": 2499}, "message": "'create' is false"}, {"location": {"col": 7, "file": 0, "line": 2499}, "message": "Left side of '||' is false"}, {"location": {"col": 18, "file": 0, "line": 2499}, "message": "Calling 'p4d_none_or_clear_bad'"}, {"location": {"col": 6, "file": 3, "line": 749}, "message": "Calling 'p4d_none'"}, {"location": {"col": 9, "file": 5, "line": 901}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 5, "line": 901}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 3, "line": 749}, "message": "Returning from 'p4d_none'"}, {"location": {"col": 2, "file": 3, "line": 749}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 751}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 755}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 18, "file": 0, "line": 2499}, "message": "Returning from 'p4d_none_or_clear_bad'"}, {"location": {"col": 3, "file": 0, "line": 2499}, "message": "Taking true branch"}, {"location": {"col": 10, "file": 0, "line": 2500}, "message": "Calling 'apply_to_pud_range'"}, {"location": {"col": 6, "file": 0, "line": 2462}, "message": "'create' is false"}, {"location": {"col": 2, "file": 0, "line": 2462}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 0, "line": 2470}, "message": "Assuming the condition is false"}, {"location": {"col": 3, "file": 3, "line": 704}, "message": "expanded from macro 'pud_addr_end'"}, {"location": {"col": 10, "file": 0, "line": 2470}, "message": "'?' condition is false"}, {"location": {"col": 2, "file": 3, "line": 704}, "message": "expanded from macro 'pud_addr_end'"}, {"location": {"col": 7, "file": 0, "line": 2471}, "message": "'create' is false"}, {"location": {"col": 7, "file": 0, "line": 2471}, "message": "Left side of '||' is false"}, {"location": {"col": 18, "file": 0, "line": 2471}, "message": "Calling 'pud_none_or_clear_bad'"}, {"location": {"col": 6, "file": 3, "line": 760}, "message": "Calling 'pud_none'"}, {"location": {"col": 9, "file": 5, "line": 860}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 5, "line": 860}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 3, "line": 760}, "message": "Returning from 'pud_none'"}, {"location": {"col": 2, "file": 3, "line": 760}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 762}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 766}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 18, "file": 0, "line": 2471}, "message": "Returning from 'pud_none_or_clear_bad'"}, {"location": {"col": 3, "file": 0, "line": 2471}, "message": "Taking true branch"}, {"location": {"col": 10, "file": 0, "line": 2472}, "message": "Calling 'apply_to_pmd_range'"}, {"location": {"col": 2, "file": 0, "line": 2432}, "message": "Assuming the condition is true"}, {"location": {"col": 36, "file": 2, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 40, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 2432}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 2, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 2432}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 2, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 6, "file": 0, "line": 2434}, "message": "'create' is false"}, {"location": {"col": 2, "file": 0, "line": 2434}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 0, "line": 2442}, "message": "Assuming the condition is true"}, {"location": {"col": 3, "file": 3, "line": 711}, "message": "expanded from macro 'pmd_addr_end'"}, {"location": {"col": 10, "file": 0, "line": 2442}, "message": "'?' condition is true"}, {"location": {"col": 2, "file": 3, "line": 711}, "message": "expanded from macro 'pmd_addr_end'"}, {"location": {"col": 7, "file": 0, "line": 2443}, "message": "'create' is false"}, {"location": {"col": 7, "file": 0, "line": 2443}, "message": "Left side of '||' is false"}, {"location": {"col": 18, "file": 0, "line": 2443}, "message": "Calling 'pmd_none_or_clear_bad'"}, {"location": {"col": 6, "file": 3, "line": 771}, "message": "Calling 'pmd_none'"}, {"location": {"col": 9, "file": 5, "line": 824}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 5, "line": 824}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 3, "line": 771}, "message": "Returning from 'pmd_none'"}, {"location": {"col": 2, "file": 3, "line": 771}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 3, "line": 773}, "message": "Assuming the condition is false"}, {"location": {"col": 22, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 3, "line": 773}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 777}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 18, "file": 0, "line": 2443}, "message": "Returning from 'pmd_none_or_clear_bad'"}, {"location": {"col": 3, "file": 0, "line": 2443}, "message": "Taking true branch"}, {"location": {"col": 10, "file": 0, "line": 2444}, "message": "Calling 'apply_to_pte_range'"}, {"location": {"col": 2, "file": 0, "line": 2387}, "message": "'ptl' declared without an initial value"}, {"location": {"col": 6, "file": 0, "line": 2389}, "message": "'create' is false"}, {"location": {"col": 2, "file": 0, "line": 2389}, "message": "Taking false branch"}, {"location": {"col": 10, "file": 0, "line": 2396}, "message": "Assuming the condition is true"}, {"location": {"col": 9, "file": 0, "line": 2396}, "message": "'?' condition is true"}, {"location": {"col": 2, "file": 0, "line": 2401}, "message": "Assuming the condition is true"}, {"location": {"col": 36, "file": 2, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 40, "file": 4, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 0, "line": 2401}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 2, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 2401}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 2, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 2403}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 36, "file": 3, "line": 903}, "message": "expanded from macro 'arch_enter_lazy_mmu_mode'"}, {"location": {"col": 6, "file": 0, "line": 2405}, "message": "Assuming 'fn' is null"}, {"location": {"col": 2, "file": 0, "line": 2405}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 2416}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 36, "file": 3, "line": 904}, "message": "expanded from macro 'arch_leave_lazy_mmu_mode'"}, {"location": {"col": 6, "file": 0, "line": 2418}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 0, "line": 2418}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 2419}, "message": "1st function call argument is an uninitialized value"}, {"location": {"col": 2, "file": 1, "line": 2216}, "message": "expanded from macro 'pte_unmap_unlock'"}, {"location": {"col": 3, "file": 0, "line": 2419}, "message": "1st function call argument is an uninitialized value"}], "macros": [], "notes": [], "path": "/src/mm/memory.c", "reportHash": "2238fa64ebe3a0022ff344e76edeae86", "checkerName": "clang-analyzer-core.CallAndMessage", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
