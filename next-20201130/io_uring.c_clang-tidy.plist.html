<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/fs/io_uring.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Shared application/kernel submission and completion ring pairs, for\n * supporting fast/efficient IO.\n *\n * A note on the read/write ordering memory barriers that are matched between\n * the application and kernel side.\n *\n * After the application reads the CQ ring tail, it must use an\n * appropriate smp_rmb() to pair with the smp_wmb() the kernel uses\n * before writing the tail (using smp_load_acquire to read the tail will\n * do). It also needs a smp_mb() before updating CQ head (ordering the\n * entry load(s) with the head store), pairing with an implicit barrier\n * through a control-dependency in io_get_cqring (smp_store_release to\n * store head will do). Failure to do so could lead to reading invalid\n * CQ entries.\n *\n * Likewise, the application must use an appropriate smp_wmb() before\n * writing the SQ tail (ordering SQ entry stores with the tail store),\n * which pairs with smp_load_acquire in io_get_sqring (smp_store_release\n * to store the tail will do). And it needs a barrier ordering the SQ\n * head load before writing new SQ entries (smp_load_acquire to read\n * head will do).\n *\n * When using the SQ poll thread (IORING_SETUP_SQPOLL), the application\n * needs to check the SQ flags for IORING_SQ_NEED_WAKEUP *after*\n * updating the SQ tail; a full memory barrier smp_mb() is needed\n * between.\n *\n * Also see the examples in the liburing library:\n *\n *\tgit://git.kernel.dk/liburing\n *\n * io_uring also uses READ/WRITE_ONCE() for _any_ store or load that happens\n * from data shared between the kernel and application. This is done both\n * for ordering purposes, but also to ensure that once a value is loaded from\n * data that the application could potentially modify, it remains stable.\n *\n * Copyright (C) 2018-2019 Jens Axboe\n * Copyright (c) 2018-2019 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <net/compat.h>\n#include <linux/refcount.h>\n#include <linux/uio.h>\n#include <linux/bits.h>\n\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/kthread.h>\n#include <linux/blkdev.h>\n#include <linux/bvec.h>\n#include <linux/net.h>\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <linux/anon_inodes.h>\n#include <linux/sched/mm.h>\n#include <linux/uaccess.h>\n#include <linux/nospec.h>\n#include <linux/sizes.h>\n#include <linux/hugetlb.h>\n#include <linux/highmem.h>\n#include <linux/namei.h>\n#include <linux/fsnotify.h>\n#include <linux/fadvise.h>\n#include <linux/eventpoll.h>\n#include <linux/fs_struct.h>\n#include <linux/splice.h>\n#include <linux/task_work.h>\n#include <linux/pagemap.h>\n#include <linux/io_uring.h>\n#include <linux/blk-cgroup.h>\n#include <linux/audit.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"internal.h\"\n#include \"io-wq.h\"\n\n#define IORING_MAX_ENTRIES\t32768\n#define IORING_MAX_CQ_ENTRIES\t(2 * IORING_MAX_ENTRIES)\n\n/*\n * Shift of 9 is 512 entries, or exactly one page on 64-bit archs\n */\n#define IORING_FILE_TABLE_SHIFT\t9\n#define IORING_MAX_FILES_TABLE\t(1U << IORING_FILE_TABLE_SHIFT)\n#define IORING_FILE_TABLE_MASK\t(IORING_MAX_FILES_TABLE - 1)\n#define IORING_MAX_FIXED_FILES\t(64 * IORING_MAX_FILES_TABLE)\n#define IORING_MAX_RESTRICTIONS\t(IORING_RESTRICTION_LAST + \\\n\t\t\t\t IORING_REGISTER_LAST + IORING_OP_LAST)\n\nstruct io_uring {\n\tu32 head ____cacheline_aligned_in_smp;\n\tu32 tail ____cacheline_aligned_in_smp;\n};\n\n/*\n * This data is shared with the application through the mmap at offsets\n * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.\n *\n * The offsets to the member fields are published through struct\n * io_sqring_offsets when calling io_uring_setup.\n */\nstruct io_rings {\n\t/*\n\t * Head and tail offsets into the ring; the offsets need to be\n\t * masked to get valid indices.\n\t *\n\t * The kernel controls head of the sq ring and the tail of the cq ring,\n\t * and the application controls tail of the sq ring and the head of the\n\t * cq ring.\n\t */\n\tstruct io_uring\t\tsq, cq;\n\t/*\n\t * Bitmasks to apply to head and tail offsets (constant, equals\n\t * ring_entries - 1)\n\t */\n\tu32\t\t\tsq_ring_mask, cq_ring_mask;\n\t/* Ring sizes (constant, power of 2) */\n\tu32\t\t\tsq_ring_entries, cq_ring_entries;\n\t/*\n\t * Number of invalid entries dropped by the kernel due to\n\t * invalid index stored in array\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * After a new SQ head value was read by the application this\n\t * counter includes all submissions that were dropped reaching\n\t * the new SQ head (and possibly more).\n\t */\n\tu32\t\t\tsq_dropped;\n\t/*\n\t * Runtime SQ flags\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application.\n\t *\n\t * The application needs a full memory barrier before checking\n\t * for IORING_SQ_NEED_WAKEUP after updating the sq tail.\n\t */\n\tu32\t\t\tsq_flags;\n\t/*\n\t * Runtime CQ flags\n\t *\n\t * Written by the application, shouldn't be modified by the\n\t * kernel.\n\t */\n\tu32                     cq_flags;\n\t/*\n\t * Number of completion events lost because the queue was full;\n\t * this should be avoided by the application by making sure\n\t * there are not more requests pending than there is space in\n\t * the completion queue.\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * As completion events come in out of order this counter is not\n\t * ordered with any other data.\n\t */\n\tu32\t\t\tcq_overflow;\n\t/*\n\t * Ring buffer of completion events.\n\t *\n\t * The kernel writes completion events fresh every time they are\n\t * produced, so the application is allowed to modify pending\n\t * entries.\n\t */\n\tstruct io_uring_cqe\tcqes[] ____cacheline_aligned_in_smp;\n};\n\nstruct io_mapped_ubuf {\n\tu64\t\tubuf;\n\tsize_t\t\tlen;\n\tstruct\t\tbio_vec *bvec;\n\tunsigned int\tnr_bvecs;\n\tunsigned long\tacct_pages;\n};\n\nstruct fixed_file_table {\n\tstruct file\t\t**files;\n};\n\nstruct fixed_file_ref_node {\n\tstruct percpu_ref\t\trefs;\n\tstruct list_head\t\tnode;\n\tstruct list_head\t\tfile_list;\n\tstruct fixed_file_data\t\t*file_data;\n\tstruct llist_node\t\tllist;\n\tbool\t\t\t\tdone;\n};\n\nstruct fixed_file_data {\n\tstruct fixed_file_table\t\t*table;\n\tstruct io_ring_ctx\t\t*ctx;\n\n\tstruct fixed_file_ref_node\t*node;\n\tstruct percpu_ref\t\trefs;\n\tstruct completion\t\tdone;\n\tstruct list_head\t\tref_list;\n\tspinlock_t\t\t\tlock;\n};\n\nstruct io_buffer {\n\tstruct list_head list;\n\t__u64 addr;\n\t__s32 len;\n\t__u16 bid;\n};\n\nstruct io_restriction {\n\tDECLARE_BITMAP(register_op, IORING_REGISTER_LAST);\n\tDECLARE_BITMAP(sqe_op, IORING_OP_LAST);\n\tu8 sqe_flags_allowed;\n\tu8 sqe_flags_required;\n\tbool registered;\n};\n\nstruct io_sq_data {\n\trefcount_t\t\trefs;\n\tstruct mutex\t\tlock;\n\n\t/* ctx's that are using this sqd */\n\tstruct list_head\tctx_list;\n\tstruct list_head\tctx_new_list;\n\tstruct mutex\t\tctx_lock;\n\n\tstruct task_struct\t*thread;\n\tstruct wait_queue_head\twait;\n\n\tunsigned\t\tsq_thread_idle;\n};\n\nstruct io_ring_ctx {\n\tstruct {\n\t\tstruct percpu_ref\trefs;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned int\t\tflags;\n\t\tunsigned int\t\tcompat: 1;\n\t\tunsigned int\t\tlimit_mem: 1;\n\t\tunsigned int\t\tcq_overflow_flushed: 1;\n\t\tunsigned int\t\tdrain_next: 1;\n\t\tunsigned int\t\teventfd_async: 1;\n\t\tunsigned int\t\trestricted: 1;\n\n\t\t/*\n\t\t * Ring buffer of indices into array of io_uring_sqe, which is\n\t\t * mmapped by the application using the IORING_OFF_SQES offset.\n\t\t *\n\t\t * This indirection could e.g. be used to assign fixed\n\t\t * io_uring_sqe entries to operations and only submit them to\n\t\t * the queue when needed.\n\t\t *\n\t\t * The kernel modifies neither the indices array nor the entries\n\t\t * array.\n\t\t */\n\t\tu32\t\t\t*sq_array;\n\t\tunsigned\t\tcached_sq_head;\n\t\tunsigned\t\tsq_entries;\n\t\tunsigned\t\tsq_mask;\n\t\tunsigned\t\tsq_thread_idle;\n\t\tunsigned\t\tcached_sq_dropped;\n\t\tunsigned\t\tcached_cq_overflow;\n\t\tunsigned long\t\tsq_check_overflow;\n\n\t\tstruct list_head\tdefer_list;\n\t\tstruct list_head\ttimeout_list;\n\t\tstruct list_head\tcq_overflow_list;\n\n\t\tstruct io_uring_sqe\t*sq_sqes;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct io_rings\t*rings;\n\n\t/* IO offload */\n\tstruct io_wq\t\t*io_wq;\n\n\t/*\n\t * For SQPOLL usage - we hold a reference to the parent task, so we\n\t * have access to the ->files\n\t */\n\tstruct task_struct\t*sqo_task;\n\n\t/* Only used for accounting purposes */\n\tstruct mm_struct\t*mm_account;\n\n#ifdef CONFIG_BLK_CGROUP\n\tstruct cgroup_subsys_state\t*sqo_blkcg_css;\n#endif\n\n\tstruct io_sq_data\t*sq_data;\t/* if using sq thread polling */\n\n\tstruct wait_queue_head\tsqo_sq_wait;\n\tstruct list_head\tsqd_list;\n\n\t/*\n\t * If used, fixed file set. Writers must ensure that ->refs is dead,\n\t * readers must ensure that ->refs is alive as long as the file* is\n\t * used. Only updated through io_uring_register(2).\n\t */\n\tstruct fixed_file_data\t*file_data;\n\tunsigned\t\tnr_user_files;\n\n\t/* if used, fixed mapped user buffers */\n\tunsigned\t\tnr_user_bufs;\n\tstruct io_mapped_ubuf\t*user_bufs;\n\n\tstruct user_struct\t*user;\n\n\tconst struct cred\t*creds;\n\n#ifdef CONFIG_AUDIT\n\tkuid_t\t\t\tloginuid;\n\tunsigned int\t\tsessionid;\n#endif\n\n\tstruct completion\tref_comp;\n\tstruct completion\tsq_thread_comp;\n\n\t/* if all else fails... */\n\tstruct io_kiocb\t\t*fallback_req;\n\n#if defined(CONFIG_UNIX)\n\tstruct socket\t\t*ring_sock;\n#endif\n\n\tstruct idr\t\tio_buffer_idr;\n\n\tstruct idr\t\tpersonality_idr;\n\n\tstruct {\n\t\tunsigned\t\tcached_cq_tail;\n\t\tunsigned\t\tcq_entries;\n\t\tunsigned\t\tcq_mask;\n\t\tatomic_t\t\tcq_timeouts;\n\t\tunsigned long\t\tcq_check_overflow;\n\t\tstruct wait_queue_head\tcq_wait;\n\t\tstruct fasync_struct\t*cq_fasync;\n\t\tstruct eventfd_ctx\t*cq_ev_fd;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\t\turing_lock;\n\t\twait_queue_head_t\twait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\t\tcompletion_lock;\n\n\t\t/*\n\t\t * ->iopoll_list is protected by the ctx->uring_lock for\n\t\t * io_uring instances that don't use IORING_SETUP_SQPOLL.\n\t\t * For SQPOLL, only the single threaded io_sq_thread() will\n\t\t * manipulate the list, hence no extra locking is needed there.\n\t\t */\n\t\tstruct list_head\tiopoll_list;\n\t\tstruct hlist_head\t*cancel_hash;\n\t\tunsigned\t\tcancel_hash_bits;\n\t\tbool\t\t\tpoll_multi_file;\n\n\t\tspinlock_t\t\tinflight_lock;\n\t\tstruct list_head\tinflight_list;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct delayed_work\t\tfile_put_work;\n\tstruct llist_head\t\tfile_put_llist;\n\n\tstruct work_struct\t\texit_work;\n\tstruct io_restriction\t\trestrictions;\n};\n\n/*\n * First field must be the file pointer in all the\n * iocb unions! See also 'struct kiocb' in <linux/fs.h>\n */\nstruct io_poll_iocb {\n\tstruct file\t\t\t*file;\n\tstruct wait_queue_head\t\t*head;\n\t__poll_t\t\t\tevents;\n\tbool\t\t\t\tdone;\n\tbool\t\t\t\tcanceled;\n\tstruct wait_queue_entry\t\twait;\n};\n\nstruct io_poll_remove {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_close {\n\tstruct file\t\t\t*file;\n\tstruct file\t\t\t*put_file;\n\tint\t\t\t\tfd;\n};\n\nstruct io_timeout_data {\n\tstruct io_kiocb\t\t\t*req;\n\tstruct hrtimer\t\t\ttimer;\n\tstruct timespec64\t\tts;\n\tenum hrtimer_mode\t\tmode;\n};\n\nstruct io_accept {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint __user\t\t\t*addr_len;\n\tint\t\t\t\tflags;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_sync {\n\tstruct file\t\t\t*file;\n\tloff_t\t\t\t\tlen;\n\tloff_t\t\t\t\toff;\n\tint\t\t\t\tflags;\n\tint\t\t\t\tmode;\n};\n\nstruct io_cancel {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_timeout {\n\tstruct file\t\t\t*file;\n\tu32\t\t\t\toff;\n\tu32\t\t\t\ttarget_seq;\n\tstruct list_head\t\tlist;\n\t/* head of the link, used by linked timeouts only */\n\tstruct io_kiocb\t\t\t*head;\n};\n\nstruct io_timeout_rem {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_rw {\n\t/* NOTE: kiocb has the file as the first member, so don't do it here */\n\tstruct kiocb\t\t\tkiocb;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tlen;\n};\n\nstruct io_connect {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint\t\t\t\taddr_len;\n};\n\nstruct io_sr_msg {\n\tstruct file\t\t\t*file;\n\tunion {\n\t\tstruct user_msghdr __user *umsg;\n\t\tvoid __user\t\t*buf;\n\t};\n\tint\t\t\t\tmsg_flags;\n\tint\t\t\t\tbgid;\n\tsize_t\t\t\t\tlen;\n\tstruct io_buffer\t\t*kbuf;\n};\n\nstruct io_open {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tbool\t\t\t\tignore_nonblock;\n\tstruct filename\t\t\t*filename;\n\tstruct open_how\t\t\thow;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_files_update {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\targ;\n\tu32\t\t\t\tnr_args;\n\tu32\t\t\t\toffset;\n};\n\nstruct io_fadvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\toffset;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_madvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_epoll {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tepfd;\n\tint\t\t\t\top;\n\tint\t\t\t\tfd;\n\tstruct epoll_event\t\tevent;\n};\n\nstruct io_splice {\n\tstruct file\t\t\t*file_out;\n\tstruct file\t\t\t*file_in;\n\tloff_t\t\t\t\toff_out;\n\tloff_t\t\t\t\toff_in;\n\tu64\t\t\t\tlen;\n\tunsigned int\t\t\tflags;\n};\n\nstruct io_provide_buf {\n\tstruct file\t\t\t*file;\n\t__u64\t\t\t\taddr;\n\t__s32\t\t\t\tlen;\n\t__u32\t\t\t\tbgid;\n\t__u16\t\t\t\tnbufs;\n\t__u16\t\t\t\tbid;\n};\n\nstruct io_statx {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tunsigned int\t\t\tmask;\n\tunsigned int\t\t\tflags;\n\tconst char __user\t\t*filename;\n\tstruct statx __user\t\t*buffer;\n};\n\nstruct io_shutdown {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\thow;\n};\n\nstruct io_rename {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\told_dfd;\n\tint\t\t\t\tnew_dfd;\n\tstruct filename\t\t\t*oldpath;\n\tstruct filename\t\t\t*newpath;\n\tint\t\t\t\tflags;\n};\n\nstruct io_unlink {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tint\t\t\t\tflags;\n\tstruct filename\t\t\t*filename;\n};\n\nstruct io_completion {\n\tstruct file\t\t\t*file;\n\tstruct list_head\t\tlist;\n\tint\t\t\t\tcflags;\n};\n\nstruct io_async_connect {\n\tstruct sockaddr_storage\t\taddress;\n};\n\nstruct io_async_msghdr {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\tstruct iovec\t\t\t*iov;\n\tstruct sockaddr __user\t\t*uaddr;\n\tstruct msghdr\t\t\tmsg;\n\tstruct sockaddr_storage\t\taddr;\n};\n\nstruct io_async_rw {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\tconst struct iovec\t\t*free_iovec;\n\tstruct iov_iter\t\t\titer;\n\tsize_t\t\t\t\tbytes_done;\n\tstruct wait_page_queue\t\twpq;\n};\n\nenum {\n\tREQ_F_FIXED_FILE_BIT\t= IOSQE_FIXED_FILE_BIT,\n\tREQ_F_IO_DRAIN_BIT\t= IOSQE_IO_DRAIN_BIT,\n\tREQ_F_LINK_BIT\t\t= IOSQE_IO_LINK_BIT,\n\tREQ_F_HARDLINK_BIT\t= IOSQE_IO_HARDLINK_BIT,\n\tREQ_F_FORCE_ASYNC_BIT\t= IOSQE_ASYNC_BIT,\n\tREQ_F_BUFFER_SELECT_BIT\t= IOSQE_BUFFER_SELECT_BIT,\n\n\tREQ_F_FAIL_LINK_BIT,\n\tREQ_F_INFLIGHT_BIT,\n\tREQ_F_CUR_POS_BIT,\n\tREQ_F_NOWAIT_BIT,\n\tREQ_F_LINK_TIMEOUT_BIT,\n\tREQ_F_ISREG_BIT,\n\tREQ_F_NEED_CLEANUP_BIT,\n\tREQ_F_POLLED_BIT,\n\tREQ_F_BUFFER_SELECTED_BIT,\n\tREQ_F_NO_FILE_TABLE_BIT,\n\tREQ_F_WORK_INITIALIZED_BIT,\n\tREQ_F_LTIMEOUT_ACTIVE_BIT,\n\n\t/* not a real bit, just to check we're not overflowing the space */\n\t__REQ_F_LAST_BIT,\n};\n\nenum {\n\t/* ctx owns file */\n\tREQ_F_FIXED_FILE\t= BIT(REQ_F_FIXED_FILE_BIT),\n\t/* drain existing IO first */\n\tREQ_F_IO_DRAIN\t\t= BIT(REQ_F_IO_DRAIN_BIT),\n\t/* linked sqes */\n\tREQ_F_LINK\t\t= BIT(REQ_F_LINK_BIT),\n\t/* doesn't sever on completion < 0 */\n\tREQ_F_HARDLINK\t\t= BIT(REQ_F_HARDLINK_BIT),\n\t/* IOSQE_ASYNC */\n\tREQ_F_FORCE_ASYNC\t= BIT(REQ_F_FORCE_ASYNC_BIT),\n\t/* IOSQE_BUFFER_SELECT */\n\tREQ_F_BUFFER_SELECT\t= BIT(REQ_F_BUFFER_SELECT_BIT),\n\n\t/* fail rest of links */\n\tREQ_F_FAIL_LINK\t\t= BIT(REQ_F_FAIL_LINK_BIT),\n\t/* on inflight list */\n\tREQ_F_INFLIGHT\t\t= BIT(REQ_F_INFLIGHT_BIT),\n\t/* read/write uses file position */\n\tREQ_F_CUR_POS\t\t= BIT(REQ_F_CUR_POS_BIT),\n\t/* must not punt to workers */\n\tREQ_F_NOWAIT\t\t= BIT(REQ_F_NOWAIT_BIT),\n\t/* has or had linked timeout */\n\tREQ_F_LINK_TIMEOUT\t= BIT(REQ_F_LINK_TIMEOUT_BIT),\n\t/* regular file */\n\tREQ_F_ISREG\t\t= BIT(REQ_F_ISREG_BIT),\n\t/* needs cleanup */\n\tREQ_F_NEED_CLEANUP\t= BIT(REQ_F_NEED_CLEANUP_BIT),\n\t/* already went through poll handler */\n\tREQ_F_POLLED\t\t= BIT(REQ_F_POLLED_BIT),\n\t/* buffer already selected */\n\tREQ_F_BUFFER_SELECTED\t= BIT(REQ_F_BUFFER_SELECTED_BIT),\n\t/* doesn't need file table for this request */\n\tREQ_F_NO_FILE_TABLE\t= BIT(REQ_F_NO_FILE_TABLE_BIT),\n\t/* io_wq_work is initialized */\n\tREQ_F_WORK_INITIALIZED\t= BIT(REQ_F_WORK_INITIALIZED_BIT),\n\t/* linked timeout is active, i.e. prepared by link's head */\n\tREQ_F_LTIMEOUT_ACTIVE\t= BIT(REQ_F_LTIMEOUT_ACTIVE_BIT),\n};\n\nstruct async_poll {\n\tstruct io_poll_iocb\tpoll;\n\tstruct io_poll_iocb\t*double_poll;\n};\n\n/*\n * NOTE! Each of the iocb union members has the file pointer\n * as the first entry in their struct definition. So you can\n * access the file pointer through any of the sub-structs,\n * or directly as just 'ki_filp' in this struct.\n */\nstruct io_kiocb {\n\tunion {\n\t\tstruct file\t\t*file;\n\t\tstruct io_rw\t\trw;\n\t\tstruct io_poll_iocb\tpoll;\n\t\tstruct io_poll_remove\tpoll_remove;\n\t\tstruct io_accept\taccept;\n\t\tstruct io_sync\t\tsync;\n\t\tstruct io_cancel\tcancel;\n\t\tstruct io_timeout\ttimeout;\n\t\tstruct io_timeout_rem\ttimeout_rem;\n\t\tstruct io_connect\tconnect;\n\t\tstruct io_sr_msg\tsr_msg;\n\t\tstruct io_open\t\topen;\n\t\tstruct io_close\t\tclose;\n\t\tstruct io_files_update\tfiles_update;\n\t\tstruct io_fadvise\tfadvise;\n\t\tstruct io_madvise\tmadvise;\n\t\tstruct io_epoll\t\tepoll;\n\t\tstruct io_splice\tsplice;\n\t\tstruct io_provide_buf\tpbuf;\n\t\tstruct io_statx\t\tstatx;\n\t\tstruct io_shutdown\tshutdown;\n\t\tstruct io_rename\trename;\n\t\tstruct io_unlink\tunlink;\n\t\t/* use only after cleaning per-op data, see io_clean_op() */\n\t\tstruct io_completion\tcompl;\n\t};\n\n\t/* opcode allocated if it needs to store data for async defer */\n\tvoid\t\t\t\t*async_data;\n\tu8\t\t\t\topcode;\n\t/* polled IO has completed */\n\tu8\t\t\t\tiopoll_completed;\n\n\tu16\t\t\t\tbuf_index;\n\tu32\t\t\t\tresult;\n\n\tstruct io_ring_ctx\t\t*ctx;\n\tunsigned int\t\t\tflags;\n\trefcount_t\t\t\trefs;\n\tstruct task_struct\t\t*task;\n\tu64\t\t\t\tuser_data;\n\n\tstruct io_kiocb\t\t\t*link;\n\tstruct percpu_ref\t\t*fixed_file_refs;\n\n\t/*\n\t * 1. used with ctx->iopoll_list with reads/writes\n\t * 2. to track reqs with ->files (see io_op_def::file_table)\n\t */\n\tstruct list_head\t\tinflight_entry;\n\tstruct callback_head\t\ttask_work;\n\t/* for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll */\n\tstruct hlist_node\t\thash_node;\n\tstruct async_poll\t\t*apoll;\n\tstruct io_wq_work\t\twork;\n};\n\nstruct io_defer_entry {\n\tstruct list_head\tlist;\n\tstruct io_kiocb\t\t*req;\n\tu32\t\t\tseq;\n};\n\n#define IO_IOPOLL_BATCH\t\t\t8\n\nstruct io_comp_state {\n\tunsigned int\t\tnr;\n\tstruct list_head\tlist;\n\tstruct io_ring_ctx\t*ctx;\n};\n\nstruct io_submit_state {\n\tstruct blk_plug\t\tplug;\n\n\t/*\n\t * io_kiocb alloc cache\n\t */\n\tvoid\t\t\t*reqs[IO_IOPOLL_BATCH];\n\tunsigned int\t\tfree_reqs;\n\n\tbool\t\t\tplug_started;\n\n\t/*\n\t * Batch completion logic\n\t */\n\tstruct io_comp_state\tcomp;\n\n\t/*\n\t * File reference cache\n\t */\n\tstruct file\t\t*file;\n\tunsigned int\t\tfd;\n\tunsigned int\t\tfile_refs;\n\tunsigned int\t\tios_left;\n};\n\nstruct io_op_def {\n\t/* needs req->file assigned */\n\tunsigned\t\tneeds_file : 1;\n\t/* don't fail if file grab fails */\n\tunsigned\t\tneeds_file_no_error : 1;\n\t/* hash wq insertion if file is a regular file */\n\tunsigned\t\thash_reg_file : 1;\n\t/* unbound wq insertion if file is a non-regular file */\n\tunsigned\t\tunbound_nonreg_file : 1;\n\t/* opcode is not supported by this kernel */\n\tunsigned\t\tnot_supported : 1;\n\t/* set if opcode supports polled \"wait\" */\n\tunsigned\t\tpollin : 1;\n\tunsigned\t\tpollout : 1;\n\t/* op supports buffer selection */\n\tunsigned\t\tbuffer_select : 1;\n\t/* must always have async data allocated */\n\tunsigned\t\tneeds_async_data : 1;\n\t/* should block plug */\n\tunsigned\t\tplug : 1;\n\t/* size of async data needed, if any */\n\tunsigned short\t\tasync_size;\n\tunsigned\t\twork_flags;\n};\n\nstatic const struct io_op_def io_op_defs[] = {\n\t[IORING_OP_NOP] = {},\n\t[IORING_OP_READV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_WRITEV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG |\n\t\t\t\t\t\tIO_WQ_WORK_FSIZE,\n\t},\n\t[IORING_OP_FSYNC] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_READ_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_WRITE_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG | IO_WQ_WORK_FSIZE |\n\t\t\t\t\t\tIO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_POLL_ADD] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_POLL_REMOVE] = {},\n\t[IORING_OP_SYNC_FILE_RANGE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_SENDMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_RECVMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_TIMEOUT] = {\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_TIMEOUT_REMOVE] = {},\n\t[IORING_OP_ACCEPT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_FILES,\n\t},\n\t[IORING_OP_ASYNC_CANCEL] = {},\n\t[IORING_OP_LINK_TIMEOUT] = {\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_CONNECT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_data\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_connect),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_FALLOCATE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG | IO_WQ_WORK_FSIZE,\n\t},\n\t[IORING_OP_OPENAT] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_BLKCG |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_CLOSE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.needs_file_no_error\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_FILES_UPDATE] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_STATX] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_MM |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_READ] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_WRITE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG |\n\t\t\t\t\t\tIO_WQ_WORK_FSIZE,\n\t},\n\t[IORING_OP_FADVISE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_MADVISE] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_SEND] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_RECV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_OPENAT2] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES | IO_WQ_WORK_FS |\n\t\t\t\t\t\tIO_WQ_WORK_BLKCG | IO_WQ_WORK_MM,\n\t},\n\t[IORING_OP_EPOLL_CTL] = {\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_FILES,\n\t},\n\t[IORING_OP_SPLICE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.work_flags\t\t= IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_PROVIDE_BUFFERS] = {},\n\t[IORING_OP_REMOVE_BUFFERS] = {},\n\t[IORING_OP_TEE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_SHUTDOWN] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_RENAMEAT] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_FILES |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_BLKCG,\n\t},\n\t[IORING_OP_UNLINKAT] = {\n\t\t.work_flags\t\t= IO_WQ_WORK_MM | IO_WQ_WORK_FILES |\n\t\t\t\t\t\tIO_WQ_WORK_FS | IO_WQ_WORK_BLKCG,\n\t},\n};\n\nenum io_mem_account {\n\tACCT_LOCKED,\n\tACCT_PINNED,\n};\n\nstatic void __io_complete_rw(struct io_kiocb *req, long res, long res2,\n\t\t\t     struct io_comp_state *cs);\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res);\nstatic void io_put_req(struct io_kiocb *req);\nstatic void io_put_req_deferred(struct io_kiocb *req, int nr);\nstatic void io_double_put_req(struct io_kiocb *req);\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);\nstatic void __io_queue_linked_timeout(struct io_kiocb *req);\nstatic void io_queue_linked_timeout(struct io_kiocb *req);\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_files_update *ip,\n\t\t\t\t unsigned nr_args);\nstatic void __io_clean_op(struct io_kiocb *req);\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed);\nstatic void __io_queue_sqe(struct io_kiocb *req, struct io_comp_state *cs);\nstatic void io_file_put_work(struct work_struct *work);\n\nstatic ssize_t io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t       struct iovec **iovec, struct iov_iter *iter,\n\t\t\t       bool needs_lock);\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force);\n\nstatic struct kmem_cache *req_cachep;\n\nstatic const struct file_operations io_uring_fops;\n\nstruct sock *io_uring_get_socket(struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tif (file->f_op == &io_uring_fops) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\treturn ctx->ring_sock->sk;\n\t}\n#endif\n\treturn NULL;\n}\nEXPORT_SYMBOL(io_uring_get_socket);\n\n#define io_for_each_link(pos, head) \\\n\tfor (pos = (head); pos; pos = pos->link)\n\nstatic inline void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & (REQ_F_NEED_CLEANUP | REQ_F_BUFFER_SELECTED |\n\t\t\t  REQ_F_INFLIGHT))\n\t\t__io_clean_op(req);\n}\n\nstatic inline void io_set_resource_node(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->fixed_file_refs) {\n\t\treq->fixed_file_refs = &ctx->file_data->node->refs;\n\t\tpercpu_ref_get(req->fixed_file_refs);\n\t}\n}\n\nstatic bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t\t    (req->work.flags & IO_WQ_WORK_FILES) &&\n\t\t    req->work.identity->files == files)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void io_sq_thread_drop_mm_files(void)\n{\n\tstruct files_struct *files = current->files;\n\tstruct mm_struct *mm = current->mm;\n\n\tif (mm) {\n\t\tkthread_unuse_mm(mm);\n\t\tmmput(mm);\n\t\tcurrent->mm = NULL;\n\t}\n\tif (files) {\n\t\tstruct nsproxy *nsproxy = current->nsproxy;\n\t\tstruct pid *thread_pid = current->thread_pid;\n\n\t\ttask_lock(current);\n\t\tcurrent->files = NULL;\n\t\tcurrent->nsproxy = NULL;\n\t\tcurrent->thread_pid = NULL;\n\t\ttask_unlock(current);\n\t\tput_files_struct(files);\n\t\tput_nsproxy(nsproxy);\n\t\tput_pid(thread_pid);\n\t}\n}\n\nstatic int __io_sq_thread_acquire_files(struct io_ring_ctx *ctx)\n{\n\tif (!current->files) {\n\t\tstruct files_struct *files;\n\t\tstruct nsproxy *nsproxy;\n\t\tstruct pid *thread_pid;\n\n\t\ttask_lock(ctx->sqo_task);\n\t\tfiles = ctx->sqo_task->files;\n\t\tif (!files) {\n\t\t\ttask_unlock(ctx->sqo_task);\n\t\t\treturn -EOWNERDEAD;\n\t\t}\n\t\tatomic_inc(&files->count);\n\t\tget_nsproxy(ctx->sqo_task->nsproxy);\n\t\tnsproxy = ctx->sqo_task->nsproxy;\n\t\tthread_pid = get_pid(ctx->sqo_task->thread_pid);\n\t\ttask_unlock(ctx->sqo_task);\n\n\t\ttask_lock(current);\n\t\tcurrent->files = files;\n\t\tcurrent->nsproxy = nsproxy;\n\t\tcurrent->thread_pid = thread_pid;\n\t\ttask_unlock(current);\n\t}\n\treturn 0;\n}\n\nstatic int __io_sq_thread_acquire_mm(struct io_ring_ctx *ctx)\n{\n\tstruct mm_struct *mm;\n\n\tif (current->mm)\n\t\treturn 0;\n\n\t/* Should never happen */\n\tif (unlikely(!(ctx->flags & IORING_SETUP_SQPOLL)))\n\t\treturn -EFAULT;\n\n\ttask_lock(ctx->sqo_task);\n\tmm = ctx->sqo_task->mm;\n\tif (unlikely(!mm || !mmget_not_zero(mm)))\n\t\tmm = NULL;\n\ttask_unlock(ctx->sqo_task);\n\n\tif (mm) {\n\t\tkthread_use_mm(mm);\n\t\treturn 0;\n\t}\n\n\treturn -EFAULT;\n}\n\nstatic int io_sq_thread_acquire_mm_files(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tint ret;\n\n\tif (def->work_flags & IO_WQ_WORK_MM) {\n\t\tret = __io_sq_thread_acquire_mm(ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tif (def->needs_file || (def->work_flags & IO_WQ_WORK_FILES)) {\n\t\tret = __io_sq_thread_acquire_files(ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void io_sq_thread_associate_blkcg(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct cgroup_subsys_state **cur_css)\n\n{\n#ifdef CONFIG_BLK_CGROUP\n\t/* puts the old one when swapping */\n\tif (*cur_css != ctx->sqo_blkcg_css) {\n\t\tkthread_associate_blkcg(ctx->sqo_blkcg_css);\n\t\t*cur_css = ctx->sqo_blkcg_css;\n\t}\n#endif\n}\n\nstatic void io_sq_thread_unassociate_blkcg(void)\n{\n#ifdef CONFIG_BLK_CGROUP\n\tkthread_associate_blkcg(NULL);\n#endif\n}\n\nstatic inline void req_set_fail_links(struct io_kiocb *req)\n{\n\tif ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)\n\t\treq->flags |= REQ_F_FAIL_LINK;\n}\n\n/*\n * None of these are dereferenced, they are simply used to check if any of\n * them have changed. If we're under current and check they are still the\n * same, we're fine to grab references to them for actual out-of-line use.\n */\nstatic void io_init_identity(struct io_identity *id)\n{\n\tid->files = current->files;\n\tid->mm = current->mm;\n#ifdef CONFIG_BLK_CGROUP\n\trcu_read_lock();\n\tid->blkcg_css = blkcg_css();\n\trcu_read_unlock();\n#endif\n\tid->creds = current_cred();\n\tid->nsproxy = current->nsproxy;\n\tid->fs = current->fs;\n\tid->fsize = rlimit(RLIMIT_FSIZE);\n#ifdef CONFIG_AUDIT\n\tid->loginuid = current->loginuid;\n\tid->sessionid = current->sessionid;\n#endif\n\trefcount_set(&id->count, 1);\n}\n\nstatic inline void __io_req_init_async(struct io_kiocb *req)\n{\n\tmemset(&req->work, 0, sizeof(req->work));\n\treq->flags |= REQ_F_WORK_INITIALIZED;\n}\n\n/*\n * Note: must call io_req_init_async() for the first time you\n * touch any members of io_wq_work.\n */\nstatic inline void io_req_init_async(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (req->flags & REQ_F_WORK_INITIALIZED)\n\t\treturn;\n\n\t__io_req_init_async(req);\n\n\t/* Grab a ref if this isn't our static identity */\n\treq->work.identity = tctx->identity;\n\tif (tctx->identity != &tctx->__identity)\n\t\trefcount_inc(&req->work.identity->count);\n}\n\nstatic inline bool io_async_submit(struct io_ring_ctx *ctx)\n{\n\treturn ctx->flags & IORING_SETUP_SQPOLL;\n}\n\nstatic void io_ring_ctx_ref_free(struct percpu_ref *ref)\n{\n\tstruct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);\n\n\tcomplete(&ctx->ref_comp);\n}\n\nstatic inline bool io_is_timeout_noseq(struct io_kiocb *req)\n{\n\treturn !req->timeout.off;\n}\n\nstatic struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\tctx->fallback_req = kmem_cache_alloc(req_cachep, GFP_KERNEL);\n\tif (!ctx->fallback_req)\n\t\tgoto err;\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread.\n\t */\n\thash_bits = ilog2(p->cq_entries);\n\thash_bits -= 5;\n\tif (hash_bits <= 0)\n\t\thash_bits = 1;\n\tctx->cancel_hash_bits = hash_bits;\n\tctx->cancel_hash = kmalloc((1U << hash_bits) * sizeof(struct hlist_head),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->cancel_hash)\n\t\tgoto err;\n\t__hash_init(ctx->cancel_hash, 1U << hash_bits);\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tinit_completion(&ctx->ref_comp);\n\tinit_completion(&ctx->sq_thread_comp);\n\tidr_init(&ctx->io_buffer_idr);\n\tidr_init(&ctx->personality_idr);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tINIT_LIST_HEAD(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tspin_lock_init(&ctx->inflight_lock);\n\tINIT_LIST_HEAD(&ctx->inflight_list);\n\tINIT_DELAYED_WORK(&ctx->file_put_work, io_file_put_work);\n\tinit_llist_head(&ctx->file_put_llist);\n\treturn ctx;\nerr:\n\tif (ctx->fallback_req)\n\t\tkmem_cache_free(req_cachep, ctx->fallback_req);\n\tkfree(ctx->cancel_hash);\n\tkfree(ctx);\n\treturn NULL;\n}\n\nstatic bool req_need_defer(struct io_kiocb *req, u32 seq)\n{\n\tif (unlikely(req->flags & REQ_F_IO_DRAIN)) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\treturn seq != ctx->cached_cq_tail\n\t\t\t\t+ READ_ONCE(ctx->cached_cq_overflow);\n\t}\n\n\treturn false;\n}\n\nstatic void __io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/* order cqe stores with ring update */\n\tsmp_store_release(&rings->cq.tail, ctx->cached_cq_tail);\n\n\tif (wq_has_sleeper(&ctx->cq_wait)) {\n\t\twake_up_interruptible(&ctx->cq_wait);\n\t\tkill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);\n\t}\n}\n\nstatic void io_put_identity(struct io_uring_task *tctx, struct io_kiocb *req)\n{\n\tif (req->work.identity == &tctx->__identity)\n\t\treturn;\n\tif (refcount_dec_and_test(&req->work.identity->count))\n\t\tkfree(req->work.identity);\n}\n\nstatic void io_req_clean_work(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_WORK_INITIALIZED))\n\t\treturn;\n\n\treq->flags &= ~REQ_F_WORK_INITIALIZED;\n\n\tif (req->work.flags & IO_WQ_WORK_MM) {\n\t\tmmdrop(req->work.identity->mm);\n\t\treq->work.flags &= ~IO_WQ_WORK_MM;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (req->work.flags & IO_WQ_WORK_BLKCG) {\n\t\tcss_put(req->work.identity->blkcg_css);\n\t\treq->work.flags &= ~IO_WQ_WORK_BLKCG;\n\t}\n#endif\n\tif (req->work.flags & IO_WQ_WORK_CREDS) {\n\t\tput_cred(req->work.identity->creds);\n\t\treq->work.flags &= ~IO_WQ_WORK_CREDS;\n\t}\n\tif (req->work.flags & IO_WQ_WORK_FS) {\n\t\tstruct fs_struct *fs = req->work.identity->fs;\n\n\t\tspin_lock(&req->work.identity->fs->lock);\n\t\tif (--fs->users)\n\t\t\tfs = NULL;\n\t\tspin_unlock(&req->work.identity->fs->lock);\n\t\tif (fs)\n\t\t\tfree_fs_struct(fs);\n\t\treq->work.flags &= ~IO_WQ_WORK_FS;\n\t}\n\n\tio_put_identity(req->task->io_uring, req);\n}\n\n/*\n * Create a private copy of io_identity, since some fields don't match\n * the current context.\n */\nstatic bool io_identity_cow(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tconst struct cred *creds = NULL;\n\tstruct io_identity *id;\n\n\tif (req->work.flags & IO_WQ_WORK_CREDS)\n\t\tcreds = req->work.identity->creds;\n\n\tid = kmemdup(req->work.identity, sizeof(*id), GFP_KERNEL);\n\tif (unlikely(!id)) {\n\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\t\treturn false;\n\t}\n\n\t/*\n\t * We can safely just re-init the creds we copied  Either the field\n\t * matches the current one, or we haven't grabbed it yet. The only\n\t * exception is ->creds, through registered personalities, so handle\n\t * that one separately.\n\t */\n\tio_init_identity(id);\n\tif (creds)\n\t\treq->work.identity->creds = creds;\n\n\t/* add one for this request */\n\trefcount_inc(&id->count);\n\n\t/* drop tctx and req identity references, if needed */\n\tif (tctx->identity != &tctx->__identity &&\n\t    refcount_dec_and_test(&tctx->identity->count))\n\t\tkfree(tctx->identity);\n\tif (req->work.identity != &tctx->__identity &&\n\t    refcount_dec_and_test(&req->work.identity->count))\n\t\tkfree(req->work.identity);\n\n\treq->work.identity = id;\n\ttctx->identity = id;\n\treturn true;\n}\n\nstatic bool io_grab_identity(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_identity *id = req->work.identity;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (def->work_flags & IO_WQ_WORK_FSIZE) {\n\t\tif (id->fsize != rlimit(RLIMIT_FSIZE))\n\t\t\treturn false;\n\t\treq->work.flags |= IO_WQ_WORK_FSIZE;\n\t}\n#ifdef CONFIG_BLK_CGROUP\n\tif (!(req->work.flags & IO_WQ_WORK_BLKCG) &&\n\t    (def->work_flags & IO_WQ_WORK_BLKCG)) {\n\t\trcu_read_lock();\n\t\tif (id->blkcg_css != blkcg_css()) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\t\t/*\n\t\t * This should be rare, either the cgroup is dying or the task\n\t\t * is moving cgroups. Just punt to root for the handful of ios.\n\t\t */\n\t\tif (css_tryget_online(id->blkcg_css))\n\t\t\treq->work.flags |= IO_WQ_WORK_BLKCG;\n\t\trcu_read_unlock();\n\t}\n#endif\n\tif (!(req->work.flags & IO_WQ_WORK_CREDS)) {\n\t\tif (id->creds != current_cred())\n\t\t\treturn false;\n\t\tget_cred(id->creds);\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n#ifdef CONFIG_AUDIT\n\tif (!uid_eq(current->loginuid, id->loginuid) ||\n\t    current->sessionid != id->sessionid)\n\t\treturn false;\n#endif\n\tif (!(req->work.flags & IO_WQ_WORK_FS) &&\n\t    (def->work_flags & IO_WQ_WORK_FS)) {\n\t\tif (current->fs != id->fs)\n\t\t\treturn false;\n\t\tspin_lock(&id->fs->lock);\n\t\tif (!id->fs->in_exec) {\n\t\t\tid->fs->users++;\n\t\t\treq->work.flags |= IO_WQ_WORK_FS;\n\t\t} else {\n\t\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\t\t}\n\t\tspin_unlock(&current->fs->lock);\n\t}\n\tif (!(req->work.flags & IO_WQ_WORK_FILES) &&\n\t    (def->work_flags & IO_WQ_WORK_FILES) &&\n\t    !(req->flags & REQ_F_NO_FILE_TABLE)) {\n\t\tif (id->files != current->files ||\n\t\t    id->nsproxy != current->nsproxy)\n\t\t\treturn false;\n\t\tatomic_inc(&id->files->count);\n\t\tget_nsproxy(id->nsproxy);\n\t\treq->flags |= REQ_F_INFLIGHT;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\t\treq->work.flags |= IO_WQ_WORK_FILES;\n\t}\n\n\treturn true;\n}\n\nstatic void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_identity *id;\n\n\tio_req_init_async(req);\n\tid = req->work.identity;\n\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\t/* ->mm can never change on us */\n\tif (!(req->work.flags & IO_WQ_WORK_MM) &&\n\t    (def->work_flags & IO_WQ_WORK_MM)) {\n\t\tmmgrab(id->mm);\n\t\treq->work.flags |= IO_WQ_WORK_MM;\n\t}\n\n\t/* if we fail grabbing identity, we must COW, regrab, and retry */\n\tif (io_grab_identity(req))\n\t\treturn;\n\n\tif (!io_identity_cow(req))\n\t\treturn;\n\n\t/* can't fail at this point */\n\tif (!io_grab_identity(req))\n\t\tWARN_ON(1);\n}\n\nstatic void io_prep_async_link(struct io_kiocb *req)\n{\n\tstruct io_kiocb *cur;\n\n\tio_for_each_link(cur, req)\n\t\tio_prep_async_work(cur);\n}\n\nstatic struct io_kiocb *__io_queue_async_work(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link = io_prep_linked_timeout(req);\n\n\ttrace_io_uring_queue_async_work(ctx, io_wq_is_hashed(&req->work), req,\n\t\t\t\t\t&req->work, req->flags);\n\tio_wq_enqueue(ctx->io_wq, &req->work);\n\treturn link;\n}\n\nstatic void io_queue_async_work(struct io_kiocb *req)\n{\n\tstruct io_kiocb *link;\n\n\t/* init ->work of the whole link before punting */\n\tio_prep_async_link(req);\n\tlink = __io_queue_async_work(req);\n\n\tif (link)\n\t\tio_queue_linked_timeout(link);\n}\n\nstatic void io_kill_timeout(struct io_kiocb *req)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\tint ret;\n\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret != -1) {\n\t\tatomic_set(&req->ctx->cq_timeouts,\n\t\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_cqring_fill_event(req, 0);\n\t\tio_put_req_deferred(req, 1);\n\t}\n}\n\n/*\n * Returns true if we found and killed one or more timeouts\n */\nstatic bool io_kill_timeouts(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t     struct files_struct *files)\n{\n\tstruct io_kiocb *req, *tmp;\n\tint canceled = 0;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n\t\tif (io_match_task(req, tsk, files)) {\n\t\t\tio_kill_timeout(req);\n\t\t\tcanceled++;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn canceled != 0;\n}\n\nstatic void __io_queue_deferred(struct io_ring_ctx *ctx)\n{\n\tdo {\n\t\tstruct io_defer_entry *de = list_first_entry(&ctx->defer_list,\n\t\t\t\t\t\tstruct io_defer_entry, list);\n\t\tstruct io_kiocb *link;\n\n\t\tif (req_need_defer(de->req, de->seq))\n\t\t\tbreak;\n\t\tlist_del_init(&de->list);\n\t\t/* punt-init is done before queueing for defer */\n\t\tlink = __io_queue_async_work(de->req);\n\t\tif (link) {\n\t\t\t__io_queue_linked_timeout(link);\n\t\t\t/* drop submission reference */\n\t\t\tio_put_req_deferred(link, 1);\n\t\t}\n\t\tkfree(de);\n\t} while (!list_empty(&ctx->defer_list));\n}\n\nstatic void io_flush_timeouts(struct io_ring_ctx *ctx)\n{\n\twhile (!list_empty(&ctx->timeout_list)) {\n\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\t\tif (req->timeout.target_seq != ctx->cached_cq_tail\n\t\t\t\t\t- atomic_read(&ctx->cq_timeouts))\n\t\t\tbreak;\n\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_kill_timeout(req);\n\t}\n}\n\nstatic void io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\tio_flush_timeouts(ctx);\n\t__io_commit_cqring(ctx);\n\n\tif (unlikely(!list_empty(&ctx->defer_list)))\n\t\t__io_queue_deferred(ctx);\n}\n\nstatic inline bool io_sqring_full(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *r = ctx->rings;\n\n\treturn READ_ONCE(r->sq.tail) - ctx->cached_sq_head == r->sq_ring_entries;\n}\n\nstatic struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned tail;\n\n\ttail = ctx->cached_cq_tail;\n\t/*\n\t * writes to the cq entry need to come after reading head; the\n\t * control dependency is enough as we're using WRITE_ONCE to\n\t * fill the cq entry\n\t */\n\tif (tail - READ_ONCE(rings->cq.head) == rings->cq_ring_entries)\n\t\treturn NULL;\n\n\tctx->cached_cq_tail++;\n\treturn &rings->cqes[tail & ctx->cq_mask];\n}\n\nstatic inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)\n{\n\tif (!ctx->cq_ev_fd)\n\t\treturn false;\n\tif (READ_ONCE(ctx->rings->cq_flags) & IORING_CQ_EVENTFD_DISABLED)\n\t\treturn false;\n\tif (!ctx->eventfd_async)\n\t\treturn true;\n\treturn io_wq_current_is_worker();\n}\n\nstatic void io_cqring_ev_posted(struct io_ring_ctx *ctx)\n{\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\tif (ctx->sq_data && waitqueue_active(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n\tif (io_should_trigger_evfd(ctx))\n\t\teventfd_signal(ctx->cq_ev_fd, 1);\n}\n\nstatic void io_cqring_mark_overflow(struct io_ring_ctx *ctx)\n{\n\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\tclear_bit(0, &ctx->sq_check_overflow);\n\t\tclear_bit(0, &ctx->cq_check_overflow);\n\t\tctx->rings->sq_flags &= ~IORING_SQ_CQ_OVERFLOW;\n\t}\n}\n\n/* Returns true if there are no backlogged entries after the flush */\nstatic bool io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force,\n\t\t\t\t     struct task_struct *tsk,\n\t\t\t\t     struct files_struct *files)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tstruct io_kiocb *req, *tmp;\n\tstruct io_uring_cqe *cqe;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\n\tif (!force) {\n\t\tif (list_empty_careful(&ctx->cq_overflow_list))\n\t\t\treturn true;\n\t\tif ((ctx->cached_cq_tail - READ_ONCE(rings->cq.head) ==\n\t\t    rings->cq_ring_entries))\n\t\t\treturn false;\n\t}\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\n\t/* if force is set, the ring is going away. always drop after that */\n\tif (force)\n\t\tctx->cq_overflow_flushed = 1;\n\n\tcqe = NULL;\n\tlist_for_each_entry_safe(req, tmp, &ctx->cq_overflow_list, compl.list) {\n\t\tif (!io_match_task(req, tsk, files))\n\t\t\tcontinue;\n\n\t\tcqe = io_get_cqring(ctx);\n\t\tif (!cqe && !force)\n\t\t\tbreak;\n\n\t\tlist_move(&req->compl.list, &list);\n\t\tif (cqe) {\n\t\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\t\tWRITE_ONCE(cqe->res, req->result);\n\t\t\tWRITE_ONCE(cqe->flags, req->compl.cflags);\n\t\t} else {\n\t\t\tctx->cached_cq_overflow++;\n\t\t\tWRITE_ONCE(ctx->rings->cq_overflow,\n\t\t\t\t   ctx->cached_cq_overflow);\n\t\t}\n\t}\n\n\tio_commit_cqring(ctx);\n\tio_cqring_mark_overflow(ctx);\n\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tio_cqring_ev_posted(ctx);\n\n\twhile (!list_empty(&list)) {\n\t\treq = list_first_entry(&list, struct io_kiocb, compl.list);\n\t\tlist_del(&req->compl.list);\n\t\tio_put_req(req);\n\t}\n\n\treturn cqe != NULL;\n}\n\nstatic void __io_cqring_fill_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_cqe *cqe;\n\n\ttrace_io_uring_complete(ctx, req->user_data, res);\n\n\t/*\n\t * If we can't get a cq entry, userspace overflowed the\n\t * submission (by quite a lot). Increment the overflow count in\n\t * the ring.\n\t */\n\tcqe = io_get_cqring(ctx);\n\tif (likely(cqe)) {\n\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\t} else if (ctx->cq_overflow_flushed ||\n\t\t   atomic_read(&req->task->io_uring->in_idle)) {\n\t\t/*\n\t\t * If we're in ring overflow flush mode, or in task cancel mode,\n\t\t * then we cannot store the request for later flushing, we need\n\t\t * to drop it on the floor.\n\t\t */\n\t\tctx->cached_cq_overflow++;\n\t\tWRITE_ONCE(ctx->rings->cq_overflow, ctx->cached_cq_overflow);\n\t} else {\n\t\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\t\tset_bit(0, &ctx->sq_check_overflow);\n\t\t\tset_bit(0, &ctx->cq_check_overflow);\n\t\t\tctx->rings->sq_flags |= IORING_SQ_CQ_OVERFLOW;\n\t\t}\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\trefcount_inc(&req->refs);\n\t\tlist_add_tail(&req->compl.list, &ctx->cq_overflow_list);\n\t}\n}\n\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res)\n{\n\t__io_cqring_fill_event(req, res, 0);\n}\n\nstatic void io_cqring_add_event(struct io_kiocb *req, long res, long cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t__io_cqring_fill_event(req, res, cflags);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n}\n\nstatic void io_submit_flush_completions(struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = cs->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\twhile (!list_empty(&cs->list)) {\n\t\tstruct io_kiocb *req;\n\n\t\treq = list_first_entry(&cs->list, struct io_kiocb, compl.list);\n\t\tlist_del(&req->compl.list);\n\t\t__io_cqring_fill_event(req, req->result, req->compl.cflags);\n\n\t\t/*\n\t\t * io_free_req() doesn't care about completion_lock unless one\n\t\t * of these flags is set. REQ_F_WORK_INITIALIZED is in the list\n\t\t * because of a potential deadlock with req->work.fs->lock\n\t\t */\n\t\tif (req->flags & (REQ_F_FAIL_LINK|REQ_F_LINK_TIMEOUT\n\t\t\t\t |REQ_F_WORK_INITIALIZED)) {\n\t\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\t\tio_put_req(req);\n\t\t\tspin_lock_irq(&ctx->completion_lock);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\tcs->nr = 0;\n}\n\nstatic void __io_req_complete(struct io_kiocb *req, long res, unsigned cflags,\n\t\t\t      struct io_comp_state *cs)\n{\n\tif (!cs) {\n\t\tio_cqring_add_event(req, res, cflags);\n\t\tio_put_req(req);\n\t} else {\n\t\tio_clean_op(req);\n\t\treq->result = res;\n\t\treq->compl.cflags = cflags;\n\t\tlist_add_tail(&req->compl.list, &cs->list);\n\t\tif (++cs->nr >= 32)\n\t\t\tio_submit_flush_completions(cs);\n\t}\n}\n\nstatic void io_req_complete(struct io_kiocb *req, long res)\n{\n\t__io_req_complete(req, res, 0, NULL);\n}\n\nstatic inline bool io_is_fallback_req(struct io_kiocb *req)\n{\n\treturn req == (struct io_kiocb *)\n\t\t\t((unsigned long) req->ctx->fallback_req & ~1UL);\n}\n\nstatic struct io_kiocb *io_get_fallback_req(struct io_ring_ctx *ctx)\n{\n\tstruct io_kiocb *req;\n\n\treq = ctx->fallback_req;\n\tif (!test_and_set_bit_lock(0, (unsigned long *) &ctx->fallback_req))\n\t\treturn req;\n\n\treturn NULL;\n}\n\nstatic struct io_kiocb *io_alloc_req(struct io_ring_ctx *ctx,\n\t\t\t\t     struct io_submit_state *state)\n{\n\tif (!state->free_reqs) {\n\t\tgfp_t gfp = GFP_KERNEL | __GFP_NOWARN;\n\t\tsize_t sz;\n\t\tint ret;\n\n\t\tsz = min_t(size_t, state->ios_left, ARRAY_SIZE(state->reqs));\n\t\tret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);\n\n\t\t/*\n\t\t * Bulk alloc is all-or-nothing. If we fail to get a batch,\n\t\t * retry single alloc to be on the safe side.\n\t\t */\n\t\tif (unlikely(ret <= 0)) {\n\t\t\tstate->reqs[0] = kmem_cache_alloc(req_cachep, gfp);\n\t\t\tif (!state->reqs[0])\n\t\t\t\tgoto fallback;\n\t\t\tret = 1;\n\t\t}\n\t\tstate->free_reqs = ret;\n\t}\n\n\tstate->free_reqs--;\n\treturn state->reqs[state->free_reqs];\nfallback:\n\treturn io_get_fallback_req(ctx);\n}\n\nstatic inline void io_put_file(struct io_kiocb *req, struct file *file,\n\t\t\t  bool fixed)\n{\n\tif (!fixed)\n\t\tfput(file);\n}\n\nstatic void io_dismantle_req(struct io_kiocb *req)\n{\n\tio_clean_op(req);\n\n\tif (req->async_data)\n\t\tkfree(req->async_data);\n\tif (req->file)\n\t\tio_put_file(req, req->file, (req->flags & REQ_F_FIXED_FILE));\n\tif (req->fixed_file_refs)\n\t\tpercpu_ref_put(req->fixed_file_refs);\n\tio_req_clean_work(req);\n}\n\nstatic void __io_free_req(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_dismantle_req(req);\n\n\tpercpu_counter_dec(&tctx->inflight);\n\tif (atomic_read(&tctx->in_idle))\n\t\twake_up(&tctx->wait);\n\tput_task_struct(req->task);\n\n\tif (likely(!io_is_fallback_req(req)))\n\t\tkmem_cache_free(req_cachep, req);\n\telse\n\t\tclear_bit_unlock(0, (unsigned long *) &ctx->fallback_req);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic inline void io_remove_next_linked(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\treq->link = nxt->link;\n\tnxt->link = NULL;\n}\n\nstatic void io_kill_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link;\n\tbool cancelled = false;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlink = req->link;\n\n\t/*\n\t * Can happen if a linked timeout fired and link had been like\n\t * req -> link t-out -> link t-out [-> ...]\n\t */\n\tif (link && (link->flags & REQ_F_LTIMEOUT_ACTIVE)) {\n\t\tstruct io_timeout_data *io = link->async_data;\n\t\tint ret;\n\n\t\tio_remove_next_linked(req);\n\t\tlink->timeout.head = NULL;\n\t\tret = hrtimer_try_to_cancel(&io->timer);\n\t\tif (ret != -1) {\n\t\t\tio_cqring_fill_event(link, -ECANCELED);\n\t\t\tio_commit_cqring(ctx);\n\t\t\tcancelled = true;\n\t\t}\n\t}\n\treq->flags &= ~REQ_F_LINK_TIMEOUT;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (cancelled) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tio_put_req(link);\n\t}\n}\n\n\nstatic void io_fail_links(struct io_kiocb *req)\n{\n\tstruct io_kiocb *link, *nxt;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlink = req->link;\n\treq->link = NULL;\n\n\twhile (link) {\n\t\tnxt = link->link;\n\t\tlink->link = NULL;\n\n\t\ttrace_io_uring_fail_link(req, link);\n\t\tio_cqring_fill_event(link, -ECANCELED);\n\n\t\t/*\n\t\t * It's ok to free under spinlock as they're not linked anymore,\n\t\t * but avoid REQ_F_WORK_INITIALIZED because it may deadlock on\n\t\t * work.fs->lock.\n\t\t */\n\t\tif (link->flags & REQ_F_WORK_INITIALIZED)\n\t\t\tio_put_req_deferred(link, 2);\n\t\telse\n\t\t\tio_double_put_req(link);\n\t\tlink = nxt;\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n}\n\nstatic struct io_kiocb *__io_req_find_next(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_LINK_TIMEOUT)\n\t\tio_kill_linked_timeout(req);\n\n\t/*\n\t * If LINK is set, we have dependent requests in this chain. If we\n\t * didn't fail this request, queue the first one up, moving any other\n\t * dependencies to the next request. In case of failure, fail the rest\n\t * of the chain.\n\t */\n\tif (likely(!(req->flags & REQ_F_FAIL_LINK))) {\n\t\tstruct io_kiocb *nxt = req->link;\n\n\t\treq->link = NULL;\n\t\treturn nxt;\n\t}\n\tio_fail_links(req);\n\treturn NULL;\n}\n\nstatic inline struct io_kiocb *io_req_find_next(struct io_kiocb *req)\n{\n\tif (likely(!(req->link) && !(req->flags & REQ_F_LINK_TIMEOUT)))\n\t\treturn NULL;\n\treturn __io_req_find_next(req);\n}\n\nstatic int io_req_task_work_add(struct io_kiocb *req)\n{\n\tstruct task_struct *tsk = req->task;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tenum task_work_notify_mode notify;\n\tint ret;\n\n\tif (tsk->flags & PF_EXITING)\n\t\treturn -ESRCH;\n\n\t/*\n\t * SQPOLL kernel thread doesn't need notification, just a wakeup. For\n\t * all other cases, use TWA_SIGNAL unconditionally to ensure we're\n\t * processing task_work. There's no reliable way to tell if TWA_RESUME\n\t * will do the job.\n\t */\n\tnotify = TWA_NONE;\n\tif (!(ctx->flags & IORING_SETUP_SQPOLL))\n\t\tnotify = TWA_SIGNAL;\n\n\tret = task_work_add(tsk, &req->task_work, notify);\n\tif (!ret)\n\t\twake_up_process(tsk);\n\n\treturn ret;\n}\n\nstatic void __io_req_task_cancel(struct io_kiocb *req, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tio_cqring_fill_event(req, error);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_double_put_req(req);\n}\n\nstatic void io_req_task_cancel(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t__io_req_task_cancel(req, -ECANCELED);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!__io_sq_thread_acquire_mm(ctx) &&\n\t    !__io_sq_thread_acquire_files(ctx)) {\n\t\tmutex_lock(&ctx->uring_lock);\n\t\t__io_queue_sqe(req, NULL);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t} else {\n\t\t__io_req_task_cancel(req, -EFAULT);\n\t}\n}\n\nstatic void io_req_task_submit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t__io_req_task_submit(req);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic void io_req_task_queue(struct io_kiocb *req)\n{\n\tint ret;\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n}\n\nstatic inline void io_queue_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = io_req_find_next(req);\n\n\tif (nxt)\n\t\tio_req_task_queue(nxt);\n}\n\nstatic void io_free_req(struct io_kiocb *req)\n{\n\tio_queue_next(req);\n\t__io_free_req(req);\n}\n\nstruct req_batch {\n\tvoid *reqs[IO_IOPOLL_BATCH];\n\tint to_free;\n\n\tstruct task_struct\t*task;\n\tint\t\t\ttask_refs;\n};\n\nstatic inline void io_init_req_batch(struct req_batch *rb)\n{\n\trb->to_free = 0;\n\trb->task_refs = 0;\n\trb->task = NULL;\n}\n\nstatic void __io_req_free_batch_flush(struct io_ring_ctx *ctx,\n\t\t\t\t      struct req_batch *rb)\n{\n\tkmem_cache_free_bulk(req_cachep, rb->to_free, rb->reqs);\n\tpercpu_ref_put_many(&ctx->refs, rb->to_free);\n\trb->to_free = 0;\n}\n\nstatic void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->to_free)\n\t\t__io_req_free_batch_flush(ctx, rb);\n\tif (rb->task) {\n\t\tstruct io_uring_task *tctx = rb->task->io_uring;\n\n\t\tpercpu_counter_sub(&tctx->inflight, rb->task_refs);\n\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\trb->task = NULL;\n\t}\n}\n\nstatic void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req)\n{\n\tif (unlikely(io_is_fallback_req(req))) {\n\t\tio_free_req(req);\n\t\treturn;\n\t}\n\tio_queue_next(req);\n\n\tif (req->task != rb->task) {\n\t\tif (rb->task) {\n\t\t\tstruct io_uring_task *tctx = rb->task->io_uring;\n\n\t\t\tpercpu_counter_sub(&tctx->inflight, rb->task_refs);\n\t\t\tput_task_struct_many(rb->task, rb->task_refs);\n\t\t}\n\t\trb->task = req->task;\n\t\trb->task_refs = 0;\n\t}\n\trb->task_refs++;\n\n\tio_dismantle_req(req);\n\trb->reqs[rb->to_free++] = req;\n\tif (unlikely(rb->to_free == ARRAY_SIZE(rb->reqs)))\n\t\t__io_req_free_batch_flush(req->ctx, rb);\n}\n\n/*\n * Drop reference to request, return next in chain (if there is one) if this\n * was the last reference to this request.\n */\nstatic struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = NULL;\n\n\tif (refcount_dec_and_test(&req->refs)) {\n\t\tnxt = io_req_find_next(req);\n\t\t__io_free_req(req);\n\t}\n\treturn nxt;\n}\n\nstatic void io_put_req(struct io_kiocb *req)\n{\n\tif (refcount_dec_and_test(&req->refs))\n\t\tio_free_req(req);\n}\n\nstatic void io_put_req_deferred_cb(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\tio_free_req(req);\n}\n\nstatic void io_free_req_deferred(struct io_kiocb *req)\n{\n\tint ret;\n\n\tinit_task_work(&req->task_work, io_put_req_deferred_cb);\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n}\n\nstatic inline void io_put_req_deferred(struct io_kiocb *req, int refs)\n{\n\tif (refcount_sub_and_test(refs, &req->refs))\n\t\tio_free_req_deferred(req);\n}\n\nstatic struct io_wq_work *io_steal_work(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt;\n\n\t/*\n\t * A ref is owned by io-wq in which context we're. So, if that's the\n\t * last one, it's safe to steal next work. False negatives are Ok,\n\t * it just will be re-punted async in io_put_work()\n\t */\n\tif (refcount_read(&req->refs) != 1)\n\t\treturn NULL;\n\n\tnxt = io_req_find_next(req);\n\treturn nxt ? &nxt->work : NULL;\n}\n\nstatic void io_double_put_req(struct io_kiocb *req)\n{\n\t/* drop both submit and complete references */\n\tif (refcount_sub_and_test(2, &req->refs))\n\t\tio_free_req(req);\n}\n\nstatic unsigned io_cqring_events(struct io_ring_ctx *ctx, bool noflush)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\tif (test_bit(0, &ctx->cq_check_overflow)) {\n\t\t/*\n\t\t * noflush == true is from the waitqueue handler, just ensure\n\t\t * we wake up the task, and the next invocation will flush the\n\t\t * entries. We cannot safely to it from here.\n\t\t */\n\t\tif (noflush && !list_empty(&ctx->cq_overflow_list))\n\t\t\treturn -1U;\n\n\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t}\n\n\t/* See comment at the top of this file */\n\tsmp_rmb();\n\treturn ctx->cached_cq_tail - READ_ONCE(rings->cq.head);\n}\n\nstatic inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/* make sure SQ entry isn't read before tail */\n\treturn smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;\n}\n\nstatic unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)\n{\n\tunsigned int cflags;\n\n\tcflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;\n\tcflags |= IORING_CQE_F_BUFFER;\n\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\tkfree(kbuf);\n\treturn cflags;\n}\n\nstatic inline unsigned int io_put_rw_kbuf(struct io_kiocb *req)\n{\n\tstruct io_buffer *kbuf;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\treturn io_put_kbuf(req, kbuf);\n}\n\nstatic inline bool io_run_task_work(void)\n{\n\t/*\n\t * Not safe to run on exiting task, and the task_work handling will\n\t * not add work to such a task.\n\t */\n\tif (unlikely(current->flags & PF_EXITING))\n\t\treturn false;\n\tif (current->task_works) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\ttask_work_run();\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void io_iopoll_queue(struct list_head *again)\n{\n\tstruct io_kiocb *req;\n\n\tdo {\n\t\treq = list_first_entry(again, struct io_kiocb, inflight_entry);\n\t\tlist_del(&req->inflight_entry);\n\t\t__io_complete_rw(req, -EAGAIN, 0, NULL);\n\t} while (!list_empty(again));\n}\n\n/*\n * Find and free completed poll iocbs\n */\nstatic void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t       struct list_head *done)\n{\n\tstruct req_batch rb;\n\tstruct io_kiocb *req;\n\tLIST_HEAD(again);\n\n\t/* order with ->result store in io_complete_rw_iopoll() */\n\tsmp_rmb();\n\n\tio_init_req_batch(&rb);\n\twhile (!list_empty(done)) {\n\t\tint cflags = 0;\n\n\t\treq = list_first_entry(done, struct io_kiocb, inflight_entry);\n\t\tif (READ_ONCE(req->result) == -EAGAIN) {\n\t\t\treq->result = 0;\n\t\t\treq->iopoll_completed = 0;\n\t\t\tlist_move_tail(&req->inflight_entry, &again);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&req->inflight_entry);\n\n\t\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\t\tcflags = io_put_rw_kbuf(req);\n\n\t\t__io_cqring_fill_event(req, req->result, cflags);\n\t\t(*nr_events)++;\n\n\t\tif (refcount_dec_and_test(&req->refs))\n\t\t\tio_req_free_batch(&rb, req);\n\t}\n\n\tio_commit_cqring(ctx);\n\tif (ctx->flags & IORING_SETUP_SQPOLL)\n\t\tio_cqring_ev_posted(ctx);\n\tio_req_free_batch_finish(ctx, &rb);\n\n\tif (!list_empty(&again))\n\t\tio_iopoll_queue(&again);\n}\n\nstatic int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\tlong min)\n{\n\tstruct io_kiocb *req, *tmp;\n\tLIST_HEAD(done);\n\tbool spin;\n\tint ret;\n\n\t/*\n\t * Only spin for completions if we don't have multiple devices hanging\n\t * off our complete list, and we're under the requested amount.\n\t */\n\tspin = !ctx->poll_multi_file && *nr_events < min;\n\n\tret = 0;\n\tlist_for_each_entry_safe(req, tmp, &ctx->iopoll_list, inflight_entry) {\n\t\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t\t/*\n\t\t * Move completed and retryable entries to our local lists.\n\t\t * If we find a request that requires polling, break out\n\t\t * and complete those lists first, if we have entries there.\n\t\t */\n\t\tif (READ_ONCE(req->iopoll_completed)) {\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!list_empty(&done))\n\t\t\tbreak;\n\n\t\tret = kiocb->ki_filp->f_op->iopoll(kiocb, spin);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t/* iopoll may have completed current req */\n\t\tif (READ_ONCE(req->iopoll_completed))\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\n\t\tif (ret && spin)\n\t\t\tspin = false;\n\t\tret = 0;\n\t}\n\n\tif (!list_empty(&done))\n\t\tio_iopoll_complete(ctx, nr_events, &done);\n\n\treturn ret;\n}\n\n/*\n * Poll for a minimum of 'min' events. Note that if min == 0 we consider that a\n * non-spinning poll check - we'll still enter the driver poll loop, but only\n * as a non-spinning completion check.\n */\nstatic int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t\tlong min)\n{\n\twhile (!list_empty(&ctx->iopoll_list) && !need_resched()) {\n\t\tint ret;\n\n\t\tret = io_do_iopoll(ctx, nr_events, min);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (*nr_events >= min)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/*\n * We can't just wait for polled events to come to us, we have to actively\n * find and complete them.\n */\nstatic void io_iopoll_try_reap_events(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn;\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!list_empty(&ctx->iopoll_list)) {\n\t\tunsigned int nr_events = 0;\n\n\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\t/* let it sleep and repeat later if can't complete a request */\n\t\tif (nr_events == 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * Ensure we allow local-to-the-cpu processing to take place,\n\t\t * in this case we need to ensure that we reap all events.\n\t\t * Also let task_work, etc. to progress by releasing the mutex\n\t\t */\n\t\tif (need_resched()) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tcond_resched();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic int io_iopoll_check(struct io_ring_ctx *ctx, long min)\n{\n\tunsigned int nr_events = 0;\n\tint iters = 0, ret = 0;\n\n\t/*\n\t * We disallow the app entering submit/complete with polling, but we\n\t * still need to lock the ring to prevent racing with polled issue\n\t * that got punted to a workqueue.\n\t */\n\tmutex_lock(&ctx->uring_lock);\n\tdo {\n\t\t/*\n\t\t * Don't enter poll loop if we already have events pending.\n\t\t * If we do, we can potentially be spinning for commands that\n\t\t * already triggered a CQE (eg in error).\n\t\t */\n\t\tif (io_cqring_events(ctx, false))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If a submit got punted to a workqueue, we can have the\n\t\t * application entering polling for a command before it gets\n\t\t * issued. That app will hold the uring_lock for the duration\n\t\t * of the poll right here, so we need to take a breather every\n\t\t * now and then to ensure that the issue has a chance to add\n\t\t * the poll to the issued list. Otherwise we can spin here\n\t\t * forever, while the workqueue is stuck trying to acquire the\n\t\t * very same mutex.\n\t\t */\n\t\tif (!(++iters & 7)) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tio_run_task_work();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\n\t\tret = io_iopoll_getevents(ctx, &nr_events, min);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\tret = 0;\n\t} while (min && !nr_events && !need_resched());\n\n\tmutex_unlock(&ctx->uring_lock);\n\treturn ret;\n}\n\nstatic void kiocb_end_write(struct io_kiocb *req)\n{\n\t/*\n\t * Tell lockdep we inherited freeze protection from submission\n\t * thread.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tstruct inode *inode = file_inode(req->file);\n\n\t\t__sb_writers_acquired(inode->i_sb, SB_FREEZE_WRITE);\n\t}\n\tfile_end_write(req->file);\n}\n\nstatic void io_complete_rw_common(struct kiocb *kiocb, long res,\n\t\t\t\t  struct io_comp_state *cs)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\tint cflags = 0;\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\n\tif (res != req->result)\n\t\treq_set_fail_links(req);\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_rw_kbuf(req);\n\t__io_req_complete(req, res, cflags, cs);\n}\n\n#ifdef CONFIG_BLOCK\nstatic bool io_resubmit_prep(struct io_kiocb *req, int error)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tssize_t ret = -ECANCELED;\n\tstruct iov_iter iter;\n\tint rw;\n\n\tif (error) {\n\t\tret = error;\n\t\tgoto end_req;\n\t}\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\trw = READ;\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\trw = WRITE;\n\t\tbreak;\n\tdefault:\n\t\tprintk_once(KERN_WARNING \"io_uring: bad opcode in resubmit %d\\n\",\n\t\t\t\treq->opcode);\n\t\tgoto end_req;\n\t}\n\n\tif (!req->async_data) {\n\t\tret = io_import_iovec(rw, req, &iovec, &iter, false);\n\t\tif (ret < 0)\n\t\t\tgoto end_req;\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, &iter, false);\n\t\tif (!ret)\n\t\t\treturn true;\n\t\tkfree(iovec);\n\t} else {\n\t\treturn true;\n\t}\nend_req:\n\treq_set_fail_links(req);\n\treturn false;\n}\n#endif\n\nstatic bool io_rw_reissue(struct io_kiocb *req, long res)\n{\n#ifdef CONFIG_BLOCK\n\tumode_t mode = file_inode(req->file)->i_mode;\n\tint ret;\n\n\tif (!S_ISBLK(mode) && !S_ISREG(mode))\n\t\treturn false;\n\tif ((res != -EAGAIN && res != -EOPNOTSUPP) || io_wq_current_is_worker())\n\t\treturn false;\n\n\tret = io_sq_thread_acquire_mm_files(req->ctx, req);\n\n\tif (io_resubmit_prep(req, ret)) {\n\t\trefcount_inc(&req->refs);\n\t\tio_queue_async_work(req);\n\t\treturn true;\n\t}\n\n#endif\n\treturn false;\n}\n\nstatic void __io_complete_rw(struct io_kiocb *req, long res, long res2,\n\t\t\t     struct io_comp_state *cs)\n{\n\tif (!io_rw_reissue(req, res))\n\t\tio_complete_rw_common(&req->rw.kiocb, res, cs);\n}\n\nstatic void io_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\t__io_complete_rw(req, res, res2, NULL);\n}\n\nstatic void io_complete_rw_iopoll(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\n\tif (res != -EAGAIN && res != req->result)\n\t\treq_set_fail_links(req);\n\n\tWRITE_ONCE(req->result, res);\n\t/* order with io_poll_complete() checking ->result */\n\tsmp_wmb();\n\tWRITE_ONCE(req->iopoll_completed, 1);\n}\n\n/*\n * After the iocb has been issued, it's safe to be found on the poll list.\n * Adding the kiocb to the list AFTER submission ensures that we don't\n * find it from a io_iopoll_getevents() thread before the issuer is done\n * accessing the kiocb cookie.\n */\nstatic void io_iopoll_req_issued(struct io_kiocb *req, bool in_async)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/*\n\t * Track whether we have multiple files in our lists. This will impact\n\t * how we do polling eventually, not spinning if we're on potentially\n\t * different devices.\n\t */\n\tif (list_empty(&ctx->iopoll_list)) {\n\t\tctx->poll_multi_file = false;\n\t} else if (!ctx->poll_multi_file) {\n\t\tstruct io_kiocb *list_req;\n\n\t\tlist_req = list_first_entry(&ctx->iopoll_list, struct io_kiocb,\n\t\t\t\t\t\tinflight_entry);\n\t\tif (list_req->file != req->file)\n\t\t\tctx->poll_multi_file = true;\n\t}\n\n\t/*\n\t * For fast devices, IO may have already completed. If it has, add\n\t * it to the front so we find it first.\n\t */\n\tif (READ_ONCE(req->iopoll_completed))\n\t\tlist_add(&req->inflight_entry, &ctx->iopoll_list);\n\telse\n\t\tlist_add_tail(&req->inflight_entry, &ctx->iopoll_list);\n\n\t/*\n\t * If IORING_SETUP_SQPOLL is enabled, sqes are either handled in sq thread\n\t * task context or in io worker task context. If current task context is\n\t * sq thread, we don't need to check whether should wake up sq thread.\n\t */\n\tif (in_async && (ctx->flags & IORING_SETUP_SQPOLL) &&\n\t    wq_has_sleeper(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n}\n\nstatic inline void __io_state_file_put(struct io_submit_state *state)\n{\n\tfput_many(state->file, state->file_refs);\n\tstate->file_refs = 0;\n}\n\nstatic inline void io_state_file_put(struct io_submit_state *state)\n{\n\tif (state->file_refs)\n\t\t__io_state_file_put(state);\n}\n\n/*\n * Get as many references to a file as we have IOs left in this submission,\n * assuming most submissions are for one file, or at least that each file\n * has more than one submission.\n */\nstatic struct file *__io_file_get(struct io_submit_state *state, int fd)\n{\n\tif (!state)\n\t\treturn fget(fd);\n\n\tif (state->file_refs) {\n\t\tif (state->fd == fd) {\n\t\t\tstate->file_refs--;\n\t\t\treturn state->file;\n\t\t}\n\t\t__io_state_file_put(state);\n\t}\n\tstate->file = fget_many(fd, state->ios_left);\n\tif (unlikely(!state->file))\n\t\treturn NULL;\n\n\tstate->fd = fd;\n\tstate->file_refs = state->ios_left - 1;\n\treturn state->file;\n}\n\nstatic bool io_bdev_nowait(struct block_device *bdev)\n{\n#ifdef CONFIG_BLOCK\n\treturn !bdev || blk_queue_nowait(bdev_get_queue(bdev));\n#else\n\treturn true;\n#endif\n}\n\n/*\n * If we tracked the file through the SCM inflight mechanism, we could support\n * any file. For now, just ensure that anything potentially problematic is done\n * inline.\n */\nstatic bool io_file_supports_async(struct file *file, int rw)\n{\n\tumode_t mode = file_inode(file)->i_mode;\n\n\tif (S_ISBLK(mode)) {\n\t\tif (io_bdev_nowait(file->f_inode->i_bdev))\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (S_ISCHR(mode) || S_ISSOCK(mode))\n\t\treturn true;\n\tif (S_ISREG(mode)) {\n\t\tif (io_bdev_nowait(file->f_inode->i_sb->s_bdev) &&\n\t\t    file->f_op != &io_uring_fops)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\t/* any ->read/write should understand O_NONBLOCK */\n\tif (file->f_flags & O_NONBLOCK)\n\t\treturn true;\n\n\tif (!(file->f_mode & FMODE_NOWAIT))\n\t\treturn false;\n\n\tif (rw == READ)\n\t\treturn file->f_op->read_iter != NULL;\n\n\treturn file->f_op->write_iter != NULL;\n}\n\nstatic int io_prep_rw(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tunsigned ioprio;\n\tint ret;\n\n\tif (S_ISREG(file_inode(req->file)->i_mode))\n\t\treq->flags |= REQ_F_ISREG;\n\n\tkiocb->ki_pos = READ_ONCE(sqe->off);\n\tif (kiocb->ki_pos == -1 && !(req->file->f_mode & FMODE_STREAM)) {\n\t\treq->flags |= REQ_F_CUR_POS;\n\t\tkiocb->ki_pos = req->file->f_pos;\n\t}\n\tkiocb->ki_hint = ki_hint_validate(file_write_hint(kiocb->ki_filp));\n\tkiocb->ki_flags = iocb_flags(kiocb->ki_filp);\n\tret = kiocb_set_rw_flags(kiocb, READ_ONCE(sqe->rw_flags));\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tioprio = READ_ONCE(sqe->ioprio);\n\tif (ioprio) {\n\t\tret = ioprio_check_cap(ioprio);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tkiocb->ki_ioprio = ioprio;\n\t} else\n\t\tkiocb->ki_ioprio = get_current_ioprio();\n\n\t/* don't allow async punt if RWF_NOWAIT was requested */\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tif (!(kiocb->ki_flags & IOCB_DIRECT) ||\n\t\t    !kiocb->ki_filp->f_op->iopoll)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tkiocb->ki_flags |= IOCB_HIPRI;\n\t\tkiocb->ki_complete = io_complete_rw_iopoll;\n\t\treq->iopoll_completed = 0;\n\t} else {\n\t\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\t\treturn -EINVAL;\n\t\tkiocb->ki_complete = io_complete_rw;\n\t}\n\n\treq->rw.addr = READ_ONCE(sqe->addr);\n\treq->rw.len = READ_ONCE(sqe->len);\n\treq->buf_index = READ_ONCE(sqe->buf_index);\n\treturn 0;\n}\n\nstatic inline void io_rw_done(struct kiocb *kiocb, ssize_t ret)\n{\n\tswitch (ret) {\n\tcase -EIOCBQUEUED:\n\t\tbreak;\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\tcase -ERESTART_RESTARTBLOCK:\n\t\t/*\n\t\t * We can't just restart the syscall, since previously\n\t\t * submitted sqes may already be in progress. Just fail this\n\t\t * IO with EINTR.\n\t\t */\n\t\tret = -EINTR;\n\t\tfallthrough;\n\tdefault:\n\t\tkiocb->ki_complete(kiocb, ret, 0);\n\t}\n}\n\nstatic void kiocb_done(struct kiocb *kiocb, ssize_t ret,\n\t\t       struct io_comp_state *cs)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\tstruct io_async_rw *io = req->async_data;\n\n\t/* add previously done IO, if any */\n\tif (io && io->bytes_done > 0) {\n\t\tif (ret < 0)\n\t\t\tret = io->bytes_done;\n\t\telse\n\t\t\tret += io->bytes_done;\n\t}\n\n\tif (req->flags & REQ_F_CUR_POS)\n\t\treq->file->f_pos = kiocb->ki_pos;\n\tif (ret >= 0 && kiocb->ki_complete == io_complete_rw)\n\t\t__io_complete_rw(req, ret, 0, cs);\n\telse\n\t\tio_rw_done(kiocb, ret);\n}\n\nstatic ssize_t io_import_fixed(struct io_kiocb *req, int rw,\n\t\t\t       struct iov_iter *iter)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tsize_t len = req->rw.len;\n\tstruct io_mapped_ubuf *imu;\n\tu16 index, buf_index = req->buf_index;\n\tsize_t offset;\n\tu64 buf_addr;\n\n\tif (unlikely(buf_index >= ctx->nr_user_bufs))\n\t\treturn -EFAULT;\n\tindex = array_index_nospec(buf_index, ctx->nr_user_bufs);\n\timu = &ctx->user_bufs[index];\n\tbuf_addr = req->rw.addr;\n\n\t/* overflow */\n\tif (buf_addr + len < buf_addr)\n\t\treturn -EFAULT;\n\t/* not inside the mapped region */\n\tif (buf_addr < imu->ubuf || buf_addr + len > imu->ubuf + imu->len)\n\t\treturn -EFAULT;\n\n\t/*\n\t * May not be a start of buffer, set size appropriately\n\t * and advance us to the beginning.\n\t */\n\toffset = buf_addr - imu->ubuf;\n\tiov_iter_bvec(iter, rw, imu->bvec, imu->nr_bvecs, offset + len);\n\n\tif (offset) {\n\t\t/*\n\t\t * Don't use iov_iter_advance() here, as it's really slow for\n\t\t * using the latter parts of a big fixed buffer - it iterates\n\t\t * over each segment manually. We can cheat a bit here, because\n\t\t * we know that:\n\t\t *\n\t\t * 1) it's a BVEC iter, we set it up\n\t\t * 2) all bvecs are PAGE_SIZE in size, except potentially the\n\t\t *    first and last bvec\n\t\t *\n\t\t * So just find our index, and adjust the iterator afterwards.\n\t\t * If the offset is within the first bvec (or the whole first\n\t\t * bvec, just use iov_iter_advance(). This makes it easier\n\t\t * since we can just skip the first segment, which may not\n\t\t * be PAGE_SIZE aligned.\n\t\t */\n\t\tconst struct bio_vec *bvec = imu->bvec;\n\n\t\tif (offset <= bvec->bv_len) {\n\t\t\tiov_iter_advance(iter, offset);\n\t\t} else {\n\t\t\tunsigned long seg_skip;\n\n\t\t\t/* skip first vec */\n\t\t\toffset -= bvec->bv_len;\n\t\t\tseg_skip = 1 + (offset >> PAGE_SHIFT);\n\n\t\t\titer->bvec = bvec + seg_skip;\n\t\t\titer->nr_segs -= seg_skip;\n\t\t\titer->count -= bvec->bv_len + offset;\n\t\t\titer->iov_offset = offset & ~PAGE_MASK;\n\t\t}\n\t}\n\n\treturn len;\n}\n\nstatic void io_ring_submit_unlock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\tif (needs_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_ring_submit_lock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\t/*\n\t * \"Normal\" inline submissions always hold the uring_lock, since we\n\t * grab it from the system call. Same is true for the SQPOLL offload.\n\t * The only exception is when we've detached the request and issue it\n\t * from an async worker thread, grab the lock for that case.\n\t */\n\tif (needs_lock)\n\t\tmutex_lock(&ctx->uring_lock);\n}\n\nstatic struct io_buffer *io_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\t  int bgid, struct io_buffer *kbuf,\n\t\t\t\t\t  bool needs_lock)\n{\n\tstruct io_buffer *head;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\treturn kbuf;\n\n\tio_ring_submit_lock(req->ctx, needs_lock);\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\thead = idr_find(&req->ctx->io_buffer_idr, bgid);\n\tif (head) {\n\t\tif (!list_empty(&head->list)) {\n\t\t\tkbuf = list_last_entry(&head->list, struct io_buffer,\n\t\t\t\t\t\t\tlist);\n\t\t\tlist_del(&kbuf->list);\n\t\t} else {\n\t\t\tkbuf = head;\n\t\t\tidr_remove(&req->ctx->io_buffer_idr, bgid);\n\t\t}\n\t\tif (*len > kbuf->len)\n\t\t\t*len = kbuf->len;\n\t} else {\n\t\tkbuf = ERR_PTR(-ENOBUFS);\n\t}\n\n\tio_ring_submit_unlock(req->ctx, needs_lock);\n\n\treturn kbuf;\n}\n\nstatic void __user *io_rw_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\tbool needs_lock)\n{\n\tstruct io_buffer *kbuf;\n\tu16 bgid;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\tbgid = req->buf_index;\n\tkbuf = io_buffer_select(req, len, bgid, kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\treq->rw.addr = (u64) (unsigned long) kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn u64_to_user_ptr(kbuf->addr);\n}\n\n#ifdef CONFIG_COMPAT\nstatic ssize_t io_compat_import(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\tbool needs_lock)\n{\n\tstruct compat_iovec __user *uiov;\n\tcompat_ssize_t clen;\n\tvoid __user *buf;\n\tssize_t len;\n\n\tuiov = u64_to_user_ptr(req->rw.addr);\n\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\tif (__get_user(clen, &uiov->iov_len))\n\t\treturn -EFAULT;\n\tif (clen < 0)\n\t\treturn -EINVAL;\n\n\tlen = clen;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = (compat_size_t) len;\n\treturn 0;\n}\n#endif\n\nstatic ssize_t __io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t      bool needs_lock)\n{\n\tstruct iovec __user *uiov = u64_to_user_ptr(req->rw.addr);\n\tvoid __user *buf;\n\tssize_t len;\n\n\tif (copy_from_user(iov, uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\n\tlen = iov[0].iov_len;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = len;\n\treturn 0;\n}\n\nstatic ssize_t io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t    bool needs_lock)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tstruct io_buffer *kbuf;\n\n\t\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\t\tiov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tiov[0].iov_len = kbuf->len;\n\t\treturn 0;\n\t}\n\tif (!req->rw.len)\n\t\treturn 0;\n\telse if (req->rw.len > 1)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn io_compat_import(req, iov, needs_lock);\n#endif\n\n\treturn __io_iov_buffer_select(req, iov, needs_lock);\n}\n\nstatic ssize_t io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t\t struct iovec **iovec, struct iov_iter *iter,\n\t\t\t\t bool needs_lock)\n{\n\tvoid __user *buf = u64_to_user_ptr(req->rw.addr);\n\tsize_t sqe_len = req->rw.len;\n\tssize_t ret;\n\tu8 opcode;\n\n\topcode = req->opcode;\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\t*iovec = NULL;\n\t\treturn io_import_fixed(req, rw, iter);\n\t}\n\n\t/* buffer index only valid with fixed read/write, or buffer select  */\n\tif (req->buf_index && !(req->flags & REQ_F_BUFFER_SELECT))\n\t\treturn -EINVAL;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {\n\t\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\t\tbuf = io_rw_buffer_select(req, &sqe_len, needs_lock);\n\t\t\tif (IS_ERR(buf))\n\t\t\t\treturn PTR_ERR(buf);\n\t\t\treq->rw.len = sqe_len;\n\t\t}\n\n\t\tret = import_single_range(rw, buf, sqe_len, *iovec, iter);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select(req, *iovec, needs_lock);\n\t\tif (!ret) {\n\t\t\tret = (*iovec)->iov_len;\n\t\t\tiov_iter_init(iter, rw, *iovec, 1, ret);\n\t\t}\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\treturn __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, iovec, iter,\n\t\t\t      req->ctx->compat);\n}\n\nstatic inline loff_t *io_kiocb_ppos(struct kiocb *kiocb)\n{\n\treturn (kiocb->ki_filp->f_mode & FMODE_STREAM) ? NULL : &kiocb->ki_pos;\n}\n\n/*\n * For files that don't have ->read_iter() and ->write_iter(), handle them\n * by looping over ->read() or ->write() manually.\n */\nstatic ssize_t loop_rw_iter(int rw, struct io_kiocb *req, struct iov_iter *iter)\n{\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct file *file = req->file;\n\tssize_t ret = 0;\n\n\t/*\n\t * Don't support polled IO through this interface, and we can't\n\t * support non-blocking either. For the latter, this just causes\n\t * the kiocb to be handled from an async context.\n\t */\n\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\treturn -EOPNOTSUPP;\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\treturn -EAGAIN;\n\n\twhile (iov_iter_count(iter)) {\n\t\tstruct iovec iovec;\n\t\tssize_t nr;\n\n\t\tif (!iov_iter_is_bvec(iter)) {\n\t\t\tiovec = iov_iter_iovec(iter);\n\t\t} else {\n\t\t\tiovec.iov_base = u64_to_user_ptr(req->rw.addr);\n\t\t\tiovec.iov_len = req->rw.len;\n\t\t}\n\n\t\tif (rw == READ) {\n\t\t\tnr = file->f_op->read(file, iovec.iov_base,\n\t\t\t\t\t      iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t} else {\n\t\t\tnr = file->f_op->write(file, iovec.iov_base,\n\t\t\t\t\t       iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t}\n\n\t\tif (nr < 0) {\n\t\t\tif (!ret)\n\t\t\t\tret = nr;\n\t\t\tbreak;\n\t\t}\n\t\tret += nr;\n\t\tif (nr != iovec.iov_len)\n\t\t\tbreak;\n\t\treq->rw.len -= nr;\n\t\treq->rw.addr += nr;\n\t\tiov_iter_advance(iter, nr);\n\t}\n\n\treturn ret;\n}\n\nstatic void io_req_map_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t  const struct iovec *fast_iov, struct iov_iter *iter)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\n\tmemcpy(&rw->iter, iter, sizeof(*iter));\n\trw->free_iovec = iovec;\n\trw->bytes_done = 0;\n\t/* can only be fixed buffers, no need to do anything */\n\tif (iov_iter_is_bvec(iter))\n\t\treturn;\n\tif (!iovec) {\n\t\tunsigned iov_off = 0;\n\n\t\trw->iter.iov = rw->fast_iov;\n\t\tif (iter->iov != fast_iov) {\n\t\t\tiov_off = iter->iov - fast_iov;\n\t\t\trw->iter.iov += iov_off;\n\t\t}\n\t\tif (rw->fast_iov != fast_iov)\n\t\t\tmemcpy(rw->fast_iov + iov_off, fast_iov + iov_off,\n\t\t\t       sizeof(struct iovec) * iter->nr_segs);\n\t} else {\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic inline int __io_alloc_async_data(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(!io_op_defs[req->opcode].async_size);\n\treq->async_data = kmalloc(io_op_defs[req->opcode].async_size, GFP_KERNEL);\n\treturn req->async_data == NULL;\n}\n\nstatic int io_alloc_async_data(struct io_kiocb *req)\n{\n\tif (!io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\n\treturn  __io_alloc_async_data(req);\n}\n\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force)\n{\n\tif (!force && !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tif (!req->async_data) {\n\t\tif (__io_alloc_async_data(req))\n\t\t\treturn -ENOMEM;\n\n\t\tio_req_map_rw(req, iovec, fast_iov, iter);\n\t}\n\treturn 0;\n}\n\nstatic inline int io_rw_prep_async(struct io_kiocb *req, int rw)\n{\n\tstruct io_async_rw *iorw = req->async_data;\n\tstruct iovec *iov = iorw->fast_iov;\n\tssize_t ret;\n\n\tret = io_import_iovec(rw, req, &iov, &iorw->iter, false);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tiorw->bytes_done = 0;\n\tiorw->free_iovec = iov;\n\tif (iov)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tssize_t ret;\n\n\tret = io_prep_rw(req, sqe);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(!(req->file->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\n\t/* either don't need iovec imported or already have it */\n\tif (!req->async_data)\n\t\treturn 0;\n\treturn io_rw_prep_async(req, READ);\n}\n\n/*\n * This is our waitqueue callback handler, registered through lock_page_async()\n * when we initially tried to do the IO with the iocb armed our waitqueue.\n * This gets called when the page is unlocked, and we generally expect that to\n * happen when the page IO is completed and the page is now uptodate. This will\n * queue a task_work based retry of the operation, attempting to copy the data\n * again. If the latter fails because the page was NOT uptodate, then we will\n * do a thread based blocking retry of the operation. That's the unexpected\n * slow path.\n */\nstatic int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\tint ret;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\treq->rw.kiocb.ki_flags &= ~IOCB_WAITQ;\n\tlist_del_init(&wait->entry);\n\n\tinit_task_work(&req->task_work, io_req_task_submit);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/* submit ref gets dropped, acquire a new one */\n\trefcount_inc(&req->refs);\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\t/* queue just for cancelation */\n\t\tinit_task_work(&req->task_work, io_req_task_cancel);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}\n\n/*\n * This controls whether a given IO request should be armed for async page\n * based retry. If we return false here, the request is handed to the async\n * worker threads for retry. If we're doing buffered reads on a regular file,\n * we prepare a private wait_page_queue entry and retry the operation. This\n * will either succeed because the page is now uptodate and unlocked, or it\n * will register a callback when the page is unlocked at IO completion. Through\n * that callback, io_uring uses task_work to setup a retry of the operation.\n * That retry will attempt the buffered read again. The retry will generally\n * succeed, or in rare cases where it fails, we then fall back to using the\n * async worker threads for a blocking retry.\n */\nstatic bool io_rw_should_retry(struct io_kiocb *req)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\tstruct wait_page_queue *wait = &rw->wpq;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t/* never retry for NOWAIT, we just complete with -EAGAIN */\n\tif (req->flags & REQ_F_NOWAIT)\n\t\treturn false;\n\n\t/* Only for buffered IO */\n\tif (kiocb->ki_flags & (IOCB_DIRECT | IOCB_HIPRI))\n\t\treturn false;\n\n\t/*\n\t * just use poll if we can, and don't attempt if the fs doesn't\n\t * support callback based unlocks\n\t */\n\tif (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))\n\t\treturn false;\n\n\twait->wait.func = io_async_buf_func;\n\twait->wait.private = req;\n\twait->wait.flags = 0;\n\tINIT_LIST_HEAD(&wait->wait.entry);\n\tkiocb->ki_flags |= IOCB_WAITQ;\n\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\tkiocb->ki_waitq = wait;\n\treturn true;\n}\n\nstatic int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)\n{\n\tif (req->file->f_op->read_iter)\n\t\treturn call_read_iter(req->file, &req->rw.kiocb, iter);\n\telse if (req->file->f_op->read)\n\t\treturn loop_rw_iter(READ, req, iter);\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic int io_read(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t io_size, ret, ret2;\n\tbool no_async;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(READ, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\tret = 0;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\n\t/* If the file doesn't support async, just async punt */\n\tno_async = force_nonblock && !io_file_supports_async(req->file, READ);\n\tif (no_async)\n\t\tgoto copy_iov;\n\n\tret = rw_verify_area(READ, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\tret = io_iter_do_read(req, iter);\n\n\tif (!ret) {\n\t\tgoto done;\n\t} else if (ret == -EIOCBQUEUED) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t} else if (ret == -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif (!force_nonblock && !(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tgoto done;\n\t\t/* no retry on NONBLOCK marked file */\n\t\tif (req->file->f_flags & O_NONBLOCK)\n\t\t\tgoto done;\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = 0;\n\t\tgoto copy_iov;\n\t} else if (ret < 0) {\n\t\t/* make sure -ERESTARTSYS -> -EINTR is done */\n\t\tgoto done;\n\t}\n\n\t/* read it all, or we did blocking attempt. no retry. */\n\tif (!iov_iter_count(iter) || !force_nonblock ||\n\t    (req->file->f_flags & O_NONBLOCK))\n\t\tgoto done;\n\n\tio_size -= ret;\ncopy_iov:\n\tret2 = io_setup_async_rw(req, iovec, inline_vecs, iter, true);\n\tif (ret2) {\n\t\tret = ret2;\n\t\tgoto out_free;\n\t}\n\tif (no_async)\n\t\treturn -EAGAIN;\n\trw = req->async_data;\n\t/* it's copied and will be cleaned with ->io */\n\tiovec = NULL;\n\t/* now use our persistent iterator, if we aren't already */\n\titer = &rw->iter;\nretry:\n\trw->bytes_done += ret;\n\t/* if we can retry, do so with the callbacks armed */\n\tif (!io_rw_should_retry(req)) {\n\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now retry read with the IOCB_WAITQ parts set in the iocb. If we\n\t * get -EIOCBQUEUED, then we'll get a notification when the desired\n\t * page gets unlocked. We can also get a partial read here, and if we\n\t * do, then just retry at the new offset.\n\t */\n\tret = io_iter_do_read(req, iter);\n\tif (ret == -EIOCBQUEUED) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t} else if (ret > 0 && ret < io_size) {\n\t\t/* we got some bytes, but not all. retry. */\n\t\tgoto retry;\n\t}\ndone:\n\tkiocb_done(kiocb, ret, cs);\n\tret = 0;\nout_free:\n\t/* it's reportedly faster than delegating the null check to kfree() */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tssize_t ret;\n\n\tret = io_prep_rw(req, sqe);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(!(req->file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\n\t/* either don't need iovec imported or already have it */\n\tif (!req->async_data)\n\t\treturn 0;\n\treturn io_rw_prep_async(req, WRITE);\n}\n\nstatic int io_write(struct io_kiocb *req, bool force_nonblock,\n\t\t    struct io_comp_state *cs)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t ret, ret2, io_size;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\t/* If the file doesn't support async, just async punt */\n\tif (force_nonblock && !io_file_supports_async(req->file, WRITE))\n\t\tgoto copy_iov;\n\n\t/* file path doesn't support NOWAIT for non-direct_IO */\n\tif (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&\n\t    (req->flags & REQ_F_ISREG))\n\t\tgoto copy_iov;\n\n\tret = rw_verify_area(WRITE, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\t/*\n\t * Open-code file_start_write here to grab freeze protection,\n\t * which will be released by another thread in\n\t * io_complete_rw().  Fool lockdep by telling it the lock got\n\t * released so that it doesn't complain about the held lock when\n\t * we return to userspace.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tsb_start_write(file_inode(req->file)->i_sb);\n\t\t__sb_writers_release(file_inode(req->file)->i_sb,\n\t\t\t\t\tSB_FREEZE_WRITE);\n\t}\n\tkiocb->ki_flags |= IOCB_WRITE;\n\n\tif (req->file->f_op->write_iter)\n\t\tret2 = call_write_iter(req->file, kiocb, iter);\n\telse if (req->file->f_op->write)\n\t\tret2 = loop_rw_iter(WRITE, req, iter);\n\telse\n\t\tret2 = -EINVAL;\n\n\t/*\n\t * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just\n\t * retry them without IOCB_NOWAIT.\n\t */\n\tif (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))\n\t\tret2 = -EAGAIN;\n\t/* no retry on NONBLOCK marked file */\n\tif (ret2 == -EAGAIN && (req->file->f_flags & O_NONBLOCK))\n\t\tgoto done;\n\tif (!force_nonblock || ret2 != -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN)\n\t\t\tgoto copy_iov;\ndone:\n\t\tkiocb_done(kiocb, ret2, cs);\n\t} else {\ncopy_iov:\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, iter, false);\n\t\tif (!ret)\n\t\t\treturn -EAGAIN;\n\t}\nout_free:\n\t/* it's reportedly faster than delegating the null check to kfree() */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int io_renameat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_rename *ren = &req->rename;\n\tconst char __user *oldf, *newf;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tren->old_dfd = READ_ONCE(sqe->fd);\n\toldf = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tnewf = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tren->new_dfd = READ_ONCE(sqe->len);\n\tren->flags = READ_ONCE(sqe->rename_flags);\n\n\tren->oldpath = getname(oldf);\n\tif (IS_ERR(ren->oldpath))\n\t\treturn PTR_ERR(ren->oldpath);\n\n\tren->newpath = getname(newf);\n\tif (IS_ERR(ren->newpath)) {\n\t\tputname(ren->oldpath);\n\t\treturn PTR_ERR(ren->newpath);\n\t}\n\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_renameat(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_rename *ren = &req->rename;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = do_renameat2(ren->old_dfd, ren->oldpath, ren->new_dfd,\n\t\t\t\tren->newpath, ren->flags);\n\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_unlinkat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tconst char __user *fname;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tun->dfd = READ_ONCE(sqe->fd);\n\n\tun->flags = READ_ONCE(sqe->unlink_flags);\n\tif (un->flags & ~AT_REMOVEDIR)\n\t\treturn -EINVAL;\n\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tun->filename = getname(fname);\n\tif (IS_ERR(un->filename))\n\t\treturn PTR_ERR(un->filename);\n\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_unlinkat(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tif (un->flags & AT_REMOVEDIR)\n\t\tret = do_rmdir(un->dfd, un->filename);\n\telse\n\t\tret = do_unlinkat(un->dfd, un->filename);\n\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_shutdown_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_NET)\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->rw_flags ||\n\t    sqe->buf_index)\n\t\treturn -EINVAL;\n\n\treq->shutdown.how = READ_ONCE(sqe->len);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_shutdown(struct io_kiocb *req, bool force_nonblock)\n{\n#if defined(CONFIG_NET)\n\tstruct socket *sock;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tret = __sys_shutdown_sock(sock, req->shutdown.how);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int __io_splice_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\tunsigned int valid_flags = SPLICE_F_FD_IN_FIXED | SPLICE_F_ALL;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsp->file_in = NULL;\n\tsp->len = READ_ONCE(sqe->len);\n\tsp->flags = READ_ONCE(sqe->splice_flags);\n\n\tif (unlikely(sp->flags & ~valid_flags))\n\t\treturn -EINVAL;\n\n\tsp->file_in = io_file_get(NULL, req, READ_ONCE(sqe->splice_fd_in),\n\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n\tif (!sp->file_in)\n\t\treturn -EBADF;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\n\tif (!S_ISREG(file_inode(sp->file_in)->i_mode)) {\n\t\t/*\n\t\t * Splice operation will be punted aync, and here need to\n\t\t * modify io_wq_work.flags, so initialize io_wq_work firstly.\n\t\t */\n\t\tio_req_init_async(req);\n\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_tee_prep(struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (READ_ONCE(sqe->splice_off_in) || READ_ONCE(sqe->off))\n\t\treturn -EINVAL;\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_tee(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tlong ret = 0;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\tif (sp->len)\n\t\tret = do_tee(in, out, sp->len, flags);\n\n\tio_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_splice_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\n\tsp->off_in = READ_ONCE(sqe->splice_off_in);\n\tsp->off_out = READ_ONCE(sqe->off);\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_splice(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tloff_t *poff_in, *poff_out;\n\tlong ret = 0;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tpoff_in = (sp->off_in == -1) ? NULL : &sp->off_in;\n\tpoff_out = (sp->off_out == -1) ? NULL : &sp->off_out;\n\n\tif (sp->len)\n\t\tret = do_splice(in, poff_in, out, poff_out, sp->len, flags);\n\n\tio_put_file(req, in, (sp->flags & SPLICE_F_FD_IN_FIXED));\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n/*\n * IORING_OP_NOP just posts a completion event, nothing else.\n */\nstatic int io_nop(struct io_kiocb *req, struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\t__io_req_complete(req, 0, 0, cs);\n\treturn 0;\n}\n\nstatic int io_prep_fsync(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->file)\n\t\treturn -EBADF;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.flags = READ_ONCE(sqe->fsync_flags);\n\tif (unlikely(req->sync.flags & ~IORING_FSYNC_DATASYNC))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fsync(struct io_kiocb *req, bool force_nonblock)\n{\n\tloff_t end = req->sync.off + req->sync.len;\n\tint ret;\n\n\t/* fsync always requires a blocking context */\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = vfs_fsync_range(req->file, req->sync.off,\n\t\t\t\tend > 0 ? end : LLONG_MAX,\n\t\t\t\treq->sync.flags & IORING_FSYNC_DATASYNC);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_fallocate_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->addr);\n\treq->sync.mode = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fallocate(struct io_kiocb *req, bool force_nonblock)\n{\n\tint ret;\n\n\t/* fallocate always requiring blocking context */\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\tret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,\n\t\t\t\treq->sync.len);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int __io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tconst char __user *fname;\n\tint ret;\n\n\tif (unlikely(sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\t/* open.how should be already initialised */\n\tif (!(req->open.how.flags & O_PATH) && force_o_largefile())\n\t\treq->open.how.flags |= O_LARGEFILE;\n\n\treq->open.dfd = READ_ONCE(sqe->fd);\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->open.filename = getname(fname);\n\tif (IS_ERR(req->open.filename)) {\n\t\tret = PTR_ERR(req->open.filename);\n\t\treq->open.filename = NULL;\n\t\treturn ret;\n\t}\n\treq->open.nofile = rlimit(RLIMIT_NOFILE);\n\treq->open.ignore_nonblock = false;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tu64 flags, mode;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tmode = READ_ONCE(sqe->len);\n\tflags = READ_ONCE(sqe->open_flags);\n\treq->open.how = build_open_how(flags, mode);\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct open_how __user *how;\n\tsize_t len;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\thow = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tlen = READ_ONCE(sqe->len);\n\tif (len < OPEN_HOW_SIZE_VER0)\n\t\treturn -EINVAL;\n\n\tret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,\n\t\t\t\t\tlen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct open_flags op;\n\tstruct file *file;\n\tint ret;\n\n\tif (force_nonblock && !req->open.ignore_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = build_open_flags(&req->open.how, &op);\n\tif (ret)\n\t\tgoto err;\n\n\tret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = do_filp_open(req->open.dfd, req->open.filename, &op);\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t\t/*\n\t\t * A work-around to ensure that /proc/self works that way\n\t\t * that it should - if we get -EOPNOTSUPP back, then assume\n\t\t * that proc_self_get_link() failed us because we're in async\n\t\t * context. We should be safe to retry this from the task\n\t\t * itself with force_nonblock == false set, as it should not\n\t\t * block on lookup. Would be nice to know this upfront and\n\t\t * avoid the async dance, but doesn't seem feasible.\n\t\t */\n\t\tif (ret == -EOPNOTSUPP && io_wq_current_is_worker()) {\n\t\t\treq->open.ignore_nonblock = true;\n\t\t\trefcount_inc(&req->refs);\n\t\t\tio_req_task_queue(req);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tfsnotify_open(file);\n\t\tfd_install(ret, file);\n\t}\nerr:\n\tputname(req->open.filename);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_openat(struct io_kiocb *req, bool force_nonblock)\n{\n\treturn io_openat2(req, force_nonblock);\n}\n\nstatic int io_remove_buffers_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->nbufs = tmp;\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\treturn 0;\n}\n\nstatic int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,\n\t\t\t       int bgid, unsigned nbufs)\n{\n\tunsigned i = 0;\n\n\t/* shouldn't happen */\n\tif (!nbufs)\n\t\treturn 0;\n\n\t/* the head kbuf is the list itself */\n\twhile (!list_empty(&buf->list)) {\n\t\tstruct io_buffer *nxt;\n\n\t\tnxt = list_first_entry(&buf->list, struct io_buffer, list);\n\t\tlist_del(&nxt->list);\n\t\tkfree(nxt);\n\t\tif (++i == nbufs)\n\t\t\treturn i;\n\t}\n\ti++;\n\tkfree(buf);\n\tidr_remove(&ctx->io_buffer_idr, bgid);\n\n\treturn i;\n}\n\nstatic int io_remove_buffers(struct io_kiocb *req, bool force_nonblock,\n\t\t\t     struct io_comp_state *cs)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head;\n\tint ret = 0;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tret = -ENOENT;\n\thead = idr_find(&ctx->io_buffer_idr, p->bgid);\n\tif (head)\n\t\tret = __io_remove_buffers(ctx, head, p->bgid, p->nbufs);\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_provide_buffers_prep(struct io_kiocb *req,\n\t\t\t\t   const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->nbufs = tmp;\n\tp->addr = READ_ONCE(sqe->addr);\n\tp->len = READ_ONCE(sqe->len);\n\n\tif (!access_ok(u64_to_user_ptr(p->addr), (p->len * p->nbufs)))\n\t\treturn -EFAULT;\n\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\ttmp = READ_ONCE(sqe->off);\n\tif (tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->bid = tmp;\n\treturn 0;\n}\n\nstatic int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\n\treturn i ? i : -ENOMEM;\n}\n\nstatic int io_provide_buffers(struct io_kiocb *req, bool force_nonblock,\n\t\t\t      struct io_comp_state *cs)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head, *list;\n\tint ret = 0;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tlist = head = idr_find(&ctx->io_buffer_idr, p->bgid);\n\n\tret = io_add_buffers(p, &head);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (!list) {\n\t\tret = idr_alloc(&ctx->io_buffer_idr, head, p->bgid, p->bgid + 1,\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (ret < 0) {\n\t\t\t__io_remove_buffers(ctx, head, p->bgid, -1U);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tio_ring_submit_unlock(ctx, !force_nonblock);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_epoll_ctl_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_EPOLL)\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\n\treq->epoll.epfd = READ_ONCE(sqe->fd);\n\treq->epoll.op = READ_ONCE(sqe->len);\n\treq->epoll.fd = READ_ONCE(sqe->off);\n\n\tif (ep_op_has_event(req->epoll.op)) {\n\t\tstruct epoll_event __user *ev;\n\n\t\tev = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\t\tif (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_epoll_ctl(struct io_kiocb *req, bool force_nonblock,\n\t\t\tstruct io_comp_state *cs)\n{\n#if defined(CONFIG_EPOLL)\n\tstruct io_epoll *ie = &req->epoll;\n\tint ret;\n\n\tret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tif (sqe->ioprio || sqe->buf_index || sqe->off)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->madvise.addr = READ_ONCE(sqe->addr);\n\treq->madvise.len = READ_ONCE(sqe->len);\n\treq->madvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise(struct io_kiocb *req, bool force_nonblock)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tstruct io_madvise *ma = &req->madvise;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = do_madvise(current->mm, ma->addr, ma->len, ma->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->addr)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->fadvise.offset = READ_ONCE(sqe->off);\n\treq->fadvise.len = READ_ONCE(sqe->len);\n\treq->fadvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n}\n\nstatic int io_fadvise(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_fadvise *fa = &req->fadvise;\n\tint ret;\n\n\tif (force_nonblock) {\n\t\tswitch (fa->advice) {\n\t\tcase POSIX_FADV_NORMAL:\n\t\tcase POSIX_FADV_RANDOM:\n\t\tcase POSIX_FADV_SEQUENTIAL:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->statx.dfd = READ_ONCE(sqe->fd);\n\treq->statx.mask = READ_ONCE(sqe->len);\n\treq->statx.filename = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->statx.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\treq->statx.flags = READ_ONCE(sqe->statx_flags);\n\n\treturn 0;\n}\n\nstatic int io_statx(struct io_kiocb *req, bool force_nonblock)\n{\n\tstruct io_statx *ctx = &req->statx;\n\tint ret;\n\n\tif (force_nonblock) {\n\t\t/* only need file table for an actual valid fd */\n\t\tif (ctx->dfd == -1 || ctx->dfd == AT_FDCWD)\n\t\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n\t\treturn -EAGAIN;\n\t}\n\n\tret = do_statx(ctx->dfd, ctx->filename, ctx->flags, ctx->mask,\n\t\t       ctx->buffer);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\t/*\n\t * If we queue this for async, it must not be cancellable. That would\n\t * leave the 'file' in an undeterminate state, and here need to modify\n\t * io_wq_work.flags, so initialize io_wq_work firstly.\n\t */\n\tio_req_init_async(req);\n\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\tif ((req->file && req->file->f_op == &io_uring_fops))\n\t\treturn -EBADF;\n\n\treq->close.put_file = NULL;\n\treturn 0;\n}\n\nstatic int io_close(struct io_kiocb *req, bool force_nonblock,\n\t\t    struct io_comp_state *cs)\n{\n\tstruct io_close *close = &req->close;\n\tint ret;\n\n\t/* might be already done during nonblock submission */\n\tif (!close->put_file) {\n\t\tret = close_fd_get_file(close->fd, &close->put_file);\n\t\tif (ret < 0)\n\t\t\treturn (ret == -ENOENT) ? -EBADF : ret;\n\t}\n\n\t/* if the file has a flush method, be safe and punt to async */\n\tif (close->put_file->f_op->flush && force_nonblock) {\n\t\t/* was never set, but play safe */\n\t\treq->flags &= ~REQ_F_NOWAIT;\n\t\t/* avoid grabbing files - we don't need the files */\n\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n\t\treturn -EAGAIN;\n\t}\n\n\t/* No ->flush() or already async, safely close from here */\n\tret = filp_close(close->put_file, req->work.identity->files);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tfput(close->put_file);\n\tclose->put_file = NULL;\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_prep_sfr(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->file)\n\t\treturn -EBADF;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treq->sync.flags = READ_ONCE(sqe->sync_range_flags);\n\treturn 0;\n}\n\nstatic int io_sync_file_range(struct io_kiocb *req, bool force_nonblock)\n{\n\tint ret;\n\n\t/* sync_file_range always requires a blocking context */\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tret = sync_file_range(req->file, req->sync.off, req->sync.len,\n\t\t\t\treq->sync.flags);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n#if defined(CONFIG_NET)\nstatic int io_setup_async_msg(struct io_kiocb *req,\n\t\t\t      struct io_async_msghdr *kmsg)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\n\tif (async_msg)\n\t\treturn -EAGAIN;\n\tif (io_alloc_async_data(req)) {\n\t\tif (kmsg->iov != kmsg->fast_iov)\n\t\t\tkfree(kmsg->iov);\n\t\treturn -ENOMEM;\n\t}\n\tasync_msg = req->async_data;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\tmemcpy(async_msg, kmsg, sizeof(*kmsg));\n\treturn -EAGAIN;\n}\n\nstatic int io_sendmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->iov = iomsg->fast_iov;\n\tiomsg->msg.msg_name = &iomsg->addr;\n\treturn sendmsg_copy_msghdr(&iomsg->msg, req->sr_msg.umsg,\n\t\t\t\t   req->sr_msg.msg_flags, &iomsg->iov);\n}\n\nstatic int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\n\tif (!async_msg || !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tret = io_sendmsg_copy_hdr(req, async_msg);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_sendmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint ret;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tif (req->async_data) {\n\t\tkmsg = req->async_data;\n\t\tkmsg->msg.msg_name = &kmsg->addr;\n\t\t/* if iov is set, it's allocated already */\n\t\tif (!kmsg->iov)\n\t\t\tkmsg->iov = kmsg->fast_iov;\n\t\tkmsg->msg.msg_iter.iov = kmsg->iov;\n\t} else {\n\t\tret = io_sendmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (kmsg->iov != kmsg->fast_iov)\n\t\tkfree(kmsg->iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_send(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint ret;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tret = import_single_range(WRITE, sr->buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tmsg.msg_flags = flags;\n\tret = sock_sendmsg(sock, &msg);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int __io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t struct io_async_msghdr *iomsg)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct iovec __user *uiov;\n\tsize_t iov_len;\n\tint ret;\n\n\tret = __copy_msghdr_from_user(&iomsg->msg, sr->umsg,\n\t\t\t\t\t&iomsg->uaddr, &uiov, &iov_len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tif (iov_len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(iomsg->iov, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tsr->len = iomsg->iov[0].iov_len;\n\t\tiov_iter_init(&iomsg->msg.msg_iter, READ, iomsg->iov, 1,\n\t\t\t\tsr->len);\n\t\tiomsg->iov = NULL;\n\t} else {\n\t\tret = __import_iovec(READ, uiov, iov_len, UIO_FASTIOV,\n\t\t\t\t     &iomsg->iov, &iomsg->msg.msg_iter,\n\t\t\t\t     false);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t\tstruct io_async_msghdr *iomsg)\n{\n\tstruct compat_msghdr __user *msg_compat;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct compat_iovec __user *uiov;\n\tcompat_uptr_t ptr;\n\tcompat_size_t len;\n\tint ret;\n\n\tmsg_compat = (struct compat_msghdr __user *) sr->umsg;\n\tret = __get_compat_msghdr(&iomsg->msg, msg_compat, &iomsg->uaddr,\n\t\t\t\t\t&ptr, &len);\n\tif (ret)\n\t\treturn ret;\n\n\tuiov = compat_ptr(ptr);\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tcompat_ssize_t clen;\n\n\t\tif (len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tif (__get_user(clen, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\t\tif (clen < 0)\n\t\t\treturn -EINVAL;\n\t\tsr->len = iomsg->iov[0].iov_len;\n\t\tiomsg->iov = NULL;\n\t} else {\n\t\tret = __import_iovec(READ, (struct iovec __user *)uiov, len,\n\t\t\t\t   UIO_FASTIOV, &iomsg->iov,\n\t\t\t\t   &iomsg->msg.msg_iter, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->msg.msg_name = &iomsg->addr;\n\tiomsg->iov = iomsg->fast_iov;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn __io_compat_recvmsg_copy_hdr(req, iomsg);\n#endif\n\n\treturn __io_recvmsg_copy_hdr(req, iomsg);\n}\n\nstatic struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,\n\t\t\t\t\t       bool needs_lock)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct io_buffer *kbuf;\n\n\tkbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\n\tsr->kbuf = kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn kbuf;\n}\n\nstatic inline unsigned int io_put_recv_kbuf(struct io_kiocb *req)\n{\n\treturn io_put_kbuf(req, req->sr_msg.kbuf);\n}\n\nstatic int io_recvmsg_prep(struct io_kiocb *req,\n\t\t\t   const struct io_uring_sqe *sqe)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\tsr->bgid = READ_ONCE(sqe->buf_group);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\n\tif (!async_msg || !io_op_defs[req->opcode].needs_async_data)\n\t\treturn 0;\n\tret = io_recvmsg_copy_hdr(req, async_msg);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_recvmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tstruct io_buffer *kbuf;\n\tunsigned flags;\n\tint ret, cflags = 0;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tif (req->async_data) {\n\t\tkmsg = req->async_data;\n\t\tkmsg->msg.msg_name = &kmsg->addr;\n\t\t/* if iov is set, it's allocated already */\n\t\tif (!kmsg->iov)\n\t\t\tkmsg->iov = kmsg->fast_iov;\n\t\tkmsg->msg.msg_iter.iov = kmsg->iov;\n\t} else {\n\t\tret = io_recvmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tkmsg->fast_iov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tiov_iter_init(&kmsg->msg.msg_iter, READ, kmsg->iov,\n\t\t\t\t1, req->sr_msg.len);\n\t}\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.umsg,\n\t\t\t\t\tkmsg->uaddr, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\tif (kmsg->iov != kmsg->fast_iov)\n\t\tkfree(kmsg->iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, cflags, cs);\n\treturn 0;\n}\n\nstatic int io_recv(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\tstruct io_buffer *kbuf;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tvoid __user *buf = sr->buf;\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tunsigned flags;\n\tint ret, cflags = 0;\n\n\tsock = sock_from_file(req->file, &ret);\n\tif (unlikely(!sock))\n\t\treturn ret;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tbuf = u64_to_user_ptr(kbuf->addr);\n\t}\n\n\tret = import_single_range(READ, buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tmsg.msg_iocb = NULL;\n\tmsg.msg_flags = 0;\n\n\tflags = req->sr_msg.msg_flags;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tret = sock_recvmsg(sock, &msg, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout_free:\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, cflags, cs);\n\treturn 0;\n}\n\nstatic int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_accept *accept = &req->accept;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\taccept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\taccept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\taccept->flags = READ_ONCE(sqe->accept_flags);\n\taccept->nofile = rlimit(RLIMIT_NOFILE);\n\treturn 0;\n}\n\nstatic int io_accept(struct io_kiocb *req, bool force_nonblock,\n\t\t     struct io_comp_state *cs)\n{\n\tstruct io_accept *accept = &req->accept;\n\tunsigned int file_flags = force_nonblock ? O_NONBLOCK : 0;\n\tint ret;\n\n\tif (req->file->f_flags & O_NONBLOCK)\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tret = __sys_accept4_file(req->file, file_flags, accept->addr,\n\t\t\t\t\taccept->addr_len, accept->flags,\n\t\t\t\t\taccept->nofile);\n\tif (ret == -EAGAIN && force_nonblock)\n\t\treturn -EAGAIN;\n\tif (ret < 0) {\n\t\tif (ret == -ERESTARTSYS)\n\t\t\tret = -EINTR;\n\t\treq_set_fail_links(req);\n\t}\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_connect *conn = &req->connect;\n\tstruct io_async_connect *io = req->async_data;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\tconn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tconn->addr_len =  READ_ONCE(sqe->addr2);\n\n\tif (!io)\n\t\treturn 0;\n\n\treturn move_addr_to_kernel(conn->addr, conn->addr_len,\n\t\t\t\t\t&io->address);\n}\n\nstatic int io_connect(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\tstruct io_async_connect __io, *io;\n\tunsigned file_flags;\n\tint ret;\n\n\tif (req->async_data) {\n\t\tio = req->async_data;\n\t} else {\n\t\tret = move_addr_to_kernel(req->connect.addr,\n\t\t\t\t\t\treq->connect.addr_len,\n\t\t\t\t\t\t&__io.address);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tio = &__io;\n\t}\n\n\tfile_flags = force_nonblock ? O_NONBLOCK : 0;\n\n\tret = __sys_connect_file(req->file, &io->address,\n\t\t\t\t\treq->connect.addr_len, file_flags);\n\tif ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {\n\t\tif (req->async_data)\n\t\t\treturn -EAGAIN;\n\t\tif (io_alloc_async_data(req)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tio = req->async_data;\n\t\tmemcpy(req->async_data, &__io, sizeof(__io));\n\t\treturn -EAGAIN;\n\t}\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n#else /* !CONFIG_NET */\nstatic int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_sendmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_send(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_recvmsg_prep(struct io_kiocb *req,\n\t\t\t   const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_recvmsg(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_recv(struct io_kiocb *req, bool force_nonblock,\n\t\t   struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_accept(struct io_kiocb *req, bool force_nonblock,\n\t\t     struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int io_connect(struct io_kiocb *req, bool force_nonblock,\n\t\t      struct io_comp_state *cs)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif /* CONFIG_NET */\n\nstruct io_poll_table {\n\tstruct poll_table_struct pt;\n\tstruct io_kiocb *req;\n\tint error;\n};\n\nstatic int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,\n\t\t\t   __poll_t mask, task_work_func_t func)\n{\n\tint ret;\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\ttrace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);\n\n\tlist_del_init(&poll->wait.entry);\n\n\treq->result = mask;\n\tinit_task_work(&req->task_work, func);\n\tpercpu_ref_get(&req->ctx->refs);\n\n\t/*\n\t * If this fails, then the task is exiting. When a task exits, the\n\t * work gets canceled, so just cancel this request as well instead\n\t * of executing it. We can't safely execute it anyway, as we may not\n\t * have the needed state needed for it anyway.\n\t */\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tstruct task_struct *tsk;\n\n\t\tWRITE_ONCE(poll->canceled, true);\n\t\ttsk = io_wq_get_task(req->ctx->io_wq);\n\t\ttask_work_add(tsk, &req->task_work, TWA_NONE);\n\t\twake_up_process(tsk);\n\t}\n\treturn 1;\n}\n\nstatic bool io_poll_rewait(struct io_kiocb *req, struct io_poll_iocb *poll)\n\t__acquires(&req->ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tstruct poll_table_struct pt = { ._key = poll->events };\n\n\t\treq->result = vfs_poll(req->file, &pt) & poll->events;\n\t}\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tadd_wait_queue(poll->head, &poll->wait);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic struct io_poll_iocb *io_poll_get_double(struct io_kiocb *req)\n{\n\t/* pure poll stashes this in ->async_data, poll driven retry elsewhere */\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn req->async_data;\n\treturn req->apoll->double_poll;\n}\n\nstatic struct io_poll_iocb *io_poll_get_single(struct io_kiocb *req)\n{\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn &req->poll;\n\treturn &req->apoll->poll;\n}\n\nstatic void io_poll_remove_double(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_double(req);\n\n\tlockdep_assert_held(&req->ctx->completion_lock);\n\n\tif (poll && poll->head) {\n\t\tstruct wait_queue_head *head = poll->head;\n\n\t\tspin_lock(&head->lock);\n\t\tlist_del_init(&poll->wait.entry);\n\t\tif (poll->wait.private)\n\t\t\trefcount_dec(&req->refs);\n\t\tpoll->head = NULL;\n\t\tspin_unlock(&head->lock);\n\t}\n}\n\nstatic void io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_poll_remove_double(req);\n\treq->poll.done = true;\n\tio_cqring_fill_event(req, error ? error : mangle_poll(mask));\n\tio_commit_cqring(ctx);\n}\n\nstatic void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *nxt;\n\n\tif (io_poll_rewait(req, &req->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t} else {\n\t\thash_del(&req->hash_node);\n\t\tio_poll_complete(req, req->result, 0);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\n\t\tnxt = io_put_req_find_next(req);\n\t\tio_cqring_ev_posted(ctx);\n\t\tif (nxt)\n\t\t\t__io_req_task_submit(nxt);\n\t}\n\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic int io_poll_double_wake(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t       int sync, void *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\t__poll_t mask = key_to_poll(key);\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\tlist_del_init(&wait->entry);\n\n\tif (poll && poll->head) {\n\t\tbool done;\n\n\t\tspin_lock(&poll->head->lock);\n\t\tdone = list_empty(&poll->wait.entry);\n\t\tif (!done)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t/* make sure double remove sees this as being gone */\n\t\twait->private = NULL;\n\t\tspin_unlock(&poll->head->lock);\n\t\tif (!done) {\n\t\t\t/* use wait func handler, so it matches the rq type */\n\t\t\tpoll->wait.func(&poll->wait, mode, sync, key);\n\t\t}\n\t}\n\trefcount_dec(&req->refs);\n\treturn 1;\n}\n\nstatic void io_init_poll_iocb(struct io_poll_iocb *poll, __poll_t events,\n\t\t\t      wait_queue_func_t wake_func)\n{\n\tpoll->head = NULL;\n\tpoll->done = false;\n\tpoll->canceled = false;\n\tpoll->events = events;\n\tINIT_LIST_HEAD(&poll->wait.entry);\n\tinit_waitqueue_func_entry(&poll->wait, wake_func);\n}\n\nstatic void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,\n\t\t\t    struct wait_queue_head *head,\n\t\t\t    struct io_poll_iocb **poll_ptr)\n{\n\tstruct io_kiocb *req = pt->req;\n\n\t/*\n\t * If poll->head is already set, it's because the file being polled\n\t * uses multiple waitqueues for poll handling (eg one for read, one\n\t * for write). Setup a separate io_poll_iocb if this happens.\n\t */\n\tif (unlikely(poll->head)) {\n\t\tstruct io_poll_iocb *poll_one = poll;\n\n\t\t/* already have a 2nd entry, fail a third attempt */\n\t\tif (*poll_ptr) {\n\t\t\tpt->error = -EINVAL;\n\t\t\treturn;\n\t\t}\n\t\tpoll = kmalloc(sizeof(*poll), GFP_ATOMIC);\n\t\tif (!poll) {\n\t\t\tpt->error = -ENOMEM;\n\t\t\treturn;\n\t\t}\n\t\tio_init_poll_iocb(poll, poll_one->events, io_poll_double_wake);\n\t\trefcount_inc(&req->refs);\n\t\tpoll->wait.private = req;\n\t\t*poll_ptr = poll;\n\t}\n\n\tpt->error = 0;\n\tpoll->head = head;\n\n\tif (poll->events & EPOLLEXCLUSIVE)\n\t\tadd_wait_queue_exclusive(head, &poll->wait);\n\telse\n\t\tadd_wait_queue(head, &poll->wait);\n}\n\nstatic void io_async_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\tstruct async_poll *apoll = pt->req->apoll;\n\n\t__io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);\n}\n\nstatic void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tpercpu_ref_put(&ctx->refs);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tpercpu_ref_put(&ctx->refs);\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}\n\nstatic int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->apoll->poll;\n\n\ttrace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,\n\t\t\t\t\tkey_to_poll(key));\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);\n}\n\nstatic void io_poll_req_insert(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct hlist_head *list;\n\n\tlist = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];\n\thlist_add_head(&req->hash_node, list);\n}\n\nstatic __poll_t __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t      struct io_poll_iocb *poll,\n\t\t\t\t      struct io_poll_table *ipt, __poll_t mask,\n\t\t\t\t      wait_queue_func_t wake_func)\n\t__acquires(&ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tbool cancel = false;\n\n\tINIT_HLIST_NODE(&req->hash_node);\n\tio_init_poll_iocb(poll, mask, wake_func);\n\tpoll->file = req->file;\n\tpoll->wait.private = req;\n\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = -EINVAL;\n\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (likely(poll->head)) {\n\t\tspin_lock(&poll->head->lock);\n\t\tif (unlikely(list_empty(&poll->wait.entry))) {\n\t\t\tif (ipt->error)\n\t\t\t\tcancel = true;\n\t\t\tipt->error = 0;\n\t\t\tmask = 0;\n\t\t}\n\t\tif (mask || ipt->error)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\telse if (cancel)\n\t\t\tWRITE_ONCE(poll->canceled, true);\n\t\telse if (!poll->done) /* actually waiting for an event */\n\t\t\tio_poll_req_insert(req);\n\t\tspin_unlock(&poll->head->lock);\n\t}\n\n\treturn mask;\n}\n\nstatic bool io_arm_poll_handler(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask, ret;\n\tint rw;\n\n\tif (!req->file || !file_can_poll(req->file))\n\t\treturn false;\n\tif (req->flags & REQ_F_POLLED)\n\t\treturn false;\n\tif (def->pollin)\n\t\trw = READ;\n\telse if (def->pollout)\n\t\trw = WRITE;\n\telse\n\t\treturn false;\n\t/* if we can't nonblock try, then no point in arming a poll handler */\n\tif (!io_file_supports_async(req->file, rw))\n\t\treturn false;\n\n\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\tif (unlikely(!apoll))\n\t\treturn false;\n\tapoll->double_poll = NULL;\n\n\treq->flags |= REQ_F_POLLED;\n\treq->apoll = apoll;\n\n\tmask = 0;\n\tif (def->pollin)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (def->pollout)\n\t\tmask |= POLLOUT | POLLWRNORM;\n\n\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\tif ((req->opcode == IORING_OP_RECVMSG) &&\n\t    (req->sr_msg.msg_flags & MSG_ERRQUEUE))\n\t\tmask &= ~POLLIN;\n\n\tmask |= POLLERR | POLLPRI;\n\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,\n\t\t\t\t\tio_async_wake);\n\tif (ret || ipt.error) {\n\t\tio_poll_remove_double(req);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(apoll->double_poll);\n\t\tkfree(apoll);\n\t\treturn false;\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\ttrace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,\n\t\t\t\t\tapoll->poll.events);\n\treturn true;\n}\n\nstatic bool __io_poll_remove_one(struct io_kiocb *req,\n\t\t\t\t struct io_poll_iocb *poll)\n{\n\tbool do_complete = false;\n\n\tspin_lock(&poll->head->lock);\n\tWRITE_ONCE(poll->canceled, true);\n\tif (!list_empty(&poll->wait.entry)) {\n\t\tlist_del_init(&poll->wait.entry);\n\t\tdo_complete = true;\n\t}\n\tspin_unlock(&poll->head->lock);\n\thash_del(&req->hash_node);\n\treturn do_complete;\n}\n\nstatic bool io_poll_remove_one(struct io_kiocb *req)\n{\n\tbool do_complete;\n\n\tio_poll_remove_double(req);\n\n\tif (req->opcode == IORING_OP_POLL_ADD) {\n\t\tdo_complete = __io_poll_remove_one(req, &req->poll);\n\t} else {\n\t\tstruct async_poll *apoll = req->apoll;\n\n\t\t/* non-poll requests have submit ref still */\n\t\tdo_complete = __io_poll_remove_one(req, &apoll->poll);\n\t\tif (do_complete) {\n\t\t\tio_put_req(req);\n\t\t\tkfree(apoll->double_poll);\n\t\t\tkfree(apoll);\n\t\t}\n\t}\n\n\tif (do_complete) {\n\t\tio_cqring_fill_event(req, -ECANCELED);\n\t\tio_commit_cqring(req->ctx);\n\t\treq_set_fail_links(req);\n\t\tio_put_req_deferred(req, 1);\n\t}\n\n\treturn do_complete;\n}\n\n/*\n * Returns true if we found and killed one or more poll requests\n */\nstatic bool io_poll_remove_all(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t       struct files_struct *files)\n{\n\tstruct hlist_node *tmp;\n\tstruct io_kiocb *req;\n\tint posted = 0, i;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list;\n\n\t\tlist = &ctx->cancel_hash[i];\n\t\thlist_for_each_entry_safe(req, tmp, list, hash_node) {\n\t\t\tif (io_match_task(req, tsk, files))\n\t\t\t\tposted += io_poll_remove_one(req);\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (posted)\n\t\tio_cqring_ev_posted(ctx);\n\n\treturn posted != 0;\n}\n\nstatic int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)\n{\n\tstruct hlist_head *list;\n\tstruct io_kiocb *req;\n\n\tlist = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];\n\thlist_for_each_entry(req, list, hash_node) {\n\t\tif (sqe_addr != req->user_data)\n\t\t\tcontinue;\n\t\tif (io_poll_remove_one(req))\n\t\t\treturn 0;\n\t\treturn -EALREADY;\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic int io_poll_remove_prep(struct io_kiocb *req,\n\t\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||\n\t    sqe->poll_events)\n\t\treturn -EINVAL;\n\n\treq->poll_remove.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\n/*\n * Find a running poll command that matches one specified in sqe->addr,\n * and remove it if found.\n */\nstatic int io_poll_remove(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tret = io_poll_cancel(ctx, req->poll_remove.addr);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->poll;\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);\n}\n\nstatic void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\n\t__io_queue_proc(&pt->req->poll, pt, head, (struct io_poll_iocb **) &pt->req->async_data);\n}\n\nstatic int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tu32 events;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->addr || sqe->ioprio || sqe->off || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\tevents = READ_ONCE(sqe->poll32_events);\n#ifdef __BIG_ENDIAN\n\tevents = swahw32(events);\n#endif\n\tpoll->events = demangle_poll(events) | EPOLLERR | EPOLLHUP |\n\t\t       (events & EPOLLEXCLUSIVE);\n\treturn 0;\n}\n\nstatic int io_poll_add(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_poll_table ipt;\n\t__poll_t mask;\n\n\tipt.pt._qproc = io_poll_queue_proc;\n\n\tmask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,\n\t\t\t\t\tio_poll_wake);\n\n\tif (mask) { /* no async, we'd stolen it */\n\t\tipt.error = 0;\n\t\tio_poll_complete(req, mask, 0);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (mask) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tio_put_req(req);\n\t}\n\treturn ipt.error;\n}\n\nstatic enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlist_del_init(&req->timeout.list);\n\tatomic_set(&req->ctx->cq_timeouts,\n\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\n\tio_cqring_fill_event(req, -ETIME);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic int __io_timeout_cancel(struct io_kiocb *req)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\tint ret;\n\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret == -1)\n\t\treturn -EALREADY;\n\tlist_del_init(&req->timeout.list);\n\n\treq_set_fail_links(req);\n\tio_cqring_fill_event(req, -ECANCELED);\n\tio_put_req_deferred(req, 1);\n\treturn 0;\n}\n\nstatic int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)\n{\n\tstruct io_kiocb *req;\n\tint ret = -ENOENT;\n\n\tlist_for_each_entry(req, &ctx->timeout_list, timeout.list) {\n\t\tif (user_data == req->user_data) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret == -ENOENT)\n\t\treturn ret;\n\n\treturn __io_timeout_cancel(req);\n}\n\nstatic int io_timeout_remove_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len || sqe->timeout_flags)\n\t\treturn -EINVAL;\n\n\treq->timeout_rem.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\n/*\n * Remove or update an existing timeout command\n */\nstatic int io_timeout_remove(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tret = io_timeout_cancel(ctx, req->timeout_rem.addr);\n\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn 0;\n}\n\nstatic int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~IORING_TIMEOUT_ABS)\n\t\treturn -EINVAL;\n\n\treq->timeout.off = off;\n\n\tif (!req->async_data && io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (flags & IORING_TIMEOUT_ABS)\n\t\tdata->mode = HRTIMER_MODE_ABS;\n\telse\n\t\tdata->mode = HRTIMER_MODE_REL;\n\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);\n\treturn 0;\n}\n\nstatic int io_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_timeout_data *data = req->async_data;\n\tstruct list_head *entry;\n\tu32 tail, off = req->timeout.off;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\n\t/*\n\t * sqe->off holds how many events that need to occur for this\n\t * timeout event to be satisfied. If it isn't set, then this is\n\t * a pure timeout request, sequence isn't used.\n\t */\n\tif (io_is_timeout_noseq(req)) {\n\t\tentry = ctx->timeout_list.prev;\n\t\tgoto add;\n\t}\n\n\ttail = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\treq->timeout.target_seq = tail + off;\n\n\t/*\n\t * Insertion sort, ensuring the first entry in the list is always\n\t * the one we need first.\n\t */\n\tlist_for_each_prev(entry, &ctx->timeout_list) {\n\t\tstruct io_kiocb *nxt = list_entry(entry, struct io_kiocb,\n\t\t\t\t\t\t  timeout.list);\n\n\t\tif (io_is_timeout_noseq(nxt))\n\t\t\tcontinue;\n\t\t/* nxt.seq is behind @tail, otherwise would've been completed */\n\t\tif (off >= nxt->timeout.target_seq - tail)\n\t\t\tbreak;\n\t}\nadd:\n\tlist_add(&req->timeout.list, entry);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn 0;\n}\n\nstatic bool io_cancel_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treturn req->user_data == (unsigned long) data;\n}\n\nstatic int io_async_cancel_one(struct io_ring_ctx *ctx, void *sqe_addr)\n{\n\tenum io_wq_cancel cancel_ret;\n\tint ret = 0;\n\n\tcancel_ret = io_wq_cancel_cb(ctx->io_wq, io_cancel_cb, sqe_addr, false);\n\tswitch (cancel_ret) {\n\tcase IO_WQ_CANCEL_OK:\n\t\tret = 0;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_RUNNING:\n\t\tret = -EALREADY;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_NOTFOUND:\n\t\tret = -ENOENT;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void io_async_find_and_cancel(struct io_ring_ctx *ctx,\n\t\t\t\t     struct io_kiocb *req, __u64 sqe_addr,\n\t\t\t\t     int success_ret)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tret = io_async_cancel_one(ctx, (void *) (unsigned long) sqe_addr);\n\tif (ret != -ENOENT) {\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tgoto done;\n\t}\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tret = io_timeout_cancel(ctx, sqe_addr);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tret = io_poll_cancel(ctx, sqe_addr);\ndone:\n\tif (!ret)\n\t\tret = success_ret;\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tio_cqring_ev_posted(ctx);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n}\n\nstatic int io_async_cancel_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->cancel_flags)\n\t\treturn -EINVAL;\n\n\treq->cancel.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_async_cancel(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_async_find_and_cancel(ctx, req, req->cancel.addr, 0);\n\treturn 0;\n}\n\nstatic int io_files_update_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_SQPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\treq->files_update.offset = READ_ONCE(sqe->off);\n\treq->files_update.nr_args = READ_ONCE(sqe->len);\n\tif (!req->files_update.nr_args)\n\t\treturn -EINVAL;\n\treq->files_update.arg = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_files_update(struct io_kiocb *req, bool force_nonblock,\n\t\t\t   struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_files_update up;\n\tint ret;\n\n\tif (force_nonblock)\n\t\treturn -EAGAIN;\n\n\tup.offset = req->files_update.offset;\n\tup.fds = req->files_update.arg;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}\n\nstatic int io_req_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\treturn 0;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\treturn io_read_prep(req, sqe);\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\treturn io_write_prep(req, sqe);\n\tcase IORING_OP_POLL_ADD:\n\t\treturn io_poll_add_prep(req, sqe);\n\tcase IORING_OP_POLL_REMOVE:\n\t\treturn io_poll_remove_prep(req, sqe);\n\tcase IORING_OP_FSYNC:\n\t\treturn io_prep_fsync(req, sqe);\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\treturn io_prep_sfr(req, sqe);\n\tcase IORING_OP_SENDMSG:\n\tcase IORING_OP_SEND:\n\t\treturn io_sendmsg_prep(req, sqe);\n\tcase IORING_OP_RECVMSG:\n\tcase IORING_OP_RECV:\n\t\treturn io_recvmsg_prep(req, sqe);\n\tcase IORING_OP_CONNECT:\n\t\treturn io_connect_prep(req, sqe);\n\tcase IORING_OP_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, false);\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\treturn io_timeout_remove_prep(req, sqe);\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\treturn io_async_cancel_prep(req, sqe);\n\tcase IORING_OP_LINK_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, true);\n\tcase IORING_OP_ACCEPT:\n\t\treturn io_accept_prep(req, sqe);\n\tcase IORING_OP_FALLOCATE:\n\t\treturn io_fallocate_prep(req, sqe);\n\tcase IORING_OP_OPENAT:\n\t\treturn io_openat_prep(req, sqe);\n\tcase IORING_OP_CLOSE:\n\t\treturn io_close_prep(req, sqe);\n\tcase IORING_OP_FILES_UPDATE:\n\t\treturn io_files_update_prep(req, sqe);\n\tcase IORING_OP_STATX:\n\t\treturn io_statx_prep(req, sqe);\n\tcase IORING_OP_FADVISE:\n\t\treturn io_fadvise_prep(req, sqe);\n\tcase IORING_OP_MADVISE:\n\t\treturn io_madvise_prep(req, sqe);\n\tcase IORING_OP_OPENAT2:\n\t\treturn io_openat2_prep(req, sqe);\n\tcase IORING_OP_EPOLL_CTL:\n\t\treturn io_epoll_ctl_prep(req, sqe);\n\tcase IORING_OP_SPLICE:\n\t\treturn io_splice_prep(req, sqe);\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\treturn io_provide_buffers_prep(req, sqe);\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\treturn io_remove_buffers_prep(req, sqe);\n\tcase IORING_OP_TEE:\n\t\treturn io_tee_prep(req, sqe);\n\tcase IORING_OP_SHUTDOWN:\n\t\treturn io_shutdown_prep(req, sqe);\n\tcase IORING_OP_RENAMEAT:\n\t\treturn io_renameat_prep(req, sqe);\n\tcase IORING_OP_UNLINKAT:\n\t\treturn io_unlinkat_prep(req, sqe);\n\t}\n\n\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n\t\t\treq->opcode);\n\treturn-EINVAL;\n}\n\nstatic int io_req_defer_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tif (!sqe)\n\t\treturn 0;\n\tif (io_alloc_async_data(req))\n\t\treturn -EAGAIN;\n\treturn io_req_prep(req, sqe);\n}\n\nstatic u32 io_get_sequence(struct io_kiocb *req)\n{\n\tstruct io_kiocb *pos;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tu32 total_submitted, nr_reqs = 0;\n\n\tio_for_each_link(pos, req)\n\t\tnr_reqs++;\n\n\ttotal_submitted = ctx->cached_sq_head - ctx->cached_sq_dropped;\n\treturn total_submitted - nr_reqs;\n}\n\nstatic int io_req_defer(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_defer_entry *de;\n\tint ret;\n\tu32 seq;\n\n\t/* Still need defer if there is pending req in defer list. */\n\tif (likely(list_empty_careful(&ctx->defer_list) &&\n\t\t!(req->flags & REQ_F_IO_DRAIN)))\n\t\treturn 0;\n\n\tseq = io_get_sequence(req);\n\t/* Still a chance to pass the sequence check */\n\tif (!req_need_defer(req, seq) && list_empty_careful(&ctx->defer_list))\n\t\treturn 0;\n\n\tif (!req->async_data) {\n\t\tret = io_req_defer_prep(req, sqe);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tio_prep_async_link(req);\n\tde = kmalloc(sizeof(*de), GFP_KERNEL);\n\tif (!de)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req_need_defer(req, seq) && list_empty(&ctx->defer_list)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(de);\n\t\tio_queue_async_work(req);\n\t\treturn -EIOCBQUEUED;\n\t}\n\n\ttrace_io_uring_defer(ctx, req, req->user_data);\n\tde->req = req;\n\tde->seq = seq;\n\tlist_add_tail(&de->list, &ctx->defer_list);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn -EIOCBQUEUED;\n}\n\nstatic void io_req_drop_files(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\tlist_del(&req->inflight_entry);\n\tif (atomic_read(&tctx->in_idle))\n\t\twake_up(&tctx->wait);\n\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\treq->flags &= ~REQ_F_INFLIGHT;\n\tput_files_struct(req->work.identity->files);\n\tput_nsproxy(req->work.identity->nsproxy);\n\treq->work.flags &= ~IO_WQ_WORK_FILES;\n}\n\nstatic void __io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\t\tkfree((void *)(unsigned long)req->rw.addr);\n\t\t\tbreak;\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_RECV:\n\t\t\tkfree(req->sr_msg.kbuf);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\tcase IORING_OP_WRITEV:\n\t\tcase IORING_OP_WRITE_FIXED:\n\t\tcase IORING_OP_WRITE: {\n\t\t\tstruct io_async_rw *io = req->async_data;\n\t\t\tif (io->free_iovec)\n\t\t\t\tkfree(io->free_iovec);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_SENDMSG: {\n\t\t\tstruct io_async_msghdr *io = req->async_data;\n\t\t\tif (io->iov != io->fast_iov)\n\t\t\t\tkfree(io->iov);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_SPLICE:\n\t\tcase IORING_OP_TEE:\n\t\t\tio_put_file(req, req->splice.file_in,\n\t\t\t\t    (req->splice.flags & SPLICE_F_FD_IN_FIXED));\n\t\t\tbreak;\n\t\tcase IORING_OP_OPENAT:\n\t\tcase IORING_OP_OPENAT2:\n\t\t\tif (req->open.filename)\n\t\t\t\tputname(req->open.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_RENAMEAT:\n\t\t\tputname(req->rename.oldpath);\n\t\t\tputname(req->rename.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_UNLINKAT:\n\t\t\tputname(req->unlink.filename);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\t}\n\n\tif (req->flags & REQ_F_INFLIGHT)\n\t\tio_req_drop_files(req);\n}\n\nstatic int io_issue_sqe(struct io_kiocb *req, bool force_nonblock,\n\t\t\tstruct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tret = io_nop(req, cs);\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\tret = io_read(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\tret = io_write(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tret = io_fsync(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tret = io_poll_add(req);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tret = io_poll_remove(req);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tret = io_sync_file_range(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tret = io_sendmsg(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_SEND:\n\t\tret = io_send(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tret = io_recvmsg(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_RECV:\n\t\tret = io_recv(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tret = io_timeout(req);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tret = io_timeout_remove(req);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tret = io_accept(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tret = io_connect(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tret = io_async_cancel(req);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tret = io_fallocate(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tret = io_openat(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_CLOSE:\n\t\tret = io_close(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_FILES_UPDATE:\n\t\tret = io_files_update(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_STATX:\n\t\tret = io_statx(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_FADVISE:\n\t\tret = io_fadvise(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_MADVISE:\n\t\tret = io_madvise(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_OPENAT2:\n\t\tret = io_openat2(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_EPOLL_CTL:\n\t\tret = io_epoll_ctl(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_SPLICE:\n\t\tret = io_splice(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\tret = io_provide_buffers(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\tret = io_remove_buffers(req, force_nonblock, cs);\n\t\tbreak;\n\tcase IORING_OP_TEE:\n\t\tret = io_tee(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_SHUTDOWN:\n\t\tret = io_shutdown(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_RENAMEAT:\n\t\tret = io_renameat(req, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_UNLINKAT:\n\t\tret = io_unlinkat(req, force_nonblock);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\t/* If the op doesn't have a file, we're not polling for it */\n\tif ((ctx->flags & IORING_SETUP_IOPOLL) && req->file) {\n\t\tconst bool in_async = io_wq_current_is_worker();\n\n\t\t/* workqueue context doesn't hold uring_lock, grab it now */\n\t\tif (in_async)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tio_iopoll_req_issued(req, in_async);\n\n\t\tif (in_async)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic struct io_wq_work *io_wq_submit_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_kiocb *timeout;\n\tint ret = 0;\n\n\ttimeout = io_prep_linked_timeout(req);\n\tif (timeout)\n\t\tio_queue_linked_timeout(timeout);\n\n\t/* if NO_CANCEL is set, we must still run the work */\n\tif ((work->flags & (IO_WQ_WORK_CANCEL|IO_WQ_WORK_NO_CANCEL)) ==\n\t\t\t\tIO_WQ_WORK_CANCEL) {\n\t\tret = -ECANCELED;\n\t}\n\n\tif (!ret) {\n\t\tdo {\n\t\t\tret = io_issue_sqe(req, false, NULL);\n\t\t\t/*\n\t\t\t * We can get EAGAIN for polled IO even though we're\n\t\t\t * forcing a sync submission from here, since we can't\n\t\t\t * wait for request slots on the block side.\n\t\t\t */\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tbreak;\n\t\t\tcond_resched();\n\t\t} while (1);\n\t}\n\n\tif (ret) {\n\t\treq_set_fail_links(req);\n\t\tio_req_complete(req, ret);\n\t}\n\n\treturn io_steal_work(req);\n}\n\nstatic inline struct file *io_file_from_index(struct io_ring_ctx *ctx,\n\t\t\t\t\t      int index)\n{\n\tstruct fixed_file_table *table;\n\n\ttable = &ctx->file_data->table[index >> IORING_FILE_TABLE_SHIFT];\n\treturn table->files[index & IORING_FILE_TABLE_MASK];\n}\n\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct file *file;\n\n\tif (fixed) {\n\t\tif (unlikely((unsigned int)fd >= ctx->nr_user_files))\n\t\t\treturn NULL;\n\t\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\t\tfile = io_file_from_index(ctx, fd);\n\t\tio_set_resource_node(req);\n\t} else {\n\t\ttrace_io_uring_file_get(ctx, fd);\n\t\tfile = __io_file_get(state, fd);\n\t}\n\n\treturn file;\n}\n\nstatic enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *prev, *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tprev = req->timeout.head;\n\treq->timeout.head = NULL;\n\n\t/*\n\t * We don't expect the list to be empty, that will only happen if we\n\t * race with the completion of the linked work.\n\t */\n\tif (prev && refcount_inc_not_zero(&prev->refs))\n\t\tio_remove_next_linked(prev);\n\telse\n\t\tprev = NULL;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (prev) {\n\t\treq_set_fail_links(prev);\n\t\tio_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);\n\t\tio_put_req(prev);\n\t} else {\n\t\tio_req_complete(req, -ETIME);\n\t}\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __io_queue_linked_timeout(struct io_kiocb *req)\n{\n\t/*\n\t * If the back reference is NULL, then our linked request finished\n\t * before we got a chance to setup the timer\n\t */\n\tif (req->timeout.head) {\n\t\tstruct io_timeout_data *data = req->async_data;\n\n\t\tdata->timer.function = io_link_timeout_fn;\n\t\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts),\n\t\t\t\tdata->mode);\n\t}\n}\n\nstatic void io_queue_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\t__io_queue_linked_timeout(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\t/* drop submission reference */\n\tio_put_req(req);\n}\n\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\tif (!nxt || (req->flags & REQ_F_LINK_TIMEOUT) ||\n\t    nxt->opcode != IORING_OP_LINK_TIMEOUT)\n\t\treturn NULL;\n\n\tnxt->timeout.head = req;\n\tnxt->flags |= REQ_F_LTIMEOUT_ACTIVE;\n\treq->flags |= REQ_F_LINK_TIMEOUT;\n\treturn nxt;\n}\n\nstatic void __io_queue_sqe(struct io_kiocb *req, struct io_comp_state *cs)\n{\n\tstruct io_kiocb *linked_timeout;\n\tconst struct cred *old_creds = NULL;\n\tint ret;\n\nagain:\n\tlinked_timeout = io_prep_linked_timeout(req);\n\n\tif ((req->flags & REQ_F_WORK_INITIALIZED) &&\n\t    (req->work.flags & IO_WQ_WORK_CREDS) &&\n\t    req->work.identity->creds != current_cred()) {\n\t\tif (old_creds)\n\t\t\trevert_creds(old_creds);\n\t\tif (old_creds == req->work.identity->creds)\n\t\t\told_creds = NULL; /* restored original creds */\n\t\telse\n\t\t\told_creds = override_creds(req->work.identity->creds);\n\t}\n\n\tret = io_issue_sqe(req, true, cs);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\n\t\tif (linked_timeout)\n\t\t\tio_queue_linked_timeout(linked_timeout);\n\t} else if (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\treq = io_put_req_find_next(req);\n\t\tif (linked_timeout)\n\t\t\tio_queue_linked_timeout(linked_timeout);\n\n\t\tif (req) {\n\t\t\tif (!(req->flags & REQ_F_FORCE_ASYNC))\n\t\t\t\tgoto again;\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else {\n\t\t/* un-prep timeout, so it'll be killed as any other linked */\n\t\treq->flags &= ~REQ_F_LINK_TIMEOUT;\n\t\treq_set_fail_links(req);\n\t\tio_put_req(req);\n\t\tio_req_complete(req, ret);\n\t}\n\n\tif (old_creds)\n\t\trevert_creds(old_creds);\n}\n\nstatic void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t struct io_comp_state *cs)\n{\n\tint ret;\n\n\tret = io_req_defer(req, sqe);\n\tif (ret) {\n\t\tif (ret != -EIOCBQUEUED) {\nfail_req:\n\t\t\treq_set_fail_links(req);\n\t\t\tio_put_req(req);\n\t\t\tio_req_complete(req, ret);\n\t\t}\n\t} else if (req->flags & REQ_F_FORCE_ASYNC) {\n\t\tif (!req->async_data) {\n\t\t\tret = io_req_defer_prep(req, sqe);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto fail_req;\n\t\t}\n\t\tio_queue_async_work(req);\n\t} else {\n\t\tif (sqe) {\n\t\t\tret = io_req_prep(req, sqe);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto fail_req;\n\t\t}\n\t\t__io_queue_sqe(req, cs);\n\t}\n}\n\nstatic inline void io_queue_link_head(struct io_kiocb *req,\n\t\t\t\t      struct io_comp_state *cs)\n{\n\tif (unlikely(req->flags & REQ_F_FAIL_LINK)) {\n\t\tio_put_req(req);\n\t\tio_req_complete(req, -ECANCELED);\n\t} else\n\t\tio_queue_sqe(req, NULL, cs);\n}\n\nstruct io_submit_link {\n\tstruct io_kiocb *head;\n\tstruct io_kiocb *last;\n};\n\nstatic int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t struct io_submit_link *link, struct io_comp_state *cs)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\t/*\n\t * If we already have a head request, queue this one for async\n\t * submittal once the head completes. If we don't have a head but\n\t * IOSQE_IO_LINK is set in the sqe, start a new head. This one will be\n\t * submitted sync once the chain is complete. If none of those\n\t * conditions are true (normal request), then just queue it.\n\t */\n\tif (link->head) {\n\t\tstruct io_kiocb *head = link->head;\n\n\t\t/*\n\t\t * Taking sequential execution of a link, draining both sides\n\t\t * of the link also fullfils IOSQE_IO_DRAIN semantics for all\n\t\t * requests in the link. So, it drains the head and the\n\t\t * next after the link request. The last one is done via\n\t\t * drain_next flag to persist the effect across calls.\n\t\t */\n\t\tif (req->flags & REQ_F_IO_DRAIN) {\n\t\t\thead->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 1;\n\t\t}\n\t\tret = io_req_defer_prep(req, sqe);\n\t\tif (unlikely(ret)) {\n\t\t\t/* fail even hard links since we don't submit */\n\t\t\thead->flags |= REQ_F_FAIL_LINK;\n\t\t\treturn ret;\n\t\t}\n\t\ttrace_io_uring_link(ctx, req, head);\n\t\tlink->last->link = req;\n\t\tlink->last = req;\n\n\t\t/* last request of a link, enqueue the link */\n\t\tif (!(req->flags & (REQ_F_LINK | REQ_F_HARDLINK))) {\n\t\t\tio_queue_link_head(head, cs);\n\t\t\tlink->head = NULL;\n\t\t}\n\t} else {\n\t\tif (unlikely(ctx->drain_next)) {\n\t\t\treq->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 0;\n\t\t}\n\t\tif (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {\n\t\t\tret = io_req_defer_prep(req, sqe);\n\t\t\tif (unlikely(ret))\n\t\t\t\treq->flags |= REQ_F_FAIL_LINK;\n\t\t\tlink->head = req;\n\t\t\tlink->last = req;\n\t\t} else {\n\t\t\tio_queue_sqe(req, sqe, cs);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Batched submission is done, ensure local IO is flushed out.\n */\nstatic void io_submit_state_end(struct io_submit_state *state)\n{\n\tif (!list_empty(&state->comp.list))\n\t\tio_submit_flush_completions(&state->comp);\n\tif (state->plug_started)\n\t\tblk_finish_plug(&state->plug);\n\tio_state_file_put(state);\n\tif (state->free_reqs)\n\t\tkmem_cache_free_bulk(req_cachep, state->free_reqs, state->reqs);\n}\n\n/*\n * Start submission side cache.\n */\nstatic void io_submit_state_start(struct io_submit_state *state,\n\t\t\t\t  struct io_ring_ctx *ctx, unsigned int max_ios)\n{\n\tstate->plug_started = false;\n\tstate->comp.nr = 0;\n\tINIT_LIST_HEAD(&state->comp.list);\n\tstate->comp.ctx = ctx;\n\tstate->free_reqs = 0;\n\tstate->file_refs = 0;\n\tstate->ios_left = max_ios;\n}\n\nstatic void io_commit_sqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/*\n\t * Ensure any loads from the SQEs are done at this point,\n\t * since once we write the new head, the application could\n\t * write new data to them.\n\t */\n\tsmp_store_release(&rings->sq.head, ctx->cached_sq_head);\n}\n\n/*\n * Fetch an sqe, if one is available. Note that sqe_ptr will point to memory\n * that is mapped by userspace. This means that care needs to be taken to\n * ensure that reads are stable, as we cannot rely on userspace always\n * being a good citizen. If members of the sqe are validated and then later\n * used, it's important that those reads are done through READ_ONCE() to\n * prevent a re-load down the line.\n */\nstatic const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)\n{\n\tu32 *sq_array = ctx->sq_array;\n\tunsigned head;\n\n\t/*\n\t * The cached sq head (or cq tail) serves two purposes:\n\t *\n\t * 1) allows us to batch the cost of updating the user visible\n\t *    head updates.\n\t * 2) allows the kernel side to track the head on its own, even\n\t *    though the application is the one updating it.\n\t */\n\thead = READ_ONCE(sq_array[ctx->cached_sq_head & ctx->sq_mask]);\n\tif (likely(head < ctx->sq_entries))\n\t\treturn &ctx->sq_sqes[head];\n\n\t/* drop invalid entries */\n\tctx->cached_sq_dropped++;\n\tWRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);\n\treturn NULL;\n}\n\nstatic inline void io_consume_sqe(struct io_ring_ctx *ctx)\n{\n\tctx->cached_sq_head++;\n}\n\n/*\n * Check SQE restrictions (opcode and flags).\n *\n * Returns 'true' if SQE is allowed, 'false' otherwise.\n */\nstatic inline bool io_check_restriction(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_kiocb *req,\n\t\t\t\t\tunsigned int sqe_flags)\n{\n\tif (!ctx->restricted)\n\t\treturn true;\n\n\tif (!test_bit(req->opcode, ctx->restrictions.sqe_op))\n\t\treturn false;\n\n\tif ((sqe_flags & ctx->restrictions.sqe_flags_required) !=\n\t    ctx->restrictions.sqe_flags_required)\n\t\treturn false;\n\n\tif (sqe_flags & ~(ctx->restrictions.sqe_flags_allowed |\n\t\t\t  ctx->restrictions.sqe_flags_required))\n\t\treturn false;\n\n\treturn true;\n}\n\n#define SQE_VALID_FLAGS\t(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|\t\\\n\t\t\t\tIOSQE_IO_HARDLINK | IOSQE_ASYNC | \\\n\t\t\t\tIOSQE_BUFFER_SELECT)\n\nstatic int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe,\n\t\t       struct io_submit_state *state)\n{\n\tunsigned int sqe_flags;\n\tint id, ret;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->flags = 0;\n\treq->link = NULL;\n\treq->fixed_file_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\trefcount_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(io_sq_thread_acquire_mm_files(ctx, req)))\n\t\treturn -EFAULT;\n\n\tsqe_flags = READ_ONCE(sqe->flags);\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tid = READ_ONCE(sqe->personality);\n\tif (id) {\n\t\tstruct io_identity *iod;\n\n\t\tiod = idr_find(&ctx->personality_idr, id);\n\t\tif (unlikely(!iod))\n\t\t\treturn -EINVAL;\n\t\trefcount_inc(&iod->count);\n\n\t\t__io_req_init_async(req);\n\t\tget_cred(iod->creds);\n\t\treq->work.identity = iod;\n\t\treq->work.flags |= IO_WQ_WORK_CREDS;\n\t}\n\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags |= sqe_flags;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tret = 0;\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file &&\n\t\t    !io_op_defs[req->opcode].needs_file_no_error))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}\n\nstatic int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr)\n{\n\tstruct io_submit_state state;\n\tstruct io_submit_link link;\n\tint i, submitted = 0;\n\n\t/* if we have a backlog and couldn't flush it all, return BUSY */\n\tif (test_bit(0, &ctx->sq_check_overflow)) {\n\t\tif (!list_empty(&ctx->cq_overflow_list) &&\n\t\t    !io_cqring_overflow_flush(ctx, false, NULL, NULL))\n\t\t\treturn -EBUSY;\n\t}\n\n\t/* make sure SQ entry isn't read before tail */\n\tnr = min3(nr, ctx->sq_entries, io_sqring_entries(ctx));\n\n\tif (!percpu_ref_tryget_many(&ctx->refs, nr))\n\t\treturn -EAGAIN;\n\n\tpercpu_counter_add(&current->io_uring->inflight, nr);\n\trefcount_add(nr, &current->usage);\n\n\tio_submit_state_start(&state, ctx, nr);\n\tlink.head = NULL;\n\n\tfor (i = 0; i < nr; i++) {\n\t\tconst struct io_uring_sqe *sqe;\n\t\tstruct io_kiocb *req;\n\t\tint err;\n\n\t\tsqe = io_get_sqe(ctx);\n\t\tif (unlikely(!sqe)) {\n\t\t\tio_consume_sqe(ctx);\n\t\t\tbreak;\n\t\t}\n\t\treq = io_alloc_req(ctx, &state);\n\t\tif (unlikely(!req)) {\n\t\t\tif (!submitted)\n\t\t\t\tsubmitted = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tio_consume_sqe(ctx);\n\t\t/* will complete beyond this point, count as submitted */\n\t\tsubmitted++;\n\n\t\terr = io_init_req(ctx, req, sqe, &state);\n\t\tif (unlikely(err)) {\nfail_req:\n\t\t\tio_put_req(req);\n\t\t\tio_req_complete(req, err);\n\t\t\tbreak;\n\t\t}\n\n\t\ttrace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,\n\t\t\t\t\t\ttrue, io_async_submit(ctx));\n\t\terr = io_submit_sqe(req, sqe, &link, &state.comp);\n\t\tif (err)\n\t\t\tgoto fail_req;\n\t}\n\n\tif (unlikely(submitted != nr)) {\n\t\tint ref_used = (submitted == -EAGAIN) ? 0 : submitted;\n\t\tstruct io_uring_task *tctx = current->io_uring;\n\t\tint unused = nr - ref_used;\n\n\t\tpercpu_ref_put_many(&ctx->refs, unused);\n\t\tpercpu_counter_sub(&tctx->inflight, unused);\n\t\tput_task_struct_many(current, unused);\n\t}\n\tif (link.head)\n\t\tio_queue_link_head(link.head, &state.comp);\n\tio_submit_state_end(&state);\n\n\t /* Commit SQ ring head once we've consumed and submitted all SQEs */\n\tio_commit_sqring(ctx);\n\n\treturn submitted;\n}\n\nstatic inline void io_ring_set_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\t/* Tell userspace we may need a wakeup call */\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags |= IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic inline void io_ring_clear_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > 8)\n\t\tto_submit = 8;\n\n\tif (!list_empty(&ctx->iopoll_list) || to_submit) {\n\t\tunsigned nr_events = 0;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (!io_sqring_full(ctx) && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\twake_up(&ctx->sqo_sq_wait);\n\n\treturn ret;\n}\n\nstatic void io_sqd_update_thread_idle(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\tunsigned sq_thread_idle = 0;\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\tif (sq_thread_idle < ctx->sq_thread_idle)\n\t\t\tsq_thread_idle = ctx->sq_thread_idle;\n\t}\n\n\tsqd->sq_thread_idle = sq_thread_idle;\n}\n\nstatic void io_sqd_init_new(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\n\twhile (!list_empty(&sqd->ctx_new_list)) {\n\t\tctx = list_first_entry(&sqd->ctx_new_list, struct io_ring_ctx, sqd_list);\n\t\tlist_move_tail(&ctx->sqd_list, &sqd->ctx_list);\n\t\tcomplete(&ctx->sq_thread_comp);\n\t}\n\n\tio_sqd_update_thread_idle(sqd);\n}\n\nstatic int io_sq_thread(void *data)\n{\n\tstruct cgroup_subsys_state *cur_css = NULL;\n\tstruct files_struct *old_files = current->files;\n\tstruct nsproxy *old_nsproxy = current->nsproxy;\n\tstruct pid *old_thread_pid = current->thread_pid;\n\tconst struct cred *old_cred = NULL;\n\tstruct io_sq_data *sqd = data;\n\tstruct io_ring_ctx *ctx;\n\tunsigned long timeout = 0;\n\tDEFINE_WAIT(wait);\n\n\ttask_lock(current);\n\tcurrent->files = NULL;\n\tcurrent->nsproxy = NULL;\n\tcurrent->thread_pid = NULL;\n\ttask_unlock(current);\n\n\twhile (!kthread_should_stop()) {\n\t\tint ret;\n\t\tbool cap_entries, sqt_spin, needs_sched;\n\n\t\t/*\n\t\t * Any changes to the sqd lists are synchronized through the\n\t\t * kthread parking. This synchronizes the thread vs users,\n\t\t * the users are synchronized on the sqd->ctx_lock.\n\t\t */\n\t\tif (kthread_should_park()) {\n\t\t\tkthread_parkme();\n\t\t\t/*\n\t\t\t * When sq thread is unparked, in case the previous park operation\n\t\t\t * comes from io_put_sq_data(), which means that sq thread is going\n\t\t\t * to be stopped, so here needs to have a check.\n\t\t\t */\n\t\t\tif (kthread_should_stop())\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(!list_empty(&sqd->ctx_new_list))) {\n\t\t\tio_sqd_init_new(sqd);\n\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t}\n\n\t\tsqt_spin = false;\n\t\tcap_entries = !list_is_singular(&sqd->ctx_list);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tif (current->cred != ctx->creds) {\n\t\t\t\tif (old_cred)\n\t\t\t\t\trevert_creds(old_cred);\n\t\t\t\told_cred = override_creds(ctx->creds);\n\t\t\t}\n\t\t\tio_sq_thread_associate_blkcg(ctx, &cur_css);\n#ifdef CONFIG_AUDIT\n\t\t\tcurrent->loginuid = ctx->loginuid;\n\t\t\tcurrent->sessionid = ctx->sessionid;\n#endif\n\n\t\t\tret = __io_sq_thread(ctx, cap_entries);\n\t\t\tif (!sqt_spin && (ret > 0 || !list_empty(&ctx->iopoll_list)))\n\t\t\t\tsqt_spin = true;\n\n\t\t\tio_sq_thread_drop_mm_files();\n\t\t}\n\n\t\tif (sqt_spin || !time_after(jiffies, timeout)) {\n\t\t\tio_run_task_work();\n\t\t\tcond_resched();\n\t\t\tif (sqt_spin)\n\t\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (kthread_should_park())\n\t\t\tcontinue;\n\n\t\tneeds_sched = true;\n\t\tprepare_to_wait(&sqd->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t    !list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (io_sqring_entries(ctx)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (needs_sched) {\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_set_wakeup_flag(ctx);\n\n\t\t\tschedule();\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t}\n\n\t\tfinish_wait(&sqd->wait, &wait);\n\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t}\n\n\tio_run_task_work();\n\n\tif (cur_css)\n\t\tio_sq_thread_unassociate_blkcg();\n\tif (old_cred)\n\t\trevert_creds(old_cred);\n\n\ttask_lock(current);\n\tcurrent->files = old_files;\n\tcurrent->nsproxy = old_nsproxy;\n\tcurrent->thread_pid = old_thread_pid;\n\ttask_unlock(current);\n\n\tkthread_parkme();\n\n\treturn 0;\n}\n\nstruct io_wait_queue {\n\tstruct wait_queue_entry wq;\n\tstruct io_ring_ctx *ctx;\n\tunsigned to_wait;\n\tunsigned nr_timeouts;\n};\n\nstatic inline bool io_should_wake(struct io_wait_queue *iowq, bool noflush)\n{\n\tstruct io_ring_ctx *ctx = iowq->ctx;\n\n\t/*\n\t * Wake up if we have enough events, or if a timeout occurred since we\n\t * started waiting. For timeouts, we always want to return to userspace,\n\t * regardless of event count.\n\t */\n\treturn io_cqring_events(ctx, noflush) >= iowq->to_wait ||\n\t\t\tatomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;\n}\n\nstatic int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,\n\t\t\t    int wake_flags, void *key)\n{\n\tstruct io_wait_queue *iowq = container_of(curr, struct io_wait_queue,\n\t\t\t\t\t\t\twq);\n\n\t/* use noflush == true, as we can't safely rely on locking context */\n\tif (!io_should_wake(iowq, true))\n\t\treturn -1;\n\n\treturn autoremove_wake_function(curr, mode, wake_flags, key);\n}\n\nstatic int io_run_task_work_sig(void)\n{\n\tif (io_run_task_work())\n\t\treturn 1;\n\tif (!signal_pending(current))\n\t\treturn 0;\n\tif (test_tsk_thread_flag(current, TIF_NOTIFY_SIGNAL))\n\t\treturn -ERESTARTSYS;\n\treturn -EINTR;\n}\n\n/*\n * Wait until events become available, if we don't already have some. The\n * application must reap them itself, as they reside on the shared cq ring.\n */\nstatic int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,\n\t\t\t  const sigset_t __user *sig, size_t sigsz,\n\t\t\t  struct __kernel_timespec __user *uts)\n{\n\tstruct io_wait_queue iowq = {\n\t\t.wq = {\n\t\t\t.private\t= current,\n\t\t\t.func\t\t= io_wake_function,\n\t\t\t.entry\t\t= LIST_HEAD_INIT(iowq.wq.entry),\n\t\t},\n\t\t.ctx\t\t= ctx,\n\t\t.to_wait\t= min_events,\n\t};\n\tstruct io_rings *rings = ctx->rings;\n\tstruct timespec64 ts;\n\tsigned long timeout = 0;\n\tint ret = 0;\n\n\tdo {\n\t\tif (io_cqring_events(ctx, false) >= min_events)\n\t\t\treturn 0;\n\t\tif (!io_run_task_work())\n\t\t\tbreak;\n\t} while (1);\n\n\tif (sig) {\n#ifdef CONFIG_COMPAT\n\t\tif (in_compat_syscall())\n\t\t\tret = set_compat_user_sigmask((const compat_sigset_t __user *)sig,\n\t\t\t\t\t\t      sigsz);\n\t\telse\n#endif\n\t\t\tret = set_user_sigmask(sig, sigsz);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (uts) {\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t\ttimeout = timespec64_to_jiffies(&ts);\n\t}\n\n\tiowq.nr_timeouts = atomic_read(&ctx->cq_timeouts);\n\ttrace_io_uring_cqring_wait(ctx, min_events);\n\tdo {\n\t\tprepare_to_wait_exclusive(&ctx->wait, &iowq.wq,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t/* make sure we run task_work before checking for signals */\n\t\tret = io_run_task_work_sig();\n\t\tif (ret > 0)\n\t\t\tcontinue;\n\t\telse if (ret < 0)\n\t\t\tbreak;\n\t\tif (io_should_wake(&iowq, false))\n\t\t\tbreak;\n\t\tif (uts) {\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tif (timeout == 0) {\n\t\t\t\tret = -ETIME;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tschedule();\n\t\t}\n\t} while (1);\n\tfinish_wait(&ctx->wait, &iowq.wq);\n\n\trestore_saved_sigmask_unless(ret == -EINTR);\n\n\treturn READ_ONCE(rings->cq.head) == READ_ONCE(rings->cq.tail) ? ret : 0;\n}\n\nstatic void __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#else\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file;\n\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n#endif\n}\n\nstatic void io_file_ref_kill(struct percpu_ref *ref)\n{\n\tstruct fixed_file_data *data;\n\n\tdata = container_of(ref, struct fixed_file_data, refs);\n\tcomplete(&data->done);\n}\n\nstatic int io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tstruct fixed_file_data *data = ctx->file_data;\n\tstruct fixed_file_ref_node *ref_node = NULL;\n\tunsigned nr_tables, i;\n\n\tif (!data)\n\t\treturn -ENXIO;\n\n\tspin_lock_bh(&data->lock);\n\tref_node = data->node;\n\tspin_unlock_bh(&data->lock);\n\tif (ref_node)\n\t\tpercpu_ref_kill(&ref_node->refs);\n\n\tpercpu_ref_kill(&data->refs);\n\n\t/* wait for all refs nodes to complete */\n\tflush_delayed_work(&ctx->file_put_work);\n\twait_for_completion(&data->done);\n\n\t__io_sqe_files_unregister(ctx);\n\tnr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(data->table[i].files);\n\tkfree(data->table);\n\tpercpu_ref_exit(&data->refs);\n\tkfree(data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n\treturn 0;\n}\n\nstatic void io_put_sq_data(struct io_sq_data *sqd)\n{\n\tif (refcount_dec_and_test(&sqd->refs)) {\n\t\t/*\n\t\t * The park is a bit of a work-around, without it we get\n\t\t * warning spews on shutdown with SQPOLL set and affinity\n\t\t * set to a single CPU.\n\t\t */\n\t\tif (sqd->thread) {\n\t\t\tkthread_park(sqd->thread);\n\t\t\tkthread_stop(sqd->thread);\n\t\t}\n\n\t\tkfree(sqd);\n\t}\n}\n\nstatic struct io_sq_data *io_attach_sq_data(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx_attach;\n\tstruct io_sq_data *sqd;\n\tstruct fd f;\n\n\tf = fdget(p->wq_fd);\n\tif (!f.file)\n\t\treturn ERR_PTR(-ENXIO);\n\tif (f.file->f_op != &io_uring_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx_attach = f.file->private_data;\n\tsqd = ctx_attach->sq_data;\n\tif (!sqd) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\trefcount_inc(&sqd->refs);\n\tfdput(f);\n\treturn sqd;\n}\n\nstatic struct io_sq_data *io_get_sq_data(struct io_uring_params *p)\n{\n\tstruct io_sq_data *sqd;\n\n\tif (p->flags & IORING_SETUP_ATTACH_WQ)\n\t\treturn io_attach_sq_data(p);\n\n\tsqd = kzalloc(sizeof(*sqd), GFP_KERNEL);\n\tif (!sqd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trefcount_set(&sqd->refs, 1);\n\tINIT_LIST_HEAD(&sqd->ctx_list);\n\tINIT_LIST_HEAD(&sqd->ctx_new_list);\n\tmutex_init(&sqd->ctx_lock);\n\tmutex_init(&sqd->lock);\n\tinit_waitqueue_head(&sqd->wait);\n\treturn sqd;\n}\n\nstatic void io_sq_thread_unpark(struct io_sq_data *sqd)\n\t__releases(&sqd->lock)\n{\n\tif (!sqd->thread)\n\t\treturn;\n\tkthread_unpark(sqd->thread);\n\tmutex_unlock(&sqd->lock);\n}\n\nstatic void io_sq_thread_park(struct io_sq_data *sqd)\n\t__acquires(&sqd->lock)\n{\n\tif (!sqd->thread)\n\t\treturn;\n\tmutex_lock(&sqd->lock);\n\tkthread_park(sqd->thread);\n}\n\nstatic void io_sq_thread_stop(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (sqd) {\n\t\tif (sqd->thread) {\n\t\t\t/*\n\t\t\t * We may arrive here from the error branch in\n\t\t\t * io_sq_offload_create() where the kthread is created\n\t\t\t * without being waked up, thus wake it up now to make\n\t\t\t * sure the wait will complete.\n\t\t\t */\n\t\t\twake_up_process(sqd->thread);\n\t\t\twait_for_completion(&ctx->sq_thread_comp);\n\n\t\t\tio_sq_thread_park(sqd);\n\t\t}\n\n\t\tmutex_lock(&sqd->ctx_lock);\n\t\tlist_del(&ctx->sqd_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\tmutex_unlock(&sqd->ctx_lock);\n\n\t\tif (sqd->thread)\n\t\t\tio_sq_thread_unpark(sqd);\n\n\t\tio_put_sq_data(sqd);\n\t\tctx->sq_data = NULL;\n\t}\n}\n\nstatic void io_finish_async(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_stop(ctx);\n\n\tif (ctx->io_wq) {\n\t\tio_wq_destroy(ctx->io_wq);\n\t\tctx->io_wq = NULL;\n\t}\n}\n\n#if defined(CONFIG_UNIX)\n/*\n * Ensure the UNIX gc is aware of our file set, so we are certain that\n * the io_uring can be safely unregistered on process exit, even if we have\n * loops in the file referencing.\n */\nstatic int __io_sqe_files_scm(struct io_ring_ctx *ctx, int nr, int offset)\n{\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\tint i, nr_files;\n\n\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\tif (!fpl)\n\t\treturn -ENOMEM;\n\n\tskb = alloc_skb(0, GFP_KERNEL);\n\tif (!skb) {\n\t\tkfree(fpl);\n\t\treturn -ENOMEM;\n\t}\n\n\tskb->sk = sk;\n\n\tnr_files = 0;\n\tfpl->user = get_uid(ctx->user);\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct file *file = io_file_from_index(ctx, i + offset);\n\n\t\tif (!file)\n\t\t\tcontinue;\n\t\tfpl->fp[nr_files] = get_file(file);\n\t\tunix_inflight(fpl->user, fpl->fp[nr_files]);\n\t\tnr_files++;\n\t}\n\n\tif (nr_files) {\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = nr_files;\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\n\t\tfor (i = 0; i < nr_files; i++)\n\t\t\tfput(fpl->fp[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t\tkfree(fpl);\n\t}\n\n\treturn 0;\n}\n\n/*\n * If UNIX sockets are enabled, fd passing can cause a reference cycle which\n * causes regular reference counting to break down. We rely on the UNIX\n * garbage collection to take care of this problem for us.\n */\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\tunsigned left, total;\n\tint ret = 0;\n\n\ttotal = 0;\n\tleft = ctx->nr_user_files;\n\twhile (left) {\n\t\tunsigned this_files = min_t(unsigned, left, SCM_MAX_FD);\n\n\t\tret = __io_sqe_files_scm(ctx, this_files, total);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tleft -= this_files;\n\t\ttotal += this_files;\n\t}\n\n\tif (!ret)\n\t\treturn 0;\n\n\twhile (total < ctx->nr_user_files) {\n\t\tstruct file *file = io_file_from_index(ctx, total);\n\n\t\tif (file)\n\t\t\tfput(file);\n\t\ttotal++;\n\t}\n\n\treturn ret;\n}\n#else\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\treturn 0;\n}\n#endif\n\nstatic int io_sqe_alloc_file_tables(struct fixed_file_data *file_data,\n\t\t\t\t    unsigned nr_tables, unsigned nr_files)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_file_table *table = &file_data->table[i];\n\t\tunsigned this_files;\n\n\t\tthis_files = min(nr_files, IORING_MAX_FILES_TABLE);\n\t\ttable->files = kcalloc(this_files, sizeof(struct file *),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!table->files)\n\t\t\tbreak;\n\t\tnr_files -= this_files;\n\t}\n\n\tif (i == nr_tables)\n\t\treturn 0;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_file_table *table = &file_data->table[i];\n\t\tkfree(table->files);\n\t}\n\treturn 1;\n}\n\nstatic void io_ring_file_put(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head list, *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tint i;\n\n\t__skb_queue_head_init(&list);\n\n\t/*\n\t * Find the skb that holds this file in its SCM_RIGHTS. When found,\n\t * remove this entry and rearrange the file array.\n\t */\n\tskb = skb_dequeue(head);\n\twhile (skb) {\n\t\tstruct scm_fp_list *fp;\n\n\t\tfp = UNIXCB(skb).fp;\n\t\tfor (i = 0; i < fp->count; i++) {\n\t\t\tint left;\n\n\t\t\tif (fp->fp[i] != file)\n\t\t\t\tcontinue;\n\n\t\t\tunix_notinflight(fp->user, fp->fp[i]);\n\t\t\tleft = fp->count - 1 - i;\n\t\t\tif (left) {\n\t\t\t\tmemmove(&fp->fp[i], &fp->fp[i + 1],\n\t\t\t\t\t\tleft * sizeof(struct file *));\n\t\t\t}\n\t\t\tfp->count--;\n\t\t\tif (!fp->count) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t} else {\n\t\t\t\t__skb_queue_tail(&list, skb);\n\t\t\t}\n\t\t\tfput(file);\n\t\t\tfile = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!file)\n\t\t\tbreak;\n\n\t\t__skb_queue_tail(&list, skb);\n\n\t\tskb = skb_dequeue(head);\n\t}\n\n\tif (skb_peek(&list)) {\n\t\tspin_lock_irq(&head->lock);\n\t\twhile ((skb = __skb_dequeue(&list)) != NULL)\n\t\t\t__skb_queue_tail(head, skb);\n\t\tspin_unlock_irq(&head->lock);\n\t}\n#else\n\tfput(file);\n#endif\n}\n\nstruct io_file_put {\n\tstruct list_head list;\n\tstruct file *file;\n};\n\nstatic void __io_file_put_work(struct fixed_file_ref_node *ref_node)\n{\n\tstruct fixed_file_data *file_data = ref_node->file_data;\n\tstruct io_ring_ctx *ctx = file_data->ctx;\n\tstruct io_file_put *pfile, *tmp;\n\n\tlist_for_each_entry_safe(pfile, tmp, &ref_node->file_list, list) {\n\t\tlist_del(&pfile->list);\n\t\tio_ring_file_put(ctx, pfile->file);\n\t\tkfree(pfile);\n\t}\n\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n\tpercpu_ref_put(&file_data->refs);\n}\n\nstatic void io_file_put_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct llist_node *node;\n\n\tctx = container_of(work, struct io_ring_ctx, file_put_work.work);\n\tnode = llist_del_all(&ctx->file_put_llist);\n\n\twhile (node) {\n\t\tstruct fixed_file_ref_node *ref_node;\n\t\tstruct llist_node *next = node->next;\n\n\t\tref_node = llist_entry(node, struct fixed_file_ref_node, llist);\n\t\t__io_file_put_work(ref_node);\n\t\tnode = next;\n\t}\n}\n\nstatic void io_file_data_ref_zero(struct percpu_ref *ref)\n{\n\tstruct fixed_file_ref_node *ref_node;\n\tstruct fixed_file_data *data;\n\tstruct io_ring_ctx *ctx;\n\tbool first_add = false;\n\tint delay = HZ;\n\n\tref_node = container_of(ref, struct fixed_file_ref_node, refs);\n\tdata = ref_node->file_data;\n\tctx = data->ctx;\n\n\tspin_lock_bh(&data->lock);\n\tref_node->done = true;\n\n\twhile (!list_empty(&data->ref_list)) {\n\t\tref_node = list_first_entry(&data->ref_list,\n\t\t\t\t\tstruct fixed_file_ref_node, node);\n\t\t/* recycle ref nodes in order */\n\t\tif (!ref_node->done)\n\t\t\tbreak;\n\t\tlist_del(&ref_node->node);\n\t\tfirst_add |= llist_add(&ref_node->llist, &ctx->file_put_llist);\n\t}\n\tspin_unlock_bh(&data->lock);\n\n\tif (percpu_ref_is_dying(&data->refs))\n\t\tdelay = 0;\n\n\tif (!delay)\n\t\tmod_delayed_work(system_wq, &ctx->file_put_work, 0);\n\telse if (first_add)\n\t\tqueue_delayed_work(system_wq, &ctx->file_put_work, delay);\n}\n\nstatic struct fixed_file_ref_node *alloc_fixed_file_ref_node(\n\t\t\tstruct io_ring_ctx *ctx)\n{\n\tstruct fixed_file_ref_node *ref_node;\n\n\tref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);\n\tif (!ref_node)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (percpu_ref_init(&ref_node->refs, io_file_data_ref_zero,\n\t\t\t    0, GFP_KERNEL)) {\n\t\tkfree(ref_node);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tINIT_LIST_HEAD(&ref_node->node);\n\tINIT_LIST_HEAD(&ref_node->file_list);\n\tref_node->file_data = ctx->file_data;\n\tref_node->done = false;\n\treturn ref_node;\n}\n\nstatic void destroy_fixed_file_ref_node(struct fixed_file_ref_node *ref_node)\n{\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n}\n\nstatic int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t unsigned nr_args)\n{\n\t__s32 __user *fds = (__s32 __user *) arg;\n\tunsigned nr_tables, i;\n\tstruct file *file;\n\tint fd, ret = -ENOMEM;\n\tstruct fixed_file_ref_node *ref_node;\n\tstruct fixed_file_data *file_data;\n\n\tif (ctx->file_data)\n\t\treturn -EBUSY;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (nr_args > IORING_MAX_FIXED_FILES)\n\t\treturn -EMFILE;\n\n\tfile_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);\n\tif (!file_data)\n\t\treturn -ENOMEM;\n\tfile_data->ctx = ctx;\n\tinit_completion(&file_data->done);\n\tINIT_LIST_HEAD(&file_data->ref_list);\n\tspin_lock_init(&file_data->lock);\n\n\tnr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);\n\tfile_data->table = kcalloc(nr_tables, sizeof(*file_data->table),\n\t\t\t\t   GFP_KERNEL);\n\tif (!file_data->table)\n\t\tgoto out_free;\n\n\tif (percpu_ref_init(&file_data->refs, io_file_ref_kill,\n\t\t\t\tPERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto out_free;\n\n\tif (io_sqe_alloc_file_tables(file_data, nr_tables, nr_args))\n\t\tgoto out_ref;\n\tctx->file_data = file_data;\n\n\tfor (i = 0; i < nr_args; i++, ctx->nr_user_files++) {\n\t\tstruct fixed_file_table *table;\n\t\tunsigned index;\n\n\t\tif (copy_from_user(&fd, &fds[i], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_fput;\n\t\t}\n\t\t/* allow sparse sets */\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tfile = fget(fd);\n\t\tret = -EBADF;\n\t\tif (!file)\n\t\t\tgoto out_fput;\n\n\t\t/*\n\t\t * Don't allow io_uring instances to be registered. If UNIX\n\t\t * isn't enabled, then this causes a reference cycle and this\n\t\t * instance can never get freed. If UNIX is enabled we'll\n\t\t * handle it just fine, but there's still no point in allowing\n\t\t * a ring fd as it doesn't support regular read/write anyway.\n\t\t */\n\t\tif (file->f_op == &io_uring_fops) {\n\t\t\tfput(file);\n\t\t\tgoto out_fput;\n\t\t}\n\t\ttable = &file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\t\tindex = i & IORING_FILE_TABLE_MASK;\n\t\ttable->files[index] = file;\n\t}\n\n\tret = io_sqe_files_scm(ctx);\n\tif (ret) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn ret;\n\t}\n\n\tref_node = alloc_fixed_file_ref_node(ctx);\n\tif (IS_ERR(ref_node)) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn PTR_ERR(ref_node);\n\t}\n\n\tfile_data->node = ref_node;\n\tspin_lock_bh(&file_data->lock);\n\tlist_add_tail(&ref_node->node, &file_data->ref_list);\n\tspin_unlock_bh(&file_data->lock);\n\tpercpu_ref_get(&file_data->refs);\n\treturn ret;\nout_fput:\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(file_data->table[i].files);\n\tctx->nr_user_files = 0;\nout_ref:\n\tpercpu_ref_exit(&file_data->refs);\nout_free:\n\tkfree(file_data->table);\n\tkfree(file_data);\n\tctx->file_data = NULL;\n\treturn ret;\n}\n\nstatic int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,\n\t\t\t\tint index)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb) {\n\t\tstruct scm_fp_list *fpl = UNIXCB(skb).fp;\n\n\t\tif (fpl->count < SCM_MAX_FD) {\n\t\t\t__skb_unlink(skb, head);\n\t\t\tspin_unlock_irq(&head->lock);\n\t\t\tfpl->fp[fpl->count] = get_file(file);\n\t\t\tunix_inflight(fpl->user, fpl->fp[fpl->count]);\n\t\t\tfpl->count++;\n\t\t\tspin_lock_irq(&head->lock);\n\t\t\t__skb_queue_head(head, skb);\n\t\t} else {\n\t\t\tskb = NULL;\n\t\t}\n\t}\n\tspin_unlock_irq(&head->lock);\n\n\tif (skb) {\n\t\tfput(file);\n\t\treturn 0;\n\t}\n\n\treturn __io_sqe_files_scm(ctx, 1, index);\n#else\n\treturn 0;\n#endif\n}\n\nstatic int io_queue_file_removal(struct fixed_file_data *data,\n\t\t\t\t struct file *file)\n{\n\tstruct io_file_put *pfile;\n\tstruct fixed_file_ref_node *ref_node = data->node;\n\n\tpfile = kzalloc(sizeof(*pfile), GFP_KERNEL);\n\tif (!pfile)\n\t\treturn -ENOMEM;\n\n\tpfile->file = file;\n\tlist_add(&pfile->list, &ref_node->file_list);\n\n\treturn 0;\n}\n\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_files_update *up,\n\t\t\t\t unsigned nr_args)\n{\n\tstruct fixed_file_data *data = ctx->file_data;\n\tstruct fixed_file_ref_node *ref_node;\n\tstruct file *file;\n\t__s32 __user *fds;\n\tint fd, i, err;\n\t__u32 done;\n\tbool needs_switch = false;\n\n\tif (check_add_overflow(up->offset, nr_args, &done))\n\t\treturn -EOVERFLOW;\n\tif (done > ctx->nr_user_files)\n\t\treturn -EINVAL;\n\n\tref_node = alloc_fixed_file_ref_node(ctx);\n\tif (IS_ERR(ref_node))\n\t\treturn PTR_ERR(ref_node);\n\n\tdone = 0;\n\tfds = u64_to_user_ptr(up->fds);\n\twhile (nr_args) {\n\t\tstruct fixed_file_table *table;\n\t\tunsigned index;\n\n\t\terr = 0;\n\t\tif (copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\ti = array_index_nospec(up->offset, ctx->nr_user_files);\n\t\ttable = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\t\tindex = i & IORING_FILE_TABLE_MASK;\n\t\tif (table->files[index]) {\n\t\t\tfile = table->files[index];\n\t\t\terr = io_queue_file_removal(data, file);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\ttable->files[index] = NULL;\n\t\t\tneeds_switch = true;\n\t\t}\n\t\tif (fd != -1) {\n\t\t\tfile = fget(fd);\n\t\t\tif (!file) {\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Don't allow io_uring instances to be registered. If\n\t\t\t * UNIX isn't enabled, then this causes a reference\n\t\t\t * cycle and this instance can never get freed. If UNIX\n\t\t\t * is enabled we'll handle it just fine, but there's\n\t\t\t * still no point in allowing a ring fd as it doesn't\n\t\t\t * support regular read/write anyway.\n\t\t\t */\n\t\t\tif (file->f_op == &io_uring_fops) {\n\t\t\t\tfput(file);\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttable->files[index] = file;\n\t\t\terr = io_sqe_file_register(ctx, file, i);\n\t\t\tif (err) {\n\t\t\t\ttable->files[index] = NULL;\n\t\t\t\tfput(file);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tnr_args--;\n\t\tdone++;\n\t\tup->offset++;\n\t}\n\n\tif (needs_switch) {\n\t\tpercpu_ref_kill(&data->node->refs);\n\t\tspin_lock_bh(&data->lock);\n\t\tlist_add_tail(&ref_node->node, &data->ref_list);\n\t\tdata->node = ref_node;\n\t\tspin_unlock_bh(&data->lock);\n\t\tpercpu_ref_get(&ctx->file_data->refs);\n\t} else\n\t\tdestroy_fixed_file_ref_node(ref_node);\n\n\treturn done ? done : err;\n}\n\nstatic int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t       unsigned nr_args)\n{\n\tstruct io_uring_files_update up;\n\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&up, arg, sizeof(up)))\n\t\treturn -EFAULT;\n\tif (up.resv)\n\t\treturn -EINVAL;\n\n\treturn __io_sqe_files_update(ctx, &up, nr_args);\n}\n\nstatic void io_free_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\t/* Consider that io_steal_work() relies on this ref */\n\tio_put_req(req);\n}\n\nstatic int io_init_wq_offload(struct io_ring_ctx *ctx,\n\t\t\t      struct io_uring_params *p)\n{\n\tstruct io_wq_data data;\n\tstruct fd f;\n\tstruct io_ring_ctx *ctx_attach;\n\tunsigned int concurrency;\n\tint ret = 0;\n\n\tdata.user = ctx->user;\n\tdata.free_work = io_free_work;\n\tdata.do_work = io_wq_submit_work;\n\n\tif (!(p->flags & IORING_SETUP_ATTACH_WQ)) {\n\t\t/* Do QD, or 4 * CPUS, whatever is smallest */\n\t\tconcurrency = min(ctx->sq_entries, 4 * num_online_cpus());\n\n\t\tctx->io_wq = io_wq_create(concurrency, &data);\n\t\tif (IS_ERR(ctx->io_wq)) {\n\t\t\tret = PTR_ERR(ctx->io_wq);\n\t\t\tctx->io_wq = NULL;\n\t\t}\n\t\treturn ret;\n\t}\n\n\tf = fdget(p->wq_fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (f.file->f_op != &io_uring_fops) {\n\t\tret = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\tctx_attach = f.file->private_data;\n\t/* @io_wq is protected by holding the fd */\n\tif (!io_wq_get(ctx_attach->io_wq, &data)) {\n\t\tret = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\tctx->io_wq = ctx_attach->io_wq;\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int io_uring_alloc_task_context(struct task_struct *task)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttctx->sqpoll = false;\n\tio_init_identity(&tctx->__identity);\n\ttctx->identity = &tctx->__identity;\n\ttask->io_uring = tctx;\n\treturn 0;\n}\n\nvoid __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(refcount_read(&tctx->identity->count) != 1);\n\tif (tctx->identity != &tctx->__identity)\n\t\tkfree(tctx->identity);\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}\n\nstatic int io_sq_offload_create(struct io_ring_ctx *ctx,\n\t\t\t\tstruct io_uring_params *p)\n{\n\tint ret;\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tstruct io_sq_data *sqd;\n\n\t\tret = -EPERM;\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_NICE))\n\t\t\tgoto err;\n\n\t\tsqd = io_get_sq_data(p);\n\t\tif (IS_ERR(sqd)) {\n\t\t\tret = PTR_ERR(sqd);\n\t\t\tgoto err;\n\t\t}\n\n\t\tctx->sq_data = sqd;\n\t\tio_sq_thread_park(sqd);\n\t\tmutex_lock(&sqd->ctx_lock);\n\t\tlist_add(&ctx->sqd_list, &sqd->ctx_new_list);\n\t\tmutex_unlock(&sqd->ctx_lock);\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tif (sqd->thread)\n\t\t\tgoto done;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids)\n\t\t\t\tgoto err;\n\t\t\tif (!cpu_online(cpu))\n\t\t\t\tgoto err;\n\n\t\t\tsqd->thread = kthread_create_on_cpu(io_sq_thread, sqd,\n\t\t\t\t\t\t\tcpu, \"io_uring-sq\");\n\t\t} else {\n\t\t\tsqd->thread = kthread_create(io_sq_thread, sqd,\n\t\t\t\t\t\t\t\"io_uring-sq\");\n\t\t}\n\t\tif (IS_ERR(sqd->thread)) {\n\t\t\tret = PTR_ERR(sqd->thread);\n\t\t\tsqd->thread = NULL;\n\t\t\tgoto err;\n\t\t}\n\t\tret = io_uring_alloc_task_context(sqd->thread);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\ndone:\n\tret = io_init_wq_offload(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tio_finish_async(ctx);\n\treturn ret;\n}\n\nstatic void io_sq_offload_start(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && sqd->thread)\n\t\twake_up_process(sqd->thread);\n}\n\nstatic inline void __io_unaccount_mem(struct user_struct *user,\n\t\t\t\t      unsigned long nr_pages)\n{\n\tatomic_long_sub(nr_pages, &user->locked_vm);\n}\n\nstatic inline int __io_account_mem(struct user_struct *user,\n\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long page_limit, cur_pages, new_pages;\n\n\t/* Don't allow more pages than we can safely lock */\n\tpage_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tdo {\n\t\tcur_pages = atomic_long_read(&user->locked_vm);\n\t\tnew_pages = cur_pages + nr_pages;\n\t\tif (new_pages > page_limit)\n\t\t\treturn -ENOMEM;\n\t} while (atomic_long_cmpxchg(&user->locked_vm, cur_pages,\n\t\t\t\t\tnew_pages) != cur_pages);\n\n\treturn 0;\n}\n\nstatic void io_unaccount_mem(struct io_ring_ctx *ctx, unsigned long nr_pages,\n\t\t\t     enum io_mem_account acct)\n{\n\tif (ctx->limit_mem)\n\t\t__io_unaccount_mem(ctx->user, nr_pages);\n\n\tif (ctx->mm_account) {\n\t\tif (acct == ACCT_LOCKED)\n\t\t\tctx->mm_account->locked_vm -= nr_pages;\n\t\telse if (acct == ACCT_PINNED)\n\t\t\tatomic64_sub(nr_pages, &ctx->mm_account->pinned_vm);\n\t}\n}\n\nstatic int io_account_mem(struct io_ring_ctx *ctx, unsigned long nr_pages,\n\t\t\t  enum io_mem_account acct)\n{\n\tint ret;\n\n\tif (ctx->limit_mem) {\n\t\tret = __io_account_mem(ctx->user, nr_pages);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ctx->mm_account) {\n\t\tif (acct == ACCT_LOCKED)\n\t\t\tctx->mm_account->locked_vm += nr_pages;\n\t\telse if (acct == ACCT_PINNED)\n\t\t\tatomic64_add(nr_pages, &ctx->mm_account->pinned_vm);\n\t}\n\n\treturn 0;\n}\n\nstatic void io_mem_free(void *ptr)\n{\n\tstruct page *page;\n\n\tif (!ptr)\n\t\treturn;\n\n\tpage = virt_to_head_page(ptr);\n\tif (put_page_testzero(page))\n\t\tfree_compound_page(page);\n}\n\nstatic void *io_mem_alloc(size_t size)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP |\n\t\t\t\t__GFP_NORETRY;\n\n\treturn (void *) __get_free_pages(gfp_flags, get_order(size));\n}\n\nstatic unsigned long rings_size(unsigned sq_entries, unsigned cq_entries,\n\t\t\t\tsize_t *sq_offset)\n{\n\tstruct io_rings *rings;\n\tsize_t off, sq_array_size;\n\n\toff = struct_size(rings, cqes, cq_entries);\n\tif (off == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n#ifdef CONFIG_SMP\n\toff = ALIGN(off, SMP_CACHE_BYTES);\n\tif (off == 0)\n\t\treturn SIZE_MAX;\n#endif\n\n\tif (sq_offset)\n\t\t*sq_offset = off;\n\n\tsq_array_size = array_size(sizeof(u32), sq_entries);\n\tif (sq_array_size == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n\tif (check_add_overflow(off, sq_array_size, &off))\n\t\treturn SIZE_MAX;\n\n\treturn off;\n}\n\nstatic unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)\n{\n\tsize_t pages;\n\n\tpages = (size_t)1 << get_order(\n\t\trings_size(sq_entries, cq_entries, NULL));\n\tpages += (size_t)1 << get_order(\n\t\tarray_size(sizeof(struct io_uring_sqe), sq_entries));\n\n\treturn pages;\n}\n\nstatic int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)\n{\n\tint i, j;\n\n\tif (!ctx->user_bufs)\n\t\treturn -ENXIO;\n\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++)\n\t\t\tunpin_user_page(imu->bvec[j].bv_page);\n\n\t\tif (imu->acct_pages)\n\t\t\tio_unaccount_mem(ctx, imu->acct_pages, ACCT_PINNED);\n\t\tkvfree(imu->bvec);\n\t\timu->nr_bvecs = 0;\n\t}\n\n\tkfree(ctx->user_bufs);\n\tctx->user_bufs = NULL;\n\tctx->nr_user_bufs = 0;\n\treturn 0;\n}\n\nstatic int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,\n\t\t       void __user *arg, unsigned index)\n{\n\tstruct iovec __user *src;\n\n#ifdef CONFIG_COMPAT\n\tif (ctx->compat) {\n\t\tstruct compat_iovec __user *ciovs;\n\t\tstruct compat_iovec ciov;\n\n\t\tciovs = (struct compat_iovec __user *) arg;\n\t\tif (copy_from_user(&ciov, &ciovs[index], sizeof(ciov)))\n\t\t\treturn -EFAULT;\n\n\t\tdst->iov_base = u64_to_user_ptr((u64)ciov.iov_base);\n\t\tdst->iov_len = ciov.iov_len;\n\t\treturn 0;\n\t}\n#endif\n\tsrc = (struct iovec __user *) arg;\n\tif (copy_from_user(dst, &src[index], sizeof(*dst)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Not super efficient, but this is just a registration time. And we do cache\n * the last compound head, so generally we'll only do a full search if we don't\n * match that one.\n *\n * We check if the given compound head page has already been accounted, to\n * avoid double accounting it. This allows us to account the full size of the\n * page, not just the constituent pages of a huge page.\n */\nstatic bool headpage_already_acct(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t  int nr_pages, struct page *hpage)\n{\n\tint i, j;\n\n\t/* check current page array */\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i]))\n\t\t\tcontinue;\n\t\tif (compound_head(pages[i]) == hpage)\n\t\t\treturn true;\n\t}\n\n\t/* check previously registered pages */\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++) {\n\t\t\tif (!PageCompound(imu->bvec[j].bv_page))\n\t\t\t\tcontinue;\n\t\t\tif (compound_head(imu->bvec[j].bv_page) == hpage)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int io_buffer_account_pin(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t int nr_pages, struct io_mapped_ubuf *imu,\n\t\t\t\t struct page **last_hpage)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i])) {\n\t\t\timu->acct_pages++;\n\t\t} else {\n\t\t\tstruct page *hpage;\n\n\t\t\thpage = compound_head(pages[i]);\n\t\t\tif (hpage == *last_hpage)\n\t\t\t\tcontinue;\n\t\t\t*last_hpage = hpage;\n\t\t\tif (headpage_already_acct(ctx, pages, i, hpage))\n\t\t\t\tcontinue;\n\t\t\timu->acct_pages += page_size(hpage) >> PAGE_SHIFT;\n\t\t}\n\t}\n\n\tif (!imu->acct_pages)\n\t\treturn 0;\n\n\tret = io_account_mem(ctx, imu->acct_pages, ACCT_PINNED);\n\tif (ret)\n\t\timu->acct_pages = 0;\n\treturn ret;\n}\n\nstatic int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t  unsigned nr_args)\n{\n\tstruct vm_area_struct **vmas = NULL;\n\tstruct page **pages = NULL;\n\tstruct page *last_hpage = NULL;\n\tint i, j, got_pages = 0;\n\tint ret = -EINVAL;\n\n\tif (ctx->user_bufs)\n\t\treturn -EBUSY;\n\tif (!nr_args || nr_args > UIO_MAXIOV)\n\t\treturn -EINVAL;\n\n\tctx->user_bufs = kcalloc(nr_args, sizeof(struct io_mapped_ubuf),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->user_bufs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\t\tunsigned long off, start, end, ubuf;\n\t\tint pret, nr_pages;\n\t\tstruct iovec iov;\n\t\tsize_t size;\n\n\t\tret = io_copy_iov(ctx, &iov, arg, i);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\t/*\n\t\t * Don't impose further limits on the size and buffer\n\t\t * constraints here, we'll -EINVAL later when IO is\n\t\t * submitted if they are wrong.\n\t\t */\n\t\tret = -EFAULT;\n\t\tif (!iov.iov_base || !iov.iov_len)\n\t\t\tgoto err;\n\n\t\t/* arbitrary limit, but we need something */\n\t\tif (iov.iov_len > SZ_1G)\n\t\t\tgoto err;\n\n\t\tubuf = (unsigned long) iov.iov_base;\n\t\tend = (ubuf + iov.iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tstart = ubuf >> PAGE_SHIFT;\n\t\tnr_pages = end - start;\n\n\t\tret = 0;\n\t\tif (!pages || nr_pages > got_pages) {\n\t\t\tkvfree(vmas);\n\t\t\tkvfree(pages);\n\t\t\tpages = kvmalloc_array(nr_pages, sizeof(struct page *),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tvmas = kvmalloc_array(nr_pages,\n\t\t\t\t\tsizeof(struct vm_area_struct *),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!pages || !vmas) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tgot_pages = nr_pages;\n\t\t}\n\n\t\timu->bvec = kvmalloc_array(nr_pages, sizeof(struct bio_vec),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tret = -ENOMEM;\n\t\tif (!imu->bvec)\n\t\t\tgoto err;\n\n\t\tret = 0;\n\t\tmmap_read_lock(current->mm);\n\t\tpret = pin_user_pages(ubuf, nr_pages,\n\t\t\t\t      FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t\t      pages, vmas);\n\t\tif (pret == nr_pages) {\n\t\t\t/* don't support file backed memory */\n\t\t\tfor (j = 0; j < nr_pages; j++) {\n\t\t\t\tstruct vm_area_struct *vma = vmas[j];\n\n\t\t\t\tif (vma->vm_file &&\n\t\t\t\t    !is_file_hugepages(vma->vm_file)) {\n\t\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tret = pret < 0 ? pret : -EFAULT;\n\t\t}\n\t\tmmap_read_unlock(current->mm);\n\t\tif (ret) {\n\t\t\t/*\n\t\t\t * if we did partial map, or found file backed vmas,\n\t\t\t * release any pages we did get\n\t\t\t */\n\t\t\tif (pret > 0)\n\t\t\t\tunpin_user_pages(pages, pret);\n\t\t\tkvfree(imu->bvec);\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = io_buffer_account_pin(ctx, pages, pret, imu, &last_hpage);\n\t\tif (ret) {\n\t\t\tunpin_user_pages(pages, pret);\n\t\t\tkvfree(imu->bvec);\n\t\t\tgoto err;\n\t\t}\n\n\t\toff = ubuf & ~PAGE_MASK;\n\t\tsize = iov.iov_len;\n\t\tfor (j = 0; j < nr_pages; j++) {\n\t\t\tsize_t vec_len;\n\n\t\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\t\timu->bvec[j].bv_page = pages[j];\n\t\t\timu->bvec[j].bv_len = vec_len;\n\t\t\timu->bvec[j].bv_offset = off;\n\t\t\toff = 0;\n\t\t\tsize -= vec_len;\n\t\t}\n\t\t/* store original address for later verification */\n\t\timu->ubuf = ubuf;\n\t\timu->len = iov.iov_len;\n\t\timu->nr_bvecs = nr_pages;\n\n\t\tctx->nr_user_bufs++;\n\t}\n\tkvfree(pages);\n\tkvfree(vmas);\n\treturn 0;\nerr:\n\tkvfree(pages);\n\tkvfree(vmas);\n\tio_sqe_buffer_unregister(ctx);\n\treturn ret;\n}\n\nstatic int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)\n{\n\t__s32 __user *fds = arg;\n\tint fd;\n\n\tif (ctx->cq_ev_fd)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&fd, fds, sizeof(*fds)))\n\t\treturn -EFAULT;\n\n\tctx->cq_ev_fd = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(ctx->cq_ev_fd)) {\n\t\tint ret = PTR_ERR(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_eventfd_unregister(struct io_ring_ctx *ctx)\n{\n\tif (ctx->cq_ev_fd) {\n\t\teventfd_ctx_put(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn 0;\n\t}\n\n\treturn -ENXIO;\n}\n\nstatic int __io_destroy_buffers(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tstruct io_buffer *buf = p;\n\n\t__io_remove_buffers(ctx, buf, id, -1U);\n\treturn 0;\n}\n\nstatic void io_destroy_buffers(struct io_ring_ctx *ctx)\n{\n\tidr_for_each(&ctx->io_buffer_idr, __io_destroy_buffers, ctx);\n\tidr_destroy(&ctx->io_buffer_idr);\n}\n\nstatic void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_finish_async(ctx);\n\tio_sqe_buffer_unregister(ctx);\n\n\tif (ctx->sqo_task) {\n\t\tput_task_struct(ctx->sqo_task);\n\t\tctx->sqo_task = NULL;\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\n#ifdef CONFIG_BLK_CGROUP\n\tif (ctx->sqo_blkcg_css)\n\t\tcss_put(ctx->sqo_blkcg_css);\n#endif\n\n\tio_sqe_files_unregister(ctx);\n\tio_eventfd_unregister(ctx);\n\tio_destroy_buffers(ctx);\n\tidr_destroy(&ctx->personality_idr);\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tput_cred(ctx->creds);\n\tkfree(ctx->cancel_hash);\n\tkmem_cache_free(req_cachep, ctx->fallback_req);\n\tkfree(ctx);\n}\n\nstatic __poll_t io_uring_poll(struct file *file, poll_table *wait)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\t__poll_t mask = 0;\n\n\tpoll_wait(file, &ctx->cq_wait, wait);\n\t/*\n\t * synchronizes with barrier from wq_has_sleeper call in\n\t * io_commit_cqring\n\t */\n\tsmp_rmb();\n\tif (!io_sqring_full(ctx))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\tif (io_cqring_events(ctx, false))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\treturn mask;\n}\n\nstatic int io_uring_fasync(int fd, struct file *file, int on)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\treturn fasync_helper(fd, file, on, &ctx->cq_fasync);\n}\n\nstatic int io_remove_personalities(int id, void *p, void *data)\n{\n\tstruct io_ring_ctx *ctx = data;\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t}\n\treturn 0;\n}\n\nstatic void io_ring_exit_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx,\n\t\t\t\t\t       exit_work);\n\n\t/*\n\t * If we're doing polled IO and end up having requests being\n\t * submitted async (out-of-line), then completions can come in while\n\t * we're waiting for refs to drop. We need to reap these manually,\n\t * as nobody else will be looking for them.\n\t */\n\tdo {\n\t\tif (ctx->rings)\n\t\t\tio_cqring_overflow_flush(ctx, true, NULL, NULL);\n\t\tio_iopoll_try_reap_events(ctx);\n\t} while (!wait_for_completion_timeout(&ctx->ref_comp, HZ/20));\n\tio_ring_ctx_free(ctx);\n}\n\nstatic void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\tif (ctx->io_wq)\n\t\tio_wq_cancel_all(ctx->io_wq);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tif (ctx->rings)\n\t\tio_cqring_overflow_flush(ctx, true, NULL, NULL);\n\tio_iopoll_try_reap_events(ctx);\n\tidr_for_each(&ctx->personality_idr, io_remove_personalities, ctx);\n\n\t/*\n\t * Do this upfront, so we won't have a grace period where the ring\n\t * is closed but resources aren't reaped yet. This can cause\n\t * spurious failure in setting up a new ring.\n\t */\n\tio_unaccount_mem(ctx, ring_pages(ctx->sq_entries, ctx->cq_entries),\n\t\t\t ACCT_LOCKED);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}\n\nstatic int io_uring_release(struct inode *inode, struct file *file)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\tfile->private_data = NULL;\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn 0;\n}\n\nstruct io_task_cancel {\n\tstruct task_struct *task;\n\tstruct files_struct *files;\n};\n\nstatic bool io_cancel_task_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_task_cancel *cancel = data;\n\tbool ret;\n\n\tif (cancel->files && (req->flags & REQ_F_LINK_TIMEOUT)) {\n\t\tunsigned long flags;\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\t/* protect against races with linked timeouts */\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\t} else {\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t}\n\treturn ret;\n}\n\nstatic void io_cancel_defer_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\tstruct io_defer_entry *de = NULL;\n\tLIST_HEAD(list);\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_reverse(de, &ctx->defer_list, list) {\n\t\tif (io_match_task(de->req, task, files)) {\n\t\t\tlist_cut_position(&list, &ctx->defer_list, &de->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tde = list_first_entry(&list, struct io_defer_entry, list);\n\t\tlist_del_init(&de->list);\n\t\treq_set_fail_links(de->req);\n\t\tio_put_req(de->req);\n\t\tio_req_complete(de->req, -ECANCELED);\n\t\tkfree(de);\n\t}\n}\n\nstatic void io_uring_cancel_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\twhile (!list_empty_careful(&ctx->inflight_list)) {\n\t\tstruct io_task_cancel cancel = { .task = task, .files = files };\n\t\tstruct io_kiocb *req;\n\t\tDEFINE_WAIT(wait);\n\t\tbool found = false;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry) {\n\t\t\tif (req->task != task ||\n\t\t\t    req->work.identity->files != files)\n\t\t\t\tcontinue;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t\tif (found)\n\t\t\tprepare_to_wait(&task->io_uring->wait, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\n\t\t/* We need to keep going until we don't find a matching req */\n\t\tif (!found)\n\t\t\tbreak;\n\n\t\tio_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, &cancel, true);\n\t\tio_poll_remove_all(ctx, task, files);\n\t\tio_kill_timeouts(ctx, task, files);\n\t\t/* cancellations _may_ trigger task work */\n\t\tio_run_task_work();\n\t\tschedule();\n\t\tfinish_wait(&task->io_uring->wait, &wait);\n\t}\n}\n\nstatic void __io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t    struct task_struct *task)\n{\n\twhile (1) {\n\t\tstruct io_task_cancel cancel = { .task = task, .files = NULL, };\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tcret = io_wq_cancel_cb(ctx->io_wq, io_cancel_task_cb, &cancel, true);\n\t\tif (cret != IO_WQ_CANCEL_NOTFOUND)\n\t\t\tret = true;\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif (!(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\twhile (!list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_poll_remove_all(ctx, task, NULL);\n\t\tret |= io_kill_timeouts(ctx, task, NULL);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tio_run_task_work();\n\t\tcond_resched();\n\t}\n}\n\n/*\n * We need to iteratively cancel requests, in case a request has dependent\n * hard links. These persist even for failure of cancelations, hence keep\n * looping until none are found.\n */\nstatic void io_uring_cancel_task_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct files_struct *files)\n{\n\tstruct task_struct *task = current;\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\ttask = ctx->sq_data->thread;\n\t\tatomic_inc(&task->io_uring->in_idle);\n\t\tio_sq_thread_park(ctx->sq_data);\n\t}\n\n\tio_cancel_defer_files(ctx, task, files);\n\tio_cqring_overflow_flush(ctx, true, task, files);\n\n\tif (!files)\n\t\t__io_uring_cancel_task_requests(ctx, task);\n\telse\n\t\tio_uring_cancel_files(ctx, task, files);\n\n\tif ((ctx->flags & IORING_SETUP_SQPOLL) && ctx->sq_data) {\n\t\tatomic_dec(&task->io_uring->in_idle);\n\t\t/*\n\t\t * If the files that are going away are the ones in the thread\n\t\t * identity, clear them out.\n\t\t */\n\t\tif (task->io_uring->identity->files == files)\n\t\t\ttask->io_uring->identity->files = NULL;\n\t\tio_sq_thread_unpark(ctx->sq_data);\n\t}\n}\n\n/*\n * Note that this task has used io_uring. We use it for cancelation purposes.\n */\nstatic int io_uring_add_task_file(struct io_ring_ctx *ctx, struct file *file)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (unlikely(!tctx)) {\n\t\tint ret;\n\n\t\tret = io_uring_alloc_task_context(current);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\ttctx = current->io_uring;\n\t}\n\tif (tctx->last != file) {\n\t\tvoid *old = xa_load(&tctx->xa, (unsigned long)file);\n\n\t\tif (!old) {\n\t\t\tget_file(file);\n\t\t\txa_store(&tctx->xa, (unsigned long)file, file, GFP_KERNEL);\n\t\t}\n\t\ttctx->last = file;\n\t}\n\n\t/*\n\t * This is race safe in that the task itself is doing this, hence it\n\t * cannot be going through the exit/cancel paths at the same time.\n\t * This cannot be modified while exit/cancel is running.\n\t */\n\tif (!tctx->sqpoll && (ctx->flags & IORING_SETUP_SQPOLL))\n\t\ttctx->sqpoll = true;\n\n\treturn 0;\n}\n\n/*\n * Remove this io_uring_file -> task mapping.\n */\nstatic void io_uring_del_task_file(struct file *file)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (tctx->last == file)\n\t\ttctx->last = NULL;\n\tfile = xa_erase(&tctx->xa, (unsigned long)file);\n\tif (file)\n\t\tfput(file);\n}\n\n/*\n * Drop task note for this file if we're the only ones that hold it after\n * pending fput()\n */\nstatic void io_uring_attempt_task_drop(struct file *file)\n{\n\tif (!current->io_uring)\n\t\treturn;\n\t/*\n\t * fput() is pending, will be 2 if the only other ref is our potential\n\t * task file note. If the task is exiting, drop regardless of count.\n\t */\n\tif (fatal_signal_pending(current) || (current->flags & PF_EXITING) ||\n\t    atomic_long_read(&file->f_count) == 2)\n\t\tio_uring_del_task_file(file);\n}\n\nvoid __io_uring_files_cancel(struct files_struct *files)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct file *file;\n\tunsigned long index;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\n\txa_for_each(&tctx->xa, index, file) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\tio_uring_cancel_task_requests(ctx, files);\n\t\tif (files)\n\t\t\tio_uring_del_task_file(file);\n\t}\n\n\tatomic_dec(&tctx->in_idle);\n}\n\nstatic s64 tctx_inflight(struct io_uring_task *tctx)\n{\n\tunsigned long index;\n\tstruct file *file;\n\ts64 inflight;\n\n\tinflight = percpu_counter_sum(&tctx->inflight);\n\tif (!tctx->sqpoll)\n\t\treturn inflight;\n\n\t/*\n\t * If we have SQPOLL rings, then we need to iterate and find them, and\n\t * add the pending count for those.\n\t */\n\txa_for_each(&tctx->xa, index, file) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\t\tstruct io_uring_task *__tctx = ctx->sqo_task->io_uring;\n\n\t\t\tinflight += percpu_counter_sum(&__tctx->inflight);\n\t\t}\n\t}\n\n\treturn inflight;\n}\n\n/*\n * Find any io_uring fd that this task has registered or done IO on, and cancel\n * requests.\n */\nvoid __io_uring_task_cancel(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tDEFINE_WAIT(wait);\n\ts64 inflight;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\t__io_uring_files_cancel(NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\t/*\n\t\t * If we've seen completions, retry. This avoids a race where\n\t\t * a completion comes in before we did prepare_to_wait().\n\t\t */\n\t\tif (inflight != tctx_inflight(tctx))\n\t\t\tcontinue;\n\t\tschedule();\n\t} while (1);\n\n\tfinish_wait(&tctx->wait, &wait);\n\tatomic_dec(&tctx->in_idle);\n}\n\nstatic int io_uring_flush(struct file *file, void *data)\n{\n\tio_uring_attempt_task_drop(file);\n\treturn 0;\n}\n\nstatic void *io_uring_validate_mmap_request(struct file *file,\n\t\t\t\t\t    loff_t pgoff, size_t sz)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\tloff_t offset = pgoff << PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tswitch (offset) {\n\tcase IORING_OFF_SQ_RING:\n\tcase IORING_OFF_CQ_RING:\n\t\tptr = ctx->rings;\n\t\tbreak;\n\tcase IORING_OFF_SQES:\n\t\tptr = ctx->sq_sqes;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tpage = virt_to_head_page(ptr);\n\tif (sz > page_size(page))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ptr;\n}\n\n#ifdef CONFIG_MMU\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t sz = vma->vm_end - vma->vm_start;\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tpfn = virt_to_phys(ptr) >> PAGE_SHIFT;\n\treturn remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);\n}\n\n#else /* !CONFIG_MMU */\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & (VM_SHARED | VM_MAYSHARE) ? 0 : -EINVAL;\n}\n\nstatic unsigned int io_uring_nommu_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT | NOMMU_MAP_READ | NOMMU_MAP_WRITE;\n}\n\nstatic unsigned long io_uring_nommu_get_unmapped_area(struct file *file,\n\tunsigned long addr, unsigned long len,\n\tunsigned long pgoff, unsigned long flags)\n{\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, pgoff, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\treturn (unsigned long) ptr;\n}\n\n#endif /* !CONFIG_MMU */\n\nstatic void io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n}\n\nstatic int io_get_ext_arg(unsigned flags, const void __user *argp, size_t *argsz,\n\t\t\t  struct __kernel_timespec __user **ts,\n\t\t\t  const sigset_t __user **sig)\n{\n\tstruct io_uring_getevents_arg arg;\n\n\t/*\n\t * If EXT_ARG isn't set, then we have no timespec and the argp pointer\n\t * is just a pointer to the sigset_t.\n\t */\n\tif (!(flags & IORING_ENTER_EXT_ARG)) {\n\t\t*sig = (const sigset_t __user *) argp;\n\t\t*ts = NULL;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * EXT_ARG is set - ensure we agree on the size of it and copy in our\n\t * timespec and sigset_t pointers if good.\n\t */\n\tif (*argsz != sizeof(arg))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&arg, argp, sizeof(arg)))\n\t\treturn -EFAULT;\n\t*sig = u64_to_user_ptr(arg.sigmask);\n\t*argsz = arg.sigmask_sz;\n\t*ts = u64_to_user_ptr(arg.ts);\n\treturn 0;\n}\n\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tint submitted = 0;\n\tstruct fd f;\n\n\tio_run_task_work();\n\n\tif (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\tIORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (!percpu_ref_tryget(&ctx->refs))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tif (!list_empty_careful(&ctx->cq_overflow_list))\n\t\t\tio_cqring_overflow_flush(ctx, false, NULL, NULL);\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT)\n\t\t\tio_sqpoll_wait_sq(ctx);\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx, f.file);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int io_uring_show_cred(int id, void *p, void *data)\n{\n\tstruct io_identity *iod = p;\n\tconst struct cred *cred = iod->creds;\n\tstruct seq_file *m = data;\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}\n\nstatic void __io_uring_show_fdinfo(struct io_ring_ctx *ctx, struct seq_file *m)\n{\n\tstruct io_sq_data *sq = NULL;\n\tbool has_lock;\n\tint i;\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL))\n\t\tsq = ctx->sq_data;\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq ? task_pid_nr(sq->thread) : -1);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq ? task_cpu(sq->thread) : -1);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct fixed_file_table *table;\n\t\tstruct file *f;\n\n\t\ttable = &ctx->file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\t\tf = table->files[i & IORING_FILE_TABLE_MASK];\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = &ctx->user_bufs[i];\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf,\n\t\t\t\t\t\t(unsigned int) buf->len);\n\t}\n\tif (has_lock && !idr_is_empty(&ctx->personality_idr)) {\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\tidr_for_each(&ctx->personality_idr, io_uring_show_cred, m);\n\t}\n\tseq_printf(m, \"PollList:\\n\");\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list = &ctx->cancel_hash[i];\n\t\tstruct io_kiocb *req;\n\n\t\thlist_for_each_entry(req, list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\treq->task->task_works != NULL);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\n\tif (percpu_ref_tryget(&ctx->refs)) {\n\t\t__io_uring_show_fdinfo(ctx, m);\n\t\tpercpu_ref_put(&ctx->refs);\n\t}\n}\n#endif\n\nstatic const struct file_operations io_uring_fops = {\n\t.release\t= io_uring_release,\n\t.flush\t\t= io_uring_flush,\n\t.mmap\t\t= io_uring_mmap,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = io_uring_nommu_get_unmapped_area,\n\t.mmap_capabilities = io_uring_nommu_mmap_capabilities,\n#endif\n\t.poll\t\t= io_uring_poll,\n\t.fasync\t\t= io_uring_fasync,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= io_uring_show_fdinfo,\n#endif\n};\n\nstatic int io_allocate_scq_urings(struct io_ring_ctx *ctx,\n\t\t\t\t  struct io_uring_params *p)\n{\n\tstruct io_rings *rings;\n\tsize_t size, sq_array_offset;\n\n\t/* make sure these are sane, as we already accounted them */\n\tctx->sq_entries = p->sq_entries;\n\tctx->cq_entries = p->cq_entries;\n\n\tsize = rings_size(p->sq_entries, p->cq_entries, &sq_array_offset);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\trings = io_mem_alloc(size);\n\tif (!rings)\n\t\treturn -ENOMEM;\n\n\tctx->rings = rings;\n\tctx->sq_array = (u32 *)((char *)rings + sq_array_offset);\n\trings->sq_ring_mask = p->sq_entries - 1;\n\trings->cq_ring_mask = p->cq_entries - 1;\n\trings->sq_ring_entries = p->sq_entries;\n\trings->cq_ring_entries = p->cq_entries;\n\tctx->sq_mask = rings->sq_ring_mask;\n\tctx->cq_mask = rings->cq_ring_mask;\n\n\tsize = array_size(sizeof(struct io_uring_sqe), p->sq_entries);\n\tif (size == SIZE_MAX) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tctx->sq_sqes = io_mem_alloc(size);\n\tif (!ctx->sq_sqes) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Allocate an anonymous fd, this is what constitutes the application\n * visible backing of an io_uring instance. The application mmaps this\n * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,\n * we have to tie this fd to a socket for file garbage collection purposes.\n */\nstatic int io_uring_get_fd(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n\tint ret;\n\n#if defined(CONFIG_UNIX)\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\tO_RDWR | O_CLOEXEC);\n\tif (IS_ERR(file)) {\nerr_fd:\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n#if defined(CONFIG_UNIX)\n\tctx->ring_sock->file = file;\n#endif\n\tif (unlikely(io_uring_add_task_file(ctx, file))) {\n\t\tfile = ERR_PTR(-ENOMEM);\n\t\tgoto err_fd;\n\t}\n\tfd_install(ret, file);\n\treturn ret;\nerr:\n#if defined(CONFIG_UNIX)\n\tsock_release(ctx->ring_sock);\n\tctx->ring_sock = NULL;\n#endif\n\treturn ret;\n}\n\nstatic int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct user_struct *user = NULL;\n\tstruct io_ring_ctx *ctx;\n\tbool limit_mem;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tuser = get_uid(current_user());\n\tlimit_mem = !capable(CAP_IPC_LOCK);\n\n\tif (limit_mem) {\n\t\tret = __io_account_mem(user,\n\t\t\t\tring_pages(p->sq_entries, p->cq_entries));\n\t\tif (ret) {\n\t\t\tfree_uid(user);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx) {\n\t\tif (limit_mem)\n\t\t\t__io_unaccount_mem(user, ring_pages(p->sq_entries,\n\t\t\t\t\t\t\t\tp->cq_entries));\n\t\tfree_uid(user);\n\t\treturn -ENOMEM;\n\t}\n\tctx->compat = in_compat_syscall();\n\tctx->user = user;\n\tctx->creds = get_current_cred();\n#ifdef CONFIG_AUDIT\n\tctx->loginuid = current->loginuid;\n\tctx->sessionid = current->sessionid;\n#endif\n\tctx->sqo_task = get_task_struct(current);\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n#ifdef CONFIG_BLK_CGROUP\n\t/*\n\t * The sq thread will belong to the original cgroup it was inited in.\n\t * If the cgroup goes offline (e.g. disabling the io controller), then\n\t * issued bios will be associated with the closest cgroup later in the\n\t * block layer.\n\t */\n\trcu_read_lock();\n\tctx->sqo_blkcg_css = blkcg_css();\n\tret = css_tryget_online(ctx->sqo_blkcg_css);\n\trcu_read_unlock();\n\tif (!ret) {\n\t\t/* don't init against a dying cgroup, have the user try again */\n\t\tctx->sqo_blkcg_css = NULL;\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n#endif\n\n\t/*\n\t * Account memory _before_ installing the file descriptor. Once\n\t * the descriptor is installed, it can get closed at any time. Also\n\t * do this before hitting the general error path, as ring freeing\n\t * will un-account as well.\n\t */\n\tio_account_mem(ctx, ring_pages(p->sq_entries, p->cq_entries),\n\t\t       ACCT_LOCKED);\n\tctx->limit_mem = limit_mem;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tif (!(p->flags & IORING_SETUP_R_DISABLED))\n\t\tio_sq_offload_start(ctx);\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_get_fd(ctx);\n\tif (ret < 0)\n\t\tgoto err;\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}\n\n/*\n * Sets up an aio uring context, and returns the fd. Applications asks for a\n * ring size, we return the actual sq/cq ring sizes (among other things) in the\n * params structure passed in.\n */\nstatic long io_uring_setup(u32 entries, struct io_uring_params __user *params)\n{\n\tstruct io_uring_params p;\n\tint i;\n\n\tif (copy_from_user(&p, params, sizeof(p)))\n\t\treturn -EFAULT;\n\tfor (i = 0; i < ARRAY_SIZE(p.resv); i++) {\n\t\tif (p.resv[i])\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |\n\t\t\tIORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |\n\t\t\tIORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |\n\t\t\tIORING_SETUP_R_DISABLED))\n\t\treturn -EINVAL;\n\n\treturn  io_uring_create(entries, &p, params);\n}\n\nSYSCALL_DEFINE2(io_uring_setup, u32, entries,\n\t\tstruct io_uring_params __user *, params)\n{\n\treturn io_uring_setup(entries, params);\n}\n\nstatic int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)\n{\n\tstruct io_uring_probe *p;\n\tsize_t size;\n\tint i, ret;\n\n\tsize = struct_size(p, ops, nr_args);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\tp = kzalloc(size, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tret = -EFAULT;\n\tif (copy_from_user(p, arg, size))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (memchr_inv(p, 0, size))\n\t\tgoto out;\n\n\tp->last_op = IORING_OP_LAST - 1;\n\tif (nr_args > IORING_OP_LAST)\n\t\tnr_args = IORING_OP_LAST;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tp->ops[i].op = i;\n\t\tif (!io_op_defs[i].not_supported)\n\t\t\tp->ops[i].flags = IO_URING_OP_SUPPORTED;\n\t}\n\tp->ops_len = i;\n\n\tret = 0;\n\tif (copy_to_user(arg, p, size))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tstruct io_identity *id;\n\tint ret;\n\n\tid = kmalloc(sizeof(*id), GFP_KERNEL);\n\tif (unlikely(!id))\n\t\treturn -ENOMEM;\n\n\tio_init_identity(id);\n\tid->creds = get_current_cred();\n\n\tret = idr_alloc_cyclic(&ctx->personality_idr, id, 1, USHRT_MAX, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_cred(id->creds);\n\t\tkfree(id);\n\t}\n\treturn ret;\n}\n\nstatic int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tstruct io_identity *iod;\n\n\tiod = idr_remove(&ctx->personality_idr, id);\n\tif (iod) {\n\t\tput_cred(iod->creds);\n\t\tif (refcount_dec_and_test(&iod->count))\n\t\t\tkfree(iod);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int io_register_restrictions(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t    unsigned int nr_args)\n{\n\tstruct io_uring_restriction *res;\n\tsize_t size;\n\tint i, ret;\n\n\t/* Restrictions allowed only if rings started disabled */\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\t/* We allow only a single restrictions registration */\n\tif (ctx->restrictions.registered)\n\t\treturn -EBUSY;\n\n\tif (!arg || nr_args > IORING_MAX_RESTRICTIONS)\n\t\treturn -EINVAL;\n\n\tsize = array_size(nr_args, sizeof(*res));\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\tres = memdup_user(arg, size);\n\tif (IS_ERR(res))\n\t\treturn PTR_ERR(res);\n\n\tret = 0;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tswitch (res[i].opcode) {\n\t\tcase IORING_RESTRICTION_REGISTER_OP:\n\t\t\tif (res[i].register_op >= IORING_REGISTER_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].register_op,\n\t\t\t\t  ctx->restrictions.register_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_OP:\n\t\t\tif (res[i].sqe_op >= IORING_OP_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].sqe_op, ctx->restrictions.sqe_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_ALLOWED:\n\t\t\tctx->restrictions.sqe_flags_allowed = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_REQUIRED:\n\t\t\tctx->restrictions.sqe_flags_required = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t/* Reset all restrictions if an error happened */\n\tif (ret != 0)\n\t\tmemset(&ctx->restrictions, 0, sizeof(ctx->restrictions));\n\telse\n\t\tctx->restrictions.registered = true;\n\n\tkfree(res);\n\treturn ret;\n}\n\nstatic int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\n\tio_sq_offload_start(ctx);\n\n\treturn 0;\n}\n\nstatic bool io_register_op_must_quiesce(int op)\n{\n\tswitch (op) {\n\tcase IORING_UNREGISTER_FILES:\n\tcase IORING_REGISTER_FILES_UPDATE:\n\tcase IORING_REGISTER_PROBE:\n\tcase IORING_REGISTER_PERSONALITY:\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n\t\t\t       void __user *arg, unsigned nr_args)\n\t__releases(ctx->uring_lock)\n\t__acquires(ctx->uring_lock)\n{\n\tint ret;\n\n\t/*\n\t * We're inside the ring mutex, if the ref is already dying, then\n\t * someone else killed the ctx or is already going through\n\t * io_uring_register().\n\t */\n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn -ENXIO;\n\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\tpercpu_ref_kill(&ctx->refs);\n\n\t\t/*\n\t\t * Drop uring mutex before waiting for references to exit. If\n\t\t * another thread is currently inside io_uring_enter() it might\n\t\t * need to grab the uring_lock to make progress. If we hold it\n\t\t * here across the drain wait, then we can deadlock. It's safe\n\t\t * to drop the mutex here, since no new references will come in\n\t\t * after we've killed the percpu ref.\n\t\t */\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tdo {\n\t\t\tret = wait_for_completion_interruptible(&ctx->ref_comp);\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t\tret = io_run_task_work_sig();\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tif (ret) {\n\t\t\tpercpu_ref_resurrect(&ctx->refs);\n\t\t\tgoto out_quiesce;\n\t\t}\n\t}\n\n\tif (ctx->restricted) {\n\t\tif (opcode >= IORING_REGISTER_LAST) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!test_bit(opcode, ctx->restrictions.register_op)) {\n\t\t\tret = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (opcode) {\n\tcase IORING_REGISTER_BUFFERS:\n\t\tret = io_sqe_buffer_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_BUFFERS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_buffer_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES:\n\t\tret = io_sqe_files_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_FILES:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_files_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES_UPDATE:\n\t\tret = io_sqe_files_update(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_EVENTFD:\n\tcase IORING_REGISTER_EVENTFD_ASYNC:\n\t\tret = -EINVAL;\n\t\tif (nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_eventfd_register(ctx, arg);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (opcode == IORING_REGISTER_EVENTFD_ASYNC)\n\t\t\tctx->eventfd_async = 1;\n\t\telse\n\t\t\tctx->eventfd_async = 0;\n\t\tbreak;\n\tcase IORING_UNREGISTER_EVENTFD:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_eventfd_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_PROBE:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args > 256)\n\t\t\tbreak;\n\t\tret = io_probe(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_personality(ctx);\n\t\tbreak;\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg)\n\t\t\tbreak;\n\t\tret = io_unregister_personality(ctx, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_ENABLE_RINGS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_enable_rings(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_RESTRICTIONS:\n\t\tret = io_register_restrictions(ctx, arg, nr_args);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\t/* bring the ctx back to life */\n\t\tpercpu_ref_reinit(&ctx->refs);\nout_quiesce:\n\t\treinit_completion(&ctx->ref_comp);\n\t}\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,\n\t\tvoid __user *, arg, unsigned int, nr_args)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tstruct fd f;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tctx = f.file->private_data;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_uring_register(ctx, opcode, arg, nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,\n\t\t\t\t\t\t\tctx->cq_ev_fd != NULL, ret);\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int __init io_uring_init(void)\n{\n#define __BUILD_BUG_VERIFY_ELEMENT(stype, eoffset, etype, ename) do { \\\n\tBUILD_BUG_ON(offsetof(stype, ename) != eoffset); \\\n\tBUILD_BUG_ON(sizeof(etype) != sizeof_field(stype, ename)); \\\n} while (0)\n\n#define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \\\n\t__BUILD_BUG_VERIFY_ELEMENT(struct io_uring_sqe, eoffset, etype, ename)\n\tBUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);\n\tBUILD_BUG_SQE_ELEM(0,  __u8,   opcode);\n\tBUILD_BUG_SQE_ELEM(1,  __u8,   flags);\n\tBUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);\n\tBUILD_BUG_SQE_ELEM(4,  __s32,  fd);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  off);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  addr2);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  addr);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);\n\tBUILD_BUG_SQE_ELEM(24, __u32,  len);\n\tBUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */   int, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u32, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u16,  poll_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  poll32_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  open_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);\n\tBUILD_BUG_SQE_ELEM(32, __u64,  user_data);\n\tBUILD_BUG_SQE_ELEM(40, __u16,  buf_index);\n\tBUILD_BUG_SQE_ELEM(42, __u16,  personality);\n\tBUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);\n\n\tBUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);\n\tBUILD_BUG_ON(__REQ_F_LAST_BIT >= 8 * sizeof(int));\n\treq_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);\n\treturn 0;\n};\n__initcall(io_uring_init);\n"}, "1": {"id": 1, "path": "/src/include/linux/syscalls.h", "content": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n * syscalls.h - Linux syscall interfaces (non-arch-specific)\n *\n * Copyright (c) 2004 Randy Dunlap\n * Copyright (c) 2004 Open Source Development Labs\n */\n\n#ifndef _LINUX_SYSCALLS_H\n#define _LINUX_SYSCALLS_H\n\nstruct __aio_sigset;\nstruct epoll_event;\nstruct iattr;\nstruct inode;\nstruct iocb;\nstruct io_event;\nstruct iovec;\nstruct __kernel_old_itimerval;\nstruct kexec_segment;\nstruct linux_dirent;\nstruct linux_dirent64;\nstruct list_head;\nstruct mmap_arg_struct;\nstruct msgbuf;\nstruct user_msghdr;\nstruct mmsghdr;\nstruct msqid_ds;\nstruct new_utsname;\nstruct nfsctl_arg;\nstruct __old_kernel_stat;\nstruct oldold_utsname;\nstruct old_utsname;\nstruct pollfd;\nstruct rlimit;\nstruct rlimit64;\nstruct rusage;\nstruct sched_param;\nstruct sched_attr;\nstruct sel_arg_struct;\nstruct semaphore;\nstruct sembuf;\nstruct shmid_ds;\nstruct sockaddr;\nstruct stat;\nstruct stat64;\nstruct statfs;\nstruct statfs64;\nstruct statx;\nstruct sysinfo;\nstruct timespec;\nstruct __kernel_old_timeval;\nstruct __kernel_timex;\nstruct timezone;\nstruct tms;\nstruct utimbuf;\nstruct mq_attr;\nstruct compat_stat;\nstruct old_timeval32;\nstruct robust_list_head;\nstruct getcpu_cache;\nstruct old_linux_dirent;\nstruct perf_event_attr;\nstruct file_handle;\nstruct sigaltstack;\nstruct rseq;\nunion bpf_attr;\nstruct io_uring_params;\nstruct clone_args;\nstruct open_how;\n\n#include <linux/types.h>\n#include <linux/aio_abi.h>\n#include <linux/capability.h>\n#include <linux/signal.h>\n#include <linux/list.h>\n#include <linux/bug.h>\n#include <linux/sem.h>\n#include <asm/siginfo.h>\n#include <linux/unistd.h>\n#include <linux/quota.h>\n#include <linux/key.h>\n#include <linux/personality.h>\n#include <trace/syscall.h>\n\n#ifdef CONFIG_ARCH_HAS_SYSCALL_WRAPPER\n/*\n * It may be useful for an architecture to override the definitions of the\n * SYSCALL_DEFINE0() and __SYSCALL_DEFINEx() macros, in particular to use a\n * different calling convention for syscalls. To allow for that, the prototypes\n * for the sys_*() functions below will *not* be included if\n * CONFIG_ARCH_HAS_SYSCALL_WRAPPER is enabled.\n */\n#include <asm/syscall_wrapper.h>\n#endif /* CONFIG_ARCH_HAS_SYSCALL_WRAPPER */\n\n/*\n * __MAP - apply a macro to syscall arguments\n * __MAP(n, m, t1, a1, t2, a2, ..., tn, an) will expand to\n *    m(t1, a1), m(t2, a2), ..., m(tn, an)\n * The first argument must be equal to the amount of type/name\n * pairs given.  Note that this list of pairs (i.e. the arguments\n * of __MAP starting at the third one) is in the same format as\n * for SYSCALL_DEFINE<n>/COMPAT_SYSCALL_DEFINE<n>\n */\n#define __MAP0(m,...)\n#define __MAP1(m,t,a,...) m(t,a)\n#define __MAP2(m,t,a,...) m(t,a), __MAP1(m,__VA_ARGS__)\n#define __MAP3(m,t,a,...) m(t,a), __MAP2(m,__VA_ARGS__)\n#define __MAP4(m,t,a,...) m(t,a), __MAP3(m,__VA_ARGS__)\n#define __MAP5(m,t,a,...) m(t,a), __MAP4(m,__VA_ARGS__)\n#define __MAP6(m,t,a,...) m(t,a), __MAP5(m,__VA_ARGS__)\n#define __MAP(n,...) __MAP##n(__VA_ARGS__)\n\n#define __SC_DECL(t, a)\tt a\n#define __TYPE_AS(t, v)\t__same_type((__force t)0, v)\n#define __TYPE_IS_L(t)\t(__TYPE_AS(t, 0L))\n#define __TYPE_IS_UL(t)\t(__TYPE_AS(t, 0UL))\n#define __TYPE_IS_LL(t) (__TYPE_AS(t, 0LL) || __TYPE_AS(t, 0ULL))\n#define __SC_LONG(t, a) __typeof(__builtin_choose_expr(__TYPE_IS_LL(t), 0LL, 0L)) a\n#define __SC_CAST(t, a)\t(__force t) a\n#define __SC_ARGS(t, a)\ta\n#define __SC_TEST(t, a) (void)BUILD_BUG_ON_ZERO(!__TYPE_IS_LL(t) && sizeof(t) > sizeof(long))\n\n#ifdef CONFIG_FTRACE_SYSCALLS\n#define __SC_STR_ADECL(t, a)\t#a\n#define __SC_STR_TDECL(t, a)\t#t\n\nextern struct trace_event_class event_class_syscall_enter;\nextern struct trace_event_class event_class_syscall_exit;\nextern struct trace_event_functions enter_syscall_print_funcs;\nextern struct trace_event_functions exit_syscall_print_funcs;\n\n#define SYSCALL_TRACE_ENTER_EVENT(sname)\t\t\t\t\\\n\tstatic struct syscall_metadata __syscall_meta_##sname;\t\t\\\n\tstatic struct trace_event_call __used\t\t\t\t\\\n\t  event_enter_##sname = {\t\t\t\t\t\\\n\t\t.class\t\t\t= &event_class_syscall_enter,\t\\\n\t\t{\t\t\t\t\t\t\t\\\n\t\t\t.name                   = \"sys_enter\"#sname,\t\\\n\t\t},\t\t\t\t\t\t\t\\\n\t\t.event.funcs            = &enter_syscall_print_funcs,\t\\\n\t\t.data\t\t\t= (void *)&__syscall_meta_##sname,\\\n\t\t.flags                  = TRACE_EVENT_FL_CAP_ANY,\t\\\n\t};\t\t\t\t\t\t\t\t\\\n\tstatic struct trace_event_call __used\t\t\t\t\\\n\t  __section(\"_ftrace_events\")\t\t\t\t\t\\\n\t *__event_enter_##sname = &event_enter_##sname;\n\n#define SYSCALL_TRACE_EXIT_EVENT(sname)\t\t\t\t\t\\\n\tstatic struct syscall_metadata __syscall_meta_##sname;\t\t\\\n\tstatic struct trace_event_call __used\t\t\t\t\\\n\t  event_exit_##sname = {\t\t\t\t\t\\\n\t\t.class\t\t\t= &event_class_syscall_exit,\t\\\n\t\t{\t\t\t\t\t\t\t\\\n\t\t\t.name                   = \"sys_exit\"#sname,\t\\\n\t\t},\t\t\t\t\t\t\t\\\n\t\t.event.funcs\t\t= &exit_syscall_print_funcs,\t\\\n\t\t.data\t\t\t= (void *)&__syscall_meta_##sname,\\\n\t\t.flags                  = TRACE_EVENT_FL_CAP_ANY,\t\\\n\t};\t\t\t\t\t\t\t\t\\\n\tstatic struct trace_event_call __used\t\t\t\t\\\n\t  __section(\"_ftrace_events\")\t\t\t\t\t\\\n\t*__event_exit_##sname = &event_exit_##sname;\n\n#define SYSCALL_METADATA(sname, nb, ...)\t\t\t\\\n\tstatic const char *types_##sname[] = {\t\t\t\\\n\t\t__MAP(nb,__SC_STR_TDECL,__VA_ARGS__)\t\t\\\n\t};\t\t\t\t\t\t\t\\\n\tstatic const char *args_##sname[] = {\t\t\t\\\n\t\t__MAP(nb,__SC_STR_ADECL,__VA_ARGS__)\t\t\\\n\t};\t\t\t\t\t\t\t\\\n\tSYSCALL_TRACE_ENTER_EVENT(sname);\t\t\t\\\n\tSYSCALL_TRACE_EXIT_EVENT(sname);\t\t\t\\\n\tstatic struct syscall_metadata __used\t\t\t\\\n\t  __syscall_meta_##sname = {\t\t\t\t\\\n\t\t.name \t\t= \"sys\"#sname,\t\t\t\\\n\t\t.syscall_nr\t= -1,\t/* Filled in at boot */\t\\\n\t\t.nb_args \t= nb,\t\t\t\t\\\n\t\t.types\t\t= nb ? types_##sname : NULL,\t\\\n\t\t.args\t\t= nb ? args_##sname : NULL,\t\\\n\t\t.enter_event\t= &event_enter_##sname,\t\t\\\n\t\t.exit_event\t= &event_exit_##sname,\t\t\\\n\t\t.enter_fields\t= LIST_HEAD_INIT(__syscall_meta_##sname.enter_fields), \\\n\t};\t\t\t\t\t\t\t\\\n\tstatic struct syscall_metadata __used\t\t\t\\\n\t  __section(\"__syscalls_metadata\")\t\t\t\\\n\t *__p_syscall_meta_##sname = &__syscall_meta_##sname;\n\nstatic inline int is_syscall_trace_event(struct trace_event_call *tp_event)\n{\n\treturn tp_event->class == &event_class_syscall_enter ||\n\t       tp_event->class == &event_class_syscall_exit;\n}\n\n#else\n#define SYSCALL_METADATA(sname, nb, ...)\n\nstatic inline int is_syscall_trace_event(struct trace_event_call *tp_event)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef SYSCALL_DEFINE0\n#define SYSCALL_DEFINE0(sname)\t\t\t\t\t\\\n\tSYSCALL_METADATA(_##sname, 0);\t\t\t\t\\\n\tasmlinkage long sys_##sname(void);\t\t\t\\\n\tALLOW_ERROR_INJECTION(sys_##sname, ERRNO);\t\t\\\n\tasmlinkage long sys_##sname(void)\n#endif /* SYSCALL_DEFINE0 */\n\n#define SYSCALL_DEFINE1(name, ...) SYSCALL_DEFINEx(1, _##name, __VA_ARGS__)\n#define SYSCALL_DEFINE2(name, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)\n#define SYSCALL_DEFINE3(name, ...) SYSCALL_DEFINEx(3, _##name, __VA_ARGS__)\n#define SYSCALL_DEFINE4(name, ...) SYSCALL_DEFINEx(4, _##name, __VA_ARGS__)\n#define SYSCALL_DEFINE5(name, ...) SYSCALL_DEFINEx(5, _##name, __VA_ARGS__)\n#define SYSCALL_DEFINE6(name, ...) SYSCALL_DEFINEx(6, _##name, __VA_ARGS__)\n\n#define SYSCALL_DEFINE_MAXARGS\t6\n\n#define SYSCALL_DEFINEx(x, sname, ...)\t\t\t\t\\\n\tSYSCALL_METADATA(sname, x, __VA_ARGS__)\t\t\t\\\n\t__SYSCALL_DEFINEx(x, sname, __VA_ARGS__)\n\n#define __PROTECT(...) asmlinkage_protect(__VA_ARGS__)\n\n/*\n * The asmlinkage stub is aliased to a function named __se_sys_*() which\n * sign-extends 32-bit ints to longs whenever needed. The actual work is\n * done within __do_sys_*().\n */\n#ifndef __SYSCALL_DEFINEx\n#define __SYSCALL_DEFINEx(x, name, ...)\t\t\t\t\t\\\n\t__diag_push();\t\t\t\t\t\t\t\\\n\t__diag_ignore(GCC, 8, \"-Wattribute-alias\",\t\t\t\\\n\t\t      \"Type aliasing is used to sanitize syscall arguments\");\\\n\tasmlinkage long sys##name(__MAP(x,__SC_DECL,__VA_ARGS__))\t\\\n\t\t__attribute__((alias(__stringify(__se_sys##name))));\t\\\n\tALLOW_ERROR_INJECTION(sys##name, ERRNO);\t\t\t\\\n\tstatic inline long __do_sys##name(__MAP(x,__SC_DECL,__VA_ARGS__));\\\n\tasmlinkage long __se_sys##name(__MAP(x,__SC_LONG,__VA_ARGS__));\t\\\n\tasmlinkage long __se_sys##name(__MAP(x,__SC_LONG,__VA_ARGS__))\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tlong ret = __do_sys##name(__MAP(x,__SC_CAST,__VA_ARGS__));\\\n\t\t__MAP(x,__SC_TEST,__VA_ARGS__);\t\t\t\t\\\n\t\t__PROTECT(x, ret,__MAP(x,__SC_ARGS,__VA_ARGS__));\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__diag_pop();\t\t\t\t\t\t\t\\\n\tstatic inline long __do_sys##name(__MAP(x,__SC_DECL,__VA_ARGS__))\n#endif /* __SYSCALL_DEFINEx */\n\n/*\n * Called before coming back to user-mode. Returning to user-mode with an\n * address limit different than USER_DS can allow to overwrite kernel memory.\n */\nstatic inline void addr_limit_user_check(void)\n{\n#ifdef TIF_FSCHECK\n\tif (!test_thread_flag(TIF_FSCHECK))\n\t\treturn;\n#endif\n\n\tif (CHECK_DATA_CORRUPTION(uaccess_kernel(),\n\t\t\t\t  \"Invalid address limit on user-mode return\"))\n\t\tforce_sig(SIGKILL);\n\n#ifdef TIF_FSCHECK\n\tclear_thread_flag(TIF_FSCHECK);\n#endif\n}\n\n/*\n * These syscall function prototypes are kept in the same order as\n * include/uapi/asm-generic/unistd.h. Architecture specific entries go below,\n * followed by deprecated or obsolete system calls.\n *\n * Please note that these prototypes here are only provided for information\n * purposes, for static analysis, and for linking from the syscall table.\n * These functions should not be called elsewhere from kernel code.\n *\n * As the syscall calling convention may be different from the default\n * for architectures overriding the syscall calling convention, do not\n * include the prototypes if CONFIG_ARCH_HAS_SYSCALL_WRAPPER is enabled.\n */\n#ifndef CONFIG_ARCH_HAS_SYSCALL_WRAPPER\nasmlinkage long sys_io_setup(unsigned nr_reqs, aio_context_t __user *ctx);\nasmlinkage long sys_io_destroy(aio_context_t ctx);\nasmlinkage long sys_io_submit(aio_context_t, long,\n\t\t\tstruct iocb __user * __user *);\nasmlinkage long sys_io_cancel(aio_context_t ctx_id, struct iocb __user *iocb,\n\t\t\t      struct io_event __user *result);\nasmlinkage long sys_io_getevents(aio_context_t ctx_id,\n\t\t\t\tlong min_nr,\n\t\t\t\tlong nr,\n\t\t\t\tstruct io_event __user *events,\n\t\t\t\tstruct __kernel_timespec __user *timeout);\nasmlinkage long sys_io_getevents_time32(__u32 ctx_id,\n\t\t\t\t__s32 min_nr,\n\t\t\t\t__s32 nr,\n\t\t\t\tstruct io_event __user *events,\n\t\t\t\tstruct old_timespec32 __user *timeout);\nasmlinkage long sys_io_pgetevents(aio_context_t ctx_id,\n\t\t\t\tlong min_nr,\n\t\t\t\tlong nr,\n\t\t\t\tstruct io_event __user *events,\n\t\t\t\tstruct __kernel_timespec __user *timeout,\n\t\t\t\tconst struct __aio_sigset *sig);\nasmlinkage long sys_io_pgetevents_time32(aio_context_t ctx_id,\n\t\t\t\tlong min_nr,\n\t\t\t\tlong nr,\n\t\t\t\tstruct io_event __user *events,\n\t\t\t\tstruct old_timespec32 __user *timeout,\n\t\t\t\tconst struct __aio_sigset *sig);\nasmlinkage long sys_io_uring_setup(u32 entries,\n\t\t\t\tstruct io_uring_params __user *p);\nasmlinkage long sys_io_uring_enter(unsigned int fd, u32 to_submit,\n\t\t\t\tu32 min_complete, u32 flags,\n\t\t\t\tconst void __user *argp, size_t argsz);\nasmlinkage long sys_io_uring_register(unsigned int fd, unsigned int op,\n\t\t\t\tvoid __user *arg, unsigned int nr_args);\n\n/* fs/xattr.c */\nasmlinkage long sys_setxattr(const char __user *path, const char __user *name,\n\t\t\t     const void __user *value, size_t size, int flags);\nasmlinkage long sys_lsetxattr(const char __user *path, const char __user *name,\n\t\t\t      const void __user *value, size_t size, int flags);\nasmlinkage long sys_fsetxattr(int fd, const char __user *name,\n\t\t\t      const void __user *value, size_t size, int flags);\nasmlinkage long sys_getxattr(const char __user *path, const char __user *name,\n\t\t\t     void __user *value, size_t size);\nasmlinkage long sys_lgetxattr(const char __user *path, const char __user *name,\n\t\t\t      void __user *value, size_t size);\nasmlinkage long sys_fgetxattr(int fd, const char __user *name,\n\t\t\t      void __user *value, size_t size);\nasmlinkage long sys_listxattr(const char __user *path, char __user *list,\n\t\t\t      size_t size);\nasmlinkage long sys_llistxattr(const char __user *path, char __user *list,\n\t\t\t       size_t size);\nasmlinkage long sys_flistxattr(int fd, char __user *list, size_t size);\nasmlinkage long sys_removexattr(const char __user *path,\n\t\t\t\tconst char __user *name);\nasmlinkage long sys_lremovexattr(const char __user *path,\n\t\t\t\t const char __user *name);\nasmlinkage long sys_fremovexattr(int fd, const char __user *name);\n\n/* fs/dcache.c */\nasmlinkage long sys_getcwd(char __user *buf, unsigned long size);\n\n/* fs/cookies.c */\nasmlinkage long sys_lookup_dcookie(u64 cookie64, char __user *buf, size_t len);\n\n/* fs/eventfd.c */\nasmlinkage long sys_eventfd2(unsigned int count, int flags);\n\n/* fs/eventpoll.c */\nasmlinkage long sys_epoll_create1(int flags);\nasmlinkage long sys_epoll_ctl(int epfd, int op, int fd,\n\t\t\t\tstruct epoll_event __user *event);\nasmlinkage long sys_epoll_pwait(int epfd, struct epoll_event __user *events,\n\t\t\t\tint maxevents, int timeout,\n\t\t\t\tconst sigset_t __user *sigmask,\n\t\t\t\tsize_t sigsetsize);\n\n/* fs/fcntl.c */\nasmlinkage long sys_dup(unsigned int fildes);\nasmlinkage long sys_dup3(unsigned int oldfd, unsigned int newfd, int flags);\nasmlinkage long sys_fcntl(unsigned int fd, unsigned int cmd, unsigned long arg);\n#if BITS_PER_LONG == 32\nasmlinkage long sys_fcntl64(unsigned int fd,\n\t\t\t\tunsigned int cmd, unsigned long arg);\n#endif\n\n/* fs/inotify_user.c */\nasmlinkage long sys_inotify_init1(int flags);\nasmlinkage long sys_inotify_add_watch(int fd, const char __user *path,\n\t\t\t\t\tu32 mask);\nasmlinkage long sys_inotify_rm_watch(int fd, __s32 wd);\n\n/* fs/ioctl.c */\nasmlinkage long sys_ioctl(unsigned int fd, unsigned int cmd,\n\t\t\t\tunsigned long arg);\n\n/* fs/ioprio.c */\nasmlinkage long sys_ioprio_set(int which, int who, int ioprio);\nasmlinkage long sys_ioprio_get(int which, int who);\n\n/* fs/locks.c */\nasmlinkage long sys_flock(unsigned int fd, unsigned int cmd);\n\n/* fs/namei.c */\nasmlinkage long sys_mknodat(int dfd, const char __user * filename, umode_t mode,\n\t\t\t    unsigned dev);\nasmlinkage long sys_mkdirat(int dfd, const char __user * pathname, umode_t mode);\nasmlinkage long sys_unlinkat(int dfd, const char __user * pathname, int flag);\nasmlinkage long sys_symlinkat(const char __user * oldname,\n\t\t\t      int newdfd, const char __user * newname);\nasmlinkage long sys_linkat(int olddfd, const char __user *oldname,\n\t\t\t   int newdfd, const char __user *newname, int flags);\nasmlinkage long sys_renameat(int olddfd, const char __user * oldname,\n\t\t\t     int newdfd, const char __user * newname);\n\n/* fs/namespace.c */\nasmlinkage long sys_umount(char __user *name, int flags);\nasmlinkage long sys_mount(char __user *dev_name, char __user *dir_name,\n\t\t\t\tchar __user *type, unsigned long flags,\n\t\t\t\tvoid __user *data);\nasmlinkage long sys_pivot_root(const char __user *new_root,\n\t\t\t\tconst char __user *put_old);\n\n/* fs/nfsctl.c */\n\n/* fs/open.c */\nasmlinkage long sys_statfs(const char __user * path,\n\t\t\t\tstruct statfs __user *buf);\nasmlinkage long sys_statfs64(const char __user *path, size_t sz,\n\t\t\t\tstruct statfs64 __user *buf);\nasmlinkage long sys_fstatfs(unsigned int fd, struct statfs __user *buf);\nasmlinkage long sys_fstatfs64(unsigned int fd, size_t sz,\n\t\t\t\tstruct statfs64 __user *buf);\nasmlinkage long sys_truncate(const char __user *path, long length);\nasmlinkage long sys_ftruncate(unsigned int fd, unsigned long length);\n#if BITS_PER_LONG == 32\nasmlinkage long sys_truncate64(const char __user *path, loff_t length);\nasmlinkage long sys_ftruncate64(unsigned int fd, loff_t length);\n#endif\nasmlinkage long sys_fallocate(int fd, int mode, loff_t offset, loff_t len);\nasmlinkage long sys_faccessat(int dfd, const char __user *filename, int mode);\nasmlinkage long sys_faccessat2(int dfd, const char __user *filename, int mode,\n\t\t\t       int flags);\nasmlinkage long sys_chdir(const char __user *filename);\nasmlinkage long sys_fchdir(unsigned int fd);\nasmlinkage long sys_chroot(const char __user *filename);\nasmlinkage long sys_fchmod(unsigned int fd, umode_t mode);\nasmlinkage long sys_fchmodat(int dfd, const char __user * filename,\n\t\t\t     umode_t mode);\nasmlinkage long sys_fchownat(int dfd, const char __user *filename, uid_t user,\n\t\t\t     gid_t group, int flag);\nasmlinkage long sys_fchown(unsigned int fd, uid_t user, gid_t group);\nasmlinkage long sys_openat(int dfd, const char __user *filename, int flags,\n\t\t\t   umode_t mode);\nasmlinkage long sys_openat2(int dfd, const char __user *filename,\n\t\t\t    struct open_how *how, size_t size);\nasmlinkage long sys_close(unsigned int fd);\nasmlinkage long sys_close_range(unsigned int fd, unsigned int max_fd,\n\t\t\t\tunsigned int flags);\nasmlinkage long sys_vhangup(void);\n\n/* fs/pipe.c */\nasmlinkage long sys_pipe2(int __user *fildes, int flags);\n\n/* fs/quota.c */\nasmlinkage long sys_quotactl(unsigned int cmd, const char __user *special,\n\t\t\t\tqid_t id, void __user *addr);\n\n/* fs/readdir.c */\nasmlinkage long sys_getdents64(unsigned int fd,\n\t\t\t\tstruct linux_dirent64 __user *dirent,\n\t\t\t\tunsigned int count);\n\n/* fs/read_write.c */\nasmlinkage long sys_llseek(unsigned int fd, unsigned long offset_high,\n\t\t\tunsigned long offset_low, loff_t __user *result,\n\t\t\tunsigned int whence);\nasmlinkage long sys_lseek(unsigned int fd, off_t offset,\n\t\t\t  unsigned int whence);\nasmlinkage long sys_read(unsigned int fd, char __user *buf, size_t count);\nasmlinkage long sys_write(unsigned int fd, const char __user *buf,\n\t\t\t  size_t count);\nasmlinkage long sys_readv(unsigned long fd,\n\t\t\t  const struct iovec __user *vec,\n\t\t\t  unsigned long vlen);\nasmlinkage long sys_writev(unsigned long fd,\n\t\t\t   const struct iovec __user *vec,\n\t\t\t   unsigned long vlen);\nasmlinkage long sys_pread64(unsigned int fd, char __user *buf,\n\t\t\t    size_t count, loff_t pos);\nasmlinkage long sys_pwrite64(unsigned int fd, const char __user *buf,\n\t\t\t     size_t count, loff_t pos);\nasmlinkage long sys_preadv(unsigned long fd, const struct iovec __user *vec,\n\t\t\t   unsigned long vlen, unsigned long pos_l, unsigned long pos_h);\nasmlinkage long sys_pwritev(unsigned long fd, const struct iovec __user *vec,\n\t\t\t    unsigned long vlen, unsigned long pos_l, unsigned long pos_h);\n\n/* fs/sendfile.c */\nasmlinkage long sys_sendfile64(int out_fd, int in_fd,\n\t\t\t       loff_t __user *offset, size_t count);\n\n/* fs/select.c */\nasmlinkage long sys_pselect6(int, fd_set __user *, fd_set __user *,\n\t\t\t     fd_set __user *, struct __kernel_timespec __user *,\n\t\t\t     void __user *);\nasmlinkage long sys_pselect6_time32(int, fd_set __user *, fd_set __user *,\n\t\t\t     fd_set __user *, struct old_timespec32 __user *,\n\t\t\t     void __user *);\nasmlinkage long sys_ppoll(struct pollfd __user *, unsigned int,\n\t\t\t  struct __kernel_timespec __user *, const sigset_t __user *,\n\t\t\t  size_t);\nasmlinkage long sys_ppoll_time32(struct pollfd __user *, unsigned int,\n\t\t\t  struct old_timespec32 __user *, const sigset_t __user *,\n\t\t\t  size_t);\n\n/* fs/signalfd.c */\nasmlinkage long sys_signalfd4(int ufd, sigset_t __user *user_mask, size_t sizemask, int flags);\n\n/* fs/splice.c */\nasmlinkage long sys_vmsplice(int fd, const struct iovec __user *iov,\n\t\t\t     unsigned long nr_segs, unsigned int flags);\nasmlinkage long sys_splice(int fd_in, loff_t __user *off_in,\n\t\t\t   int fd_out, loff_t __user *off_out,\n\t\t\t   size_t len, unsigned int flags);\nasmlinkage long sys_tee(int fdin, int fdout, size_t len, unsigned int flags);\n\n/* fs/stat.c */\nasmlinkage long sys_readlinkat(int dfd, const char __user *path, char __user *buf,\n\t\t\t       int bufsiz);\nasmlinkage long sys_newfstatat(int dfd, const char __user *filename,\n\t\t\t       struct stat __user *statbuf, int flag);\nasmlinkage long sys_newfstat(unsigned int fd, struct stat __user *statbuf);\n#if defined(__ARCH_WANT_STAT64) || defined(__ARCH_WANT_COMPAT_STAT64)\nasmlinkage long sys_fstat64(unsigned long fd, struct stat64 __user *statbuf);\nasmlinkage long sys_fstatat64(int dfd, const char __user *filename,\n\t\t\t       struct stat64 __user *statbuf, int flag);\n#endif\n\n/* fs/sync.c */\nasmlinkage long sys_sync(void);\nasmlinkage long sys_fsync(unsigned int fd);\nasmlinkage long sys_fdatasync(unsigned int fd);\nasmlinkage long sys_sync_file_range2(int fd, unsigned int flags,\n\t\t\t\t     loff_t offset, loff_t nbytes);\nasmlinkage long sys_sync_file_range(int fd, loff_t offset, loff_t nbytes,\n\t\t\t\t\tunsigned int flags);\n\n/* fs/timerfd.c */\nasmlinkage long sys_timerfd_create(int clockid, int flags);\nasmlinkage long sys_timerfd_settime(int ufd, int flags,\n\t\t\t\t    const struct __kernel_itimerspec __user *utmr,\n\t\t\t\t    struct __kernel_itimerspec __user *otmr);\nasmlinkage long sys_timerfd_gettime(int ufd, struct __kernel_itimerspec __user *otmr);\nasmlinkage long sys_timerfd_gettime32(int ufd,\n\t\t\t\t   struct old_itimerspec32 __user *otmr);\nasmlinkage long sys_timerfd_settime32(int ufd, int flags,\n\t\t\t\t   const struct old_itimerspec32 __user *utmr,\n\t\t\t\t   struct old_itimerspec32 __user *otmr);\n\n/* fs/utimes.c */\nasmlinkage long sys_utimensat(int dfd, const char __user *filename,\n\t\t\t\tstruct __kernel_timespec __user *utimes,\n\t\t\t\tint flags);\nasmlinkage long sys_utimensat_time32(unsigned int dfd,\n\t\t\t\tconst char __user *filename,\n\t\t\t\tstruct old_timespec32 __user *t, int flags);\n\n/* kernel/acct.c */\nasmlinkage long sys_acct(const char __user *name);\n\n/* kernel/capability.c */\nasmlinkage long sys_capget(cap_user_header_t header,\n\t\t\t\tcap_user_data_t dataptr);\nasmlinkage long sys_capset(cap_user_header_t header,\n\t\t\t\tconst cap_user_data_t data);\n\n/* kernel/exec_domain.c */\nasmlinkage long sys_personality(unsigned int personality);\n\n/* kernel/exit.c */\nasmlinkage long sys_exit(int error_code);\nasmlinkage long sys_exit_group(int error_code);\nasmlinkage long sys_waitid(int which, pid_t pid,\n\t\t\t   struct siginfo __user *infop,\n\t\t\t   int options, struct rusage __user *ru);\n\n/* kernel/fork.c */\nasmlinkage long sys_set_tid_address(int __user *tidptr);\nasmlinkage long sys_unshare(unsigned long unshare_flags);\n\n/* kernel/futex.c */\nasmlinkage long sys_futex(u32 __user *uaddr, int op, u32 val,\n\t\t\tstruct __kernel_timespec __user *utime, u32 __user *uaddr2,\n\t\t\tu32 val3);\nasmlinkage long sys_futex_time32(u32 __user *uaddr, int op, u32 val,\n\t\t\tstruct old_timespec32 __user *utime, u32 __user *uaddr2,\n\t\t\tu32 val3);\nasmlinkage long sys_get_robust_list(int pid,\n\t\t\t\t    struct robust_list_head __user * __user *head_ptr,\n\t\t\t\t    size_t __user *len_ptr);\nasmlinkage long sys_set_robust_list(struct robust_list_head __user *head,\n\t\t\t\t    size_t len);\n\n/* kernel/hrtimer.c */\nasmlinkage long sys_nanosleep(struct __kernel_timespec __user *rqtp,\n\t\t\t      struct __kernel_timespec __user *rmtp);\nasmlinkage long sys_nanosleep_time32(struct old_timespec32 __user *rqtp,\n\t\t\t\t     struct old_timespec32 __user *rmtp);\n\n/* kernel/itimer.c */\nasmlinkage long sys_getitimer(int which, struct __kernel_old_itimerval __user *value);\nasmlinkage long sys_setitimer(int which,\n\t\t\t\tstruct __kernel_old_itimerval __user *value,\n\t\t\t\tstruct __kernel_old_itimerval __user *ovalue);\n\n/* kernel/kexec.c */\nasmlinkage long sys_kexec_load(unsigned long entry, unsigned long nr_segments,\n\t\t\t\tstruct kexec_segment __user *segments,\n\t\t\t\tunsigned long flags);\n\n/* kernel/module.c */\nasmlinkage long sys_init_module(void __user *umod, unsigned long len,\n\t\t\t\tconst char __user *uargs);\nasmlinkage long sys_delete_module(const char __user *name_user,\n\t\t\t\tunsigned int flags);\n\n/* kernel/posix-timers.c */\nasmlinkage long sys_timer_create(clockid_t which_clock,\n\t\t\t\t struct sigevent __user *timer_event_spec,\n\t\t\t\t timer_t __user * created_timer_id);\nasmlinkage long sys_timer_gettime(timer_t timer_id,\n\t\t\t\tstruct __kernel_itimerspec __user *setting);\nasmlinkage long sys_timer_getoverrun(timer_t timer_id);\nasmlinkage long sys_timer_settime(timer_t timer_id, int flags,\n\t\t\t\tconst struct __kernel_itimerspec __user *new_setting,\n\t\t\t\tstruct __kernel_itimerspec __user *old_setting);\nasmlinkage long sys_timer_delete(timer_t timer_id);\nasmlinkage long sys_clock_settime(clockid_t which_clock,\n\t\t\t\tconst struct __kernel_timespec __user *tp);\nasmlinkage long sys_clock_gettime(clockid_t which_clock,\n\t\t\t\tstruct __kernel_timespec __user *tp);\nasmlinkage long sys_clock_getres(clockid_t which_clock,\n\t\t\t\tstruct __kernel_timespec __user *tp);\nasmlinkage long sys_clock_nanosleep(clockid_t which_clock, int flags,\n\t\t\t\tconst struct __kernel_timespec __user *rqtp,\n\t\t\t\tstruct __kernel_timespec __user *rmtp);\nasmlinkage long sys_timer_gettime32(timer_t timer_id,\n\t\t\t\t struct old_itimerspec32 __user *setting);\nasmlinkage long sys_timer_settime32(timer_t timer_id, int flags,\n\t\t\t\t\t struct old_itimerspec32 __user *new,\n\t\t\t\t\t struct old_itimerspec32 __user *old);\nasmlinkage long sys_clock_settime32(clockid_t which_clock,\n\t\t\t\tstruct old_timespec32 __user *tp);\nasmlinkage long sys_clock_gettime32(clockid_t which_clock,\n\t\t\t\tstruct old_timespec32 __user *tp);\nasmlinkage long sys_clock_getres_time32(clockid_t which_clock,\n\t\t\t\tstruct old_timespec32 __user *tp);\nasmlinkage long sys_clock_nanosleep_time32(clockid_t which_clock, int flags,\n\t\t\t\tstruct old_timespec32 __user *rqtp,\n\t\t\t\tstruct old_timespec32 __user *rmtp);\n\n/* kernel/printk.c */\nasmlinkage long sys_syslog(int type, char __user *buf, int len);\n\n/* kernel/ptrace.c */\nasmlinkage long sys_ptrace(long request, long pid, unsigned long addr,\n\t\t\t   unsigned long data);\n/* kernel/sched/core.c */\n\nasmlinkage long sys_sched_setparam(pid_t pid,\n\t\t\t\t\tstruct sched_param __user *param);\nasmlinkage long sys_sched_setscheduler(pid_t pid, int policy,\n\t\t\t\t\tstruct sched_param __user *param);\nasmlinkage long sys_sched_getscheduler(pid_t pid);\nasmlinkage long sys_sched_getparam(pid_t pid,\n\t\t\t\t\tstruct sched_param __user *param);\nasmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,\n\t\t\t\t\tunsigned long __user *user_mask_ptr);\nasmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,\n\t\t\t\t\tunsigned long __user *user_mask_ptr);\nasmlinkage long sys_sched_yield(void);\nasmlinkage long sys_sched_get_priority_max(int policy);\nasmlinkage long sys_sched_get_priority_min(int policy);\nasmlinkage long sys_sched_rr_get_interval(pid_t pid,\n\t\t\t\tstruct __kernel_timespec __user *interval);\nasmlinkage long sys_sched_rr_get_interval_time32(pid_t pid,\n\t\t\t\t\t\t struct old_timespec32 __user *interval);\n\n/* kernel/signal.c */\nasmlinkage long sys_restart_syscall(void);\nasmlinkage long sys_kill(pid_t pid, int sig);\nasmlinkage long sys_tkill(pid_t pid, int sig);\nasmlinkage long sys_tgkill(pid_t tgid, pid_t pid, int sig);\nasmlinkage long sys_sigaltstack(const struct sigaltstack __user *uss,\n\t\t\t\tstruct sigaltstack __user *uoss);\nasmlinkage long sys_rt_sigsuspend(sigset_t __user *unewset, size_t sigsetsize);\n#ifndef CONFIG_ODD_RT_SIGACTION\nasmlinkage long sys_rt_sigaction(int,\n\t\t\t\t const struct sigaction __user *,\n\t\t\t\t struct sigaction __user *,\n\t\t\t\t size_t);\n#endif\nasmlinkage long sys_rt_sigprocmask(int how, sigset_t __user *set,\n\t\t\t\tsigset_t __user *oset, size_t sigsetsize);\nasmlinkage long sys_rt_sigpending(sigset_t __user *set, size_t sigsetsize);\nasmlinkage long sys_rt_sigtimedwait(const sigset_t __user *uthese,\n\t\t\t\tsiginfo_t __user *uinfo,\n\t\t\t\tconst struct __kernel_timespec __user *uts,\n\t\t\t\tsize_t sigsetsize);\nasmlinkage long sys_rt_sigtimedwait_time32(const sigset_t __user *uthese,\n\t\t\t\tsiginfo_t __user *uinfo,\n\t\t\t\tconst struct old_timespec32 __user *uts,\n\t\t\t\tsize_t sigsetsize);\nasmlinkage long sys_rt_sigqueueinfo(pid_t pid, int sig, siginfo_t __user *uinfo);\n\n/* kernel/sys.c */\nasmlinkage long sys_setpriority(int which, int who, int niceval);\nasmlinkage long sys_getpriority(int which, int who);\nasmlinkage long sys_reboot(int magic1, int magic2, unsigned int cmd,\n\t\t\t\tvoid __user *arg);\nasmlinkage long sys_setregid(gid_t rgid, gid_t egid);\nasmlinkage long sys_setgid(gid_t gid);\nasmlinkage long sys_setreuid(uid_t ruid, uid_t euid);\nasmlinkage long sys_setuid(uid_t uid);\nasmlinkage long sys_setresuid(uid_t ruid, uid_t euid, uid_t suid);\nasmlinkage long sys_getresuid(uid_t __user *ruid, uid_t __user *euid, uid_t __user *suid);\nasmlinkage long sys_setresgid(gid_t rgid, gid_t egid, gid_t sgid);\nasmlinkage long sys_getresgid(gid_t __user *rgid, gid_t __user *egid, gid_t __user *sgid);\nasmlinkage long sys_setfsuid(uid_t uid);\nasmlinkage long sys_setfsgid(gid_t gid);\nasmlinkage long sys_times(struct tms __user *tbuf);\nasmlinkage long sys_setpgid(pid_t pid, pid_t pgid);\nasmlinkage long sys_getpgid(pid_t pid);\nasmlinkage long sys_getsid(pid_t pid);\nasmlinkage long sys_setsid(void);\nasmlinkage long sys_getgroups(int gidsetsize, gid_t __user *grouplist);\nasmlinkage long sys_setgroups(int gidsetsize, gid_t __user *grouplist);\nasmlinkage long sys_newuname(struct new_utsname __user *name);\nasmlinkage long sys_sethostname(char __user *name, int len);\nasmlinkage long sys_setdomainname(char __user *name, int len);\nasmlinkage long sys_getrlimit(unsigned int resource,\n\t\t\t\tstruct rlimit __user *rlim);\nasmlinkage long sys_setrlimit(unsigned int resource,\n\t\t\t\tstruct rlimit __user *rlim);\nasmlinkage long sys_getrusage(int who, struct rusage __user *ru);\nasmlinkage long sys_umask(int mask);\nasmlinkage long sys_prctl(int option, unsigned long arg2, unsigned long arg3,\n\t\t\tunsigned long arg4, unsigned long arg5);\nasmlinkage long sys_getcpu(unsigned __user *cpu, unsigned __user *node, struct getcpu_cache __user *cache);\n\n/* kernel/time.c */\nasmlinkage long sys_gettimeofday(struct __kernel_old_timeval __user *tv,\n\t\t\t\tstruct timezone __user *tz);\nasmlinkage long sys_settimeofday(struct __kernel_old_timeval __user *tv,\n\t\t\t\tstruct timezone __user *tz);\nasmlinkage long sys_adjtimex(struct __kernel_timex __user *txc_p);\nasmlinkage long sys_adjtimex_time32(struct old_timex32 __user *txc_p);\n\n/* kernel/sys.c */\nasmlinkage long sys_getpid(void);\nasmlinkage long sys_getppid(void);\nasmlinkage long sys_getuid(void);\nasmlinkage long sys_geteuid(void);\nasmlinkage long sys_getgid(void);\nasmlinkage long sys_getegid(void);\nasmlinkage long sys_gettid(void);\nasmlinkage long sys_sysinfo(struct sysinfo __user *info);\n\n/* ipc/mqueue.c */\nasmlinkage long sys_mq_open(const char __user *name, int oflag, umode_t mode, struct mq_attr __user *attr);\nasmlinkage long sys_mq_unlink(const char __user *name);\nasmlinkage long sys_mq_timedsend(mqd_t mqdes, const char __user *msg_ptr, size_t msg_len, unsigned int msg_prio, const struct __kernel_timespec __user *abs_timeout);\nasmlinkage long sys_mq_timedreceive(mqd_t mqdes, char __user *msg_ptr, size_t msg_len, unsigned int __user *msg_prio, const struct __kernel_timespec __user *abs_timeout);\nasmlinkage long sys_mq_notify(mqd_t mqdes, const struct sigevent __user *notification);\nasmlinkage long sys_mq_getsetattr(mqd_t mqdes, const struct mq_attr __user *mqstat, struct mq_attr __user *omqstat);\nasmlinkage long sys_mq_timedreceive_time32(mqd_t mqdes,\n\t\t\tchar __user *u_msg_ptr,\n\t\t\tunsigned int msg_len, unsigned int __user *u_msg_prio,\n\t\t\tconst struct old_timespec32 __user *u_abs_timeout);\nasmlinkage long sys_mq_timedsend_time32(mqd_t mqdes,\n\t\t\tconst char __user *u_msg_ptr,\n\t\t\tunsigned int msg_len, unsigned int msg_prio,\n\t\t\tconst struct old_timespec32 __user *u_abs_timeout);\n\n/* ipc/msg.c */\nasmlinkage long sys_msgget(key_t key, int msgflg);\nasmlinkage long sys_old_msgctl(int msqid, int cmd, struct msqid_ds __user *buf);\nasmlinkage long sys_msgctl(int msqid, int cmd, struct msqid_ds __user *buf);\nasmlinkage long sys_msgrcv(int msqid, struct msgbuf __user *msgp,\n\t\t\t\tsize_t msgsz, long msgtyp, int msgflg);\nasmlinkage long sys_msgsnd(int msqid, struct msgbuf __user *msgp,\n\t\t\t\tsize_t msgsz, int msgflg);\n\n/* ipc/sem.c */\nasmlinkage long sys_semget(key_t key, int nsems, int semflg);\nasmlinkage long sys_semctl(int semid, int semnum, int cmd, unsigned long arg);\nasmlinkage long sys_old_semctl(int semid, int semnum, int cmd, unsigned long arg);\nasmlinkage long sys_semtimedop(int semid, struct sembuf __user *sops,\n\t\t\t\tunsigned nsops,\n\t\t\t\tconst struct __kernel_timespec __user *timeout);\nasmlinkage long sys_semtimedop_time32(int semid, struct sembuf __user *sops,\n\t\t\t\tunsigned nsops,\n\t\t\t\tconst struct old_timespec32 __user *timeout);\nasmlinkage long sys_semop(int semid, struct sembuf __user *sops,\n\t\t\t\tunsigned nsops);\n\n/* ipc/shm.c */\nasmlinkage long sys_shmget(key_t key, size_t size, int flag);\nasmlinkage long sys_old_shmctl(int shmid, int cmd, struct shmid_ds __user *buf);\nasmlinkage long sys_shmctl(int shmid, int cmd, struct shmid_ds __user *buf);\nasmlinkage long sys_shmat(int shmid, char __user *shmaddr, int shmflg);\nasmlinkage long sys_shmdt(char __user *shmaddr);\n\n/* net/socket.c */\nasmlinkage long sys_socket(int, int, int);\nasmlinkage long sys_socketpair(int, int, int, int __user *);\nasmlinkage long sys_bind(int, struct sockaddr __user *, int);\nasmlinkage long sys_listen(int, int);\nasmlinkage long sys_accept(int, struct sockaddr __user *, int __user *);\nasmlinkage long sys_connect(int, struct sockaddr __user *, int);\nasmlinkage long sys_getsockname(int, struct sockaddr __user *, int __user *);\nasmlinkage long sys_getpeername(int, struct sockaddr __user *, int __user *);\nasmlinkage long sys_sendto(int, void __user *, size_t, unsigned,\n\t\t\t\tstruct sockaddr __user *, int);\nasmlinkage long sys_recvfrom(int, void __user *, size_t, unsigned,\n\t\t\t\tstruct sockaddr __user *, int __user *);\nasmlinkage long sys_setsockopt(int fd, int level, int optname,\n\t\t\t\tchar __user *optval, int optlen);\nasmlinkage long sys_getsockopt(int fd, int level, int optname,\n\t\t\t\tchar __user *optval, int __user *optlen);\nasmlinkage long sys_shutdown(int, int);\nasmlinkage long sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned flags);\nasmlinkage long sys_recvmsg(int fd, struct user_msghdr __user *msg, unsigned flags);\n\n/* mm/filemap.c */\nasmlinkage long sys_readahead(int fd, loff_t offset, size_t count);\n\n/* mm/nommu.c, also with MMU */\nasmlinkage long sys_brk(unsigned long brk);\nasmlinkage long sys_munmap(unsigned long addr, size_t len);\nasmlinkage long sys_mremap(unsigned long addr,\n\t\t\t   unsigned long old_len, unsigned long new_len,\n\t\t\t   unsigned long flags, unsigned long new_addr);\n\n/* security/keys/keyctl.c */\nasmlinkage long sys_add_key(const char __user *_type,\n\t\t\t    const char __user *_description,\n\t\t\t    const void __user *_payload,\n\t\t\t    size_t plen,\n\t\t\t    key_serial_t destringid);\nasmlinkage long sys_request_key(const char __user *_type,\n\t\t\t\tconst char __user *_description,\n\t\t\t\tconst char __user *_callout_info,\n\t\t\t\tkey_serial_t destringid);\nasmlinkage long sys_keyctl(int cmd, unsigned long arg2, unsigned long arg3,\n\t\t\t   unsigned long arg4, unsigned long arg5);\n\n/* arch/example/kernel/sys_example.c */\n#ifdef CONFIG_CLONE_BACKWARDS\nasmlinkage long sys_clone(unsigned long, unsigned long, int __user *, unsigned long,\n\t       int __user *);\n#else\n#ifdef CONFIG_CLONE_BACKWARDS3\nasmlinkage long sys_clone(unsigned long, unsigned long, int, int __user *,\n\t\t\t  int __user *, unsigned long);\n#else\nasmlinkage long sys_clone(unsigned long, unsigned long, int __user *,\n\t       int __user *, unsigned long);\n#endif\n#endif\n\nasmlinkage long sys_clone3(struct clone_args __user *uargs, size_t size);\n\nasmlinkage long sys_execve(const char __user *filename,\n\t\tconst char __user *const __user *argv,\n\t\tconst char __user *const __user *envp);\n\n/* mm/fadvise.c */\nasmlinkage long sys_fadvise64_64(int fd, loff_t offset, loff_t len, int advice);\n\n/* mm/, CONFIG_MMU only */\nasmlinkage long sys_swapon(const char __user *specialfile, int swap_flags);\nasmlinkage long sys_swapoff(const char __user *specialfile);\nasmlinkage long sys_mprotect(unsigned long start, size_t len,\n\t\t\t\tunsigned long prot);\nasmlinkage long sys_msync(unsigned long start, size_t len, int flags);\nasmlinkage long sys_mlock(unsigned long start, size_t len);\nasmlinkage long sys_munlock(unsigned long start, size_t len);\nasmlinkage long sys_mlockall(int flags);\nasmlinkage long sys_munlockall(void);\nasmlinkage long sys_mincore(unsigned long start, size_t len,\n\t\t\t\tunsigned char __user * vec);\nasmlinkage long sys_madvise(unsigned long start, size_t len, int behavior);\nasmlinkage long sys_process_madvise(int pidfd, const struct iovec __user *vec,\n\t\t\tsize_t vlen, int behavior, unsigned int flags);\nasmlinkage long sys_remap_file_pages(unsigned long start, unsigned long size,\n\t\t\tunsigned long prot, unsigned long pgoff,\n\t\t\tunsigned long flags);\nasmlinkage long sys_mbind(unsigned long start, unsigned long len,\n\t\t\t\tunsigned long mode,\n\t\t\t\tconst unsigned long __user *nmask,\n\t\t\t\tunsigned long maxnode,\n\t\t\t\tunsigned flags);\nasmlinkage long sys_get_mempolicy(int __user *policy,\n\t\t\t\tunsigned long __user *nmask,\n\t\t\t\tunsigned long maxnode,\n\t\t\t\tunsigned long addr, unsigned long flags);\nasmlinkage long sys_set_mempolicy(int mode, const unsigned long __user *nmask,\n\t\t\t\tunsigned long maxnode);\nasmlinkage long sys_migrate_pages(pid_t pid, unsigned long maxnode,\n\t\t\t\tconst unsigned long __user *from,\n\t\t\t\tconst unsigned long __user *to);\nasmlinkage long sys_move_pages(pid_t pid, unsigned long nr_pages,\n\t\t\t\tconst void __user * __user *pages,\n\t\t\t\tconst int __user *nodes,\n\t\t\t\tint __user *status,\n\t\t\t\tint flags);\n\nasmlinkage long sys_rt_tgsigqueueinfo(pid_t tgid, pid_t  pid, int sig,\n\t\tsiginfo_t __user *uinfo);\nasmlinkage long sys_perf_event_open(\n\t\tstruct perf_event_attr __user *attr_uptr,\n\t\tpid_t pid, int cpu, int group_fd, unsigned long flags);\nasmlinkage long sys_accept4(int, struct sockaddr __user *, int __user *, int);\nasmlinkage long sys_recvmmsg(int fd, struct mmsghdr __user *msg,\n\t\t\t     unsigned int vlen, unsigned flags,\n\t\t\t     struct __kernel_timespec __user *timeout);\nasmlinkage long sys_recvmmsg_time32(int fd, struct mmsghdr __user *msg,\n\t\t\t     unsigned int vlen, unsigned flags,\n\t\t\t     struct old_timespec32 __user *timeout);\n\nasmlinkage long sys_wait4(pid_t pid, int __user *stat_addr,\n\t\t\t\tint options, struct rusage __user *ru);\nasmlinkage long sys_prlimit64(pid_t pid, unsigned int resource,\n\t\t\t\tconst struct rlimit64 __user *new_rlim,\n\t\t\t\tstruct rlimit64 __user *old_rlim);\nasmlinkage long sys_fanotify_init(unsigned int flags, unsigned int event_f_flags);\nasmlinkage long sys_fanotify_mark(int fanotify_fd, unsigned int flags,\n\t\t\t\t  u64 mask, int fd,\n\t\t\t\t  const char  __user *pathname);\nasmlinkage long sys_name_to_handle_at(int dfd, const char __user *name,\n\t\t\t\t      struct file_handle __user *handle,\n\t\t\t\t      int __user *mnt_id, int flag);\nasmlinkage long sys_open_by_handle_at(int mountdirfd,\n\t\t\t\t      struct file_handle __user *handle,\n\t\t\t\t      int flags);\nasmlinkage long sys_clock_adjtime(clockid_t which_clock,\n\t\t\t\tstruct __kernel_timex __user *tx);\nasmlinkage long sys_clock_adjtime32(clockid_t which_clock,\n\t\t\t\tstruct old_timex32 __user *tx);\nasmlinkage long sys_syncfs(int fd);\nasmlinkage long sys_setns(int fd, int nstype);\nasmlinkage long sys_pidfd_open(pid_t pid, unsigned int flags);\nasmlinkage long sys_sendmmsg(int fd, struct mmsghdr __user *msg,\n\t\t\t     unsigned int vlen, unsigned flags);\nasmlinkage long sys_process_vm_readv(pid_t pid,\n\t\t\t\t     const struct iovec __user *lvec,\n\t\t\t\t     unsigned long liovcnt,\n\t\t\t\t     const struct iovec __user *rvec,\n\t\t\t\t     unsigned long riovcnt,\n\t\t\t\t     unsigned long flags);\nasmlinkage long sys_process_vm_writev(pid_t pid,\n\t\t\t\t      const struct iovec __user *lvec,\n\t\t\t\t      unsigned long liovcnt,\n\t\t\t\t      const struct iovec __user *rvec,\n\t\t\t\t      unsigned long riovcnt,\n\t\t\t\t      unsigned long flags);\nasmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,\n\t\t\t unsigned long idx1, unsigned long idx2);\nasmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);\nasmlinkage long sys_sched_setattr(pid_t pid,\n\t\t\t\t\tstruct sched_attr __user *attr,\n\t\t\t\t\tunsigned int flags);\nasmlinkage long sys_sched_getattr(pid_t pid,\n\t\t\t\t\tstruct sched_attr __user *attr,\n\t\t\t\t\tunsigned int size,\n\t\t\t\t\tunsigned int flags);\nasmlinkage long sys_renameat2(int olddfd, const char __user *oldname,\n\t\t\t      int newdfd, const char __user *newname,\n\t\t\t      unsigned int flags);\nasmlinkage long sys_seccomp(unsigned int op, unsigned int flags,\n\t\t\t    void __user *uargs);\nasmlinkage long sys_getrandom(char __user *buf, size_t count,\n\t\t\t      unsigned int flags);\nasmlinkage long sys_memfd_create(const char __user *uname_ptr, unsigned int flags);\nasmlinkage long sys_bpf(int cmd, union bpf_attr *attr, unsigned int size);\nasmlinkage long sys_execveat(int dfd, const char __user *filename,\n\t\t\tconst char __user *const __user *argv,\n\t\t\tconst char __user *const __user *envp, int flags);\nasmlinkage long sys_userfaultfd(int flags);\nasmlinkage long sys_membarrier(int cmd, unsigned int flags, int cpu_id);\nasmlinkage long sys_mlock2(unsigned long start, size_t len, int flags);\nasmlinkage long sys_copy_file_range(int fd_in, loff_t __user *off_in,\n\t\t\t\t    int fd_out, loff_t __user *off_out,\n\t\t\t\t    size_t len, unsigned int flags);\nasmlinkage long sys_preadv2(unsigned long fd, const struct iovec __user *vec,\n\t\t\t    unsigned long vlen, unsigned long pos_l, unsigned long pos_h,\n\t\t\t    rwf_t flags);\nasmlinkage long sys_pwritev2(unsigned long fd, const struct iovec __user *vec,\n\t\t\t    unsigned long vlen, unsigned long pos_l, unsigned long pos_h,\n\t\t\t    rwf_t flags);\nasmlinkage long sys_pkey_mprotect(unsigned long start, size_t len,\n\t\t\t\t  unsigned long prot, int pkey);\nasmlinkage long sys_pkey_alloc(unsigned long flags, unsigned long init_val);\nasmlinkage long sys_pkey_free(int pkey);\nasmlinkage long sys_statx(int dfd, const char __user *path, unsigned flags,\n\t\t\t  unsigned mask, struct statx __user *buffer);\nasmlinkage long sys_rseq(struct rseq __user *rseq, uint32_t rseq_len,\n\t\t\t int flags, uint32_t sig);\nasmlinkage long sys_open_tree(int dfd, const char __user *path, unsigned flags);\nasmlinkage long sys_move_mount(int from_dfd, const char __user *from_path,\n\t\t\t       int to_dfd, const char __user *to_path,\n\t\t\t       unsigned int ms_flags);\nasmlinkage long sys_fsopen(const char __user *fs_name, unsigned int flags);\nasmlinkage long sys_fsconfig(int fs_fd, unsigned int cmd, const char __user *key,\n\t\t\t     const void __user *value, int aux);\nasmlinkage long sys_fsmount(int fs_fd, unsigned int flags, unsigned int ms_flags);\nasmlinkage long sys_fspick(int dfd, const char __user *path, unsigned int flags);\nasmlinkage long sys_pidfd_send_signal(int pidfd, int sig,\n\t\t\t\t       siginfo_t __user *info,\n\t\t\t\t       unsigned int flags);\nasmlinkage long sys_pidfd_getfd(int pidfd, int fd, unsigned int flags);\nasmlinkage long sys_watch_mount(int dfd, const char __user *path,\n\t\t\t\tunsigned int at_flags, int watch_fd, int watch_id);\n\n/*\n * Architecture-specific system calls\n */\n\n/* arch/x86/kernel/ioport.c */\nasmlinkage long sys_ioperm(unsigned long from, unsigned long num, int on);\n\n/* pciconfig: alpha, arm, arm64, ia64, sparc */\nasmlinkage long sys_pciconfig_read(unsigned long bus, unsigned long dfn,\n\t\t\t\tunsigned long off, unsigned long len,\n\t\t\t\tvoid __user *buf);\nasmlinkage long sys_pciconfig_write(unsigned long bus, unsigned long dfn,\n\t\t\t\tunsigned long off, unsigned long len,\n\t\t\t\tvoid __user *buf);\nasmlinkage long sys_pciconfig_iobase(long which, unsigned long bus, unsigned long devfn);\n\n/* powerpc */\nasmlinkage long sys_spu_run(int fd, __u32 __user *unpc,\n\t\t\t\t __u32 __user *ustatus);\nasmlinkage long sys_spu_create(const char __user *name,\n\t\tunsigned int flags, umode_t mode, int fd);\n\n\n/*\n * Deprecated system calls which are still defined in\n * include/uapi/asm-generic/unistd.h and wanted by >= 1 arch\n */\n\n/* __ARCH_WANT_SYSCALL_NO_AT */\nasmlinkage long sys_open(const char __user *filename,\n\t\t\t\tint flags, umode_t mode);\nasmlinkage long sys_link(const char __user *oldname,\n\t\t\t\tconst char __user *newname);\nasmlinkage long sys_unlink(const char __user *pathname);\nasmlinkage long sys_mknod(const char __user *filename, umode_t mode,\n\t\t\t\tunsigned dev);\nasmlinkage long sys_chmod(const char __user *filename, umode_t mode);\nasmlinkage long sys_chown(const char __user *filename,\n\t\t\t\tuid_t user, gid_t group);\nasmlinkage long sys_mkdir(const char __user *pathname, umode_t mode);\nasmlinkage long sys_rmdir(const char __user *pathname);\nasmlinkage long sys_lchown(const char __user *filename,\n\t\t\t\tuid_t user, gid_t group);\nasmlinkage long sys_access(const char __user *filename, int mode);\nasmlinkage long sys_rename(const char __user *oldname,\n\t\t\t\tconst char __user *newname);\nasmlinkage long sys_symlink(const char __user *old, const char __user *new);\n#if defined(__ARCH_WANT_STAT64) || defined(__ARCH_WANT_COMPAT_STAT64)\nasmlinkage long sys_stat64(const char __user *filename,\n\t\t\t\tstruct stat64 __user *statbuf);\nasmlinkage long sys_lstat64(const char __user *filename,\n\t\t\t\tstruct stat64 __user *statbuf);\n#endif\n\n/* __ARCH_WANT_SYSCALL_NO_FLAGS */\nasmlinkage long sys_pipe(int __user *fildes);\nasmlinkage long sys_dup2(unsigned int oldfd, unsigned int newfd);\nasmlinkage long sys_epoll_create(int size);\nasmlinkage long sys_inotify_init(void);\nasmlinkage long sys_eventfd(unsigned int count);\nasmlinkage long sys_signalfd(int ufd, sigset_t __user *user_mask, size_t sizemask);\n\n/* __ARCH_WANT_SYSCALL_OFF_T */\nasmlinkage long sys_sendfile(int out_fd, int in_fd,\n\t\t\t     off_t __user *offset, size_t count);\nasmlinkage long sys_newstat(const char __user *filename,\n\t\t\t\tstruct stat __user *statbuf);\nasmlinkage long sys_newlstat(const char __user *filename,\n\t\t\t\tstruct stat __user *statbuf);\nasmlinkage long sys_fadvise64(int fd, loff_t offset, size_t len, int advice);\n\n/* __ARCH_WANT_SYSCALL_DEPRECATED */\nasmlinkage long sys_alarm(unsigned int seconds);\nasmlinkage long sys_getpgrp(void);\nasmlinkage long sys_pause(void);\nasmlinkage long sys_time(__kernel_old_time_t __user *tloc);\nasmlinkage long sys_time32(old_time32_t __user *tloc);\n#ifdef __ARCH_WANT_SYS_UTIME\nasmlinkage long sys_utime(char __user *filename,\n\t\t\t\tstruct utimbuf __user *times);\nasmlinkage long sys_utimes(char __user *filename,\n\t\t\t\tstruct __kernel_old_timeval __user *utimes);\nasmlinkage long sys_futimesat(int dfd, const char __user *filename,\n\t\t\t      struct __kernel_old_timeval __user *utimes);\n#endif\nasmlinkage long sys_futimesat_time32(unsigned int dfd,\n\t\t\t\t     const char __user *filename,\n\t\t\t\t     struct old_timeval32 __user *t);\nasmlinkage long sys_utime32(const char __user *filename,\n\t\t\t\t struct old_utimbuf32 __user *t);\nasmlinkage long sys_utimes_time32(const char __user *filename,\n\t\t\t\t  struct old_timeval32 __user *t);\nasmlinkage long sys_creat(const char __user *pathname, umode_t mode);\nasmlinkage long sys_getdents(unsigned int fd,\n\t\t\t\tstruct linux_dirent __user *dirent,\n\t\t\t\tunsigned int count);\nasmlinkage long sys_select(int n, fd_set __user *inp, fd_set __user *outp,\n\t\t\tfd_set __user *exp, struct __kernel_old_timeval __user *tvp);\nasmlinkage long sys_poll(struct pollfd __user *ufds, unsigned int nfds,\n\t\t\t\tint timeout);\nasmlinkage long sys_epoll_wait(int epfd, struct epoll_event __user *events,\n\t\t\t\tint maxevents, int timeout);\nasmlinkage long sys_ustat(unsigned dev, struct ustat __user *ubuf);\nasmlinkage long sys_vfork(void);\nasmlinkage long sys_recv(int, void __user *, size_t, unsigned);\nasmlinkage long sys_send(int, void __user *, size_t, unsigned);\nasmlinkage long sys_bdflush(int func, long data);\nasmlinkage long sys_oldumount(char __user *name);\nasmlinkage long sys_uselib(const char __user *library);\nasmlinkage long sys_sysfs(int option,\n\t\t\t\tunsigned long arg1, unsigned long arg2);\nasmlinkage long sys_fork(void);\n\n/* obsolete: kernel/time/time.c */\nasmlinkage long sys_stime(__kernel_old_time_t __user *tptr);\nasmlinkage long sys_stime32(old_time32_t __user *tptr);\n\n/* obsolete: kernel/signal.c */\nasmlinkage long sys_sigpending(old_sigset_t __user *uset);\nasmlinkage long sys_sigprocmask(int how, old_sigset_t __user *set,\n\t\t\t\told_sigset_t __user *oset);\n#ifdef CONFIG_OLD_SIGSUSPEND\nasmlinkage long sys_sigsuspend(old_sigset_t mask);\n#endif\n\n#ifdef CONFIG_OLD_SIGSUSPEND3\nasmlinkage long sys_sigsuspend(int unused1, int unused2, old_sigset_t mask);\n#endif\n\n#ifdef CONFIG_OLD_SIGACTION\nasmlinkage long sys_sigaction(int, const struct old_sigaction __user *,\n\t\t\t\tstruct old_sigaction __user *);\n#endif\nasmlinkage long sys_sgetmask(void);\nasmlinkage long sys_ssetmask(int newmask);\nasmlinkage long sys_signal(int sig, __sighandler_t handler);\n\n/* obsolete: kernel/sched/core.c */\nasmlinkage long sys_nice(int increment);\n\n/* obsolete: kernel/kexec_file.c */\nasmlinkage long sys_kexec_file_load(int kernel_fd, int initrd_fd,\n\t\t\t\t    unsigned long cmdline_len,\n\t\t\t\t    const char __user *cmdline_ptr,\n\t\t\t\t    unsigned long flags);\n\n/* obsolete: kernel/exit.c */\nasmlinkage long sys_waitpid(pid_t pid, int __user *stat_addr, int options);\n\n/* obsolete: kernel/uid16.c */\n#ifdef CONFIG_HAVE_UID16\nasmlinkage long sys_chown16(const char __user *filename,\n\t\t\t\told_uid_t user, old_gid_t group);\nasmlinkage long sys_lchown16(const char __user *filename,\n\t\t\t\told_uid_t user, old_gid_t group);\nasmlinkage long sys_fchown16(unsigned int fd, old_uid_t user, old_gid_t group);\nasmlinkage long sys_setregid16(old_gid_t rgid, old_gid_t egid);\nasmlinkage long sys_setgid16(old_gid_t gid);\nasmlinkage long sys_setreuid16(old_uid_t ruid, old_uid_t euid);\nasmlinkage long sys_setuid16(old_uid_t uid);\nasmlinkage long sys_setresuid16(old_uid_t ruid, old_uid_t euid, old_uid_t suid);\nasmlinkage long sys_getresuid16(old_uid_t __user *ruid,\n\t\t\t\told_uid_t __user *euid, old_uid_t __user *suid);\nasmlinkage long sys_setresgid16(old_gid_t rgid, old_gid_t egid, old_gid_t sgid);\nasmlinkage long sys_getresgid16(old_gid_t __user *rgid,\n\t\t\t\told_gid_t __user *egid, old_gid_t __user *sgid);\nasmlinkage long sys_setfsuid16(old_uid_t uid);\nasmlinkage long sys_setfsgid16(old_gid_t gid);\nasmlinkage long sys_getgroups16(int gidsetsize, old_gid_t __user *grouplist);\nasmlinkage long sys_setgroups16(int gidsetsize, old_gid_t __user *grouplist);\nasmlinkage long sys_getuid16(void);\nasmlinkage long sys_geteuid16(void);\nasmlinkage long sys_getgid16(void);\nasmlinkage long sys_getegid16(void);\n#endif\n\n/* obsolete: net/socket.c */\nasmlinkage long sys_socketcall(int call, unsigned long __user *args);\n\n/* obsolete: fs/stat.c */\nasmlinkage long sys_stat(const char __user *filename,\n\t\t\tstruct __old_kernel_stat __user *statbuf);\nasmlinkage long sys_lstat(const char __user *filename,\n\t\t\tstruct __old_kernel_stat __user *statbuf);\nasmlinkage long sys_fstat(unsigned int fd,\n\t\t\tstruct __old_kernel_stat __user *statbuf);\nasmlinkage long sys_readlink(const char __user *path,\n\t\t\t\tchar __user *buf, int bufsiz);\n\n/* obsolete: fs/select.c */\nasmlinkage long sys_old_select(struct sel_arg_struct __user *arg);\n\n/* obsolete: fs/readdir.c */\nasmlinkage long sys_old_readdir(unsigned int, struct old_linux_dirent __user *, unsigned int);\n\n/* obsolete: kernel/sys.c */\nasmlinkage long sys_gethostname(char __user *name, int len);\nasmlinkage long sys_uname(struct old_utsname __user *);\nasmlinkage long sys_olduname(struct oldold_utsname __user *);\n#ifdef __ARCH_WANT_SYS_OLD_GETRLIMIT\nasmlinkage long sys_old_getrlimit(unsigned int resource, struct rlimit __user *rlim);\n#endif\n\n/* obsolete: ipc */\nasmlinkage long sys_ipc(unsigned int call, int first, unsigned long second,\n\t\tunsigned long third, void __user *ptr, long fifth);\n\n/* obsolete: mm/ */\nasmlinkage long sys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\tunsigned long prot, unsigned long flags,\n\t\t\tunsigned long fd, unsigned long pgoff);\nasmlinkage long sys_old_mmap(struct mmap_arg_struct __user *arg);\n\n\n/*\n * Not a real system call, but a placeholder for syscalls which are\n * not implemented -- see kernel/sys_ni.c\n */\nasmlinkage long sys_ni_syscall(void);\n\n#endif /* CONFIG_ARCH_HAS_SYSCALL_WRAPPER */\n\n\n/*\n * Kernel code should not call syscalls (i.e., sys_xyzyyz()) directly.\n * Instead, use one of the functions which work equivalently, such as\n * the ksys_xyzyyz() functions prototyped below.\n */\nssize_t ksys_write(unsigned int fd, const char __user *buf, size_t count);\nint ksys_fchown(unsigned int fd, uid_t user, gid_t group);\nssize_t ksys_read(unsigned int fd, char __user *buf, size_t count);\nvoid ksys_sync(void);\nint ksys_unshare(unsigned long unshare_flags);\nint ksys_setsid(void);\nint ksys_sync_file_range(int fd, loff_t offset, loff_t nbytes,\n\t\t\t unsigned int flags);\nssize_t ksys_pread64(unsigned int fd, char __user *buf, size_t count,\n\t\t     loff_t pos);\nssize_t ksys_pwrite64(unsigned int fd, const char __user *buf,\n\t\t      size_t count, loff_t pos);\nint ksys_fallocate(int fd, int mode, loff_t offset, loff_t len);\n#ifdef CONFIG_ADVISE_SYSCALLS\nint ksys_fadvise64_64(int fd, loff_t offset, loff_t len, int advice);\n#else\nstatic inline int ksys_fadvise64_64(int fd, loff_t offset, loff_t len,\n\t\t\t\t    int advice)\n{\n\treturn -EINVAL;\n}\n#endif\nunsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff);\nssize_t ksys_readahead(int fd, loff_t offset, size_t count);\nint ksys_ipc(unsigned int call, int first, unsigned long second,\n\tunsigned long third, void __user * ptr, long fifth);\nint compat_ksys_ipc(u32 call, int first, int second,\n\tu32 third, u32 ptr, u32 fifth);\n\n/*\n * The following kernel syscall equivalents are just wrappers to fs-internal\n * functions. Therefore, provide stubs to be inlined at the callsites.\n */\nextern int do_fchownat(int dfd, const char __user *filename, uid_t user,\n\t\t       gid_t group, int flag);\n\nstatic inline long ksys_chown(const char __user *filename, uid_t user,\n\t\t\t      gid_t group)\n{\n\treturn do_fchownat(AT_FDCWD, filename, user, group, 0);\n}\n\nstatic inline long ksys_lchown(const char __user *filename, uid_t user,\n\t\t\t       gid_t group)\n{\n\treturn do_fchownat(AT_FDCWD, filename, user, group,\n\t\t\t     AT_SYMLINK_NOFOLLOW);\n}\n\nextern long do_sys_ftruncate(unsigned int fd, loff_t length, int small);\n\nstatic inline long ksys_ftruncate(unsigned int fd, loff_t length)\n{\n\treturn do_sys_ftruncate(fd, length, 1);\n}\n\nextern long do_sys_truncate(const char __user *pathname, loff_t length);\n\nstatic inline long ksys_truncate(const char __user *pathname, loff_t length)\n{\n\treturn do_sys_truncate(pathname, length);\n}\n\nstatic inline unsigned int ksys_personality(unsigned int personality)\n{\n\tunsigned int old = current->personality;\n\n\tif (personality != 0xffffffff)\n\t\tset_personality(personality);\n\n\treturn old;\n}\n\n/* for __ARCH_WANT_SYS_IPC */\nlong ksys_semtimedop(int semid, struct sembuf __user *tsops,\n\t\t     unsigned int nsops,\n\t\t     const struct __kernel_timespec __user *timeout);\nlong ksys_semget(key_t key, int nsems, int semflg);\nlong ksys_old_semctl(int semid, int semnum, int cmd, unsigned long arg);\nlong ksys_msgget(key_t key, int msgflg);\nlong ksys_old_msgctl(int msqid, int cmd, struct msqid_ds __user *buf);\nlong ksys_msgrcv(int msqid, struct msgbuf __user *msgp, size_t msgsz,\n\t\t long msgtyp, int msgflg);\nlong ksys_msgsnd(int msqid, struct msgbuf __user *msgp, size_t msgsz,\n\t\t int msgflg);\nlong ksys_shmget(key_t key, size_t size, int shmflg);\nlong ksys_shmdt(char __user *shmaddr);\nlong ksys_old_shmctl(int shmid, int cmd, struct shmid_ds __user *buf);\nlong compat_ksys_semtimedop(int semid, struct sembuf __user *tsems,\n\t\t\t    unsigned int nsops,\n\t\t\t    const struct old_timespec32 __user *timeout);\n\nint __sys_getsockopt(int fd, int level, int optname, char __user *optval,\n\t\tint __user *optlen);\nint __sys_setsockopt(int fd, int level, int optname, char __user *optval,\n\t\tint optlen);\n#endif\n"}, "2": {"id": 2, "path": "/src/arch/x86/include/asm/syscall_wrapper.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * syscall_wrapper.h - x86 specific wrappers to syscall definitions\n */\n\n#ifndef _ASM_X86_SYSCALL_WRAPPER_H\n#define _ASM_X86_SYSCALL_WRAPPER_H\n\nstruct pt_regs;\n\nextern long __x64_sys_ni_syscall(const struct pt_regs *regs);\nextern long __ia32_sys_ni_syscall(const struct pt_regs *regs);\n\n/*\n * Instead of the generic __SYSCALL_DEFINEx() definition, the x86 version takes\n * struct pt_regs *regs as the only argument of the syscall stub(s) named as:\n * __x64_sys_*()         - 64-bit native syscall\n * __ia32_sys_*()        - 32-bit native syscall or common compat syscall\n * __ia32_compat_sys_*() - 32-bit compat syscall\n * __x32_compat_sys_*()  - 64-bit X32 compat syscall\n *\n * The registers are decoded according to the ABI:\n * 64-bit: RDI, RSI, RDX, R10, R8, R9\n * 32-bit: EBX, ECX, EDX, ESI, EDI, EBP\n *\n * The stub then passes the decoded arguments to the __se_sys_*() wrapper to\n * perform sign-extension (omitted for zero-argument syscalls).  Finally the\n * arguments are passed to the __do_sys_*() function which is the actual\n * syscall.  These wrappers are marked as inline so the compiler can optimize\n * the functions where appropriate.\n *\n * Example assembly (slightly re-ordered for better readability):\n *\n * <__x64_sys_recv>:\t\t<-- syscall with 4 parameters\n *\tcallq\t<__fentry__>\n *\n *\tmov\t0x70(%rdi),%rdi\t<-- decode regs->di\n *\tmov\t0x68(%rdi),%rsi\t<-- decode regs->si\n *\tmov\t0x60(%rdi),%rdx\t<-- decode regs->dx\n *\tmov\t0x38(%rdi),%rcx\t<-- decode regs->r10\n *\n *\txor\t%r9d,%r9d\t<-- clear %r9\n *\txor\t%r8d,%r8d\t<-- clear %r8\n *\n *\tcallq\t__sys_recvfrom\t<-- do the actual work in __sys_recvfrom()\n *\t\t\t\t    which takes 6 arguments\n *\n *\tcltq\t\t\t<-- extend return value to 64-bit\n *\tretq\t\t\t<-- return\n *\n * This approach avoids leaking random user-provided register content down\n * the call chain.\n */\n\n/* Mapping of registers to parameters for syscalls on x86-64 and x32 */\n#define SC_X86_64_REGS_TO_ARGS(x, ...)\t\t\t\t\t\\\n\t__MAP(x,__SC_ARGS\t\t\t\t\t\t\\\n\t\t,,regs->di,,regs->si,,regs->dx\t\t\t\t\\\n\t\t,,regs->r10,,regs->r8,,regs->r9)\t\t\t\\\n\n/* Mapping of registers to parameters for syscalls on i386 */\n#define SC_IA32_REGS_TO_ARGS(x, ...)\t\t\t\t\t\\\n\t__MAP(x,__SC_ARGS\t\t\t\t\t\t\\\n\t      ,,(unsigned int)regs->bx,,(unsigned int)regs->cx\t\t\\\n\t      ,,(unsigned int)regs->dx,,(unsigned int)regs->si\t\t\\\n\t      ,,(unsigned int)regs->di,,(unsigned int)regs->bp)\n\n#define __SYS_STUB0(abi, name)\t\t\t\t\t\t\\\n\tlong __##abi##_##name(const struct pt_regs *regs);\t\t\\\n\tALLOW_ERROR_INJECTION(__##abi##_##name, ERRNO);\t\t\t\\\n\tlong __##abi##_##name(const struct pt_regs *regs)\t\t\\\n\t\t__alias(\"__do_\" #name);\n\n#define __SYS_STUBx(abi, name, ...)\t\t\t\t\t\\\n\tlong __##abi##_##name(const struct pt_regs *regs);\t\t\\\n\tALLOW_ERROR_INJECTION(__##abi##_##name, ERRNO);\t\t\t\\\n\tlong __##abi##_##name(const struct pt_regs *regs)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn __se_##name(__VA_ARGS__);\t\t\t\\\n\t}\n\n#define __COND_SYSCALL(abi, name)\t\t\t\t\t\\\n\t__weak long __##abi##_##name(const struct pt_regs *__unused)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn sys_ni_syscall();\t\t\t\t\\\n\t}\n\n#define __SYS_NI(abi, name)\t\t\t\t\t\t\\\n\tSYSCALL_ALIAS(__##abi##_##name, sys_ni_posix_timers);\n\n#ifdef CONFIG_X86_64\n#define __X64_SYS_STUB0(name)\t\t\t\t\t\t\\\n\t__SYS_STUB0(x64, sys_##name)\n\n#define __X64_SYS_STUBx(x, name, ...)\t\t\t\t\t\\\n\t__SYS_STUBx(x64, sys##name,\t\t\t\t\t\\\n\t\t    SC_X86_64_REGS_TO_ARGS(x, __VA_ARGS__))\n\n#define __X64_COND_SYSCALL(name)\t\t\t\t\t\\\n\t__COND_SYSCALL(x64, sys_##name)\n\n#define __X64_SYS_NI(name)\t\t\t\t\t\t\\\n\t__SYS_NI(x64, sys_##name)\n#else /* CONFIG_X86_64 */\n#define __X64_SYS_STUB0(name)\n#define __X64_SYS_STUBx(x, name, ...)\n#define __X64_COND_SYSCALL(name)\n#define __X64_SYS_NI(name)\n#endif /* CONFIG_X86_64 */\n\n#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)\n#define __IA32_SYS_STUB0(name)\t\t\t\t\t\t\\\n\t__SYS_STUB0(ia32, sys_##name)\n\n#define __IA32_SYS_STUBx(x, name, ...)\t\t\t\t\t\\\n\t__SYS_STUBx(ia32, sys##name,\t\t\t\t\t\\\n\t\t    SC_IA32_REGS_TO_ARGS(x, __VA_ARGS__))\n\n#define __IA32_COND_SYSCALL(name)\t\t\t\t\t\\\n\t__COND_SYSCALL(ia32, sys_##name)\n\n#define __IA32_SYS_NI(name)\t\t\t\t\t\t\\\n\t__SYS_NI(ia32, sys_##name)\n#else /* CONFIG_X86_32 || CONFIG_IA32_EMULATION */\n#define __IA32_SYS_STUB0(name)\n#define __IA32_SYS_STUBx(x, name, ...)\n#define __IA32_COND_SYSCALL(name)\n#define __IA32_SYS_NI(name)\n#endif /* CONFIG_X86_32 || CONFIG_IA32_EMULATION */\n\n#ifdef CONFIG_IA32_EMULATION\n/*\n * For IA32 emulation, we need to handle \"compat\" syscalls *and* create\n * additional wrappers (aptly named __ia32_sys_xyzzy) which decode the\n * ia32 regs in the proper order for shared or \"common\" syscalls. As some\n * syscalls may not be implemented, we need to expand COND_SYSCALL in\n * kernel/sys_ni.c and SYS_NI in kernel/time/posix-stubs.c to cover this\n * case as well.\n */\n#define __IA32_COMPAT_SYS_STUB0(name)\t\t\t\t\t\\\n\t__SYS_STUB0(ia32, compat_sys_##name)\n\n#define __IA32_COMPAT_SYS_STUBx(x, name, ...)\t\t\t\t\\\n\t__SYS_STUBx(ia32, compat_sys##name,\t\t\t\t\\\n\t\t    SC_IA32_REGS_TO_ARGS(x, __VA_ARGS__))\n\n#define __IA32_COMPAT_COND_SYSCALL(name)\t\t\t\t\\\n\t__COND_SYSCALL(ia32, compat_sys_##name)\n\n#define __IA32_COMPAT_SYS_NI(name)\t\t\t\t\t\\\n\t__SYS_NI(ia32, compat_sys_##name)\n\n#else /* CONFIG_IA32_EMULATION */\n#define __IA32_COMPAT_SYS_STUB0(name)\n#define __IA32_COMPAT_SYS_STUBx(x, name, ...)\n#define __IA32_COMPAT_COND_SYSCALL(name)\n#define __IA32_COMPAT_SYS_NI(name)\n#endif /* CONFIG_IA32_EMULATION */\n\n\n#ifdef CONFIG_X86_X32\n/*\n * For the x32 ABI, we need to create a stub for compat_sys_*() which is aware\n * of the x86-64-style parameter ordering of x32 syscalls. The syscalls common\n * with x86_64 obviously do not need such care.\n */\n#define __X32_COMPAT_SYS_STUB0(name)\t\t\t\t\t\\\n\t__SYS_STUB0(x32, compat_sys_##name)\n\n#define __X32_COMPAT_SYS_STUBx(x, name, ...)\t\t\t\t\\\n\t__SYS_STUBx(x32, compat_sys##name,\t\t\t\t\\\n\t\t    SC_X86_64_REGS_TO_ARGS(x, __VA_ARGS__))\n\n#define __X32_COMPAT_COND_SYSCALL(name)\t\t\t\t\t\\\n\t__COND_SYSCALL(x32, compat_sys_##name)\n\n#define __X32_COMPAT_SYS_NI(name)\t\t\t\t\t\\\n\t__SYS_NI(x32, compat_sys_##name)\n#else /* CONFIG_X86_X32 */\n#define __X32_COMPAT_SYS_STUB0(name)\n#define __X32_COMPAT_SYS_STUBx(x, name, ...)\n#define __X32_COMPAT_COND_SYSCALL(name)\n#define __X32_COMPAT_SYS_NI(name)\n#endif /* CONFIG_X86_X32 */\n\n\n#ifdef CONFIG_COMPAT\n/*\n * Compat means IA32_EMULATION and/or X86_X32. As they use a different\n * mapping of registers to parameters, we need to generate stubs for each\n * of them.\n */\n#define COMPAT_SYSCALL_DEFINE0(name)\t\t\t\t\t\\\n\tstatic long\t\t\t\t\t\t\t\\\n\t__do_compat_sys_##name(const struct pt_regs *__unused);\t\t\\\n\t__IA32_COMPAT_SYS_STUB0(name)\t\t\t\t\t\\\n\t__X32_COMPAT_SYS_STUB0(name)\t\t\t\t\t\\\n\tstatic long\t\t\t\t\t\t\t\\\n\t__do_compat_sys_##name(const struct pt_regs *__unused)\n\n#define COMPAT_SYSCALL_DEFINEx(x, name, ...)\t\t\t\t\t\\\n\tstatic long __se_compat_sys##name(__MAP(x,__SC_LONG,__VA_ARGS__));\t\\\n\tstatic inline long __do_compat_sys##name(__MAP(x,__SC_DECL,__VA_ARGS__));\\\n\t__IA32_COMPAT_SYS_STUBx(x, name, __VA_ARGS__)\t\t\t\t\\\n\t__X32_COMPAT_SYS_STUBx(x, name, __VA_ARGS__)\t\t\t\t\\\n\tstatic long __se_compat_sys##name(__MAP(x,__SC_LONG,__VA_ARGS__))\t\\\n\t{\t\t\t\t\t\t\t\t\t\\\n\t\treturn __do_compat_sys##name(__MAP(x,__SC_DELOUSE,__VA_ARGS__));\\\n\t}\t\t\t\t\t\t\t\t\t\\\n\tstatic inline long __do_compat_sys##name(__MAP(x,__SC_DECL,__VA_ARGS__))\n\n/*\n * As some compat syscalls may not be implemented, we need to expand\n * COND_SYSCALL_COMPAT in kernel/sys_ni.c and COMPAT_SYS_NI in\n * kernel/time/posix-stubs.c to cover this case as well.\n */\n#define COND_SYSCALL_COMPAT(name) \t\t\t\t\t\\\n\t__IA32_COMPAT_COND_SYSCALL(name)\t\t\t\t\\\n\t__X32_COMPAT_COND_SYSCALL(name)\n\n#define COMPAT_SYS_NI(name)\t\t\t\t\t\t\\\n\t__IA32_COMPAT_SYS_NI(name)\t\t\t\t\t\\\n\t__X32_COMPAT_SYS_NI(name)\n\n#endif /* CONFIG_COMPAT */\n\n#define __SYSCALL_DEFINEx(x, name, ...)\t\t\t\t\t\\\n\tstatic long __se_sys##name(__MAP(x,__SC_LONG,__VA_ARGS__));\t\\\n\tstatic inline long __do_sys##name(__MAP(x,__SC_DECL,__VA_ARGS__));\\\n\t__X64_SYS_STUBx(x, name, __VA_ARGS__)\t\t\t\t\\\n\t__IA32_SYS_STUBx(x, name, __VA_ARGS__)\t\t\t\t\\\n\tstatic long __se_sys##name(__MAP(x,__SC_LONG,__VA_ARGS__))\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tlong ret = __do_sys##name(__MAP(x,__SC_CAST,__VA_ARGS__));\\\n\t\t__MAP(x,__SC_TEST,__VA_ARGS__);\t\t\t\t\\\n\t\t__PROTECT(x, ret,__MAP(x,__SC_ARGS,__VA_ARGS__));\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tstatic inline long __do_sys##name(__MAP(x,__SC_DECL,__VA_ARGS__))\n\n/*\n * As the generic SYSCALL_DEFINE0() macro does not decode any parameters for\n * obvious reasons, and passing struct pt_regs *regs to it in %rdi does not\n * hurt, we only need to re-define it here to keep the naming congruent to\n * SYSCALL_DEFINEx() -- which is essential for the COND_SYSCALL() and SYS_NI()\n * macros to work correctly.\n */\n#define SYSCALL_DEFINE0(sname)\t\t\t\t\t\t\\\n\tSYSCALL_METADATA(_##sname, 0);\t\t\t\t\t\\\n\tstatic long __do_sys_##sname(const struct pt_regs *__unused);\t\\\n\t__X64_SYS_STUB0(sname)\t\t\t\t\t\t\\\n\t__IA32_SYS_STUB0(sname)\t\t\t\t\t\t\\\n\tstatic long __do_sys_##sname(const struct pt_regs *__unused)\n\n#define COND_SYSCALL(name)\t\t\t\t\t\t\\\n\t__X64_COND_SYSCALL(name)\t\t\t\t\t\\\n\t__IA32_COND_SYSCALL(name)\n\n#define SYS_NI(name)\t\t\t\t\t\t\t\\\n\t__X64_SYS_NI(name)\t\t\t\t\t\t\\\n\t__IA32_SYS_NI(name)\n\n\n/*\n * For VSYSCALLS, we need to declare these three syscalls with the new\n * pt_regs-based calling convention for in-kernel use.\n */\nlong __x64_sys_getcpu(const struct pt_regs *regs);\nlong __x64_sys_gettimeofday(const struct pt_regs *regs);\nlong __x64_sys_time(const struct pt_regs *regs);\n\n#endif /* _ASM_X86_SYSCALL_WRAPPER_H */\n"}, "3": {"id": 3, "path": "/src/include/linux/kernel.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_KERNEL_H\n#define _LINUX_KERNEL_H\n\n#include <stdarg.h>\n#include <linux/limits.h>\n#include <linux/linkage.h>\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/math.h>\n#include <linux/minmax.h>\n#include <linux/typecheck.h>\n#include <linux/printk.h>\n#include <linux/build_bug.h>\n\n#include <asm/byteorder.h>\n\n#include <uapi/linux/kernel.h>\n\n#define STACK_MAGIC\t0xdeadbeef\n\n/**\n * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value\n * @x: value to repeat\n *\n * NOTE: @x is not checked for > 0xff; larger values produce odd results.\n */\n#define REPEAT_BYTE(x)\t((~0ul / 0xff) * (x))\n\n/* @a is a power of 2 value */\n#define ALIGN(x, a)\t\t__ALIGN_KERNEL((x), (a))\n#define ALIGN_DOWN(x, a)\t__ALIGN_KERNEL((x) - ((a) - 1), (a))\n#define __ALIGN_MASK(x, mask)\t__ALIGN_KERNEL_MASK((x), (mask))\n#define PTR_ALIGN(p, a)\t\t((typeof(p))ALIGN((unsigned long)(p), (a)))\n#define PTR_ALIGN_DOWN(p, a)\t((typeof(p))ALIGN_DOWN((unsigned long)(p), (a)))\n#define IS_ALIGNED(x, a)\t\t(((x) & ((typeof(x))(a) - 1)) == 0)\n\n/* generic data direction definitions */\n#define READ\t\t\t0\n#define WRITE\t\t\t1\n\n/**\n * ARRAY_SIZE - get the number of elements in array @arr\n * @arr: array to be sized\n */\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n\n#define u64_to_user_ptr(x) (\t\t\\\n{\t\t\t\t\t\\\n\ttypecheck(u64, (x));\t\t\\\n\t(void __user *)(uintptr_t)(x);\t\\\n}\t\t\t\t\t\\\n)\n\n#define typeof_member(T, m)\ttypeof(((T*)0)->m)\n\n#define _RET_IP_\t\t(unsigned long)__builtin_return_address(0)\n#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })\n\n/**\n * upper_32_bits - return bits 32-63 of a number\n * @n: the number we're accessing\n *\n * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress\n * the \"right shift count >= width of type\" warning when that quantity is\n * 32-bits.\n */\n#define upper_32_bits(n) ((u32)(((n) >> 16) >> 16))\n\n/**\n * lower_32_bits - return bits 0-31 of a number\n * @n: the number we're accessing\n */\n#define lower_32_bits(n) ((u32)((n) & 0xffffffff))\n\nstruct completion;\nstruct pt_regs;\nstruct user;\n\n#ifdef CONFIG_PREEMPT_VOLUNTARY\nextern int _cond_resched(void);\n# define might_resched() _cond_resched()\n#else\n# define might_resched() do { } while (0)\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nextern void ___might_sleep(const char *file, int line, int preempt_offset);\nextern void __might_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_sleep(const char *file, int line, int preempt_offset);\nextern void __cant_migrate(const char *file, int line);\n\n/**\n * might_sleep - annotation for functions that can sleep\n *\n * this macro will print a stack trace if it is executed in an atomic\n * context (spinlock, irq-handler, ...). Additional sections where blocking is\n * not allowed can be annotated with non_block_start() and non_block_end()\n * pairs.\n *\n * This is a useful debugging help to be able to catch problems early and not\n * be bitten later when the calling function happens to sleep when it is not\n * supposed to.\n */\n# define might_sleep() \\\n\tdo { __might_sleep(__FILE__, __LINE__, 0); might_resched(); } while (0)\n/**\n * cant_sleep - annotation for functions that cannot sleep\n *\n * this macro will print a stack trace if it is executed with preemption enabled\n */\n# define cant_sleep() \\\n\tdo { __cant_sleep(__FILE__, __LINE__, 0); } while (0)\n# define sched_annotate_sleep()\t(current->task_state_change = 0)\n\n/**\n * cant_migrate - annotation for functions that cannot migrate\n *\n * Will print a stack trace if executed in code which is migratable\n */\n# define cant_migrate()\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (IS_ENABLED(CONFIG_SMP))\t\t\t\t\\\n\t\t\t__cant_migrate(__FILE__, __LINE__);\t\t\\\n\t} while (0)\n\n/**\n * non_block_start - annotate the start of section where sleeping is prohibited\n *\n * This is on behalf of the oom reaper, specifically when it is calling the mmu\n * notifiers. The problem is that if the notifier were to block on, for example,\n * mutex_lock() and if the process which holds that mutex were to perform a\n * sleeping memory allocation, the oom reaper is now blocked on completion of\n * that memory allocation. Other blocking calls like wait_event() pose similar\n * issues.\n */\n# define non_block_start() (current->non_block_count++)\n/**\n * non_block_end - annotate the end of section where sleeping is prohibited\n *\n * Closes a section opened by non_block_start().\n */\n# define non_block_end() WARN_ON(current->non_block_count-- == 0)\n#else\n  static inline void ___might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n  static inline void __might_sleep(const char *file, int line,\n\t\t\t\t   int preempt_offset) { }\n# define might_sleep() do { might_resched(); } while (0)\n# define cant_sleep() do { } while (0)\n# define cant_migrate()\t\tdo { } while (0)\n# define sched_annotate_sleep() do { } while (0)\n# define non_block_start() do { } while (0)\n# define non_block_end() do { } while (0)\n#endif\n\n#define might_sleep_if(cond) do { if (cond) might_sleep(); } while (0)\n\n#if defined(CONFIG_MMU) && \\\n\t(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))\n#define might_fault() __might_fault(__FILE__, __LINE__)\nvoid __might_fault(const char *file, int line);\n#else\nstatic inline void might_fault(void) { }\n#endif\n\nextern struct atomic_notifier_head panic_notifier_list;\nextern long (*panic_blink)(int state);\n__printf(1, 2)\nvoid panic(const char *fmt, ...) __noreturn __cold;\nvoid nmi_panic(struct pt_regs *regs, const char *msg);\nextern void oops_enter(void);\nextern void oops_exit(void);\nextern bool oops_may_print(void);\nvoid do_exit(long error_code) __noreturn;\nvoid complete_and_exit(struct completion *, long) __noreturn;\n\n/* Internal, do not use. */\nint __must_check _kstrtoul(const char *s, unsigned int base, unsigned long *res);\nint __must_check _kstrtol(const char *s, unsigned int base, long *res);\n\nint __must_check kstrtoull(const char *s, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll(const char *s, unsigned int base, long long *res);\n\n/**\n * kstrtoul - convert a string to an unsigned long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign, but not a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtoul(). Return code must be checked.\n*/\nstatic inline int __must_check kstrtoul(const char *s, unsigned int base, unsigned long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.\n\t */\n\tif (sizeof(unsigned long) == sizeof(unsigned long long) &&\n\t    __alignof__(unsigned long) == __alignof__(unsigned long long))\n\t\treturn kstrtoull(s, base, (unsigned long long *)res);\n\telse\n\t\treturn _kstrtoul(s, base, res);\n}\n\n/**\n * kstrtol - convert a string to a long\n * @s: The start of the string. The string must be null-terminated, and may also\n *  include a single newline before its terminating null. The first character\n *  may also be a plus sign or a minus sign.\n * @base: The number base to use. The maximum supported base is 16. If base is\n *  given as 0, then the base of the string is automatically detected with the\n *  conventional semantics - If it begins with 0x the number will be parsed as a\n *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be\n *  parsed as an octal number. Otherwise it will be parsed as a decimal.\n * @res: Where to write the result of the conversion on success.\n *\n * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.\n * Preferred over simple_strtol(). Return code must be checked.\n */\nstatic inline int __must_check kstrtol(const char *s, unsigned int base, long *res)\n{\n\t/*\n\t * We want to shortcut function call, but\n\t * __builtin_types_compatible_p(long, long long) = 0.\n\t */\n\tif (sizeof(long) == sizeof(long long) &&\n\t    __alignof__(long) == __alignof__(long long))\n\t\treturn kstrtoll(s, base, (long long *)res);\n\telse\n\t\treturn _kstrtol(s, base, res);\n}\n\nint __must_check kstrtouint(const char *s, unsigned int base, unsigned int *res);\nint __must_check kstrtoint(const char *s, unsigned int base, int *res);\n\nstatic inline int __must_check kstrtou64(const char *s, unsigned int base, u64 *res)\n{\n\treturn kstrtoull(s, base, res);\n}\n\nstatic inline int __must_check kstrtos64(const char *s, unsigned int base, s64 *res)\n{\n\treturn kstrtoll(s, base, res);\n}\n\nstatic inline int __must_check kstrtou32(const char *s, unsigned int base, u32 *res)\n{\n\treturn kstrtouint(s, base, res);\n}\n\nstatic inline int __must_check kstrtos32(const char *s, unsigned int base, s32 *res)\n{\n\treturn kstrtoint(s, base, res);\n}\n\nint __must_check kstrtou16(const char *s, unsigned int base, u16 *res);\nint __must_check kstrtos16(const char *s, unsigned int base, s16 *res);\nint __must_check kstrtou8(const char *s, unsigned int base, u8 *res);\nint __must_check kstrtos8(const char *s, unsigned int base, s8 *res);\nint __must_check kstrtobool(const char *s, bool *res);\n\nint __must_check kstrtoull_from_user(const char __user *s, size_t count, unsigned int base, unsigned long long *res);\nint __must_check kstrtoll_from_user(const char __user *s, size_t count, unsigned int base, long long *res);\nint __must_check kstrtoul_from_user(const char __user *s, size_t count, unsigned int base, unsigned long *res);\nint __must_check kstrtol_from_user(const char __user *s, size_t count, unsigned int base, long *res);\nint __must_check kstrtouint_from_user(const char __user *s, size_t count, unsigned int base, unsigned int *res);\nint __must_check kstrtoint_from_user(const char __user *s, size_t count, unsigned int base, int *res);\nint __must_check kstrtou16_from_user(const char __user *s, size_t count, unsigned int base, u16 *res);\nint __must_check kstrtos16_from_user(const char __user *s, size_t count, unsigned int base, s16 *res);\nint __must_check kstrtou8_from_user(const char __user *s, size_t count, unsigned int base, u8 *res);\nint __must_check kstrtos8_from_user(const char __user *s, size_t count, unsigned int base, s8 *res);\nint __must_check kstrtobool_from_user(const char __user *s, size_t count, bool *res);\n\nstatic inline int __must_check kstrtou64_from_user(const char __user *s, size_t count, unsigned int base, u64 *res)\n{\n\treturn kstrtoull_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos64_from_user(const char __user *s, size_t count, unsigned int base, s64 *res)\n{\n\treturn kstrtoll_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtou32_from_user(const char __user *s, size_t count, unsigned int base, u32 *res)\n{\n\treturn kstrtouint_from_user(s, count, base, res);\n}\n\nstatic inline int __must_check kstrtos32_from_user(const char __user *s, size_t count, unsigned int base, s32 *res)\n{\n\treturn kstrtoint_from_user(s, count, base, res);\n}\n\n/*\n * Use kstrto<foo> instead.\n *\n * NOTE: simple_strto<foo> does not check for the range overflow and,\n *\t depending on the input, may give interesting results.\n *\n * Use these functions if and only if you cannot use kstrto<foo>, because\n * the conversion ends on the first non-digit character, which may be far\n * beyond the supported range. It might be useful to parse the strings like\n * 10x50 or 12:21 without altering original string or temporary buffer in use.\n * Keep in mind above caveat.\n */\n\nextern unsigned long simple_strtoul(const char *,char **,unsigned int);\nextern long simple_strtol(const char *,char **,unsigned int);\nextern unsigned long long simple_strtoull(const char *,char **,unsigned int);\nextern long long simple_strtoll(const char *,char **,unsigned int);\n\nextern int num_to_str(char *buf, int size,\n\t\t      unsigned long long num, unsigned int width);\n\n/* lib/printf utilities */\n\nextern __printf(2, 3) int sprintf(char *buf, const char * fmt, ...);\nextern __printf(2, 0) int vsprintf(char *buf, const char *, va_list);\nextern __printf(3, 4)\nint snprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vsnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(3, 4)\nint scnprintf(char *buf, size_t size, const char *fmt, ...);\nextern __printf(3, 0)\nint vscnprintf(char *buf, size_t size, const char *fmt, va_list args);\nextern __printf(2, 3) __malloc\nchar *kasprintf(gfp_t gfp, const char *fmt, ...);\nextern __printf(2, 0) __malloc\nchar *kvasprintf(gfp_t gfp, const char *fmt, va_list args);\nextern __printf(2, 0)\nconst char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);\n\nextern __scanf(2, 3)\nint sscanf(const char *, const char *, ...);\nextern __scanf(2, 0)\nint vsscanf(const char *, const char *, va_list);\n\nextern int get_option(char **str, int *pint);\nextern char *get_options(const char *str, int nints, int *ints);\nextern unsigned long long memparse(const char *ptr, char **retptr);\nextern bool parse_option_str(const char *str, const char *option);\nextern char *next_arg(char *args, char **param, char **val);\n\nextern int core_kernel_text(unsigned long addr);\nextern int init_kernel_text(unsigned long addr);\nextern int core_kernel_data(unsigned long addr);\nextern int __kernel_text_address(unsigned long addr);\nextern int kernel_text_address(unsigned long addr);\nextern int func_ptr_is_kernel_text(void *ptr);\n\n#ifdef CONFIG_SMP\nextern unsigned int sysctl_oops_all_cpu_backtrace;\n#else\n#define sysctl_oops_all_cpu_backtrace 0\n#endif /* CONFIG_SMP */\n\nextern void bust_spinlocks(int yes);\nextern int panic_timeout;\nextern unsigned long panic_print;\nextern int panic_on_oops;\nextern int panic_on_unrecovered_nmi;\nextern int panic_on_io_nmi;\nextern int panic_on_warn;\nextern unsigned long panic_on_taint;\nextern bool panic_on_taint_nousertaint;\nextern int sysctl_panic_on_rcu_stall;\nextern int sysctl_max_rcu_stall_to_panic;\nextern int sysctl_panic_on_stackoverflow;\n\nextern bool crash_kexec_post_notifiers;\n\n/*\n * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It\n * holds a CPU number which is executing panic() currently. A value of\n * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().\n */\nextern atomic_t panic_cpu;\n#define PANIC_CPU_INVALID\t-1\n\n/*\n * Only to be used by arch init code. If the user over-wrote the default\n * CONFIG_PANIC_TIMEOUT, honor it.\n */\nstatic inline void set_arch_panic_timeout(int timeout, int arch_default_timeout)\n{\n\tif (panic_timeout == arch_default_timeout)\n\t\tpanic_timeout = timeout;\n}\nextern const char *print_tainted(void);\nenum lockdep_ok {\n\tLOCKDEP_STILL_OK,\n\tLOCKDEP_NOW_UNRELIABLE\n};\nextern void add_taint(unsigned flag, enum lockdep_ok);\nextern int test_taint(unsigned flag);\nextern unsigned long get_taint(void);\nextern int root_mountflags;\n\nextern bool early_boot_irqs_disabled;\n\n/*\n * Values used for system_state. Ordering of the states must not be changed\n * as code checks for <, <=, >, >= STATE.\n */\nextern enum system_states {\n\tSYSTEM_BOOTING,\n\tSYSTEM_SCHEDULING,\n\tSYSTEM_RUNNING,\n\tSYSTEM_HALT,\n\tSYSTEM_POWER_OFF,\n\tSYSTEM_RESTART,\n\tSYSTEM_SUSPEND,\n} system_state;\n\n/* This cannot be an enum because some may be used in assembly source. */\n#define TAINT_PROPRIETARY_MODULE\t0\n#define TAINT_FORCED_MODULE\t\t1\n#define TAINT_CPU_OUT_OF_SPEC\t\t2\n#define TAINT_FORCED_RMMOD\t\t3\n#define TAINT_MACHINE_CHECK\t\t4\n#define TAINT_BAD_PAGE\t\t\t5\n#define TAINT_USER\t\t\t6\n#define TAINT_DIE\t\t\t7\n#define TAINT_OVERRIDDEN_ACPI_TABLE\t8\n#define TAINT_WARN\t\t\t9\n#define TAINT_CRAP\t\t\t10\n#define TAINT_FIRMWARE_WORKAROUND\t11\n#define TAINT_OOT_MODULE\t\t12\n#define TAINT_UNSIGNED_MODULE\t\t13\n#define TAINT_SOFTLOCKUP\t\t14\n#define TAINT_LIVEPATCH\t\t\t15\n#define TAINT_AUX\t\t\t16\n#define TAINT_RANDSTRUCT\t\t17\n#define TAINT_FLAGS_COUNT\t\t18\n#define TAINT_FLAGS_MAX\t\t\t((1UL << TAINT_FLAGS_COUNT) - 1)\n\nstruct taint_flag {\n\tchar c_true;\t/* character printed when tainted */\n\tchar c_false;\t/* character printed when not tainted */\n\tbool module;\t/* also show as a per-module taint flag */\n};\n\nextern const struct taint_flag taint_flags[TAINT_FLAGS_COUNT];\n\nextern const char hex_asc[];\n#define hex_asc_lo(x)\thex_asc[((x) & 0x0f)]\n#define hex_asc_hi(x)\thex_asc[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_hi(byte);\n\t*buf++ = hex_asc_lo(byte);\n\treturn buf;\n}\n\nextern const char hex_asc_upper[];\n#define hex_asc_upper_lo(x)\thex_asc_upper[((x) & 0x0f)]\n#define hex_asc_upper_hi(x)\thex_asc_upper[((x) & 0xf0) >> 4]\n\nstatic inline char *hex_byte_pack_upper(char *buf, u8 byte)\n{\n\t*buf++ = hex_asc_upper_hi(byte);\n\t*buf++ = hex_asc_upper_lo(byte);\n\treturn buf;\n}\n\nextern int hex_to_bin(char ch);\nextern int __must_check hex2bin(u8 *dst, const char *src, size_t count);\nextern char *bin2hex(char *dst, const void *src, size_t count);\n\nbool mac_pton(const char *s, u8 *mac);\n\n/*\n * General tracing related utility functions - trace_printk(),\n * tracing_on/tracing_off and tracing_start()/tracing_stop\n *\n * Use tracing_on/tracing_off when you want to quickly turn on or off\n * tracing. It simply enables or disables the recording of the trace events.\n * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on\n * file, which gives a means for the kernel and userspace to interact.\n * Place a tracing_off() in the kernel where you want tracing to end.\n * From user space, examine the trace, and then echo 1 > tracing_on\n * to continue tracing.\n *\n * tracing_stop/tracing_start has slightly more overhead. It is used\n * by things like suspend to ram where disabling the recording of the\n * trace is not enough, but tracing must actually stop because things\n * like calling smp_processor_id() may crash the system.\n *\n * Most likely, you want to use tracing_on/tracing_off.\n */\n\nenum ftrace_dump_mode {\n\tDUMP_NONE,\n\tDUMP_ALL,\n\tDUMP_ORIG,\n};\n\n#ifdef CONFIG_TRACING\nvoid tracing_on(void);\nvoid tracing_off(void);\nint tracing_is_on(void);\nvoid tracing_snapshot(void);\nvoid tracing_snapshot_alloc(void);\n\nextern void tracing_start(void);\nextern void tracing_stop(void);\n\nstatic inline __printf(1, 2)\nvoid ____trace_printk_check_format(const char *fmt, ...)\n{\n}\n#define __trace_printk_check_format(fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\t____trace_printk_check_format(fmt, ##args);\t\t\\\n} while (0)\n\n/**\n * trace_printk - printf formatting in the ftrace buffer\n * @fmt: the printf format for printing\n *\n * Note: __trace_printk is an internal function for trace_printk() and\n *       the @ip is passed in via the trace_printk() macro.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_printks scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_printk() is used.)\n *\n * A little optimization trick is done here. If there's only one\n * argument, there's no need to scan the string for printf formats.\n * The trace_puts() will suffice. But how can we take advantage of\n * using trace_puts() when trace_printk() has only one argument?\n * By stringifying the args and checking the size we can tell\n * whether or not there are args. __stringify((__VA_ARGS__)) will\n * turn into \"()\\0\" with a size of 3 when there are no args, anything\n * else will be bigger. All we need to do is define a string to this,\n * and then take its size and compare to 3. If it's bigger, use\n * do_trace_printk() otherwise, optimize it to trace_puts(). Then just\n * let gcc optimize the rest.\n */\n\n#define trace_printk(fmt, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar _______STR[] = __stringify((__VA_ARGS__));\t\\\n\tif (sizeof(_______STR) > 3)\t\t\t\\\n\t\tdo_trace_printk(fmt, ##__VA_ARGS__);\t\\\n\telse\t\t\t\t\t\t\\\n\t\ttrace_puts(fmt);\t\t\t\\\n} while (0)\n\n#define do_trace_printk(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt))\t\t\t\t\t\\\n\t\t__trace_bprintk(_THIS_IP_, trace_printk_fmt, ##args);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(_THIS_IP_, fmt, ##args);\t\t\t\\\n} while (0)\n\nextern __printf(2, 3)\nint __trace_bprintk(unsigned long ip, const char *fmt, ...);\n\nextern __printf(2, 3)\nint __trace_printk(unsigned long ip, const char *fmt, ...);\n\n/**\n * trace_puts - write a string into the ftrace buffer\n * @str: the string to record\n *\n * Note: __trace_bputs is an internal function for trace_puts and\n *       the @ip is passed in via the trace_puts macro.\n *\n * This is similar to trace_printk() but is made for those really fast\n * paths that a developer wants the least amount of \"Heisenbug\" effects,\n * where the processing of the print format is still too much.\n *\n * This function allows a kernel developer to debug fast path sections\n * that printk is not appropriate for. By scattering in various\n * printk like tracing in the code, a developer can quickly see\n * where problems are occurring.\n *\n * This is intended as a debugging tool for the developer only.\n * Please refrain from leaving trace_puts scattered around in\n * your code. (Extra memory is used for special buffers that are\n * allocated when trace_puts() is used.)\n *\n * Returns: 0 if nothing was written, positive # if string was.\n *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)\n */\n\n#define trace_puts(str) ({\t\t\t\t\t\t\\\n\tstatic const char *trace_printk_fmt __used\t\t\t\\\n\t\t__section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t__builtin_constant_p(str) ? str : NULL;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(str))\t\t\t\t\t\\\n\t\t__trace_bputs(_THIS_IP_, trace_printk_fmt);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t__trace_puts(_THIS_IP_, str, strlen(str));\t\t\\\n})\nextern int __trace_bputs(unsigned long ip, const char *str);\nextern int __trace_puts(unsigned long ip, const char *str, int size);\n\nextern void trace_dump_stack(int skip);\n\n/*\n * The double __builtin_constant_p is because gcc will give us an error\n * if we try to allocate the static variable to fmt if it is not a\n * constant. Even with the outer if statement.\n */\n#define ftrace_vprintk(fmt, vargs)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt __used\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vbprintk(_THIS_IP_, trace_printk_fmt, vargs);\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__ftrace_vprintk(_THIS_IP_, fmt, vargs);\t\t\\\n} while (0)\n\nextern __printf(2, 0) int\n__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern __printf(2, 0) int\n__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);\n\nextern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);\n#else\nstatic inline void tracing_start(void) { }\nstatic inline void tracing_stop(void) { }\nstatic inline void trace_dump_stack(int skip) { }\n\nstatic inline void tracing_on(void) { }\nstatic inline void tracing_off(void) { }\nstatic inline int tracing_is_on(void) { return 0; }\nstatic inline void tracing_snapshot(void) { }\nstatic inline void tracing_snapshot_alloc(void) { }\n\nstatic inline __printf(1, 2)\nint trace_printk(const char *fmt, ...)\n{\n\treturn 0;\n}\nstatic __printf(1, 0) inline int\nftrace_vprintk(const char *fmt, va_list ap)\n{\n\treturn 0;\n}\nstatic inline void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }\n#endif /* CONFIG_TRACING */\n\n/* This counts to 12. Any more, it will return 13th argument. */\n#define __COUNT_ARGS(_0, _1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _n, X...) _n\n#define COUNT_ARGS(X...) __COUNT_ARGS(, ##X, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n#define __CONCAT(a, b) a ## b\n#define CONCATENATE(a, b) __CONCAT(a, b)\n\n/**\n * container_of - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n */\n#define container_of(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\t((type *)(__mptr - offsetof(type, member))); })\n\n/**\n * container_of_safe - cast a member of a structure out to the containing structure\n * @ptr:\tthe pointer to the member.\n * @type:\tthe type of the container struct this is embedded in.\n * @member:\tthe name of the member within the struct.\n *\n * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.\n */\n#define container_of_safe(ptr, type, member) ({\t\t\t\t\\\n\tvoid *__mptr = (void *)(ptr);\t\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(!__same_type(*(ptr), ((type *)0)->member) &&\t\\\n\t\t\t !__same_type(*(ptr), void),\t\t\t\\\n\t\t\t \"pointer type mismatch in container_of()\");\t\\\n\tIS_ERR_OR_NULL(__mptr) ? ERR_CAST(__mptr) :\t\t\t\\\n\t\t((type *)(__mptr - offsetof(type, member))); })\n\n/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */\n#ifdef CONFIG_FTRACE_MCOUNT_RECORD\n# define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD\n#endif\n\n/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */\n#define VERIFY_OCTAL_PERMISSIONS(perms)\t\t\t\t\t\t\\\n\t(BUILD_BUG_ON_ZERO((perms) < 0) +\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) > 0777) +\t\t\t\t\t\\\n\t /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 4) < (((perms) >> 3) & 4)) +\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 3) & 4) < ((perms) & 4)) +\t\t\\\n\t /* USER_WRITABLE >= GROUP_WRITABLE */\t\t\t\t\t\\\n\t BUILD_BUG_ON_ZERO((((perms) >> 6) & 2) < (((perms) >> 3) & 2)) +\t\\\n\t /* OTHER_WRITABLE?  Generally considered a bad idea. */\t\t\\\n\t BUILD_BUG_ON_ZERO((perms) & 2) +\t\t\t\t\t\\\n\t (perms))\n#endif\n"}, "4": {"id": 4, "path": "/src/include/linux/build_bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BUILD_BUG_H\n#define _LINUX_BUILD_BUG_H\n\n#include <linux/compiler.h>\n\n#ifdef __CHECKER__\n#define BUILD_BUG_ON_ZERO(e) (0)\n#else /* __CHECKER__ */\n/*\n * Force a compilation error if condition is true, but also produce a\n * result (of value 0 and type int), so the expression can be used\n * e.g. in a structure initializer (or where-ever else comma expressions\n * aren't permitted).\n */\n#define BUILD_BUG_ON_ZERO(e) ((int)(sizeof(struct { int:(-!!(e)); })))\n#endif /* __CHECKER__ */\n\n/* Force a compilation error if a constant expression is not a power of 2 */\n#define __BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\\\n\tBUILD_BUG_ON(((n) & ((n) - 1)) != 0)\n#define BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\t\t\\\n\tBUILD_BUG_ON((n) == 0 || (((n) & ((n) - 1)) != 0))\n\n/*\n * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the\n * expression but avoids the generation of any code, even if that expression\n * has side-effects.\n */\n#define BUILD_BUG_ON_INVALID(e) ((void)(sizeof((__force long)(e))))\n\n/**\n * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied\n *\t\t      error message.\n * @condition: the condition which the compiler should know is false.\n *\n * See BUILD_BUG_ON for description.\n */\n#define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)\n\n/**\n * BUILD_BUG_ON - break compile if a condition is true.\n * @condition: the condition which the compiler should know is false.\n *\n * If you have some code which relies on certain constants being equal, or\n * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to\n * detect if someone changes it.\n */\n#define BUILD_BUG_ON(condition) \\\n\tBUILD_BUG_ON_MSG(condition, \"BUILD_BUG_ON failed: \" #condition)\n\n/**\n * BUILD_BUG - break compile if used.\n *\n * If you have some code that you expect the compiler to eliminate at\n * build time, you should use BUILD_BUG to detect if it is\n * unexpectedly used.\n */\n#define BUILD_BUG() BUILD_BUG_ON_MSG(1, \"BUILD_BUG failed\")\n\n/**\n * static_assert - check integer constant expression at build time\n *\n * static_assert() is a wrapper for the C11 _Static_assert, with a\n * little macro magic to make the message optional (defaulting to the\n * stringification of the tested expression).\n *\n * Contrary to BUILD_BUG_ON(), static_assert() can be used at global\n * scope, but requires the expression to be an integer constant\n * expression (i.e., it is not enough that __builtin_constant_p() is\n * true for expr).\n *\n * Also note that BUILD_BUG_ON() fails the build if the condition is\n * true, while static_assert() fails the build if the expression is\n * false.\n */\n#define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr)\n#define __static_assert(expr, msg, ...) _Static_assert(expr, msg)\n\n#endif\t/* _LINUX_BUILD_BUG_H */\n"}, "5": {"id": 5, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#define __must_check\t\t__attribute__((__warn_unused_result__))\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "6": {"id": 6, "path": "/src/include/linux/overflow.h", "content": "/* SPDX-License-Identifier: GPL-2.0 OR MIT */\n#ifndef __LINUX_OVERFLOW_H\n#define __LINUX_OVERFLOW_H\n\n#include <linux/compiler.h>\n#include <linux/limits.h>\n\n/*\n * In the fallback code below, we need to compute the minimum and\n * maximum values representable in a given type. These macros may also\n * be useful elsewhere, so we provide them outside the\n * COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW block.\n *\n * It would seem more obvious to do something like\n *\n * #define type_min(T) (T)(is_signed_type(T) ? (T)1 << (8*sizeof(T)-1) : 0)\n * #define type_max(T) (T)(is_signed_type(T) ? ((T)1 << (8*sizeof(T)-1)) - 1 : ~(T)0)\n *\n * Unfortunately, the middle expressions, strictly speaking, have\n * undefined behaviour, and at least some versions of gcc warn about\n * the type_max expression (but not if -fsanitize=undefined is in\n * effect; in that case, the warning is deferred to runtime...).\n *\n * The slightly excessive casting in type_min is to make sure the\n * macros also produce sensible values for the exotic type _Bool. [The\n * overflow checkers only almost work for _Bool, but that's\n * a-feature-not-a-bug, since people shouldn't be doing arithmetic on\n * _Bools. Besides, the gcc builtins don't allow _Bool* as third\n * argument.]\n *\n * Idea stolen from\n * https://mail-index.netbsd.org/tech-misc/2007/02/05/0000.html -\n * credit to Christian Biere.\n */\n#define is_signed_type(type)       (((type)(-1)) < (type)1)\n#define __type_half_max(type) ((type)1 << (8*sizeof(type) - 1 - is_signed_type(type)))\n#define type_max(T) ((T)((__type_half_max(T) - 1) + __type_half_max(T)))\n#define type_min(T) ((T)((T)-type_max(T)-(T)1))\n\n/*\n * Avoids triggering -Wtype-limits compilation warning,\n * while using unsigned data types to check a < 0.\n */\n#define is_non_negative(a) ((a) > 0 || (a) == 0)\n#define is_negative(a) (!(is_non_negative(a)))\n\n/*\n * Allows for effectively applying __must_check to a macro so we can have\n * both the type-agnostic benefits of the macros while also being able to\n * enforce that the return value is, in fact, checked.\n */\nstatic inline bool __must_check __must_check_overflow(bool overflow)\n{\n\treturn unlikely(overflow);\n}\n\n#ifdef COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW\n/*\n * For simplicity and code hygiene, the fallback code below insists on\n * a, b and *d having the same type (similar to the min() and max()\n * macros), whereas gcc's type-generic overflow checkers accept\n * different types. Hence we don't just make check_add_overflow an\n * alias for __builtin_add_overflow, but add type checks similar to\n * below.\n */\n#define check_add_overflow(a, b, d) __must_check_overflow(({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t__builtin_add_overflow(__a, __b, __d);\t\\\n}))\n\n#define check_sub_overflow(a, b, d) __must_check_overflow(({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t__builtin_sub_overflow(__a, __b, __d);\t\\\n}))\n\n#define check_mul_overflow(a, b, d) __must_check_overflow(({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t__builtin_mul_overflow(__a, __b, __d);\t\\\n}))\n\n#else\n\n\n/* Checking for unsigned overflow is relatively easy without causing UB. */\n#define __unsigned_add_overflow(a, b, d) ({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t*__d = __a + __b;\t\t\t\\\n\t*__d < __a;\t\t\t\t\\\n})\n#define __unsigned_sub_overflow(a, b, d) ({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t*__d = __a - __b;\t\t\t\\\n\t__a < __b;\t\t\t\t\\\n})\n/*\n * If one of a or b is a compile-time constant, this avoids a division.\n */\n#define __unsigned_mul_overflow(a, b, d) ({\t\t\\\n\ttypeof(a) __a = (a);\t\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\t\\\n\t*__d = __a * __b;\t\t\t\t\\\n\t__builtin_constant_p(__b) ?\t\t\t\\\n\t  __b > 0 && __a > type_max(typeof(__a)) / __b : \\\n\t  __a > 0 && __b > type_max(typeof(__b)) / __a;\t \\\n})\n\n/*\n * For signed types, detecting overflow is much harder, especially if\n * we want to avoid UB. But the interface of these macros is such that\n * we must provide a result in *d, and in fact we must produce the\n * result promised by gcc's builtins, which is simply the possibly\n * wrapped-around value. Fortunately, we can just formally do the\n * operations in the widest relevant unsigned type (u64) and then\n * truncate the result - gcc is smart enough to generate the same code\n * with and without the (u64) casts.\n */\n\n/*\n * Adding two signed integers can overflow only if they have the same\n * sign, and overflow has happened iff the result has the opposite\n * sign.\n */\n#define __signed_add_overflow(a, b, d) ({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t*__d = (u64)__a + (u64)__b;\t\t\\\n\t(((~(__a ^ __b)) & (*__d ^ __a))\t\\\n\t\t& type_min(typeof(__a))) != 0;\t\\\n})\n\n/*\n * Subtraction is similar, except that overflow can now happen only\n * when the signs are opposite. In this case, overflow has happened if\n * the result has the opposite sign of a.\n */\n#define __signed_sub_overflow(a, b, d) ({\t\\\n\ttypeof(a) __a = (a);\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\\\n\t*__d = (u64)__a - (u64)__b;\t\t\\\n\t((((__a ^ __b)) & (*__d ^ __a))\t\t\\\n\t\t& type_min(typeof(__a))) != 0;\t\\\n})\n\n/*\n * Signed multiplication is rather hard. gcc always follows C99, so\n * division is truncated towards 0. This means that we can write the\n * overflow check like this:\n *\n * (a > 0 && (b > MAX/a || b < MIN/a)) ||\n * (a < -1 && (b > MIN/a || b < MAX/a) ||\n * (a == -1 && b == MIN)\n *\n * The redundant casts of -1 are to silence an annoying -Wtype-limits\n * (included in -Wextra) warning: When the type is u8 or u16, the\n * __b_c_e in check_mul_overflow obviously selects\n * __unsigned_mul_overflow, but unfortunately gcc still parses this\n * code and warns about the limited range of __b.\n */\n\n#define __signed_mul_overflow(a, b, d) ({\t\t\t\t\\\n\ttypeof(a) __a = (a);\t\t\t\t\t\t\\\n\ttypeof(b) __b = (b);\t\t\t\t\t\t\\\n\ttypeof(d) __d = (d);\t\t\t\t\t\t\\\n\ttypeof(a) __tmax = type_max(typeof(a));\t\t\t\t\\\n\ttypeof(a) __tmin = type_min(typeof(a));\t\t\t\t\\\n\t(void) (&__a == &__b);\t\t\t\t\t\t\\\n\t(void) (&__a == __d);\t\t\t\t\t\t\\\n\t*__d = (u64)__a * (u64)__b;\t\t\t\t\t\\\n\t(__b > 0   && (__a > __tmax/__b || __a < __tmin/__b)) ||\t\\\n\t(__b < (typeof(__b))-1  && (__a > __tmin/__b || __a < __tmax/__b)) || \\\n\t(__b == (typeof(__b))-1 && __a == __tmin);\t\t\t\\\n})\n\n\n#define check_add_overflow(a, b, d)\t__must_check_overflow(\t\t\\\n\t__builtin_choose_expr(is_signed_type(typeof(a)),\t\t\\\n\t\t\t__signed_add_overflow(a, b, d),\t\t\t\\\n\t\t\t__unsigned_add_overflow(a, b, d)))\n\n#define check_sub_overflow(a, b, d)\t__must_check_overflow(\t\t\\\n\t__builtin_choose_expr(is_signed_type(typeof(a)),\t\t\\\n\t\t\t__signed_sub_overflow(a, b, d),\t\t\t\\\n\t\t\t__unsigned_sub_overflow(a, b, d)))\n\n#define check_mul_overflow(a, b, d)\t__must_check_overflow(\t\t\\\n\t__builtin_choose_expr(is_signed_type(typeof(a)),\t\t\\\n\t\t\t__signed_mul_overflow(a, b, d),\t\t\t\\\n\t\t\t__unsigned_mul_overflow(a, b, d)))\n\n#endif /* COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW */\n\n/** check_shl_overflow() - Calculate a left-shifted value and check overflow\n *\n * @a: Value to be shifted\n * @s: How many bits left to shift\n * @d: Pointer to where to store the result\n *\n * Computes *@d = (@a << @s)\n *\n * Returns true if '*d' cannot hold the result or when 'a << s' doesn't\n * make sense. Example conditions:\n * - 'a << s' causes bits to be lost when stored in *d.\n * - 's' is garbage (e.g. negative) or so large that the result of\n *   'a << s' is guaranteed to be 0.\n * - 'a' is negative.\n * - 'a << s' sets the sign bit, if any, in '*d'.\n *\n * '*d' will hold the results of the attempted shift, but is not\n * considered \"safe for use\" if false is returned.\n */\n#define check_shl_overflow(a, s, d) __must_check_overflow(({\t\t\\\n\ttypeof(a) _a = a;\t\t\t\t\t\t\\\n\ttypeof(s) _s = s;\t\t\t\t\t\t\\\n\ttypeof(d) _d = d;\t\t\t\t\t\t\\\n\tu64 _a_full = _a;\t\t\t\t\t\t\\\n\tunsigned int _to_shift =\t\t\t\t\t\\\n\t\tis_non_negative(_s) && _s < 8 * sizeof(*d) ? _s : 0;\t\\\n\t*_d = (_a_full << _to_shift);\t\t\t\t\t\\\n\t(_to_shift != _s || is_negative(*_d) || is_negative(_a) ||\t\\\n\t(*_d >> _to_shift) != _a);\t\t\t\t\t\\\n}))\n\n/**\n * array_size() - Calculate size of 2-dimensional array.\n *\n * @a: dimension one\n * @b: dimension two\n *\n * Calculates size of 2-dimensional array: @a * @b.\n *\n * Returns: number of bytes needed to represent the array or SIZE_MAX on\n * overflow.\n */\nstatic inline __must_check size_t array_size(size_t a, size_t b)\n{\n\tsize_t bytes;\n\n\tif (check_mul_overflow(a, b, &bytes))\n\t\treturn SIZE_MAX;\n\n\treturn bytes;\n}\n\n/**\n * array3_size() - Calculate size of 3-dimensional array.\n *\n * @a: dimension one\n * @b: dimension two\n * @c: dimension three\n *\n * Calculates size of 3-dimensional array: @a * @b * @c.\n *\n * Returns: number of bytes needed to represent the array or SIZE_MAX on\n * overflow.\n */\nstatic inline __must_check size_t array3_size(size_t a, size_t b, size_t c)\n{\n\tsize_t bytes;\n\n\tif (check_mul_overflow(a, b, &bytes))\n\t\treturn SIZE_MAX;\n\tif (check_mul_overflow(bytes, c, &bytes))\n\t\treturn SIZE_MAX;\n\n\treturn bytes;\n}\n\n/*\n * Compute a*b+c, returning SIZE_MAX on overflow. Internal helper for\n * struct_size() below.\n */\nstatic inline __must_check size_t __ab_c_size(size_t a, size_t b, size_t c)\n{\n\tsize_t bytes;\n\n\tif (check_mul_overflow(a, b, &bytes))\n\t\treturn SIZE_MAX;\n\tif (check_add_overflow(bytes, c, &bytes))\n\t\treturn SIZE_MAX;\n\n\treturn bytes;\n}\n\n/**\n * struct_size() - Calculate size of structure with trailing array.\n * @p: Pointer to the structure.\n * @member: Name of the array member.\n * @count: Number of elements in the array.\n *\n * Calculates size of memory needed for structure @p followed by an\n * array of @count number of @member elements.\n *\n * Return: number of bytes needed or SIZE_MAX on overflow.\n */\n#define struct_size(p, member, count)\t\t\t\t\t\\\n\t__ab_c_size(count,\t\t\t\t\t\t\\\n\t\t    sizeof(*(p)->member) + __must_be_array((p)->member),\\\n\t\t    sizeof(*(p)))\n\n/**\n * flex_array_size() - Calculate size of a flexible array member\n *                     within an enclosing structure.\n *\n * @p: Pointer to the structure.\n * @member: Name of the flexible array member.\n * @count: Number of elements in the array.\n *\n * Calculates size of a flexible array of @count number of @member\n * elements, at the end of structure @p.\n *\n * Return: number of bytes needed or SIZE_MAX on overflow.\n */\n#define flex_array_size(p, member, count)\t\t\t\t\\\n\tarray_size(count,\t\t\t\t\t\t\\\n\t\t    sizeof(*(p)->member) + __must_be_array((p)->member))\n\n#endif /* __LINUX_OVERFLOW_H */\n"}, "7": {"id": 7, "path": "/src/include/linux/compiler.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_H\n#define __LINUX_COMPILER_H\n\n#include <linux/compiler_types.h>\n\n#ifndef __ASSEMBLY__\n\n#ifdef __KERNEL__\n\n/*\n * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code\n * to disable branch tracing on a per file basis.\n */\n#if defined(CONFIG_TRACE_BRANCH_PROFILING) \\\n    && !defined(DISABLE_BRANCH_PROFILING) && !defined(__CHECKER__)\nvoid ftrace_likely_update(struct ftrace_likely_data *f, int val,\n\t\t\t  int expect, int is_constant);\n\n#define likely_notrace(x)\t__builtin_expect(!!(x), 1)\n#define unlikely_notrace(x)\t__builtin_expect(!!(x), 0)\n\n#define __branch_check__(x, expect, is_constant) ({\t\t\t\\\n\t\t\tlong ______r;\t\t\t\t\t\\\n\t\t\tstatic struct ftrace_likely_data\t\t\\\n\t\t\t\t__aligned(4)\t\t\t\t\\\n\t\t\t\t__section(\"_ftrace_annotated_branch\")\t\\\n\t\t\t\t______f = {\t\t\t\t\\\n\t\t\t\t.data.func = __func__,\t\t\t\\\n\t\t\t\t.data.file = __FILE__,\t\t\t\\\n\t\t\t\t.data.line = __LINE__,\t\t\t\\\n\t\t\t};\t\t\t\t\t\t\\\n\t\t\t______r = __builtin_expect(!!(x), expect);\t\\\n\t\t\tftrace_likely_update(&______f, ______r,\t\t\\\n\t\t\t\t\t     expect, is_constant);\t\\\n\t\t\t______r;\t\t\t\t\t\\\n\t\t})\n\n/*\n * Using __builtin_constant_p(x) to ignore cases where the return\n * value is always the same.  This idea is taken from a similar patch\n * written by Daniel Walker.\n */\n# ifndef likely\n#  define likely(x)\t(__branch_check__(x, 1, __builtin_constant_p(x)))\n# endif\n# ifndef unlikely\n#  define unlikely(x)\t(__branch_check__(x, 0, __builtin_constant_p(x)))\n# endif\n\n#ifdef CONFIG_PROFILE_ALL_BRANCHES\n/*\n * \"Define 'is'\", Bill Clinton\n * \"Define 'if'\", Steven Rostedt\n */\n#define if(cond, ...) if ( __trace_if_var( !!(cond , ## __VA_ARGS__) ) )\n\n#define __trace_if_var(cond) (__builtin_constant_p(cond) ? (cond) : __trace_if_value(cond))\n\n#define __trace_if_value(cond) ({\t\t\t\\\n\tstatic struct ftrace_branch_data\t\t\\\n\t\t__aligned(4)\t\t\t\t\\\n\t\t__section(\"_ftrace_branch\")\t\t\\\n\t\t__if_trace = {\t\t\t\t\\\n\t\t\t.func = __func__,\t\t\\\n\t\t\t.file = __FILE__,\t\t\\\n\t\t\t.line = __LINE__,\t\t\\\n\t\t};\t\t\t\t\t\\\n\t(cond) ?\t\t\t\t\t\\\n\t\t(__if_trace.miss_hit[1]++,1) :\t\t\\\n\t\t(__if_trace.miss_hit[0]++,0);\t\t\\\n})\n\n#endif /* CONFIG_PROFILE_ALL_BRANCHES */\n\n#else\n# define likely(x)\t__builtin_expect(!!(x), 1)\n# define unlikely(x)\t__builtin_expect(!!(x), 0)\n#endif\n\n/* Optimization barrier */\n#ifndef barrier\n/* The \"volatile\" is due to gcc bugs */\n# define barrier() __asm__ __volatile__(\"\": : :\"memory\")\n#endif\n\n#ifndef barrier_data\n/*\n * This version is i.e. to prevent dead stores elimination on @ptr\n * where gcc and llvm may behave differently when otherwise using\n * normal barrier(): while gcc behavior gets along with a normal\n * barrier(), llvm needs an explicit input variable to be assumed\n * clobbered. The issue is as follows: while the inline asm might\n * access any memory it wants, the compiler could have fit all of\n * @ptr into memory registers instead, and since @ptr never escaped\n * from that, it proved that the inline asm wasn't touching any of\n * it. This version works well with both compilers, i.e. we're telling\n * the compiler that the inline asm absolutely may see the contents\n * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495\n */\n# define barrier_data(ptr) __asm__ __volatile__(\"\": :\"r\"(ptr) :\"memory\")\n#endif\n\n/* workaround for GCC PR82365 if needed */\n#ifndef barrier_before_unreachable\n# define barrier_before_unreachable() do { } while (0)\n#endif\n\n/* Unreachable code */\n#ifdef CONFIG_STACK_VALIDATION\n/*\n * These macros help objtool understand GCC code flow for unreachable code.\n * The __COUNTER__ based labels are a hack to make each instance of the macros\n * unique, to convince GCC not to merge duplicate inline asm statements.\n */\n#define annotate_reachable() ({\t\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.reachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define annotate_unreachable() ({\t\t\t\t\t\\\n\tasm volatile(\"%c0:\\n\\t\"\t\t\t\t\t\t\\\n\t\t     \".pushsection .discard.unreachable\\n\\t\"\t\t\\\n\t\t     \".long %c0b - .\\n\\t\"\t\t\t\t\\\n\t\t     \".popsection\\n\\t\" : : \"i\" (__COUNTER__));\t\t\\\n})\n#define ASM_UNREACHABLE\t\t\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\t\\\n\t\".pushsection .discard.unreachable\\n\\t\"\t\t\t\t\\\n\t\".long 999b - .\\n\\t\"\t\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\n/* Annotate a C jump table to allow objtool to follow the code flow */\n#define __annotate_jump_table __section(\".rodata..c_jump_table\")\n\n#else\n#define annotate_reachable()\n#define annotate_unreachable()\n#define __annotate_jump_table\n#endif\n\n#ifndef ASM_UNREACHABLE\n# define ASM_UNREACHABLE\n#endif\n#ifndef unreachable\n# define unreachable() do {\t\t\\\n\tannotate_unreachable();\t\t\\\n\t__builtin_unreachable();\t\\\n} while (0)\n#endif\n\n/*\n * KENTRY - kernel entry point\n * This can be used to annotate symbols (functions or data) that are used\n * without their linker symbol being referenced explicitly. For example,\n * interrupt vector handlers, or functions in the kernel image that are found\n * programatically.\n *\n * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those\n * are handled in their own way (with KEEP() in linker scripts).\n *\n * KENTRY can be avoided if the symbols in question are marked as KEEP() in the\n * linker script. For example an architecture could KEEP() its entire\n * boot/exception vector code rather than annotate each function and data.\n */\n#ifndef KENTRY\n# define KENTRY(sym)\t\t\t\t\t\t\\\n\textern typeof(sym) sym;\t\t\t\t\t\\\n\tstatic const unsigned long __kentry_##sym\t\t\\\n\t__used\t\t\t\t\t\t\t\\\n\t__attribute__((__section__(\"___kentry+\" #sym)))\t\t\\\n\t= (unsigned long)&sym;\n#endif\n\n#ifndef RELOC_HIDE\n# define RELOC_HIDE(ptr, off)\t\t\t\t\t\\\n  ({ unsigned long __ptr;\t\t\t\t\t\\\n     __ptr = (unsigned long) (ptr);\t\t\t\t\\\n    (typeof(ptr)) (__ptr + (off)); })\n#endif\n\n#ifndef OPTIMIZER_HIDE_VAR\n/* Make the optimizer believe the variable can be manipulated arbitrarily. */\n#define OPTIMIZER_HIDE_VAR(var)\t\t\t\t\t\t\\\n\t__asm__ (\"\" : \"=r\" (var) : \"0\" (var))\n#endif\n\n/* Not-quite-unique ID. */\n#ifndef __UNIQUE_ID\n# define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __LINE__)\n#endif\n\n/**\n * data_race - mark an expression as containing intentional data races\n *\n * This data_race() macro is useful for situations in which data races\n * should be forgiven.  One example is diagnostic code that accesses\n * shared variables but is not a part of the core synchronization design.\n *\n * This macro *does not* affect normal code generation, but is a hint\n * to tooling that data races here are to be ignored.\n */\n#define data_race(expr)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__unqual_scalar_typeof(({ expr; })) __v = ({\t\t\t\\\n\t\t__kcsan_disable_current();\t\t\t\t\\\n\t\texpr;\t\t\t\t\t\t\t\\\n\t});\t\t\t\t\t\t\t\t\\\n\t__kcsan_enable_current();\t\t\t\t\t\\\n\t__v;\t\t\t\t\t\t\t\t\\\n})\n\n#endif /* __KERNEL__ */\n\n/*\n * Force the compiler to emit 'sym' as a symbol, so that we can reference\n * it from inline assembler. Necessary in case 'sym' could be inlined\n * otherwise, or eliminated entirely due to lack of references that are\n * visible to the compiler.\n */\n#define __ADDRESSABLE(sym) \\\n\tstatic void * __section(\".discard.addressable\") __used \\\n\t\t__UNIQUE_ID(__PASTE(__addressable_,sym)) = (void *)&sym;\n\n/**\n * offset_to_ptr - convert a relative memory offset to an absolute pointer\n * @off:\tthe address of the 32-bit offset value\n */\nstatic inline void *offset_to_ptr(const int *off)\n{\n\treturn (void *)((unsigned long)off + *off);\n}\n\n#endif /* __ASSEMBLY__ */\n\n/* &a[0] degrades to a pointer: a different type from an array */\n#define __must_be_array(a)\tBUILD_BUG_ON_ZERO(__same_type((a), &(a)[0]))\n\n/*\n * This is needed in functions which generate the stack canary, see\n * arch/x86/kernel/smpboot.c::start_secondary() for an example.\n */\n#define prevent_tail_call_optimization()\tmb()\n\n#include <asm/rwonce.h>\n\n#endif /* __LINUX_COMPILER_H */\n"}, "8": {"id": 8, "path": "/src/include/linux/err.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_ERR_H\n#define _LINUX_ERR_H\n\n#include <linux/compiler.h>\n#include <linux/types.h>\n\n#include <asm/errno.h>\n\n/*\n * Kernel pointers have redundant information, so we can use a\n * scheme where we can return either an error code or a normal\n * pointer with the same return value.\n *\n * This should be a per-architecture thing, to allow different\n * error and pointer decisions.\n */\n#define MAX_ERRNO\t4095\n\n#ifndef __ASSEMBLY__\n\n#define IS_ERR_VALUE(x) unlikely((unsigned long)(void *)(x) >= (unsigned long)-MAX_ERRNO)\n\nstatic inline void * __must_check ERR_PTR(long error)\n{\n\treturn (void *) error;\n}\n\nstatic inline long __must_check PTR_ERR(__force const void *ptr)\n{\n\treturn (long) ptr;\n}\n\nstatic inline bool __must_check IS_ERR(__force const void *ptr)\n{\n\treturn IS_ERR_VALUE((unsigned long)ptr);\n}\n\nstatic inline bool __must_check IS_ERR_OR_NULL(__force const void *ptr)\n{\n\treturn unlikely(!ptr) || IS_ERR_VALUE((unsigned long)ptr);\n}\n\n/**\n * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type\n * @ptr: The pointer to cast.\n *\n * Explicitly cast an error-valued pointer to another pointer type in such a\n * way as to make it clear that's what's going on.\n */\nstatic inline void * __must_check ERR_CAST(__force const void *ptr)\n{\n\t/* cast away the const */\n\treturn (void *) ptr;\n}\n\nstatic inline int __must_check PTR_ERR_OR_ZERO(__force const void *ptr)\n{\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\telse\n\t\treturn 0;\n}\n\n#endif\n\n#endif /* _LINUX_ERR_H */\n"}}, "reports": [{"events": [{"location": {"col": 1, "file": 0, "line": 9056}, "message": "Calling '__se_sys_io_uring_enter'"}, {"location": {"col": 36, "file": 1, "line": 218}, "message": "expanded from macro 'SYSCALL_DEFINE6'"}, {"location": {"col": 2, "file": 1, "line": 224}, "message": "expanded from macro 'SYSCALL_DEFINEx'"}, {"location": {"col": 2, "file": 2, "line": 231}, "message": "expanded from macro '__SYSCALL_DEFINEx'"}, {"location": {"col": 2, "file": 2, "line": 116}, "message": "expanded from macro '__IA32_SYS_STUBx'"}, {"location": {"col": 10, "file": 2, "line": 79}, "message": "expanded from macro '__SYS_STUBx'"}, {"location": {"col": 22, "file": 0, "line": 1619}, "message": "Use of memory after it is freed"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "d89e5cc8b04fd6f2324650fe1011ddc3", "checkerName": "clang-analyzer-unix.Malloc", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 0, "line": 2628}, "message": "Value stored to 'ret' is never read"}, {"location": {"col": 3, "file": 0, "line": 2628}, "message": "Value stored to 'ret' is never read"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "fd2f554d8423b73c9631836a47f6ece6", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 0, "line": 6261}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 25, "file": 0, "line": 6261}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 4, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 5, "line": 322}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 310}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 5, "line": 302}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 25, "file": 0, "line": 6261}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 4, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 5, "line": 322}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 310}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 12, "file": 0, "line": 6265}, "message": "Calling 'io_prep_linked_timeout'"}, {"location": {"col": 6, "file": 0, "line": 6389}, "message": "Assuming 'nxt' is non-null"}, {"location": {"col": 6, "file": 0, "line": 6389}, "message": "Left side of '||' is false"}, {"location": {"col": 15, "file": 0, "line": 6389}, "message": "Assuming the condition is false"}, {"location": {"col": 6, "file": 0, "line": 6389}, "message": "Left side of '||' is false"}, {"location": {"col": 6, "file": 0, "line": 6390}, "message": "Assuming field 'opcode' is equal to IORING_OP_LINK_TIMEOUT"}, {"location": {"col": 2, "file": 0, "line": 6389}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 6396}, "message": "Returning without writing to 'req->buf_index', which participates in a condition later"}, {"location": {"col": 2, "file": 0, "line": 6396}, "message": "Returning without writing to 'req->.rw.len', which participates in a condition later"}, {"location": {"col": 12, "file": 0, "line": 6265}, "message": "Returning from 'io_prep_linked_timeout'"}, {"location": {"col": 6, "file": 0, "line": 6266}, "message": "'timeout' is non-null"}, {"location": {"col": 2, "file": 0, "line": 6266}, "message": "Taking true branch"}, {"location": {"col": 6, "file": 0, "line": 6270}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 6270}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 0, "line": 6275}, "message": "'ret' is 0"}, {"location": {"col": 2, "file": 0, "line": 6275}, "message": "Taking true branch"}, {"location": {"col": 10, "file": 0, "line": 6277}, "message": "Calling 'io_issue_sqe'"}, {"location": {"col": 2, "file": 0, "line": 6133}, "message": "Control jumps to 'case IORING_OP_WRITEV:'  at line 6142"}, {"location": {"col": 9, "file": 0, "line": 6145}, "message": "Calling 'io_write'"}, {"location": {"col": 6, "file": 0, "line": 3585}, "message": "Assuming 'rw' is null"}, {"location": {"col": 2, "file": 0, "line": 3585}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 0, "line": 3589}, "message": "Calling 'io_import_iovec'"}, {"location": {"col": 6, "file": 0, "line": 3163}, "message": "'opcode' is not equal to IORING_OP_READ_FIXED"}, {"location": {"col": 6, "file": 0, "line": 3163}, "message": "Left side of '||' is false"}, {"location": {"col": 40, "file": 0, "line": 3163}, "message": "'opcode' is not equal to IORING_OP_WRITE_FIXED"}, {"location": {"col": 2, "file": 0, "line": 3163}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 3169}, "message": "Assuming field 'buf_index' is 0"}, {"location": {"col": 21, "file": 0, "line": 3169}, "message": "Left side of '&&' is false"}, {"location": {"col": 6, "file": 0, "line": 3172}, "message": "'opcode' is not equal to IORING_OP_READ"}, {"location": {"col": 6, "file": 0, "line": 3172}, "message": "Left side of '||' is false"}, {"location": {"col": 34, "file": 0, "line": 3172}, "message": "'opcode' is not equal to IORING_OP_WRITE"}, {"location": {"col": 2, "file": 0, "line": 3172}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 3185}, "message": "Assuming the condition is true"}, {"location": {"col": 2, "file": 0, "line": 3185}, "message": "Taking true branch"}, {"location": {"col": 9, "file": 0, "line": 3186}, "message": "Calling 'io_iov_buffer_select'"}, {"location": {"col": 6, "file": 0, "line": 3132}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 3132}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 3140}, "message": "Assuming field 'len' is 0, which participates in a condition later"}, {"location": {"col": 2, "file": 0, "line": 3140}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 3141}, "message": "Returning without writing to 'iov->iov_len'"}, {"location": {"col": 3, "file": 0, "line": 3141}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 9, "file": 0, "line": 3186}, "message": "Returning from 'io_iov_buffer_select'"}, {"location": {"col": 8, "file": 0, "line": 3187}, "message": "'ret' is 0"}, {"location": {"col": 3, "file": 0, "line": 3187}, "message": "Taking true branch"}, {"location": {"col": 8, "file": 0, "line": 3188}, "message": "Assigned value is garbage or undefined"}, {"location": {"col": 8, "file": 0, "line": 3188}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "83da829136fa5d9fc1aebacfbb5b4312", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 0, "line": 4983}, "message": "Value stored to 'io' is never read"}, {"location": {"col": 3, "file": 0, "line": 4983}, "message": "Value stored to 'io' is never read"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "d039cb84270a4652efea0d463333023e", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 0, "line": 2146}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 25, "file": 0, "line": 2146}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 4, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 5, "line": 322}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 310}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 5, "line": 302}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 25, "file": 0, "line": 2146}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 4, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 5, "line": 322}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 310}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 2149}, "message": "Calling '__io_req_task_submit'"}, {"location": {"col": 6, "file": 0, "line": 2134}, "message": "Left side of '&&' is true"}, {"location": {"col": 2, "file": 0, "line": 2134}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 2137}, "message": "Calling '__io_queue_sqe'"}, {"location": {"col": 19, "file": 0, "line": 6406}, "message": "Calling 'io_prep_linked_timeout'"}, {"location": {"col": 6, "file": 0, "line": 6389}, "message": "Assuming 'nxt' is non-null"}, {"location": {"col": 6, "file": 0, "line": 6389}, "message": "Left side of '||' is false"}, {"location": {"col": 15, "file": 0, "line": 6389}, "message": "Assuming the condition is false"}, {"location": {"col": 6, "file": 0, "line": 6389}, "message": "Left side of '||' is false"}, {"location": {"col": 6, "file": 0, "line": 6390}, "message": "Assuming field 'opcode' is equal to IORING_OP_LINK_TIMEOUT"}, {"location": {"col": 2, "file": 0, "line": 6389}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 6396}, "message": "Returning without writing to 'req->opcode', which participates in a condition later"}, {"location": {"col": 2, "file": 0, "line": 6396}, "message": "Returning without writing to 'req->async_data', which participates in a condition later"}, {"location": {"col": 2, "file": 0, "line": 6396}, "message": "Returning without writing to 'req->async_data'"}, {"location": {"col": 19, "file": 0, "line": 6406}, "message": "Returning from 'io_prep_linked_timeout'"}, {"location": {"col": 7, "file": 0, "line": 6408}, "message": "Assuming the condition is false"}, {"location": {"col": 44, "file": 0, "line": 6408}, "message": "Left side of '&&' is false"}, {"location": {"col": 8, "file": 0, "line": 6419}, "message": "Calling 'io_issue_sqe'"}, {"location": {"col": 2, "file": 0, "line": 6133}, "message": "Control jumps to 'case IORING_OP_CONNECT:'  at line 6180"}, {"location": {"col": 9, "file": 0, "line": 6181}, "message": "Calling 'io_connect'"}, {"location": {"col": 6, "file": 0, "line": 4961}, "message": "Assuming field 'async_data' is null"}, {"location": {"col": 2, "file": 0, "line": 4961}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 0, "line": 4967}, "message": "Assuming 'ret' is 0"}, {"location": {"col": 3, "file": 0, "line": 4967}, "message": "Taking false branch"}, {"location": {"col": 15, "file": 0, "line": 4972}, "message": "'force_nonblock' is true"}, {"location": {"col": 15, "file": 0, "line": 4972}, "message": "'?' condition is true"}, {"location": {"col": 7, "file": 0, "line": 4976}, "message": "Assuming the condition is true"}, {"location": {"col": 22, "file": 0, "line": 4976}, "message": "Left side of '||' is true"}, {"location": {"col": 49, "file": 0, "line": 4976}, "message": "'force_nonblock' is true"}, {"location": {"col": 2, "file": 0, "line": 4976}, "message": "Taking true branch"}, {"location": {"col": 12, "file": 0, "line": 4977}, "message": "Field 'async_data' is null"}, {"location": {"col": 3, "file": 0, "line": 4977}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 0, "line": 4979}, "message": "Calling 'io_alloc_async_data'"}, {"location": {"col": 6, "file": 0, "line": 3295}, "message": "Assuming field 'needs_async_data' is 0, which participates in a condition later"}, {"location": {"col": 2, "file": 0, "line": 3295}, "message": "Taking true branch"}, {"location": {"col": 3, "file": 0, "line": 3296}, "message": "Returning without writing to 'req->async_data'"}, {"location": {"col": 3, "file": 0, "line": 3296}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 7, "file": 0, "line": 4979}, "message": "Returning from 'io_alloc_async_data'"}, {"location": {"col": 3, "file": 0, "line": 4979}, "message": "Taking false branch"}, {"location": {"col": 3, "file": 0, "line": 4984}, "message": "Null pointer passed as 1st argument to memory copy function"}, {"location": {"col": 3, "file": 0, "line": 4984}, "message": "Null pointer passed as 1st argument to memory copy function"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "6b294e6b410e7315a03b211a50cd3c4d", "checkerName": "clang-analyzer-unix.cstring.NullArg", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 25, "file": 0, "line": 6261}, "message": "Left side of '&&' is false"}, {"location": {"col": 61, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 25, "file": 0, "line": 6261}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 4, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 5, "line": 322}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 310}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 5, "line": 302}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 25, "file": 0, "line": 6261}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 3, "line": 694}, "message": "expanded from macro 'container_of'"}, {"location": {"col": 37, "file": 4, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 5, "line": 322}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 310}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 5, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 6, "file": 0, "line": 6266}, "message": "'timeout' is non-null"}, {"location": {"col": 2, "file": 0, "line": 6266}, "message": "Taking true branch"}, {"location": {"col": 6, "file": 0, "line": 6270}, "message": "Assuming the condition is false"}, {"location": {"col": 2, "file": 0, "line": 6270}, "message": "Taking false branch"}, {"location": {"col": 7, "file": 0, "line": 6275}, "message": "'ret' is 0"}, {"location": {"col": 2, "file": 0, "line": 6275}, "message": "Taking true branch"}, {"location": {"col": 10, "file": 0, "line": 6277}, "message": "Calling 'io_issue_sqe'"}, {"location": {"col": 2, "file": 0, "line": 6133}, "message": "Control jumps to 'case IORING_OP_FILES_UPDATE:'  at line 6195"}, {"location": {"col": 9, "file": 0, "line": 6196}, "message": "Calling 'io_files_update'"}, {"location": {"col": 6, "file": 0, "line": 5888}, "message": "'force_nonblock' is false"}, {"location": {"col": 2, "file": 0, "line": 5888}, "message": "Taking false branch"}, {"location": {"col": 8, "file": 0, "line": 5895}, "message": "Calling '__io_sqe_files_update'"}, {"location": {"col": 13, "file": 0, "line": 7791}, "message": "'err' declared without an initial value"}, {"location": {"col": 6, "file": 0, "line": 7795}, "message": "Calling '__must_check_overflow'"}, {"location": {"col": 37, "file": 6, "line": 66}, "message": "expanded from macro 'check_add_overflow'"}, {"location": {"col": 9, "file": 6, "line": 54}, "message": "Assuming 'overflow' is false"}, {"location": {"col": 40, "file": 7, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 6, "line": 54}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 0, "line": 7795}, "message": "Returning from '__must_check_overflow'"}, {"location": {"col": 37, "file": 6, "line": 66}, "message": "expanded from macro 'check_add_overflow'"}, {"location": {"col": 2, "file": 0, "line": 7795}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 7797}, "message": "Assuming 'done' is <= field 'nr_user_files'"}, {"location": {"col": 2, "file": 0, "line": 7797}, "message": "Taking false branch"}, {"location": {"col": 6, "file": 0, "line": 7801}, "message": "Calling 'IS_ERR'"}, {"location": {"col": 9, "file": 8, "line": 36}, "message": "Assuming the condition is false"}, {"location": {"col": 34, "file": 8, "line": 22}, "message": "expanded from macro 'IS_ERR_VALUE'"}, {"location": {"col": 42, "file": 7, "line": 78}, "message": "expanded from macro 'unlikely'"}, {"location": {"col": 2, "file": 8, "line": 36}, "message": "Returning zero, which participates in a condition later"}, {"location": {"col": 6, "file": 0, "line": 7801}, "message": "Returning from 'IS_ERR'"}, {"location": {"col": 2, "file": 0, "line": 7801}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 0, "line": 7806}, "message": "Loop condition is false. Execution continues on line 7858"}, {"location": {"col": 6, "file": 0, "line": 7858}, "message": "'needs_switch' is false"}, {"location": {"col": 2, "file": 0, "line": 7858}, "message": "Taking false branch"}, {"location": {"col": 9, "file": 0, "line": 7868}, "message": "'done' is 0"}, {"location": {"col": 9, "file": 0, "line": 7868}, "message": "'?' condition is false"}, {"location": {"col": 2, "file": 0, "line": 7868}, "message": "Undefined or garbage value returned to caller"}, {"location": {"col": 2, "file": 0, "line": 7868}, "message": "Undefined or garbage value returned to caller"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "7eed6c652138a8c3ed6f6a5f56a9182b", "checkerName": "clang-analyzer-core.uninitialized.UndefReturn", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 1, "file": 0, "line": 9547}, "message": "Calling '__se_sys_io_uring_setup'"}, {"location": {"col": 36, "file": 1, "line": 214}, "message": "expanded from macro 'SYSCALL_DEFINE2'"}, {"location": {"col": 2, "file": 1, "line": 224}, "message": "expanded from macro 'SYSCALL_DEFINEx'"}, {"location": {"col": 2, "file": 2, "line": 230}, "message": "expanded from macro '__SYSCALL_DEFINEx'"}, {"location": {"col": 2, "file": 2, "line": 96}, "message": "expanded from macro '__X64_SYS_STUBx'"}, {"location": {"col": 10, "file": 2, "line": 79}, "message": "expanded from macro '__SYS_STUBx'"}, {"location": {"col": 44, "file": 0, "line": 8058}, "message": "Access to field 'thread' results in a dereference of a null pointer (loaded from variable 'sqd')"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "591120b29b83306fa74295e6084fd3a9", "checkerName": "clang-analyzer-core.NullDereference", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 3, "file": 0, "line": 8349}, "message": "Value stored to 'ret' is never read"}, {"location": {"col": 3, "file": 0, "line": 8349}, "message": "Value stored to 'ret' is never read"}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "5e8ec2507459145c4304f3271c2c7153", "checkerName": "clang-analyzer-deadcode.DeadStores", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
