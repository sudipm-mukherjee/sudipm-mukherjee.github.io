<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/fs/io_uring.c", "content": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Shared application/kernel submission and completion ring pairs, for\n * supporting fast/efficient IO.\n *\n * A note on the read/write ordering memory barriers that are matched between\n * the application and kernel side.\n *\n * After the application reads the CQ ring tail, it must use an\n * appropriate smp_rmb() to pair with the smp_wmb() the kernel uses\n * before writing the tail (using smp_load_acquire to read the tail will\n * do). It also needs a smp_mb() before updating CQ head (ordering the\n * entry load(s) with the head store), pairing with an implicit barrier\n * through a control-dependency in io_get_cqring (smp_store_release to\n * store head will do). Failure to do so could lead to reading invalid\n * CQ entries.\n *\n * Likewise, the application must use an appropriate smp_wmb() before\n * writing the SQ tail (ordering SQ entry stores with the tail store),\n * which pairs with smp_load_acquire in io_get_sqring (smp_store_release\n * to store the tail will do). And it needs a barrier ordering the SQ\n * head load before writing new SQ entries (smp_load_acquire to read\n * head will do).\n *\n * When using the SQ poll thread (IORING_SETUP_SQPOLL), the application\n * needs to check the SQ flags for IORING_SQ_NEED_WAKEUP *after*\n * updating the SQ tail; a full memory barrier smp_mb() is needed\n * between.\n *\n * Also see the examples in the liburing library:\n *\n *\tgit://git.kernel.dk/liburing\n *\n * io_uring also uses READ/WRITE_ONCE() for _any_ store or load that happens\n * from data shared between the kernel and application. This is done both\n * for ordering purposes, but also to ensure that once a value is loaded from\n * data that the application could potentially modify, it remains stable.\n *\n * Copyright (C) 2018-2019 Jens Axboe\n * Copyright (c) 2018-2019 Christoph Hellwig\n */\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <net/compat.h>\n#include <linux/refcount.h>\n#include <linux/uio.h>\n#include <linux/bits.h>\n\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/bvec.h>\n#include <linux/net.h>\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <linux/anon_inodes.h>\n#include <linux/sched/mm.h>\n#include <linux/uaccess.h>\n#include <linux/nospec.h>\n#include <linux/sizes.h>\n#include <linux/hugetlb.h>\n#include <linux/highmem.h>\n#include <linux/namei.h>\n#include <linux/fsnotify.h>\n#include <linux/fadvise.h>\n#include <linux/eventpoll.h>\n#include <linux/splice.h>\n#include <linux/task_work.h>\n#include <linux/pagemap.h>\n#include <linux/io_uring.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"internal.h\"\n#include \"io-wq.h\"\n\n#define IORING_MAX_ENTRIES\t32768\n#define IORING_MAX_CQ_ENTRIES\t(2 * IORING_MAX_ENTRIES)\n\n/*\n * Shift of 9 is 512 entries, or exactly one page on 64-bit archs\n */\n#define IORING_FILE_TABLE_SHIFT\t9\n#define IORING_MAX_FILES_TABLE\t(1U << IORING_FILE_TABLE_SHIFT)\n#define IORING_FILE_TABLE_MASK\t(IORING_MAX_FILES_TABLE - 1)\n#define IORING_MAX_FIXED_FILES\t(64 * IORING_MAX_FILES_TABLE)\n#define IORING_MAX_RESTRICTIONS\t(IORING_RESTRICTION_LAST + \\\n\t\t\t\t IORING_REGISTER_LAST + IORING_OP_LAST)\n\n#define SQE_VALID_FLAGS\t(IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK|\t\\\n\t\t\t\tIOSQE_IO_HARDLINK | IOSQE_ASYNC | \\\n\t\t\t\tIOSQE_BUFFER_SELECT)\n\nstruct io_uring {\n\tu32 head ____cacheline_aligned_in_smp;\n\tu32 tail ____cacheline_aligned_in_smp;\n};\n\n/*\n * This data is shared with the application through the mmap at offsets\n * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.\n *\n * The offsets to the member fields are published through struct\n * io_sqring_offsets when calling io_uring_setup.\n */\nstruct io_rings {\n\t/*\n\t * Head and tail offsets into the ring; the offsets need to be\n\t * masked to get valid indices.\n\t *\n\t * The kernel controls head of the sq ring and the tail of the cq ring,\n\t * and the application controls tail of the sq ring and the head of the\n\t * cq ring.\n\t */\n\tstruct io_uring\t\tsq, cq;\n\t/*\n\t * Bitmasks to apply to head and tail offsets (constant, equals\n\t * ring_entries - 1)\n\t */\n\tu32\t\t\tsq_ring_mask, cq_ring_mask;\n\t/* Ring sizes (constant, power of 2) */\n\tu32\t\t\tsq_ring_entries, cq_ring_entries;\n\t/*\n\t * Number of invalid entries dropped by the kernel due to\n\t * invalid index stored in array\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * After a new SQ head value was read by the application this\n\t * counter includes all submissions that were dropped reaching\n\t * the new SQ head (and possibly more).\n\t */\n\tu32\t\t\tsq_dropped;\n\t/*\n\t * Runtime SQ flags\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application.\n\t *\n\t * The application needs a full memory barrier before checking\n\t * for IORING_SQ_NEED_WAKEUP after updating the sq tail.\n\t */\n\tu32\t\t\tsq_flags;\n\t/*\n\t * Runtime CQ flags\n\t *\n\t * Written by the application, shouldn't be modified by the\n\t * kernel.\n\t */\n\tu32                     cq_flags;\n\t/*\n\t * Number of completion events lost because the queue was full;\n\t * this should be avoided by the application by making sure\n\t * there are not more requests pending than there is space in\n\t * the completion queue.\n\t *\n\t * Written by the kernel, shouldn't be modified by the\n\t * application (i.e. get number of \"new events\" by comparing to\n\t * cached value).\n\t *\n\t * As completion events come in out of order this counter is not\n\t * ordered with any other data.\n\t */\n\tu32\t\t\tcq_overflow;\n\t/*\n\t * Ring buffer of completion events.\n\t *\n\t * The kernel writes completion events fresh every time they are\n\t * produced, so the application is allowed to modify pending\n\t * entries.\n\t */\n\tstruct io_uring_cqe\tcqes[] ____cacheline_aligned_in_smp;\n};\n\nenum io_uring_cmd_flags {\n\tIO_URING_F_NONBLOCK\t\t= 1,\n\tIO_URING_F_COMPLETE_DEFER\t= 2,\n};\n\nstruct io_mapped_ubuf {\n\tu64\t\tubuf;\n\tsize_t\t\tlen;\n\tstruct\t\tbio_vec *bvec;\n\tunsigned int\tnr_bvecs;\n\tunsigned long\tacct_pages;\n};\n\nstruct io_ring_ctx;\n\nstruct io_overflow_cqe {\n\tstruct io_uring_cqe cqe;\n\tstruct list_head list;\n};\n\nstruct io_rsrc_put {\n\tstruct list_head list;\n\tunion {\n\t\tvoid *rsrc;\n\t\tstruct file *file;\n\t};\n};\n\nstruct fixed_rsrc_table {\n\tstruct file\t\t**files;\n};\n\nstruct fixed_rsrc_ref_node {\n\tstruct percpu_ref\t\trefs;\n\tstruct list_head\t\tnode;\n\tstruct list_head\t\trsrc_list;\n\tstruct fixed_rsrc_data\t\t*rsrc_data;\n\tvoid\t\t\t\t(*rsrc_put)(struct io_ring_ctx *ctx,\n\t\t\t\t\t\t    struct io_rsrc_put *prsrc);\n\tstruct llist_node\t\tllist;\n\tbool\t\t\t\tdone;\n};\n\nstruct fixed_rsrc_data {\n\tstruct fixed_rsrc_table\t\t*table;\n\tstruct io_ring_ctx\t\t*ctx;\n\n\tstruct fixed_rsrc_ref_node\t*node;\n\tstruct percpu_ref\t\trefs;\n\tstruct completion\t\tdone;\n\tbool\t\t\t\tquiesce;\n};\n\nstruct io_buffer {\n\tstruct list_head list;\n\t__u64 addr;\n\t__s32 len;\n\t__u16 bid;\n};\n\nstruct io_restriction {\n\tDECLARE_BITMAP(register_op, IORING_REGISTER_LAST);\n\tDECLARE_BITMAP(sqe_op, IORING_OP_LAST);\n\tu8 sqe_flags_allowed;\n\tu8 sqe_flags_required;\n\tbool registered;\n};\n\nenum {\n\tIO_SQ_THREAD_SHOULD_STOP = 0,\n\tIO_SQ_THREAD_SHOULD_PARK,\n};\n\nstruct io_sq_data {\n\trefcount_t\t\trefs;\n\tatomic_t\t\tpark_pending;\n\tstruct mutex\t\tlock;\n\n\t/* ctx's that are using this sqd */\n\tstruct list_head\tctx_list;\n\n\tstruct task_struct\t*thread;\n\tstruct wait_queue_head\twait;\n\n\tunsigned\t\tsq_thread_idle;\n\tint\t\t\tsq_cpu;\n\tpid_t\t\t\ttask_pid;\n\tpid_t\t\t\ttask_tgid;\n\n\tunsigned long\t\tstate;\n\tstruct completion\texited;\n\tstruct callback_head\t*park_task_work;\n};\n\n#define IO_IOPOLL_BATCH\t\t\t8\n#define IO_COMPL_BATCH\t\t\t32\n#define IO_REQ_CACHE_SIZE\t\t32\n#define IO_REQ_ALLOC_BATCH\t\t8\n\nstruct io_comp_state {\n\tstruct io_kiocb\t\t*reqs[IO_COMPL_BATCH];\n\tunsigned int\t\tnr;\n\tunsigned int\t\tlocked_free_nr;\n\t/* inline/task_work completion list, under ->uring_lock */\n\tstruct list_head\tfree_list;\n\t/* IRQ completion list, under ->completion_lock */\n\tstruct list_head\tlocked_free_list;\n};\n\nstruct io_submit_link {\n\tstruct io_kiocb\t\t*head;\n\tstruct io_kiocb\t\t*last;\n};\n\nstruct io_submit_state {\n\tstruct blk_plug\t\tplug;\n\tstruct io_submit_link\tlink;\n\n\t/*\n\t * io_kiocb alloc cache\n\t */\n\tvoid\t\t\t*reqs[IO_REQ_CACHE_SIZE];\n\tunsigned int\t\tfree_reqs;\n\n\tbool\t\t\tplug_started;\n\n\t/*\n\t * Batch completion logic\n\t */\n\tstruct io_comp_state\tcomp;\n\n\t/*\n\t * File reference cache\n\t */\n\tstruct file\t\t*file;\n\tunsigned int\t\tfd;\n\tunsigned int\t\tfile_refs;\n\tunsigned int\t\tios_left;\n};\n\nstruct io_ring_ctx {\n\tstruct {\n\t\tstruct percpu_ref\trefs;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned int\t\tflags;\n\t\tunsigned int\t\tcompat: 1;\n\t\tunsigned int\t\tcq_overflow_flushed: 1;\n\t\tunsigned int\t\tdrain_next: 1;\n\t\tunsigned int\t\teventfd_async: 1;\n\t\tunsigned int\t\trestricted: 1;\n\n\t\t/*\n\t\t * Ring buffer of indices into array of io_uring_sqe, which is\n\t\t * mmapped by the application using the IORING_OFF_SQES offset.\n\t\t *\n\t\t * This indirection could e.g. be used to assign fixed\n\t\t * io_uring_sqe entries to operations and only submit them to\n\t\t * the queue when needed.\n\t\t *\n\t\t * The kernel modifies neither the indices array nor the entries\n\t\t * array.\n\t\t */\n\t\tu32\t\t\t*sq_array;\n\t\tunsigned\t\tcached_sq_head;\n\t\tunsigned\t\tsq_entries;\n\t\tunsigned\t\tsq_mask;\n\t\tunsigned\t\tsq_thread_idle;\n\t\tunsigned\t\tcached_sq_dropped;\n\t\tunsigned\t\tcached_cq_overflow;\n\t\tunsigned long\t\tsq_check_overflow;\n\n\t\t/* hashed buffered write serialization */\n\t\tstruct io_wq_hash\t*hash_map;\n\n\t\tstruct list_head\tdefer_list;\n\t\tstruct list_head\ttimeout_list;\n\t\tstruct list_head\tcq_overflow_list;\n\n\t\tstruct io_uring_sqe\t*sq_sqes;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\t\turing_lock;\n\t\twait_queue_head_t\twait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct io_submit_state\t\tsubmit_state;\n\n\tstruct io_rings\t*rings;\n\n\t/* Only used for accounting purposes */\n\tstruct mm_struct\t*mm_account;\n\n\tconst struct cred\t*sq_creds;\t/* cred used for __io_sq_thread() */\n\tstruct io_sq_data\t*sq_data;\t/* if using sq thread polling */\n\n\tstruct wait_queue_head\tsqo_sq_wait;\n\tstruct list_head\tsqd_list;\n\n\t/*\n\t * If used, fixed file set. Writers must ensure that ->refs is dead,\n\t * readers must ensure that ->refs is alive as long as the file* is\n\t * used. Only updated through io_uring_register(2).\n\t */\n\tstruct fixed_rsrc_data\t*file_data;\n\tunsigned\t\tnr_user_files;\n\n\t/* if used, fixed mapped user buffers */\n\tunsigned\t\tnr_user_bufs;\n\tstruct io_mapped_ubuf\t*user_bufs;\n\n\tstruct user_struct\t*user;\n\n\tstruct completion\tref_comp;\n\n#if defined(CONFIG_UNIX)\n\tstruct socket\t\t*ring_sock;\n#endif\n\n\tstruct xarray\t\tio_buffers;\n\n\tstruct xarray\t\tpersonalities;\n\tu32\t\t\tpers_next;\n\n\tstruct {\n\t\tunsigned\t\tcached_cq_tail;\n\t\tunsigned\t\tcq_entries;\n\t\tunsigned\t\tcq_mask;\n\t\tatomic_t\t\tcq_timeouts;\n\t\tunsigned\t\tcq_last_tm_flush;\n\t\tunsigned long\t\tcq_check_overflow;\n\t\tstruct wait_queue_head\tcq_wait;\n\t\tstruct fasync_struct\t*cq_fasync;\n\t\tstruct eventfd_ctx\t*cq_ev_fd;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\t\tcompletion_lock;\n\n\t\t/*\n\t\t * ->iopoll_list is protected by the ctx->uring_lock for\n\t\t * io_uring instances that don't use IORING_SETUP_SQPOLL.\n\t\t * For SQPOLL, only the single threaded io_sq_thread() will\n\t\t * manipulate the list, hence no extra locking is needed there.\n\t\t */\n\t\tstruct list_head\tiopoll_list;\n\t\tstruct hlist_head\t*cancel_hash;\n\t\tunsigned\t\tcancel_hash_bits;\n\t\tbool\t\t\tpoll_multi_file;\n\n\t\tspinlock_t\t\tinflight_lock;\n\t\tstruct list_head\tinflight_list;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct delayed_work\t\trsrc_put_work;\n\tstruct llist_head\t\trsrc_put_llist;\n\tstruct list_head\t\trsrc_ref_list;\n\tspinlock_t\t\t\trsrc_ref_lock;\n\tstruct fixed_rsrc_ref_node\t*rsrc_backup_node;\n\n\tstruct io_restriction\t\trestrictions;\n\n\t/* exit task_work */\n\tstruct callback_head\t\t*exit_task_work;\n\n\tstruct wait_queue_head\t\thash_wait;\n\n\t/* Keep this last, we don't need it for the fast path */\n\tstruct work_struct\t\texit_work;\n\tstruct list_head\t\ttctx_list;\n};\n\nstruct io_uring_task {\n\t/* submission side */\n\tstruct xarray\t\txa;\n\tstruct wait_queue_head\twait;\n\tconst struct io_ring_ctx *last;\n\tstruct io_wq\t\t*io_wq;\n\tstruct percpu_counter\tinflight;\n\tatomic_t\t\tin_idle;\n\n\tspinlock_t\t\ttask_lock;\n\tstruct io_wq_work_list\ttask_list;\n\tunsigned long\t\ttask_state;\n\tstruct callback_head\ttask_work;\n};\n\n/*\n * First field must be the file pointer in all the\n * iocb unions! See also 'struct kiocb' in <linux/fs.h>\n */\nstruct io_poll_iocb {\n\tstruct file\t\t\t*file;\n\tstruct wait_queue_head\t\t*head;\n\t__poll_t\t\t\tevents;\n\tbool\t\t\t\tdone;\n\tbool\t\t\t\tcanceled;\n\tbool\t\t\t\tupdate_events;\n\tbool\t\t\t\tupdate_user_data;\n\tunion {\n\t\tstruct wait_queue_entry\twait;\n\t\tstruct {\n\t\t\tu64\t\told_user_data;\n\t\t\tu64\t\tnew_user_data;\n\t\t};\n\t};\n};\n\nstruct io_poll_remove {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_close {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tfd;\n};\n\nstruct io_timeout_data {\n\tstruct io_kiocb\t\t\t*req;\n\tstruct hrtimer\t\t\ttimer;\n\tstruct timespec64\t\tts;\n\tenum hrtimer_mode\t\tmode;\n};\n\nstruct io_accept {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint __user\t\t\t*addr_len;\n\tint\t\t\t\tflags;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_sync {\n\tstruct file\t\t\t*file;\n\tloff_t\t\t\t\tlen;\n\tloff_t\t\t\t\toff;\n\tint\t\t\t\tflags;\n\tint\t\t\t\tmode;\n};\n\nstruct io_cancel {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n};\n\nstruct io_timeout {\n\tstruct file\t\t\t*file;\n\tu32\t\t\t\toff;\n\tu32\t\t\t\ttarget_seq;\n\tstruct list_head\t\tlist;\n\t/* head of the link, used by linked timeouts only */\n\tstruct io_kiocb\t\t\t*head;\n};\n\nstruct io_timeout_rem {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\n\t/* timeout update */\n\tstruct timespec64\t\tts;\n\tu32\t\t\t\tflags;\n};\n\nstruct io_rw {\n\t/* NOTE: kiocb has the file as the first member, so don't do it here */\n\tstruct kiocb\t\t\tkiocb;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tlen;\n};\n\nstruct io_connect {\n\tstruct file\t\t\t*file;\n\tstruct sockaddr __user\t\t*addr;\n\tint\t\t\t\taddr_len;\n};\n\nstruct io_sr_msg {\n\tstruct file\t\t\t*file;\n\tunion {\n\t\tstruct user_msghdr __user *umsg;\n\t\tvoid __user\t\t*buf;\n\t};\n\tint\t\t\t\tmsg_flags;\n\tint\t\t\t\tbgid;\n\tsize_t\t\t\t\tlen;\n\tstruct io_buffer\t\t*kbuf;\n};\n\nstruct io_open {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tstruct filename\t\t\t*filename;\n\tstruct open_how\t\t\thow;\n\tunsigned long\t\t\tnofile;\n};\n\nstruct io_rsrc_update {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\targ;\n\tu32\t\t\t\tnr_args;\n\tu32\t\t\t\toffset;\n};\n\nstruct io_fadvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\toffset;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_madvise {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\tadvice;\n};\n\nstruct io_epoll {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tepfd;\n\tint\t\t\t\top;\n\tint\t\t\t\tfd;\n\tstruct epoll_event\t\tevent;\n};\n\nstruct io_splice {\n\tstruct file\t\t\t*file_out;\n\tstruct file\t\t\t*file_in;\n\tloff_t\t\t\t\toff_out;\n\tloff_t\t\t\t\toff_in;\n\tu64\t\t\t\tlen;\n\tunsigned int\t\t\tflags;\n};\n\nstruct io_provide_buf {\n\tstruct file\t\t\t*file;\n\t__u64\t\t\t\taddr;\n\t__s32\t\t\t\tlen;\n\t__u32\t\t\t\tbgid;\n\t__u16\t\t\t\tnbufs;\n\t__u16\t\t\t\tbid;\n};\n\nstruct io_statx {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tunsigned int\t\t\tmask;\n\tunsigned int\t\t\tflags;\n\tconst char __user\t\t*filename;\n\tstruct statx __user\t\t*buffer;\n};\n\nstruct io_shutdown {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\thow;\n};\n\nstruct io_rename {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\told_dfd;\n\tint\t\t\t\tnew_dfd;\n\tstruct filename\t\t\t*oldpath;\n\tstruct filename\t\t\t*newpath;\n\tint\t\t\t\tflags;\n};\n\nstruct io_unlink {\n\tstruct file\t\t\t*file;\n\tint\t\t\t\tdfd;\n\tint\t\t\t\tflags;\n\tstruct filename\t\t\t*filename;\n};\n\nstruct io_completion {\n\tstruct file\t\t\t*file;\n\tstruct list_head\t\tlist;\n\tu32\t\t\t\tcflags;\n};\n\nstruct io_async_connect {\n\tstruct sockaddr_storage\t\taddress;\n};\n\nstruct io_async_msghdr {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\t/* points to an allocated iov, if NULL we use fast_iov instead */\n\tstruct iovec\t\t\t*free_iov;\n\tstruct sockaddr __user\t\t*uaddr;\n\tstruct msghdr\t\t\tmsg;\n\tstruct sockaddr_storage\t\taddr;\n};\n\nstruct io_async_rw {\n\tstruct iovec\t\t\tfast_iov[UIO_FASTIOV];\n\tconst struct iovec\t\t*free_iovec;\n\tstruct iov_iter\t\t\titer;\n\tsize_t\t\t\t\tbytes_done;\n\tstruct wait_page_queue\t\twpq;\n};\n\nenum {\n\tREQ_F_FIXED_FILE_BIT\t= IOSQE_FIXED_FILE_BIT,\n\tREQ_F_IO_DRAIN_BIT\t= IOSQE_IO_DRAIN_BIT,\n\tREQ_F_LINK_BIT\t\t= IOSQE_IO_LINK_BIT,\n\tREQ_F_HARDLINK_BIT\t= IOSQE_IO_HARDLINK_BIT,\n\tREQ_F_FORCE_ASYNC_BIT\t= IOSQE_ASYNC_BIT,\n\tREQ_F_BUFFER_SELECT_BIT\t= IOSQE_BUFFER_SELECT_BIT,\n\n\tREQ_F_FAIL_LINK_BIT,\n\tREQ_F_INFLIGHT_BIT,\n\tREQ_F_CUR_POS_BIT,\n\tREQ_F_NOWAIT_BIT,\n\tREQ_F_LINK_TIMEOUT_BIT,\n\tREQ_F_NEED_CLEANUP_BIT,\n\tREQ_F_POLLED_BIT,\n\tREQ_F_BUFFER_SELECTED_BIT,\n\tREQ_F_LTIMEOUT_ACTIVE_BIT,\n\tREQ_F_COMPLETE_INLINE_BIT,\n\tREQ_F_DONT_REISSUE_BIT,\n\t/* keep async read/write and isreg together and in order */\n\tREQ_F_ASYNC_READ_BIT,\n\tREQ_F_ASYNC_WRITE_BIT,\n\tREQ_F_ISREG_BIT,\n\n\t/* not a real bit, just to check we're not overflowing the space */\n\t__REQ_F_LAST_BIT,\n};\n\nenum {\n\t/* ctx owns file */\n\tREQ_F_FIXED_FILE\t= BIT(REQ_F_FIXED_FILE_BIT),\n\t/* drain existing IO first */\n\tREQ_F_IO_DRAIN\t\t= BIT(REQ_F_IO_DRAIN_BIT),\n\t/* linked sqes */\n\tREQ_F_LINK\t\t= BIT(REQ_F_LINK_BIT),\n\t/* doesn't sever on completion < 0 */\n\tREQ_F_HARDLINK\t\t= BIT(REQ_F_HARDLINK_BIT),\n\t/* IOSQE_ASYNC */\n\tREQ_F_FORCE_ASYNC\t= BIT(REQ_F_FORCE_ASYNC_BIT),\n\t/* IOSQE_BUFFER_SELECT */\n\tREQ_F_BUFFER_SELECT\t= BIT(REQ_F_BUFFER_SELECT_BIT),\n\n\t/* fail rest of links */\n\tREQ_F_FAIL_LINK\t\t= BIT(REQ_F_FAIL_LINK_BIT),\n\t/* on inflight list, should be cancelled and waited on exit reliably */\n\tREQ_F_INFLIGHT\t\t= BIT(REQ_F_INFLIGHT_BIT),\n\t/* read/write uses file position */\n\tREQ_F_CUR_POS\t\t= BIT(REQ_F_CUR_POS_BIT),\n\t/* must not punt to workers */\n\tREQ_F_NOWAIT\t\t= BIT(REQ_F_NOWAIT_BIT),\n\t/* has or had linked timeout */\n\tREQ_F_LINK_TIMEOUT\t= BIT(REQ_F_LINK_TIMEOUT_BIT),\n\t/* needs cleanup */\n\tREQ_F_NEED_CLEANUP\t= BIT(REQ_F_NEED_CLEANUP_BIT),\n\t/* already went through poll handler */\n\tREQ_F_POLLED\t\t= BIT(REQ_F_POLLED_BIT),\n\t/* buffer already selected */\n\tREQ_F_BUFFER_SELECTED\t= BIT(REQ_F_BUFFER_SELECTED_BIT),\n\t/* linked timeout is active, i.e. prepared by link's head */\n\tREQ_F_LTIMEOUT_ACTIVE\t= BIT(REQ_F_LTIMEOUT_ACTIVE_BIT),\n\t/* completion is deferred through io_comp_state */\n\tREQ_F_COMPLETE_INLINE\t= BIT(REQ_F_COMPLETE_INLINE_BIT),\n\t/* don't attempt request reissue, see io_rw_reissue() */\n\tREQ_F_DONT_REISSUE\t= BIT(REQ_F_DONT_REISSUE_BIT),\n\t/* supports async reads */\n\tREQ_F_ASYNC_READ\t= BIT(REQ_F_ASYNC_READ_BIT),\n\t/* supports async writes */\n\tREQ_F_ASYNC_WRITE\t= BIT(REQ_F_ASYNC_WRITE_BIT),\n\t/* regular file */\n\tREQ_F_ISREG\t\t= BIT(REQ_F_ISREG_BIT),\n};\n\nstruct async_poll {\n\tstruct io_poll_iocb\tpoll;\n\tstruct io_poll_iocb\t*double_poll;\n};\n\nstruct io_task_work {\n\tstruct io_wq_work_node\tnode;\n\ttask_work_func_t\tfunc;\n};\n\n/*\n * NOTE! Each of the iocb union members has the file pointer\n * as the first entry in their struct definition. So you can\n * access the file pointer through any of the sub-structs,\n * or directly as just 'ki_filp' in this struct.\n */\nstruct io_kiocb {\n\tunion {\n\t\tstruct file\t\t*file;\n\t\tstruct io_rw\t\trw;\n\t\tstruct io_poll_iocb\tpoll;\n\t\tstruct io_poll_remove\tpoll_remove;\n\t\tstruct io_accept\taccept;\n\t\tstruct io_sync\t\tsync;\n\t\tstruct io_cancel\tcancel;\n\t\tstruct io_timeout\ttimeout;\n\t\tstruct io_timeout_rem\ttimeout_rem;\n\t\tstruct io_connect\tconnect;\n\t\tstruct io_sr_msg\tsr_msg;\n\t\tstruct io_open\t\topen;\n\t\tstruct io_close\t\tclose;\n\t\tstruct io_rsrc_update\trsrc_update;\n\t\tstruct io_fadvise\tfadvise;\n\t\tstruct io_madvise\tmadvise;\n\t\tstruct io_epoll\t\tepoll;\n\t\tstruct io_splice\tsplice;\n\t\tstruct io_provide_buf\tpbuf;\n\t\tstruct io_statx\t\tstatx;\n\t\tstruct io_shutdown\tshutdown;\n\t\tstruct io_rename\trename;\n\t\tstruct io_unlink\tunlink;\n\t\t/* use only after cleaning per-op data, see io_clean_op() */\n\t\tstruct io_completion\tcompl;\n\t};\n\n\t/* opcode allocated if it needs to store data for async defer */\n\tvoid\t\t\t\t*async_data;\n\tu8\t\t\t\topcode;\n\t/* polled IO has completed */\n\tu8\t\t\t\tiopoll_completed;\n\n\tu16\t\t\t\tbuf_index;\n\tu32\t\t\t\tresult;\n\n\tstruct io_ring_ctx\t\t*ctx;\n\tunsigned int\t\t\tflags;\n\tatomic_t\t\t\trefs;\n\tstruct task_struct\t\t*task;\n\tu64\t\t\t\tuser_data;\n\n\tstruct io_kiocb\t\t\t*link;\n\tstruct percpu_ref\t\t*fixed_rsrc_refs;\n\n\t/*\n\t * 1. used with ctx->iopoll_list with reads/writes\n\t * 2. to track reqs with ->files (see io_op_def::file_table)\n\t */\n\tstruct list_head\t\tinflight_entry;\n\tunion {\n\t\tstruct io_task_work\tio_task_work;\n\t\tstruct callback_head\ttask_work;\n\t};\n\t/* for polled requests, i.e. IORING_OP_POLL_ADD and async armed poll */\n\tstruct hlist_node\t\thash_node;\n\tstruct async_poll\t\t*apoll;\n\tstruct io_wq_work\t\twork;\n};\n\nstruct io_tctx_node {\n\tstruct list_head\tctx_node;\n\tstruct task_struct\t*task;\n\tstruct io_ring_ctx\t*ctx;\n};\n\nstruct io_defer_entry {\n\tstruct list_head\tlist;\n\tstruct io_kiocb\t\t*req;\n\tu32\t\t\tseq;\n};\n\nstruct io_op_def {\n\t/* needs req->file assigned */\n\tunsigned\t\tneeds_file : 1;\n\t/* hash wq insertion if file is a regular file */\n\tunsigned\t\thash_reg_file : 1;\n\t/* unbound wq insertion if file is a non-regular file */\n\tunsigned\t\tunbound_nonreg_file : 1;\n\t/* opcode is not supported by this kernel */\n\tunsigned\t\tnot_supported : 1;\n\t/* set if opcode supports polled \"wait\" */\n\tunsigned\t\tpollin : 1;\n\tunsigned\t\tpollout : 1;\n\t/* op supports buffer selection */\n\tunsigned\t\tbuffer_select : 1;\n\t/* do prep async if is going to be punted */\n\tunsigned\t\tneeds_async_setup : 1;\n\t/* should block plug */\n\tunsigned\t\tplug : 1;\n\t/* size of async data needed, if any */\n\tunsigned short\t\tasync_size;\n};\n\nstatic const struct io_op_def io_op_defs[] = {\n\t[IORING_OP_NOP] = {},\n\t[IORING_OP_READV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_setup\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_WRITEV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_setup\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_FSYNC] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_READ_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_WRITE_FIXED] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_POLL_ADD] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_POLL_REMOVE] = {},\n\t[IORING_OP_SYNC_FILE_RANGE] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_SENDMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_setup\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t},\n\t[IORING_OP_RECVMSG] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.needs_async_setup\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_msghdr),\n\t},\n\t[IORING_OP_TIMEOUT] = {\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t},\n\t[IORING_OP_TIMEOUT_REMOVE] = {\n\t\t/* used by timeout updates' prep() */\n\t},\n\t[IORING_OP_ACCEPT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t},\n\t[IORING_OP_ASYNC_CANCEL] = {},\n\t[IORING_OP_LINK_TIMEOUT] = {\n\t\t.async_size\t\t= sizeof(struct io_timeout_data),\n\t},\n\t[IORING_OP_CONNECT] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.needs_async_setup\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_connect),\n\t},\n\t[IORING_OP_FALLOCATE] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_OPENAT] = {},\n\t[IORING_OP_CLOSE] = {},\n\t[IORING_OP_FILES_UPDATE] = {},\n\t[IORING_OP_STATX] = {},\n\t[IORING_OP_READ] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_WRITE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t\t.plug\t\t\t= 1,\n\t\t.async_size\t\t= sizeof(struct io_async_rw),\n\t},\n\t[IORING_OP_FADVISE] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_MADVISE] = {},\n\t[IORING_OP_SEND] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollout\t\t= 1,\n\t},\n\t[IORING_OP_RECV] = {\n\t\t.needs_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t\t.pollin\t\t\t= 1,\n\t\t.buffer_select\t\t= 1,\n\t},\n\t[IORING_OP_OPENAT2] = {\n\t},\n\t[IORING_OP_EPOLL_CTL] = {\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_SPLICE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_PROVIDE_BUFFERS] = {},\n\t[IORING_OP_REMOVE_BUFFERS] = {},\n\t[IORING_OP_TEE] = {\n\t\t.needs_file\t\t= 1,\n\t\t.hash_reg_file\t\t= 1,\n\t\t.unbound_nonreg_file\t= 1,\n\t},\n\t[IORING_OP_SHUTDOWN] = {\n\t\t.needs_file\t\t= 1,\n\t},\n\t[IORING_OP_RENAMEAT] = {},\n\t[IORING_OP_UNLINKAT] = {},\n};\n\nstatic bool io_disarm_next(struct io_kiocb *req);\nstatic void io_uring_del_task_file(unsigned long index);\nstatic void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct task_struct *task,\n\t\t\t\t\t struct files_struct *files);\nstatic void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx);\nstatic void destroy_fixed_rsrc_ref_node(struct fixed_rsrc_ref_node *ref_node);\nstatic struct fixed_rsrc_ref_node *alloc_fixed_rsrc_ref_node(\n\t\t\tstruct io_ring_ctx *ctx);\nstatic void io_ring_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc);\n\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res);\nstatic void io_put_req(struct io_kiocb *req);\nstatic void io_put_req_deferred(struct io_kiocb *req, int nr);\nstatic void io_dismantle_req(struct io_kiocb *req);\nstatic void io_put_task(struct task_struct *task, int nr);\nstatic void io_queue_next(struct io_kiocb *req);\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req);\nstatic void io_queue_linked_timeout(struct io_kiocb *req);\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_rsrc_update *ip,\n\t\t\t\t unsigned nr_args);\nstatic void io_clean_op(struct io_kiocb *req);\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed);\nstatic void __io_queue_sqe(struct io_kiocb *req);\nstatic void io_rsrc_put_work(struct work_struct *work);\n\nstatic void io_req_task_queue(struct io_kiocb *req);\nstatic void io_submit_flush_completions(struct io_comp_state *cs,\n\t\t\t\t\tstruct io_ring_ctx *ctx);\nstatic bool io_poll_remove_waitqs(struct io_kiocb *req);\nstatic int io_req_prep_async(struct io_kiocb *req);\n\nstatic struct kmem_cache *req_cachep;\n\nstatic const struct file_operations io_uring_fops;\n\nstruct sock *io_uring_get_socket(struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tif (file->f_op == &io_uring_fops) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\treturn ctx->ring_sock->sk;\n\t}\n#endif\n\treturn NULL;\n}\nEXPORT_SYMBOL(io_uring_get_socket);\n\n#define io_for_each_link(pos, head) \\\n\tfor (pos = (head); pos; pos = pos->link)\n\nstatic inline void io_set_resource_node(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->fixed_rsrc_refs) {\n\t\treq->fixed_rsrc_refs = &ctx->file_data->node->refs;\n\t\tpercpu_ref_get(req->fixed_rsrc_refs);\n\t}\n}\n\nstatic bool io_match_task(struct io_kiocb *head,\n\t\t\t  struct task_struct *task,\n\t\t\t  struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (!files)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (req->flags & REQ_F_INFLIGHT)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void req_set_fail_links(struct io_kiocb *req)\n{\n\tif ((req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) == REQ_F_LINK)\n\t\treq->flags |= REQ_F_FAIL_LINK;\n}\n\nstatic void io_ring_ctx_ref_free(struct percpu_ref *ref)\n{\n\tstruct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);\n\n\tcomplete(&ctx->ref_comp);\n}\n\nstatic inline bool io_is_timeout_noseq(struct io_kiocb *req)\n{\n\treturn !req->timeout.off;\n}\n\nstatic struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\t/*\n\t * Use 5 bits less than the max cq entries, that should give us around\n\t * 32 entries per hash list if totally full and uniformly spread.\n\t */\n\thash_bits = ilog2(p->cq_entries);\n\thash_bits -= 5;\n\tif (hash_bits <= 0)\n\t\thash_bits = 1;\n\tctx->cancel_hash_bits = hash_bits;\n\tctx->cancel_hash = kmalloc((1U << hash_bits) * sizeof(struct hlist_head),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->cancel_hash)\n\t\tgoto err;\n\t__hash_init(ctx->cancel_hash, 1U << hash_bits);\n\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tinit_completion(&ctx->ref_comp);\n\txa_init_flags(&ctx->io_buffers, XA_FLAGS_ALLOC1);\n\txa_init_flags(&ctx->personalities, XA_FLAGS_ALLOC1);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\tspin_lock_init(&ctx->completion_lock);\n\tINIT_LIST_HEAD(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tspin_lock_init(&ctx->inflight_lock);\n\tINIT_LIST_HEAD(&ctx->inflight_list);\n\tspin_lock_init(&ctx->rsrc_ref_lock);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tINIT_DELAYED_WORK(&ctx->rsrc_put_work, io_rsrc_put_work);\n\tinit_llist_head(&ctx->rsrc_put_llist);\n\tINIT_LIST_HEAD(&ctx->tctx_list);\n\tINIT_LIST_HEAD(&ctx->submit_state.comp.free_list);\n\tINIT_LIST_HEAD(&ctx->submit_state.comp.locked_free_list);\n\treturn ctx;\nerr:\n\tkfree(ctx->cancel_hash);\n\tkfree(ctx);\n\treturn NULL;\n}\n\nstatic bool req_need_defer(struct io_kiocb *req, u32 seq)\n{\n\tif (unlikely(req->flags & REQ_F_IO_DRAIN)) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\treturn seq != ctx->cached_cq_tail\n\t\t\t\t+ READ_ONCE(ctx->cached_cq_overflow);\n\t}\n\n\treturn false;\n}\n\nstatic void io_req_track_inflight(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!(req->flags & REQ_F_INFLIGHT)) {\n\t\treq->flags |= REQ_F_INFLIGHT;\n\n\t\tspin_lock_irq(&ctx->inflight_lock);\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\tspin_unlock_irq(&ctx->inflight_lock);\n\t}\n}\n\nstatic void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->work.creds)\n\t\treq->work.creds = get_current_cred();\n\n\treq->work.list.next = NULL;\n\treq->work.flags = 0;\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->flags & REQ_F_ISREG) {\n\t\tif (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_SPLICE:\n\tcase IORING_OP_TEE:\n\t\t/*\n\t\t * Splice operation will be punted aync, and here need to\n\t\t * modify io_wq_work.flags, so initialize io_wq_work firstly.\n\t\t */\n\t\tif (!S_ISREG(file_inode(req->splice.file_in)->i_mode))\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t\tbreak;\n\t}\n}\n\nstatic void io_prep_async_link(struct io_kiocb *req)\n{\n\tstruct io_kiocb *cur;\n\n\tio_for_each_link(cur, req)\n\t\tio_prep_async_work(cur);\n}\n\nstatic void io_queue_async_work(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *link = io_prep_linked_timeout(req);\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\n\tBUG_ON(!tctx);\n\tBUG_ON(!tctx->io_wq);\n\n\t/* init ->work of the whole link before punting */\n\tio_prep_async_link(req);\n\ttrace_io_uring_queue_async_work(ctx, io_wq_is_hashed(&req->work), req,\n\t\t\t\t\t&req->work, req->flags);\n\tio_wq_enqueue(tctx->io_wq, &req->work);\n\tif (link)\n\t\tio_queue_linked_timeout(link);\n}\n\nstatic void io_kill_timeout(struct io_kiocb *req, int status)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\tint ret;\n\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret != -1) {\n\t\tatomic_set(&req->ctx->cq_timeouts,\n\t\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_cqring_fill_event(req, status);\n\t\tio_put_req_deferred(req, 1);\n\t}\n}\n\nstatic void __io_queue_deferred(struct io_ring_ctx *ctx)\n{\n\tdo {\n\t\tstruct io_defer_entry *de = list_first_entry(&ctx->defer_list,\n\t\t\t\t\t\tstruct io_defer_entry, list);\n\n\t\tif (req_need_defer(de->req, de->seq))\n\t\t\tbreak;\n\t\tlist_del_init(&de->list);\n\t\tio_req_task_queue(de->req);\n\t\tkfree(de);\n\t} while (!list_empty(&ctx->defer_list));\n}\n\nstatic void io_flush_timeouts(struct io_ring_ctx *ctx)\n{\n\tu32 seq;\n\n\tif (list_empty(&ctx->timeout_list))\n\t\treturn;\n\n\tseq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\n\tdo {\n\t\tu32 events_needed, events_got;\n\t\tstruct io_kiocb *req = list_first_entry(&ctx->timeout_list,\n\t\t\t\t\t\tstruct io_kiocb, timeout.list);\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Since seq can easily wrap around over time, subtract\n\t\t * the last seq at which timeouts were flushed before comparing.\n\t\t * Assuming not more than 2^31-1 events have happened since,\n\t\t * these subtractions won't have wrapped, so we can check if\n\t\t * target is in [last_seq, current_seq] by comparing the two.\n\t\t */\n\t\tevents_needed = req->timeout.target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tlist_del_init(&req->timeout.list);\n\t\tio_kill_timeout(req, 0);\n\t} while (!list_empty(&ctx->timeout_list));\n\n\tctx->cq_last_tm_flush = seq;\n}\n\nstatic void io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\tio_flush_timeouts(ctx);\n\n\t/* order cqe stores with ring update */\n\tsmp_store_release(&ctx->rings->cq.tail, ctx->cached_cq_tail);\n\n\tif (unlikely(!list_empty(&ctx->defer_list)))\n\t\t__io_queue_deferred(ctx);\n}\n\nstatic inline bool io_sqring_full(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *r = ctx->rings;\n\n\treturn READ_ONCE(r->sq.tail) - ctx->cached_sq_head == r->sq_ring_entries;\n}\n\nstatic inline unsigned int __io_cqring_events(struct io_ring_ctx *ctx)\n{\n\treturn ctx->cached_cq_tail - READ_ONCE(ctx->rings->cq.head);\n}\n\nstatic struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned tail;\n\n\t/*\n\t * writes to the cq entry need to come after reading head; the\n\t * control dependency is enough as we're using WRITE_ONCE to\n\t * fill the cq entry\n\t */\n\tif (__io_cqring_events(ctx) == rings->cq_ring_entries)\n\t\treturn NULL;\n\n\ttail = ctx->cached_cq_tail++;\n\treturn &rings->cqes[tail & ctx->cq_mask];\n}\n\nstatic inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)\n{\n\tif (!ctx->cq_ev_fd)\n\t\treturn false;\n\tif (READ_ONCE(ctx->rings->cq_flags) & IORING_CQ_EVENTFD_DISABLED)\n\t\treturn false;\n\tif (!ctx->eventfd_async)\n\t\treturn true;\n\treturn io_wq_current_is_worker();\n}\n\nstatic void io_cqring_ev_posted(struct io_ring_ctx *ctx)\n{\n\t/* see waitqueue_active() comment */\n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n\tif (ctx->sq_data && waitqueue_active(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n\tif (io_should_trigger_evfd(ctx))\n\t\teventfd_signal(ctx->cq_ev_fd, 1);\n\tif (waitqueue_active(&ctx->cq_wait)) {\n\t\twake_up_interruptible(&ctx->cq_wait);\n\t\tkill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);\n\t}\n}\n\nstatic void io_cqring_ev_posted_iopoll(struct io_ring_ctx *ctx)\n{\n\t/* see waitqueue_active() comment */\n\tsmp_mb();\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tif (waitqueue_active(&ctx->wait))\n\t\t\twake_up(&ctx->wait);\n\t}\n\tif (io_should_trigger_evfd(ctx))\n\t\teventfd_signal(ctx->cq_ev_fd, 1);\n\tif (waitqueue_active(&ctx->cq_wait)) {\n\t\twake_up_interruptible(&ctx->cq_wait);\n\t\tkill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);\n\t}\n}\n\n/* Returns true if there are no backlogged entries after the flush */\nstatic bool __io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned long flags;\n\tbool all_flushed, posted;\n\n\tif (!force && __io_cqring_events(ctx) == rings->cq_ring_entries)\n\t\treturn false;\n\n\tposted = false;\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\twhile (!list_empty(&ctx->cq_overflow_list)) {\n\t\tstruct io_uring_cqe *cqe = io_get_cqring(ctx);\n\t\tstruct io_overflow_cqe *ocqe;\n\n\t\tif (!cqe && !force)\n\t\t\tbreak;\n\t\tocqe = list_first_entry(&ctx->cq_overflow_list,\n\t\t\t\t\tstruct io_overflow_cqe, list);\n\t\tif (cqe)\n\t\t\tmemcpy(cqe, &ocqe->cqe, sizeof(*cqe));\n\t\telse\n\t\t\tWRITE_ONCE(ctx->rings->cq_overflow,\n\t\t\t\t   ++ctx->cached_cq_overflow);\n\t\tposted = true;\n\t\tlist_del(&ocqe->list);\n\t\tkfree(ocqe);\n\t}\n\n\tall_flushed = list_empty(&ctx->cq_overflow_list);\n\tif (all_flushed) {\n\t\tclear_bit(0, &ctx->sq_check_overflow);\n\t\tclear_bit(0, &ctx->cq_check_overflow);\n\t\tctx->rings->sq_flags &= ~IORING_SQ_CQ_OVERFLOW;\n\t}\n\n\tif (posted)\n\t\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tif (posted)\n\t\tio_cqring_ev_posted(ctx);\n\treturn all_flushed;\n}\n\nstatic bool io_cqring_overflow_flush(struct io_ring_ctx *ctx, bool force)\n{\n\tbool ret = true;\n\n\tif (test_bit(0, &ctx->cq_check_overflow)) {\n\t\t/* iopoll syncs against uring_lock, not completion_lock */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\tret = __io_cqring_overflow_flush(ctx, force);\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn ret;\n}\n\n/*\n * Shamelessly stolen from the mm implementation of page reference checking,\n * see commit f958d7b528b1 for details.\n */\n#define req_ref_zero_or_close_to_overflow(req)\t\\\n\t((unsigned int) atomic_read(&(req->refs)) + 127u <= 127u)\n\nstatic inline bool req_ref_inc_not_zero(struct io_kiocb *req)\n{\n\treturn atomic_inc_not_zero(&req->refs);\n}\n\nstatic inline bool req_ref_sub_and_test(struct io_kiocb *req, int refs)\n{\n\tWARN_ON_ONCE(req_ref_zero_or_close_to_overflow(req));\n\treturn atomic_sub_and_test(refs, &req->refs);\n}\n\nstatic inline bool req_ref_put_and_test(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(req_ref_zero_or_close_to_overflow(req));\n\treturn atomic_dec_and_test(&req->refs);\n}\n\nstatic inline void req_ref_put(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(req_ref_put_and_test(req));\n}\n\nstatic inline void req_ref_get(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(req_ref_zero_or_close_to_overflow(req));\n\tatomic_inc(&req->refs);\n}\n\nstatic bool __io_cqring_fill_event(struct io_kiocb *req, long res,\n\t\t\t\t   unsigned int cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_cqe *cqe;\n\n\ttrace_io_uring_complete(ctx, req->user_data, res, cflags);\n\n\t/*\n\t * If we can't get a cq entry, userspace overflowed the\n\t * submission (by quite a lot). Increment the overflow count in\n\t * the ring.\n\t */\n\tcqe = io_get_cqring(ctx);\n\tif (likely(cqe)) {\n\t\tWRITE_ONCE(cqe->user_data, req->user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\t\treturn true;\n\t}\n\tif (!ctx->cq_overflow_flushed &&\n\t    !atomic_read(&req->task->io_uring->in_idle)) {\n\t\tstruct io_overflow_cqe *ocqe;\n\n\t\tocqe = kmalloc(sizeof(*ocqe), GFP_ATOMIC | __GFP_ACCOUNT);\n\t\tif (!ocqe)\n\t\t\tgoto overflow;\n\t\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\t\tset_bit(0, &ctx->sq_check_overflow);\n\t\t\tset_bit(0, &ctx->cq_check_overflow);\n\t\t\tctx->rings->sq_flags |= IORING_SQ_CQ_OVERFLOW;\n\t\t}\n\t\tocqe->cqe.user_data = req->user_data;\n\t\tocqe->cqe.res = res;\n\t\tocqe->cqe.flags = cflags;\n\t\tlist_add_tail(&ocqe->list, &ctx->cq_overflow_list);\n\t\treturn true;\n\t}\noverflow:\n\t/*\n\t * If we're in ring overflow flush mode, or in task cancel mode,\n\t * or cannot allocate an overflow entry, then we need to drop it\n\t * on the floor.\n\t */\n\tWRITE_ONCE(ctx->rings->cq_overflow, ++ctx->cached_cq_overflow);\n\treturn false;\n}\n\nstatic void io_cqring_fill_event(struct io_kiocb *req, long res)\n{\n\t__io_cqring_fill_event(req, res, 0);\n}\n\nstatic void io_req_complete_post(struct io_kiocb *req, long res,\n\t\t\t\t unsigned int cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t__io_cqring_fill_event(req, res, cflags);\n\t/*\n\t * If we're the last reference to this request, add to our locked\n\t * free_list cache.\n\t */\n\tif (req_ref_put_and_test(req)) {\n\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\tif (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {\n\t\t\tif (req->flags & (REQ_F_LINK_TIMEOUT | REQ_F_FAIL_LINK))\n\t\t\t\tio_disarm_next(req);\n\t\t\tif (req->link) {\n\t\t\t\tio_req_task_queue(req->link);\n\t\t\t\treq->link = NULL;\n\t\t\t}\n\t\t}\n\t\tio_dismantle_req(req);\n\t\tio_put_task(req->task, 1);\n\t\tlist_add(&req->compl.list, &cs->locked_free_list);\n\t\tcs->locked_free_nr++;\n\t} else {\n\t\tif (!percpu_ref_tryget(&ctx->refs))\n\t\t\treq = NULL;\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (req) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tpercpu_ref_put(&ctx->refs);\n\t}\n}\n\nstatic void io_req_complete_state(struct io_kiocb *req, long res,\n\t\t\t\t  unsigned int cflags)\n{\n\tif (req->flags & (REQ_F_NEED_CLEANUP | REQ_F_BUFFER_SELECTED))\n\t\tio_clean_op(req);\n\treq->result = res;\n\treq->compl.cflags = cflags;\n\treq->flags |= REQ_F_COMPLETE_INLINE;\n}\n\nstatic inline void __io_req_complete(struct io_kiocb *req, unsigned issue_flags,\n\t\t\t\t     long res, unsigned cflags)\n{\n\tif (issue_flags & IO_URING_F_COMPLETE_DEFER)\n\t\tio_req_complete_state(req, res, cflags);\n\telse\n\t\tio_req_complete_post(req, res, cflags);\n}\n\nstatic inline void io_req_complete(struct io_kiocb *req, long res)\n{\n\t__io_req_complete(req, 0, res, 0);\n}\n\nstatic void io_req_complete_failed(struct io_kiocb *req, long res)\n{\n\treq_set_fail_links(req);\n\tio_put_req(req);\n\tio_req_complete_post(req, res, 0);\n}\n\nstatic void io_flush_cached_locked_reqs(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_comp_state *cs)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_splice_init(&cs->locked_free_list, &cs->free_list);\n\tcs->locked_free_nr = 0;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\n/* Returns true IFF there are requests in the cache */\nstatic bool io_flush_cached_reqs(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\tstruct io_comp_state *cs = &state->comp;\n\tint nr;\n\n\t/*\n\t * If we have more than a batch's worth of requests in our IRQ side\n\t * locked cache, grab the lock and move them over to our submission\n\t * side cache.\n\t */\n\tif (READ_ONCE(cs->locked_free_nr) > IO_COMPL_BATCH)\n\t\tio_flush_cached_locked_reqs(ctx, cs);\n\n\tnr = state->free_reqs;\n\twhile (!list_empty(&cs->free_list)) {\n\t\tstruct io_kiocb *req = list_first_entry(&cs->free_list,\n\t\t\t\t\t\tstruct io_kiocb, compl.list);\n\n\t\tlist_del(&req->compl.list);\n\t\tstate->reqs[nr++] = req;\n\t\tif (nr == ARRAY_SIZE(state->reqs))\n\t\t\tbreak;\n\t}\n\n\tstate->free_reqs = nr;\n\treturn nr != 0;\n}\n\nstatic struct io_kiocb *io_alloc_req(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\n\tBUILD_BUG_ON(IO_REQ_ALLOC_BATCH > ARRAY_SIZE(state->reqs));\n\n\tif (!state->free_reqs) {\n\t\tgfp_t gfp = GFP_KERNEL | __GFP_NOWARN;\n\t\tint ret;\n\n\t\tif (io_flush_cached_reqs(ctx))\n\t\t\tgoto got_req;\n\n\t\tret = kmem_cache_alloc_bulk(req_cachep, gfp, IO_REQ_ALLOC_BATCH,\n\t\t\t\t\t    state->reqs);\n\n\t\t/*\n\t\t * Bulk alloc is all-or-nothing. If we fail to get a batch,\n\t\t * retry single alloc to be on the safe side.\n\t\t */\n\t\tif (unlikely(ret <= 0)) {\n\t\t\tstate->reqs[0] = kmem_cache_alloc(req_cachep, gfp);\n\t\t\tif (!state->reqs[0])\n\t\t\t\treturn NULL;\n\t\t\tret = 1;\n\t\t}\n\t\tstate->free_reqs = ret;\n\t}\ngot_req:\n\tstate->free_reqs--;\n\treturn state->reqs[state->free_reqs];\n}\n\nstatic inline void io_put_file(struct file *file)\n{\n\tif (file)\n\t\tfput(file);\n}\n\nstatic void io_dismantle_req(struct io_kiocb *req)\n{\n\tunsigned int flags = req->flags;\n\n\tif (!(flags & REQ_F_FIXED_FILE))\n\t\tio_put_file(req->file);\n\tif (flags & (REQ_F_NEED_CLEANUP | REQ_F_BUFFER_SELECTED |\n\t\t     REQ_F_INFLIGHT)) {\n\t\tio_clean_op(req);\n\n\t\tif (req->flags & REQ_F_INFLIGHT) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&ctx->inflight_lock, flags);\n\t\t\tlist_del(&req->inflight_entry);\n\t\t\tspin_unlock_irqrestore(&ctx->inflight_lock, flags);\n\t\t\treq->flags &= ~REQ_F_INFLIGHT;\n\t\t}\n\t}\n\tif (req->fixed_rsrc_refs)\n\t\tpercpu_ref_put(req->fixed_rsrc_refs);\n\tif (req->async_data)\n\t\tkfree(req->async_data);\n\tif (req->work.creds) {\n\t\tput_cred(req->work.creds);\n\t\treq->work.creds = NULL;\n\t}\n}\n\n/* must to be called somewhat shortly after putting a request */\nstatic inline void io_put_task(struct task_struct *task, int nr)\n{\n\tstruct io_uring_task *tctx = task->io_uring;\n\n\tpercpu_counter_sub(&tctx->inflight, nr);\n\tif (unlikely(atomic_read(&tctx->in_idle)))\n\t\twake_up(&tctx->wait);\n\tput_task_struct_many(task, nr);\n}\n\nstatic void __io_free_req(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tio_dismantle_req(req);\n\tio_put_task(req->task, 1);\n\n\tkmem_cache_free(req_cachep, req);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic inline void io_remove_next_linked(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\treq->link = nxt->link;\n\tnxt->link = NULL;\n}\n\nstatic bool io_kill_linked_timeout(struct io_kiocb *req)\n\t__must_hold(&req->ctx->completion_lock)\n{\n\tstruct io_kiocb *link = req->link;\n\n\t/*\n\t * Can happen if a linked timeout fired and link had been like\n\t * req -> link t-out -> link t-out [-> ...]\n\t */\n\tif (link && (link->flags & REQ_F_LTIMEOUT_ACTIVE)) {\n\t\tstruct io_timeout_data *io = link->async_data;\n\t\tint ret;\n\n\t\tio_remove_next_linked(req);\n\t\tlink->timeout.head = NULL;\n\t\tret = hrtimer_try_to_cancel(&io->timer);\n\t\tif (ret != -1) {\n\t\t\tio_cqring_fill_event(link, -ECANCELED);\n\t\t\tio_put_req_deferred(link, 1);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic void io_fail_links(struct io_kiocb *req)\n\t__must_hold(&req->ctx->completion_lock)\n{\n\tstruct io_kiocb *nxt, *link = req->link;\n\n\treq->link = NULL;\n\twhile (link) {\n\t\tnxt = link->link;\n\t\tlink->link = NULL;\n\n\t\ttrace_io_uring_fail_link(req, link);\n\t\tio_cqring_fill_event(link, -ECANCELED);\n\t\tio_put_req_deferred(link, 2);\n\t\tlink = nxt;\n\t}\n}\n\nstatic bool io_disarm_next(struct io_kiocb *req)\n\t__must_hold(&req->ctx->completion_lock)\n{\n\tbool posted = false;\n\n\tif (likely(req->flags & REQ_F_LINK_TIMEOUT))\n\t\tposted = io_kill_linked_timeout(req);\n\tif (unlikely(req->flags & REQ_F_FAIL_LINK)) {\n\t\tposted |= (req->link != NULL);\n\t\tio_fail_links(req);\n\t}\n\treturn posted;\n}\n\nstatic struct io_kiocb *__io_req_find_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt;\n\n\t/*\n\t * If LINK is set, we have dependent requests in this chain. If we\n\t * didn't fail this request, queue the first one up, moving any other\n\t * dependencies to the next request. In case of failure, fail the rest\n\t * of the chain.\n\t */\n\tif (req->flags & (REQ_F_LINK_TIMEOUT | REQ_F_FAIL_LINK)) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tunsigned long flags;\n\t\tbool posted;\n\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tposted = io_disarm_next(req);\n\t\tif (posted)\n\t\t\tio_commit_cqring(req->ctx);\n\t\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\t\tif (posted)\n\t\t\tio_cqring_ev_posted(ctx);\n\t}\n\tnxt = req->link;\n\treq->link = NULL;\n\treturn nxt;\n}\n\nstatic inline struct io_kiocb *io_req_find_next(struct io_kiocb *req)\n{\n\tif (likely(!(req->flags & (REQ_F_LINK|REQ_F_HARDLINK))))\n\t\treturn NULL;\n\treturn __io_req_find_next(req);\n}\n\nstatic void ctx_flush_and_put(struct io_ring_ctx *ctx)\n{\n\tif (!ctx)\n\t\treturn;\n\tif (ctx->submit_state.comp.nr) {\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tio_submit_flush_completions(&ctx->submit_state.comp, ctx);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic bool __tctx_task_work(struct io_uring_task *tctx)\n{\n\tstruct io_ring_ctx *ctx = NULL;\n\tstruct io_wq_work_list list;\n\tstruct io_wq_work_node *node;\n\n\tif (wq_list_empty(&tctx->task_list))\n\t\treturn false;\n\n\tspin_lock_irq(&tctx->task_lock);\n\tlist = tctx->task_list;\n\tINIT_WQ_LIST(&tctx->task_list);\n\tspin_unlock_irq(&tctx->task_lock);\n\n\tnode = list.first;\n\twhile (node) {\n\t\tstruct io_wq_work_node *next = node->next;\n\t\tstruct io_kiocb *req;\n\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tif (req->ctx != ctx) {\n\t\t\tctx_flush_and_put(ctx);\n\t\t\tctx = req->ctx;\n\t\t\tpercpu_ref_get(&ctx->refs);\n\t\t}\n\n\t\treq->task_work.func(&req->task_work);\n\t\tnode = next;\n\t}\n\n\tctx_flush_and_put(ctx);\n\treturn list.first != NULL;\n}\n\nstatic void tctx_task_work(struct callback_head *cb)\n{\n\tstruct io_uring_task *tctx = container_of(cb, struct io_uring_task, task_work);\n\n\tclear_bit(0, &tctx->task_state);\n\n\twhile (__tctx_task_work(tctx))\n\t\tcond_resched();\n}\n\nstatic int io_req_task_work_add(struct io_kiocb *req)\n{\n\tstruct task_struct *tsk = req->task;\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\tenum task_work_notify_mode notify;\n\tstruct io_wq_work_node *node, *prev;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (unlikely(tsk->flags & PF_EXITING))\n\t\treturn -ESRCH;\n\n\tWARN_ON_ONCE(!tctx);\n\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_add_tail(&req->io_task_work.node, &tctx->task_list);\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\n\t/* task_work already pending, we're done */\n\tif (test_bit(0, &tctx->task_state) ||\n\t    test_and_set_bit(0, &tctx->task_state))\n\t\treturn 0;\n\n\t/*\n\t * SQPOLL kernel thread doesn't need notification, just a wakeup. For\n\t * all other cases, use TWA_SIGNAL unconditionally to ensure we're\n\t * processing task_work. There's no reliable way to tell if TWA_RESUME\n\t * will do the job.\n\t */\n\tnotify = (req->ctx->flags & IORING_SETUP_SQPOLL) ? TWA_NONE : TWA_SIGNAL;\n\n\tif (!task_work_add(tsk, &tctx->task_work, notify)) {\n\t\twake_up_process(tsk);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Slow path - we failed, find and delete work. if the work is not\n\t * in the list, it got run and we're fine.\n\t */\n\tspin_lock_irqsave(&tctx->task_lock, flags);\n\twq_list_for_each(node, prev, &tctx->task_list) {\n\t\tif (&req->io_task_work.node == node) {\n\t\t\twq_list_del(&tctx->task_list, node, prev);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&tctx->task_lock, flags);\n\tclear_bit(0, &tctx->task_state);\n\treturn ret;\n}\n\nstatic bool io_run_task_work_head(struct callback_head **work_head)\n{\n\tstruct callback_head *work, *next;\n\tbool executed = false;\n\n\tdo {\n\t\twork = xchg(work_head, NULL);\n\t\tif (!work)\n\t\t\tbreak;\n\n\t\tdo {\n\t\t\tnext = work->next;\n\t\t\twork->func(work);\n\t\t\twork = next;\n\t\t\tcond_resched();\n\t\t} while (work);\n\t\texecuted = true;\n\t} while (1);\n\n\treturn executed;\n}\n\nstatic void io_task_work_add_head(struct callback_head **work_head,\n\t\t\t\t  struct callback_head *task_work)\n{\n\tstruct callback_head *head;\n\n\tdo {\n\t\thead = READ_ONCE(*work_head);\n\t\ttask_work->next = head;\n\t} while (cmpxchg(work_head, head, task_work) != head);\n}\n\nstatic void io_req_task_work_add_fallback(struct io_kiocb *req,\n\t\t\t\t\t  task_work_func_t cb)\n{\n\tinit_task_work(&req->task_work, cb);\n\tio_task_work_add_head(&req->ctx->exit_task_work, &req->task_work);\n}\n\nstatic void io_req_task_cancel(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/* ctx is guaranteed to stay alive while we hold uring_lock */\n\tmutex_lock(&ctx->uring_lock);\n\tio_req_complete_failed(req, req->result);\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void __io_req_task_submit(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/* ctx stays valid until unlock, even if we drop all ours ctx->refs */\n\tmutex_lock(&ctx->uring_lock);\n\tif (!(current->flags & PF_EXITING) && !current->in_execve)\n\t\t__io_queue_sqe(req);\n\telse\n\t\tio_req_complete_failed(req, -EFAULT);\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_req_task_submit(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\t__io_req_task_submit(req);\n}\n\nstatic void io_req_task_queue_fail(struct io_kiocb *req, int ret)\n{\n\treq->result = ret;\n\treq->task_work.func = io_req_task_cancel;\n\n\tif (unlikely(io_req_task_work_add(req)))\n\t\tio_req_task_work_add_fallback(req, io_req_task_cancel);\n}\n\nstatic void io_req_task_queue(struct io_kiocb *req)\n{\n\treq->task_work.func = io_req_task_submit;\n\n\tif (unlikely(io_req_task_work_add(req)))\n\t\tio_req_task_queue_fail(req, -ECANCELED);\n}\n\nstatic inline void io_queue_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = io_req_find_next(req);\n\n\tif (nxt)\n\t\tio_req_task_queue(nxt);\n}\n\nstatic void io_free_req(struct io_kiocb *req)\n{\n\tio_queue_next(req);\n\t__io_free_req(req);\n}\n\nstruct req_batch {\n\tstruct task_struct\t*task;\n\tint\t\t\ttask_refs;\n\tint\t\t\tctx_refs;\n};\n\nstatic inline void io_init_req_batch(struct req_batch *rb)\n{\n\trb->task_refs = 0;\n\trb->ctx_refs = 0;\n\trb->task = NULL;\n}\n\nstatic void io_req_free_batch_finish(struct io_ring_ctx *ctx,\n\t\t\t\t     struct req_batch *rb)\n{\n\tif (rb->task)\n\t\tio_put_task(rb->task, rb->task_refs);\n\tif (rb->ctx_refs)\n\t\tpercpu_ref_put_many(&ctx->refs, rb->ctx_refs);\n}\n\nstatic void io_req_free_batch(struct req_batch *rb, struct io_kiocb *req,\n\t\t\t      struct io_submit_state *state)\n{\n\tio_queue_next(req);\n\tio_dismantle_req(req);\n\n\tif (req->task != rb->task) {\n\t\tif (rb->task)\n\t\t\tio_put_task(rb->task, rb->task_refs);\n\t\trb->task = req->task;\n\t\trb->task_refs = 0;\n\t}\n\trb->task_refs++;\n\trb->ctx_refs++;\n\n\tif (state->free_reqs != ARRAY_SIZE(state->reqs))\n\t\tstate->reqs[state->free_reqs++] = req;\n\telse\n\t\tlist_add(&req->compl.list, &state->comp.free_list);\n}\n\nstatic void io_submit_flush_completions(struct io_comp_state *cs,\n\t\t\t\t\tstruct io_ring_ctx *ctx)\n{\n\tint i, nr = cs->nr;\n\tstruct io_kiocb *req;\n\tstruct req_batch rb;\n\n\tio_init_req_batch(&rb);\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < nr; i++) {\n\t\treq = cs->reqs[i];\n\t\t__io_cqring_fill_event(req, req->result, req->compl.cflags);\n\t}\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_cqring_ev_posted(ctx);\n\tfor (i = 0; i < nr; i++) {\n\t\treq = cs->reqs[i];\n\n\t\t/* submission and completion refs */\n\t\tif (req_ref_sub_and_test(req, 2))\n\t\t\tio_req_free_batch(&rb, req, &ctx->submit_state);\n\t}\n\n\tio_req_free_batch_finish(ctx, &rb);\n\tcs->nr = 0;\n}\n\n/*\n * Drop reference to request, return next in chain (if there is one) if this\n * was the last reference to this request.\n */\nstatic inline struct io_kiocb *io_put_req_find_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = NULL;\n\n\tif (req_ref_put_and_test(req)) {\n\t\tnxt = io_req_find_next(req);\n\t\t__io_free_req(req);\n\t}\n\treturn nxt;\n}\n\nstatic inline void io_put_req(struct io_kiocb *req)\n{\n\tif (req_ref_put_and_test(req))\n\t\tio_free_req(req);\n}\n\nstatic void io_put_req_deferred_cb(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\n\tio_free_req(req);\n}\n\nstatic void io_free_req_deferred(struct io_kiocb *req)\n{\n\treq->task_work.func = io_put_req_deferred_cb;\n\tif (unlikely(io_req_task_work_add(req)))\n\t\tio_req_task_work_add_fallback(req, io_put_req_deferred_cb);\n}\n\nstatic inline void io_put_req_deferred(struct io_kiocb *req, int refs)\n{\n\tif (req_ref_sub_and_test(req, refs))\n\t\tio_free_req_deferred(req);\n}\n\nstatic unsigned io_cqring_events(struct io_ring_ctx *ctx)\n{\n\t/* See comment at the top of this file */\n\tsmp_rmb();\n\treturn __io_cqring_events(ctx);\n}\n\nstatic inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/* make sure SQ entry isn't read before tail */\n\treturn smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;\n}\n\nstatic unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)\n{\n\tunsigned int cflags;\n\n\tcflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;\n\tcflags |= IORING_CQE_F_BUFFER;\n\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\tkfree(kbuf);\n\treturn cflags;\n}\n\nstatic inline unsigned int io_put_rw_kbuf(struct io_kiocb *req)\n{\n\tstruct io_buffer *kbuf;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\treturn io_put_kbuf(req, kbuf);\n}\n\nstatic inline bool io_run_task_work(void)\n{\n\t/*\n\t * Not safe to run on exiting task, and the task_work handling will\n\t * not add work to such a task.\n\t */\n\tif (unlikely(current->flags & PF_EXITING))\n\t\treturn false;\n\tif (current->task_works) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\ttask_work_run();\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Find and free completed poll iocbs\n */\nstatic void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t       struct list_head *done)\n{\n\tstruct req_batch rb;\n\tstruct io_kiocb *req;\n\n\t/* order with ->result store in io_complete_rw_iopoll() */\n\tsmp_rmb();\n\n\tio_init_req_batch(&rb);\n\twhile (!list_empty(done)) {\n\t\tint cflags = 0;\n\n\t\treq = list_first_entry(done, struct io_kiocb, inflight_entry);\n\t\tlist_del(&req->inflight_entry);\n\n\t\tif (READ_ONCE(req->result) == -EAGAIN &&\n\t\t    !(req->flags & REQ_F_DONT_REISSUE)) {\n\t\t\treq->iopoll_completed = 0;\n\t\t\treq_ref_get(req);\n\t\t\tio_queue_async_work(req);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\t\tcflags = io_put_rw_kbuf(req);\n\n\t\t__io_cqring_fill_event(req, req->result, cflags);\n\t\t(*nr_events)++;\n\n\t\tif (req_ref_put_and_test(req))\n\t\t\tio_req_free_batch(&rb, req, &ctx->submit_state);\n\t}\n\n\tio_commit_cqring(ctx);\n\tio_cqring_ev_posted_iopoll(ctx);\n\tio_req_free_batch_finish(ctx, &rb);\n}\n\nstatic int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\tlong min)\n{\n\tstruct io_kiocb *req, *tmp;\n\tLIST_HEAD(done);\n\tbool spin;\n\tint ret;\n\n\t/*\n\t * Only spin for completions if we don't have multiple devices hanging\n\t * off our complete list, and we're under the requested amount.\n\t */\n\tspin = !ctx->poll_multi_file && *nr_events < min;\n\n\tret = 0;\n\tlist_for_each_entry_safe(req, tmp, &ctx->iopoll_list, inflight_entry) {\n\t\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t\t/*\n\t\t * Move completed and retryable entries to our local lists.\n\t\t * If we find a request that requires polling, break out\n\t\t * and complete those lists first, if we have entries there.\n\t\t */\n\t\tif (READ_ONCE(req->iopoll_completed)) {\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!list_empty(&done))\n\t\t\tbreak;\n\n\t\tret = kiocb->ki_filp->f_op->iopoll(kiocb, spin);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\t/* iopoll may have completed current req */\n\t\tif (READ_ONCE(req->iopoll_completed))\n\t\t\tlist_move_tail(&req->inflight_entry, &done);\n\n\t\tif (ret && spin)\n\t\t\tspin = false;\n\t\tret = 0;\n\t}\n\n\tif (!list_empty(&done))\n\t\tio_iopoll_complete(ctx, nr_events, &done);\n\n\treturn ret;\n}\n\n/*\n * Poll for a minimum of 'min' events. Note that if min == 0 we consider that a\n * non-spinning poll check - we'll still enter the driver poll loop, but only\n * as a non-spinning completion check.\n */\nstatic int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,\n\t\t\t\tlong min)\n{\n\twhile (!list_empty(&ctx->iopoll_list) && !need_resched()) {\n\t\tint ret;\n\n\t\tret = io_do_iopoll(ctx, nr_events, min);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (*nr_events >= min)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/*\n * We can't just wait for polled events to come to us, we have to actively\n * find and complete them.\n */\nstatic void io_iopoll_try_reap_events(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn;\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!list_empty(&ctx->iopoll_list)) {\n\t\tunsigned int nr_events = 0;\n\n\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\t/* let it sleep and repeat later if can't complete a request */\n\t\tif (nr_events == 0)\n\t\t\tbreak;\n\t\t/*\n\t\t * Ensure we allow local-to-the-cpu processing to take place,\n\t\t * in this case we need to ensure that we reap all events.\n\t\t * Also let task_work, etc. to progress by releasing the mutex\n\t\t */\n\t\tif (need_resched()) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tcond_resched();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic int io_iopoll_check(struct io_ring_ctx *ctx, long min)\n{\n\tunsigned int nr_events = 0;\n\tint iters = 0, ret = 0;\n\n\t/*\n\t * We disallow the app entering submit/complete with polling, but we\n\t * still need to lock the ring to prevent racing with polled issue\n\t * that got punted to a workqueue.\n\t */\n\tmutex_lock(&ctx->uring_lock);\n\tdo {\n\t\t/*\n\t\t * Don't enter poll loop if we already have events pending.\n\t\t * If we do, we can potentially be spinning for commands that\n\t\t * already triggered a CQE (eg in error).\n\t\t */\n\t\tif (test_bit(0, &ctx->cq_check_overflow))\n\t\t\t__io_cqring_overflow_flush(ctx, false);\n\t\tif (io_cqring_events(ctx))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If a submit got punted to a workqueue, we can have the\n\t\t * application entering polling for a command before it gets\n\t\t * issued. That app will hold the uring_lock for the duration\n\t\t * of the poll right here, so we need to take a breather every\n\t\t * now and then to ensure that the issue has a chance to add\n\t\t * the poll to the issued list. Otherwise we can spin here\n\t\t * forever, while the workqueue is stuck trying to acquire the\n\t\t * very same mutex.\n\t\t */\n\t\tif (!(++iters & 7)) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tio_run_task_work();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\n\t\tret = io_iopoll_getevents(ctx, &nr_events, min);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\tret = 0;\n\t} while (min && !nr_events && !need_resched());\n\n\tmutex_unlock(&ctx->uring_lock);\n\treturn ret;\n}\n\nstatic void kiocb_end_write(struct io_kiocb *req)\n{\n\t/*\n\t * Tell lockdep we inherited freeze protection from submission\n\t * thread.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tstruct super_block *sb = file_inode(req->file)->i_sb;\n\n\t\t__sb_writers_acquired(sb, SB_FREEZE_WRITE);\n\t\tsb_end_write(sb);\n\t}\n}\n\n#ifdef CONFIG_BLOCK\nstatic bool io_resubmit_prep(struct io_kiocb *req)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\n\tif (!rw)\n\t\treturn !io_req_prep_async(req);\n\t/* may have left rw->iter inconsistent on -EIOCBQUEUED */\n\tiov_iter_revert(&rw->iter, req->result - iov_iter_count(&rw->iter));\n\treturn true;\n}\n\nstatic bool io_rw_should_reissue(struct io_kiocb *req)\n{\n\tumode_t mode = file_inode(req->file)->i_mode;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!S_ISBLK(mode) && !S_ISREG(mode))\n\t\treturn false;\n\tif ((req->flags & REQ_F_NOWAIT) || (io_wq_current_is_worker() &&\n\t    !(ctx->flags & IORING_SETUP_IOPOLL)))\n\t\treturn false;\n\t/*\n\t * If ref is dying, we might be running poll reap from the exit work.\n\t * Don't attempt to reissue from that path, just let it fail with\n\t * -EAGAIN.\n\t */\n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn false;\n\treturn true;\n}\n#endif\n\nstatic bool io_rw_reissue(struct io_kiocb *req)\n{\n#ifdef CONFIG_BLOCK\n\tif (!io_rw_should_reissue(req))\n\t\treturn false;\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\tif (io_resubmit_prep(req)) {\n\t\treq_ref_get(req);\n\t\tio_queue_async_work(req);\n\t\treturn true;\n\t}\n\treq_set_fail_links(req);\n#endif\n\treturn false;\n}\n\nstatic void __io_complete_rw(struct io_kiocb *req, long res, long res2,\n\t\t\t     unsigned int issue_flags)\n{\n\tint cflags = 0;\n\n\tif (req->rw.kiocb.ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\tif (unlikely(res != req->result)) {\n\t\tif ((res == -EAGAIN || res == -EOPNOTSUPP) && io_rw_reissue(req))\n\t\t\treturn;\n\t\treq_set_fail_links(req);\n\t}\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_rw_kbuf(req);\n\t__io_req_complete(req, issue_flags, res, cflags);\n}\n\nstatic void io_complete_rw(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\t__io_complete_rw(req, res, res2, 0);\n}\n\nstatic void io_complete_rw_iopoll(struct kiocb *kiocb, long res, long res2)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tkiocb_end_write(req);\n\tif (unlikely(res != req->result)) {\n\t\tbool fail = true;\n\n#ifdef CONFIG_BLOCK\n\t\tif (res == -EAGAIN && io_rw_should_reissue(req) &&\n\t\t    io_resubmit_prep(req))\n\t\t\tfail = false;\n#endif\n\t\tif (fail) {\n\t\t\treq_set_fail_links(req);\n\t\t\treq->flags |= REQ_F_DONT_REISSUE;\n\t\t}\n\t}\n\n\tWRITE_ONCE(req->result, res);\n\t/* order with io_iopoll_complete() checking ->result */\n\tsmp_wmb();\n\tWRITE_ONCE(req->iopoll_completed, 1);\n}\n\n/*\n * After the iocb has been issued, it's safe to be found on the poll list.\n * Adding the kiocb to the list AFTER submission ensures that we don't\n * find it from a io_iopoll_getevents() thread before the issuer is done\n * accessing the kiocb cookie.\n */\nstatic void io_iopoll_req_issued(struct io_kiocb *req, bool in_async)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t/*\n\t * Track whether we have multiple files in our lists. This will impact\n\t * how we do polling eventually, not spinning if we're on potentially\n\t * different devices.\n\t */\n\tif (list_empty(&ctx->iopoll_list)) {\n\t\tctx->poll_multi_file = false;\n\t} else if (!ctx->poll_multi_file) {\n\t\tstruct io_kiocb *list_req;\n\n\t\tlist_req = list_first_entry(&ctx->iopoll_list, struct io_kiocb,\n\t\t\t\t\t\tinflight_entry);\n\t\tif (list_req->file != req->file)\n\t\t\tctx->poll_multi_file = true;\n\t}\n\n\t/*\n\t * For fast devices, IO may have already completed. If it has, add\n\t * it to the front so we find it first.\n\t */\n\tif (READ_ONCE(req->iopoll_completed))\n\t\tlist_add(&req->inflight_entry, &ctx->iopoll_list);\n\telse\n\t\tlist_add_tail(&req->inflight_entry, &ctx->iopoll_list);\n\n\t/*\n\t * If IORING_SETUP_SQPOLL is enabled, sqes are either handled in sq thread\n\t * task context or in io worker task context. If current task context is\n\t * sq thread, we don't need to check whether should wake up sq thread.\n\t */\n\tif (in_async && (ctx->flags & IORING_SETUP_SQPOLL) &&\n\t    wq_has_sleeper(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n}\n\nstatic inline void io_state_file_put(struct io_submit_state *state)\n{\n\tif (state->file_refs) {\n\t\tfput_many(state->file, state->file_refs);\n\t\tstate->file_refs = 0;\n\t}\n}\n\n/*\n * Get as many references to a file as we have IOs left in this submission,\n * assuming most submissions are for one file, or at least that each file\n * has more than one submission.\n */\nstatic struct file *__io_file_get(struct io_submit_state *state, int fd)\n{\n\tif (!state)\n\t\treturn fget(fd);\n\n\tif (state->file_refs) {\n\t\tif (state->fd == fd) {\n\t\t\tstate->file_refs--;\n\t\t\treturn state->file;\n\t\t}\n\t\tio_state_file_put(state);\n\t}\n\tstate->file = fget_many(fd, state->ios_left);\n\tif (unlikely(!state->file))\n\t\treturn NULL;\n\n\tstate->fd = fd;\n\tstate->file_refs = state->ios_left - 1;\n\treturn state->file;\n}\n\nstatic bool io_bdev_nowait(struct block_device *bdev)\n{\n\treturn !bdev || blk_queue_nowait(bdev_get_queue(bdev));\n}\n\n/*\n * If we tracked the file through the SCM inflight mechanism, we could support\n * any file. For now, just ensure that anything potentially problematic is done\n * inline.\n */\nstatic bool __io_file_supports_async(struct file *file, int rw)\n{\n\tumode_t mode = file_inode(file)->i_mode;\n\n\tif (S_ISBLK(mode)) {\n\t\tif (IS_ENABLED(CONFIG_BLOCK) &&\n\t\t    io_bdev_nowait(I_BDEV(file->f_mapping->host)))\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\tif (S_ISCHR(mode) || S_ISSOCK(mode))\n\t\treturn true;\n\tif (S_ISREG(mode)) {\n\t\tif (IS_ENABLED(CONFIG_BLOCK) &&\n\t\t    io_bdev_nowait(file->f_inode->i_sb->s_bdev) &&\n\t\t    file->f_op != &io_uring_fops)\n\t\t\treturn true;\n\t\treturn false;\n\t}\n\n\t/* any ->read/write should understand O_NONBLOCK */\n\tif (file->f_flags & O_NONBLOCK)\n\t\treturn true;\n\n\tif (!(file->f_mode & FMODE_NOWAIT))\n\t\treturn false;\n\n\tif (rw == READ)\n\t\treturn file->f_op->read_iter != NULL;\n\n\treturn file->f_op->write_iter != NULL;\n}\n\nstatic bool io_file_supports_async(struct io_kiocb *req, int rw)\n{\n\tif (rw == READ && (req->flags & REQ_F_ASYNC_READ))\n\t\treturn true;\n\telse if (rw == WRITE && (req->flags & REQ_F_ASYNC_WRITE))\n\t\treturn true;\n\n\treturn __io_file_supports_async(req->file, rw);\n}\n\nstatic int io_prep_rw(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct file *file = req->file;\n\tunsigned ioprio;\n\tint ret;\n\n\tif (!(req->flags & REQ_F_ISREG) && S_ISREG(file_inode(file)->i_mode))\n\t\treq->flags |= REQ_F_ISREG;\n\n\tkiocb->ki_pos = READ_ONCE(sqe->off);\n\tif (kiocb->ki_pos == -1 && !(file->f_mode & FMODE_STREAM)) {\n\t\treq->flags |= REQ_F_CUR_POS;\n\t\tkiocb->ki_pos = file->f_pos;\n\t}\n\tkiocb->ki_hint = ki_hint_validate(file_write_hint(kiocb->ki_filp));\n\tkiocb->ki_flags = iocb_flags(kiocb->ki_filp);\n\tret = kiocb_set_rw_flags(kiocb, READ_ONCE(sqe->rw_flags));\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\t/* don't allow async punt for O_NONBLOCK or RWF_NOWAIT */\n\tif ((kiocb->ki_flags & IOCB_NOWAIT) || (file->f_flags & O_NONBLOCK))\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tioprio = READ_ONCE(sqe->ioprio);\n\tif (ioprio) {\n\t\tret = ioprio_check_cap(ioprio);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tkiocb->ki_ioprio = ioprio;\n\t} else\n\t\tkiocb->ki_ioprio = get_current_ioprio();\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tif (!(kiocb->ki_flags & IOCB_DIRECT) ||\n\t\t    !kiocb->ki_filp->f_op->iopoll)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tkiocb->ki_flags |= IOCB_HIPRI;\n\t\tkiocb->ki_complete = io_complete_rw_iopoll;\n\t\treq->iopoll_completed = 0;\n\t} else {\n\t\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\t\treturn -EINVAL;\n\t\tkiocb->ki_complete = io_complete_rw;\n\t}\n\n\treq->rw.addr = READ_ONCE(sqe->addr);\n\treq->rw.len = READ_ONCE(sqe->len);\n\treq->buf_index = READ_ONCE(sqe->buf_index);\n\treturn 0;\n}\n\nstatic inline void io_rw_done(struct kiocb *kiocb, ssize_t ret)\n{\n\tswitch (ret) {\n\tcase -EIOCBQUEUED:\n\t\tbreak;\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\tcase -ERESTART_RESTARTBLOCK:\n\t\t/*\n\t\t * We can't just restart the syscall, since previously\n\t\t * submitted sqes may already be in progress. Just fail this\n\t\t * IO with EINTR.\n\t\t */\n\t\tret = -EINTR;\n\t\tfallthrough;\n\tdefault:\n\t\tkiocb->ki_complete(kiocb, ret, 0);\n\t}\n}\n\nstatic void kiocb_done(struct kiocb *kiocb, ssize_t ret,\n\t\t       unsigned int issue_flags)\n{\n\tstruct io_kiocb *req = container_of(kiocb, struct io_kiocb, rw.kiocb);\n\tstruct io_async_rw *io = req->async_data;\n\n\t/* add previously done IO, if any */\n\tif (io && io->bytes_done > 0) {\n\t\tif (ret < 0)\n\t\t\tret = io->bytes_done;\n\t\telse\n\t\t\tret += io->bytes_done;\n\t}\n\n\tif (req->flags & REQ_F_CUR_POS)\n\t\treq->file->f_pos = kiocb->ki_pos;\n\tif (ret >= 0 && kiocb->ki_complete == io_complete_rw)\n\t\t__io_complete_rw(req, ret, 0, issue_flags);\n\telse\n\t\tio_rw_done(kiocb, ret);\n}\n\nstatic int io_import_fixed(struct io_kiocb *req, int rw, struct iov_iter *iter)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tsize_t len = req->rw.len;\n\tstruct io_mapped_ubuf *imu;\n\tu16 index, buf_index = req->buf_index;\n\tsize_t offset;\n\tu64 buf_addr;\n\n\tif (unlikely(buf_index >= ctx->nr_user_bufs))\n\t\treturn -EFAULT;\n\tindex = array_index_nospec(buf_index, ctx->nr_user_bufs);\n\timu = &ctx->user_bufs[index];\n\tbuf_addr = req->rw.addr;\n\n\t/* overflow */\n\tif (buf_addr + len < buf_addr)\n\t\treturn -EFAULT;\n\t/* not inside the mapped region */\n\tif (buf_addr < imu->ubuf || buf_addr + len > imu->ubuf + imu->len)\n\t\treturn -EFAULT;\n\n\t/*\n\t * May not be a start of buffer, set size appropriately\n\t * and advance us to the beginning.\n\t */\n\toffset = buf_addr - imu->ubuf;\n\tiov_iter_bvec(iter, rw, imu->bvec, imu->nr_bvecs, offset + len);\n\n\tif (offset) {\n\t\t/*\n\t\t * Don't use iov_iter_advance() here, as it's really slow for\n\t\t * using the latter parts of a big fixed buffer - it iterates\n\t\t * over each segment manually. We can cheat a bit here, because\n\t\t * we know that:\n\t\t *\n\t\t * 1) it's a BVEC iter, we set it up\n\t\t * 2) all bvecs are PAGE_SIZE in size, except potentially the\n\t\t *    first and last bvec\n\t\t *\n\t\t * So just find our index, and adjust the iterator afterwards.\n\t\t * If the offset is within the first bvec (or the whole first\n\t\t * bvec, just use iov_iter_advance(). This makes it easier\n\t\t * since we can just skip the first segment, which may not\n\t\t * be PAGE_SIZE aligned.\n\t\t */\n\t\tconst struct bio_vec *bvec = imu->bvec;\n\n\t\tif (offset <= bvec->bv_len) {\n\t\t\tiov_iter_advance(iter, offset);\n\t\t} else {\n\t\t\tunsigned long seg_skip;\n\n\t\t\t/* skip first vec */\n\t\t\toffset -= bvec->bv_len;\n\t\t\tseg_skip = 1 + (offset >> PAGE_SHIFT);\n\n\t\t\titer->bvec = bvec + seg_skip;\n\t\t\titer->nr_segs -= seg_skip;\n\t\t\titer->count -= bvec->bv_len + offset;\n\t\t\titer->iov_offset = offset & ~PAGE_MASK;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void io_ring_submit_unlock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\tif (needs_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_ring_submit_lock(struct io_ring_ctx *ctx, bool needs_lock)\n{\n\t/*\n\t * \"Normal\" inline submissions always hold the uring_lock, since we\n\t * grab it from the system call. Same is true for the SQPOLL offload.\n\t * The only exception is when we've detached the request and issue it\n\t * from an async worker thread, grab the lock for that case.\n\t */\n\tif (needs_lock)\n\t\tmutex_lock(&ctx->uring_lock);\n}\n\nstatic struct io_buffer *io_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\t  int bgid, struct io_buffer *kbuf,\n\t\t\t\t\t  bool needs_lock)\n{\n\tstruct io_buffer *head;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\treturn kbuf;\n\n\tio_ring_submit_lock(req->ctx, needs_lock);\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\thead = xa_load(&req->ctx->io_buffers, bgid);\n\tif (head) {\n\t\tif (!list_empty(&head->list)) {\n\t\t\tkbuf = list_last_entry(&head->list, struct io_buffer,\n\t\t\t\t\t\t\tlist);\n\t\t\tlist_del(&kbuf->list);\n\t\t} else {\n\t\t\tkbuf = head;\n\t\t\txa_erase(&req->ctx->io_buffers, bgid);\n\t\t}\n\t\tif (*len > kbuf->len)\n\t\t\t*len = kbuf->len;\n\t} else {\n\t\tkbuf = ERR_PTR(-ENOBUFS);\n\t}\n\n\tio_ring_submit_unlock(req->ctx, needs_lock);\n\n\treturn kbuf;\n}\n\nstatic void __user *io_rw_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\tbool needs_lock)\n{\n\tstruct io_buffer *kbuf;\n\tu16 bgid;\n\n\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\tbgid = req->buf_index;\n\tkbuf = io_buffer_select(req, len, bgid, kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\treq->rw.addr = (u64) (unsigned long) kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn u64_to_user_ptr(kbuf->addr);\n}\n\n#ifdef CONFIG_COMPAT\nstatic ssize_t io_compat_import(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\tbool needs_lock)\n{\n\tstruct compat_iovec __user *uiov;\n\tcompat_ssize_t clen;\n\tvoid __user *buf;\n\tssize_t len;\n\n\tuiov = u64_to_user_ptr(req->rw.addr);\n\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\tif (__get_user(clen, &uiov->iov_len))\n\t\treturn -EFAULT;\n\tif (clen < 0)\n\t\treturn -EINVAL;\n\n\tlen = clen;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = (compat_size_t) len;\n\treturn 0;\n}\n#endif\n\nstatic ssize_t __io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t      bool needs_lock)\n{\n\tstruct iovec __user *uiov = u64_to_user_ptr(req->rw.addr);\n\tvoid __user *buf;\n\tssize_t len;\n\n\tif (copy_from_user(iov, uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\n\tlen = iov[0].iov_len;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\tbuf = io_rw_buffer_select(req, &len, needs_lock);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\tiov[0].iov_base = buf;\n\tiov[0].iov_len = len;\n\treturn 0;\n}\n\nstatic ssize_t io_iov_buffer_select(struct io_kiocb *req, struct iovec *iov,\n\t\t\t\t    bool needs_lock)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tstruct io_buffer *kbuf;\n\n\t\tkbuf = (struct io_buffer *) (unsigned long) req->rw.addr;\n\t\tiov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tiov[0].iov_len = kbuf->len;\n\t\treturn 0;\n\t}\n\tif (req->rw.len != 1)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn io_compat_import(req, iov, needs_lock);\n#endif\n\n\treturn __io_iov_buffer_select(req, iov, needs_lock);\n}\n\nstatic int io_import_iovec(int rw, struct io_kiocb *req, struct iovec **iovec,\n\t\t\t   struct iov_iter *iter, bool needs_lock)\n{\n\tvoid __user *buf = u64_to_user_ptr(req->rw.addr);\n\tsize_t sqe_len = req->rw.len;\n\tu8 opcode = req->opcode;\n\tssize_t ret;\n\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\t*iovec = NULL;\n\t\treturn io_import_fixed(req, rw, iter);\n\t}\n\n\t/* buffer index only valid with fixed read/write, or buffer select  */\n\tif (req->buf_index && !(req->flags & REQ_F_BUFFER_SELECT))\n\t\treturn -EINVAL;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE) {\n\t\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\t\tbuf = io_rw_buffer_select(req, &sqe_len, needs_lock);\n\t\t\tif (IS_ERR(buf))\n\t\t\t\treturn PTR_ERR(buf);\n\t\t\treq->rw.len = sqe_len;\n\t\t}\n\n\t\tret = import_single_range(rw, buf, sqe_len, *iovec, iter);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select(req, *iovec, needs_lock);\n\t\tif (!ret)\n\t\t\tiov_iter_init(iter, rw, *iovec, 1, (*iovec)->iov_len);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\treturn __import_iovec(rw, buf, sqe_len, UIO_FASTIOV, iovec, iter,\n\t\t\t      req->ctx->compat);\n}\n\nstatic inline loff_t *io_kiocb_ppos(struct kiocb *kiocb)\n{\n\treturn (kiocb->ki_filp->f_mode & FMODE_STREAM) ? NULL : &kiocb->ki_pos;\n}\n\n/*\n * For files that don't have ->read_iter() and ->write_iter(), handle them\n * by looping over ->read() or ->write() manually.\n */\nstatic ssize_t loop_rw_iter(int rw, struct io_kiocb *req, struct iov_iter *iter)\n{\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct file *file = req->file;\n\tssize_t ret = 0;\n\n\t/*\n\t * Don't support polled IO through this interface, and we can't\n\t * support non-blocking either. For the latter, this just causes\n\t * the kiocb to be handled from an async context.\n\t */\n\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\treturn -EOPNOTSUPP;\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\treturn -EAGAIN;\n\n\twhile (iov_iter_count(iter)) {\n\t\tstruct iovec iovec;\n\t\tssize_t nr;\n\n\t\tif (!iov_iter_is_bvec(iter)) {\n\t\t\tiovec = iov_iter_iovec(iter);\n\t\t} else {\n\t\t\tiovec.iov_base = u64_to_user_ptr(req->rw.addr);\n\t\t\tiovec.iov_len = req->rw.len;\n\t\t}\n\n\t\tif (rw == READ) {\n\t\t\tnr = file->f_op->read(file, iovec.iov_base,\n\t\t\t\t\t      iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t} else {\n\t\t\tnr = file->f_op->write(file, iovec.iov_base,\n\t\t\t\t\t       iovec.iov_len, io_kiocb_ppos(kiocb));\n\t\t}\n\n\t\tif (nr < 0) {\n\t\t\tif (!ret)\n\t\t\t\tret = nr;\n\t\t\tbreak;\n\t\t}\n\t\tret += nr;\n\t\tif (nr != iovec.iov_len)\n\t\t\tbreak;\n\t\treq->rw.len -= nr;\n\t\treq->rw.addr += nr;\n\t\tiov_iter_advance(iter, nr);\n\t}\n\n\treturn ret;\n}\n\nstatic void io_req_map_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t  const struct iovec *fast_iov, struct iov_iter *iter)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\n\tmemcpy(&rw->iter, iter, sizeof(*iter));\n\trw->free_iovec = iovec;\n\trw->bytes_done = 0;\n\t/* can only be fixed buffers, no need to do anything */\n\tif (iov_iter_is_bvec(iter))\n\t\treturn;\n\tif (!iovec) {\n\t\tunsigned iov_off = 0;\n\n\t\trw->iter.iov = rw->fast_iov;\n\t\tif (iter->iov != fast_iov) {\n\t\t\tiov_off = iter->iov - fast_iov;\n\t\t\trw->iter.iov += iov_off;\n\t\t}\n\t\tif (rw->fast_iov != fast_iov)\n\t\t\tmemcpy(rw->fast_iov + iov_off, fast_iov + iov_off,\n\t\t\t       sizeof(struct iovec) * iter->nr_segs);\n\t} else {\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic inline int io_alloc_async_data(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(!io_op_defs[req->opcode].async_size);\n\treq->async_data = kmalloc(io_op_defs[req->opcode].async_size, GFP_KERNEL);\n\treturn req->async_data == NULL;\n}\n\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     const struct iovec *fast_iov,\n\t\t\t     struct iov_iter *iter, bool force)\n{\n\tif (!force && !io_op_defs[req->opcode].needs_async_setup)\n\t\treturn 0;\n\tif (!req->async_data) {\n\t\tif (io_alloc_async_data(req)) {\n\t\t\tkfree(iovec);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tio_req_map_rw(req, iovec, fast_iov, iter);\n\t}\n\treturn 0;\n}\n\nstatic inline int io_rw_prep_async(struct io_kiocb *req, int rw)\n{\n\tstruct io_async_rw *iorw = req->async_data;\n\tstruct iovec *iov = iorw->fast_iov;\n\tint ret;\n\n\tret = io_import_iovec(rw, req, &iov, &iorw->iter, false);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tiorw->bytes_done = 0;\n\tiorw->free_iovec = iov;\n\tif (iov)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_read_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(!(req->file->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\treturn io_prep_rw(req, sqe);\n}\n\n/*\n * This is our waitqueue callback handler, registered through lock_page_async()\n * when we initially tried to do the IO with the iocb armed our waitqueue.\n * This gets called when the page is unlocked, and we generally expect that to\n * happen when the page IO is completed and the page is now uptodate. This will\n * queue a task_work based retry of the operation, attempting to copy the data\n * again. If the latter fails because the page was NOT uptodate, then we will\n * do a thread based blocking retry of the operation. That's the unexpected\n * slow path.\n */\nstatic int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct wait_page_key *key = arg;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\treq->rw.kiocb.ki_flags &= ~IOCB_WAITQ;\n\tlist_del_init(&wait->entry);\n\n\t/* submit ref gets dropped, acquire a new one */\n\treq_ref_get(req);\n\tio_req_task_queue(req);\n\treturn 1;\n}\n\n/*\n * This controls whether a given IO request should be armed for async page\n * based retry. If we return false here, the request is handed to the async\n * worker threads for retry. If we're doing buffered reads on a regular file,\n * we prepare a private wait_page_queue entry and retry the operation. This\n * will either succeed because the page is now uptodate and unlocked, or it\n * will register a callback when the page is unlocked at IO completion. Through\n * that callback, io_uring uses task_work to setup a retry of the operation.\n * That retry will attempt the buffered read again. The retry will generally\n * succeed, or in rare cases where it fails, we then fall back to using the\n * async worker threads for a blocking retry.\n */\nstatic bool io_rw_should_retry(struct io_kiocb *req)\n{\n\tstruct io_async_rw *rw = req->async_data;\n\tstruct wait_page_queue *wait = &rw->wpq;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\n\t/* never retry for NOWAIT, we just complete with -EAGAIN */\n\tif (req->flags & REQ_F_NOWAIT)\n\t\treturn false;\n\n\t/* Only for buffered IO */\n\tif (kiocb->ki_flags & (IOCB_DIRECT | IOCB_HIPRI))\n\t\treturn false;\n\n\t/*\n\t * just use poll if we can, and don't attempt if the fs doesn't\n\t * support callback based unlocks\n\t */\n\tif (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))\n\t\treturn false;\n\n\twait->wait.func = io_async_buf_func;\n\twait->wait.private = req;\n\twait->wait.flags = 0;\n\tINIT_LIST_HEAD(&wait->wait.entry);\n\tkiocb->ki_flags |= IOCB_WAITQ;\n\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\tkiocb->ki_waitq = wait;\n\treturn true;\n}\n\nstatic int io_iter_do_read(struct io_kiocb *req, struct iov_iter *iter)\n{\n\tif (req->file->f_op->read_iter)\n\t\treturn call_read_iter(req->file, &req->rw.kiocb, iter);\n\telse if (req->file->f_op->read)\n\t\treturn loop_rw_iter(READ, req, iter);\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic int io_read(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t io_size, ret, ret2;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(READ, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\t/* If the file doesn't support async, just async punt */\n\tif (force_nonblock && !io_file_supports_async(req, READ)) {\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, iter, true);\n\t\treturn ret ?: -EAGAIN;\n\t}\n\n\tret = rw_verify_area(READ, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret)) {\n\t\tkfree(iovec);\n\t\treturn ret;\n\t}\n\n\tret = io_iter_do_read(req, iter);\n\n\tif (ret == -EIOCBQUEUED) {\n\t\tgoto out_free;\n\t} else if (ret == -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif (!force_nonblock && !(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tgoto done;\n\t\t/* no retry on NONBLOCK nor RWF_NOWAIT */\n\t\tif (req->flags & REQ_F_NOWAIT)\n\t\t\tgoto done;\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = 0;\n\t} else if (ret <= 0 || ret == io_size || !force_nonblock ||\n\t\t   (req->flags & REQ_F_NOWAIT) || !(req->flags & REQ_F_ISREG)) {\n\t\t/* read all, failed, already did sync or don't want to retry */\n\t\tgoto done;\n\t}\n\n\tret2 = io_setup_async_rw(req, iovec, inline_vecs, iter, true);\n\tif (ret2)\n\t\treturn ret2;\n\n\tiovec = NULL;\n\trw = req->async_data;\n\t/* now use our persistent iterator, if we aren't already */\n\titer = &rw->iter;\n\n\tdo {\n\t\tio_size -= ret;\n\t\trw->bytes_done += ret;\n\t\t/* if we can retry, do so with the callbacks armed */\n\t\tif (!io_rw_should_retry(req)) {\n\t\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t/*\n\t\t * Now retry read with the IOCB_WAITQ parts set in the iocb. If\n\t\t * we get -EIOCBQUEUED, then we'll get a notification when the\n\t\t * desired page gets unlocked. We can also get a partial read\n\t\t * here, and if we do, then just retry at the new offset.\n\t\t */\n\t\tret = io_iter_do_read(req, iter);\n\t\tif (ret == -EIOCBQUEUED)\n\t\t\treturn 0;\n\t\t/* we got some bytes, but not all. retry. */\n\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t} while (ret > 0 && ret < io_size);\ndone:\n\tkiocb_done(kiocb, ret, issue_flags);\nout_free:\n\t/* it's faster to check here then delegate to kfree */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn 0;\n}\n\nstatic int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(!(req->file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\treturn io_prep_rw(req, sqe);\n}\n\nstatic int io_write(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct kiocb *kiocb = &req->rw.kiocb;\n\tstruct iov_iter __iter, *iter = &__iter;\n\tstruct io_async_rw *rw = req->async_data;\n\tssize_t ret, ret2, io_size;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tif (rw) {\n\t\titer = &rw->iter;\n\t\tiovec = NULL;\n\t} else {\n\t\tret = io_import_iovec(WRITE, req, &iovec, iter, !force_nonblock);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\tio_size = iov_iter_count(iter);\n\treq->result = io_size;\n\n\t/* Ensure we clear previously set non-block flag */\n\tif (!force_nonblock)\n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\telse\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\n\t/* If the file doesn't support async, just async punt */\n\tif (force_nonblock && !io_file_supports_async(req, WRITE))\n\t\tgoto copy_iov;\n\n\t/* file path doesn't support NOWAIT for non-direct_IO */\n\tif (force_nonblock && !(kiocb->ki_flags & IOCB_DIRECT) &&\n\t    (req->flags & REQ_F_ISREG))\n\t\tgoto copy_iov;\n\n\tret = rw_verify_area(WRITE, req->file, io_kiocb_ppos(kiocb), io_size);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\t/*\n\t * Open-code file_start_write here to grab freeze protection,\n\t * which will be released by another thread in\n\t * io_complete_rw().  Fool lockdep by telling it the lock got\n\t * released so that it doesn't complain about the held lock when\n\t * we return to userspace.\n\t */\n\tif (req->flags & REQ_F_ISREG) {\n\t\tsb_start_write(file_inode(req->file)->i_sb);\n\t\t__sb_writers_release(file_inode(req->file)->i_sb,\n\t\t\t\t\tSB_FREEZE_WRITE);\n\t}\n\tkiocb->ki_flags |= IOCB_WRITE;\n\n\tif (req->file->f_op->write_iter)\n\t\tret2 = call_write_iter(req->file, kiocb, iter);\n\telse if (req->file->f_op->write)\n\t\tret2 = loop_rw_iter(WRITE, req, iter);\n\telse\n\t\tret2 = -EINVAL;\n\n\t/*\n\t * Raw bdev writes will return -EOPNOTSUPP for IOCB_NOWAIT. Just\n\t * retry them without IOCB_NOWAIT.\n\t */\n\tif (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))\n\t\tret2 = -EAGAIN;\n\t/* no retry on NONBLOCK nor RWF_NOWAIT */\n\tif (ret2 == -EAGAIN && (req->flags & REQ_F_NOWAIT))\n\t\tgoto done;\n\tif (!force_nonblock || ret2 != -EAGAIN) {\n\t\t/* IOPOLL retry should happen for io-wq threads */\n\t\tif ((req->ctx->flags & IORING_SETUP_IOPOLL) && ret2 == -EAGAIN)\n\t\t\tgoto copy_iov;\ndone:\n\t\tkiocb_done(kiocb, ret2, issue_flags);\n\t} else {\ncopy_iov:\n\t\t/* some cases will consume bytes even on error returns */\n\t\tiov_iter_revert(iter, io_size - iov_iter_count(iter));\n\t\tret = io_setup_async_rw(req, iovec, inline_vecs, iter, false);\n\t\treturn ret ?: -EAGAIN;\n\t}\nout_free:\n\t/* it's reportedly faster than delegating the null check to kfree() */\n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int io_renameat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_rename *ren = &req->rename;\n\tconst char __user *oldf, *newf;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tren->old_dfd = READ_ONCE(sqe->fd);\n\toldf = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tnewf = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tren->new_dfd = READ_ONCE(sqe->len);\n\tren->flags = READ_ONCE(sqe->rename_flags);\n\n\tren->oldpath = getname(oldf);\n\tif (IS_ERR(ren->oldpath))\n\t\treturn PTR_ERR(ren->oldpath);\n\n\tren->newpath = getname(newf);\n\tif (IS_ERR(ren->newpath)) {\n\t\tputname(ren->oldpath);\n\t\treturn PTR_ERR(ren->newpath);\n\t}\n\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_renameat(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_rename *ren = &req->rename;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = do_renameat2(ren->old_dfd, ren->oldpath, ren->new_dfd,\n\t\t\t\tren->newpath, ren->flags);\n\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_unlinkat_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tconst char __user *fname;\n\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\tun->dfd = READ_ONCE(sqe->fd);\n\n\tun->flags = READ_ONCE(sqe->unlink_flags);\n\tif (un->flags & ~AT_REMOVEDIR)\n\t\treturn -EINVAL;\n\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tun->filename = getname(fname);\n\tif (IS_ERR(un->filename))\n\t\treturn PTR_ERR(un->filename);\n\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_unlinkat(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_unlink *un = &req->unlink;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tif (un->flags & AT_REMOVEDIR)\n\t\tret = do_rmdir(un->dfd, un->filename);\n\telse\n\t\tret = do_unlinkat(un->dfd, un->filename);\n\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_shutdown_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_NET)\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->rw_flags ||\n\t    sqe->buf_index)\n\t\treturn -EINVAL;\n\n\treq->shutdown.how = READ_ONCE(sqe->len);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_shutdown(struct io_kiocb *req, unsigned int issue_flags)\n{\n#if defined(CONFIG_NET)\n\tstruct socket *sock;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tret = __sys_shutdown_sock(sock, req->shutdown.how);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int __io_splice_prep(struct io_kiocb *req,\n\t\t\t    const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\tunsigned int valid_flags = SPLICE_F_FD_IN_FIXED | SPLICE_F_ALL;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsp->file_in = NULL;\n\tsp->len = READ_ONCE(sqe->len);\n\tsp->flags = READ_ONCE(sqe->splice_flags);\n\n\tif (unlikely(sp->flags & ~valid_flags))\n\t\treturn -EINVAL;\n\n\tsp->file_in = io_file_get(NULL, req, READ_ONCE(sqe->splice_fd_in),\n\t\t\t\t  (sp->flags & SPLICE_F_FD_IN_FIXED));\n\tif (!sp->file_in)\n\t\treturn -EBADF;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_tee_prep(struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (READ_ONCE(sqe->splice_off_in) || READ_ONCE(sqe->off))\n\t\treturn -EINVAL;\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_tee(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\tif (sp->len)\n\t\tret = do_tee(in, out, sp->len, flags);\n\n\tif (!(sp->flags & SPLICE_F_FD_IN_FIXED))\n\t\tio_put_file(in);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_splice_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_splice* sp = &req->splice;\n\n\tsp->off_in = READ_ONCE(sqe->splice_off_in);\n\tsp->off_out = READ_ONCE(sqe->off);\n\treturn __io_splice_prep(req, sqe);\n}\n\nstatic int io_splice(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_splice *sp = &req->splice;\n\tstruct file *in = sp->file_in;\n\tstruct file *out = sp->file_out;\n\tunsigned int flags = sp->flags & ~SPLICE_F_FD_IN_FIXED;\n\tloff_t *poff_in, *poff_out;\n\tlong ret = 0;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tpoff_in = (sp->off_in == -1) ? NULL : &sp->off_in;\n\tpoff_out = (sp->off_out == -1) ? NULL : &sp->off_out;\n\n\tif (sp->len)\n\t\tret = do_splice(in, poff_in, out, poff_out, sp->len, flags);\n\n\tif (!(sp->flags & SPLICE_F_FD_IN_FIXED))\n\t\tio_put_file(in);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\n\tif (ret != sp->len)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n/*\n * IORING_OP_NOP just posts a completion event, nothing else.\n */\nstatic int io_nop(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\t__io_req_complete(req, issue_flags, 0, 0);\n\treturn 0;\n}\n\nstatic int io_fsync_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->file)\n\t\treturn -EBADF;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.flags = READ_ONCE(sqe->fsync_flags);\n\tif (unlikely(req->sync.flags & ~IORING_FSYNC_DATASYNC))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fsync(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tloff_t end = req->sync.off + req->sync.len;\n\tint ret;\n\n\t/* fsync always requires a blocking context */\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = vfs_fsync_range(req->file, req->sync.off,\n\t\t\t\tend > 0 ? end : LLONG_MAX,\n\t\t\t\treq->sync.flags & IORING_FSYNC_DATASYNC);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_fallocate_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->addr);\n\treq->sync.mode = READ_ONCE(sqe->len);\n\treturn 0;\n}\n\nstatic int io_fallocate(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tint ret;\n\n\t/* fallocate always requiring blocking context */\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\tret = vfs_fallocate(req->file, req->sync.mode, req->sync.off,\n\t\t\t\treq->sync.len);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int __io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tconst char __user *fname;\n\tint ret;\n\n\tif (unlikely(sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & REQ_F_FIXED_FILE))\n\t\treturn -EBADF;\n\n\t/* open.how should be already initialised */\n\tif (!(req->open.how.flags & O_PATH) && force_o_largefile())\n\t\treq->open.how.flags |= O_LARGEFILE;\n\n\treq->open.dfd = READ_ONCE(sqe->fd);\n\tfname = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->open.filename = getname(fname);\n\tif (IS_ERR(req->open.filename)) {\n\t\tret = PTR_ERR(req->open.filename);\n\t\treq->open.filename = NULL;\n\t\treturn ret;\n\t}\n\treq->open.nofile = rlimit(RLIMIT_NOFILE);\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn 0;\n}\n\nstatic int io_openat_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tu64 flags, mode;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tmode = READ_ONCE(sqe->len);\n\tflags = READ_ONCE(sqe->open_flags);\n\treq->open.how = build_open_how(flags, mode);\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct open_how __user *how;\n\tsize_t len;\n\tint ret;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\thow = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\tlen = READ_ONCE(sqe->len);\n\tif (len < OPEN_HOW_SIZE_VER0)\n\t\treturn -EINVAL;\n\n\tret = copy_struct_from_user(&req->open.how, sizeof(req->open.how), how,\n\t\t\t\t\tlen);\n\tif (ret)\n\t\treturn ret;\n\n\treturn __io_openat_prep(req, sqe);\n}\n\nstatic int io_openat2(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct open_flags op;\n\tstruct file *file;\n\tbool nonblock_set;\n\tbool resolve_nonblock;\n\tint ret;\n\n\tret = build_open_flags(&req->open.how, &op);\n\tif (ret)\n\t\tgoto err;\n\tnonblock_set = op.open_flag & O_NONBLOCK;\n\tresolve_nonblock = req->open.how.resolve & RESOLVE_CACHED;\n\tif (issue_flags & IO_URING_F_NONBLOCK) {\n\t\t/*\n\t\t * Don't bother trying for O_TRUNC, O_CREAT, or O_TMPFILE open,\n\t\t * it'll always -EAGAIN\n\t\t */\n\t\tif (req->open.how.flags & (O_TRUNC | O_CREAT | O_TMPFILE))\n\t\t\treturn -EAGAIN;\n\t\top.lookup_flags |= LOOKUP_CACHED;\n\t\top.open_flag |= O_NONBLOCK;\n\t}\n\n\tret = __get_unused_fd_flags(req->open.how.flags, req->open.nofile);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tfile = do_filp_open(req->open.dfd, req->open.filename, &op);\n\t/* only retry if RESOLVE_CACHED wasn't already set by application */\n\tif ((!resolve_nonblock && (issue_flags & IO_URING_F_NONBLOCK)) &&\n\t    file == ERR_PTR(-EAGAIN)) {\n\t\t/*\n\t\t * We could hang on to this 'fd', but seems like marginal\n\t\t * gain for something that is now known to be a slower path.\n\t\t * So just put it, and we'll get a new one when we retry.\n\t\t */\n\t\tput_unused_fd(ret);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(ret);\n\t\tret = PTR_ERR(file);\n\t} else {\n\t\tif ((issue_flags & IO_URING_F_NONBLOCK) && !nonblock_set)\n\t\t\tfile->f_flags &= ~O_NONBLOCK;\n\t\tfsnotify_open(file);\n\t\tfd_install(ret, file);\n\t}\nerr:\n\tputname(req->open.filename);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_openat(struct io_kiocb *req, unsigned int issue_flags)\n{\n\treturn io_openat2(req, issue_flags);\n}\n\nstatic int io_remove_buffers_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags || sqe->addr || sqe->len || sqe->off)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->nbufs = tmp;\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\treturn 0;\n}\n\nstatic int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer *buf,\n\t\t\t       int bgid, unsigned nbufs)\n{\n\tunsigned i = 0;\n\n\t/* shouldn't happen */\n\tif (!nbufs)\n\t\treturn 0;\n\n\t/* the head kbuf is the list itself */\n\twhile (!list_empty(&buf->list)) {\n\t\tstruct io_buffer *nxt;\n\n\t\tnxt = list_first_entry(&buf->list, struct io_buffer, list);\n\t\tlist_del(&nxt->list);\n\t\tkfree(nxt);\n\t\tif (++i == nbufs)\n\t\t\treturn i;\n\t}\n\ti++;\n\tkfree(buf);\n\txa_erase(&ctx->io_buffers, bgid);\n\n\treturn i;\n}\n\nstatic int io_remove_buffers(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head;\n\tint ret = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tret = -ENOENT;\n\thead = xa_load(&ctx->io_buffers, p->bgid);\n\tif (head)\n\t\tret = __io_remove_buffers(ctx, head, p->bgid, p->nbufs);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\n\t/* complete before unlock, IOPOLL may need the lock */\n\t__io_req_complete(req, issue_flags, ret, 0);\n\tio_ring_submit_unlock(ctx, !force_nonblock);\n\treturn 0;\n}\n\nstatic int io_provide_buffers_prep(struct io_kiocb *req,\n\t\t\t\t   const struct io_uring_sqe *sqe)\n{\n\tunsigned long size;\n\tstruct io_provide_buf *p = &req->pbuf;\n\tu64 tmp;\n\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->nbufs = tmp;\n\tp->addr = READ_ONCE(sqe->addr);\n\tp->len = READ_ONCE(sqe->len);\n\n\tsize = (unsigned long)p->len * p->nbufs;\n\tif (!access_ok(u64_to_user_ptr(p->addr), size))\n\t\treturn -EFAULT;\n\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\ttmp = READ_ONCE(sqe->off);\n\tif (tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tp->bid = tmp;\n\treturn 0;\n}\n\nstatic int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\n\treturn i ? i : -ENOMEM;\n}\n\nstatic int io_provide_buffers(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_provide_buf *p = &req->pbuf;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer *head, *list;\n\tint ret = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tio_ring_submit_lock(ctx, !force_nonblock);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tlist = head = xa_load(&ctx->io_buffers, p->bgid);\n\n\tret = io_add_buffers(p, &head);\n\tif (ret >= 0 && !list) {\n\t\tret = xa_insert(&ctx->io_buffers, p->bgid, head, GFP_KERNEL);\n\t\tif (ret < 0)\n\t\t\t__io_remove_buffers(ctx, head, p->bgid, -1U);\n\t}\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t/* complete before unlock, IOPOLL may need the lock */\n\t__io_req_complete(req, issue_flags, ret, 0);\n\tio_ring_submit_unlock(ctx, !force_nonblock);\n\treturn 0;\n}\n\nstatic int io_epoll_ctl_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_EPOLL)\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\n\treq->epoll.epfd = READ_ONCE(sqe->fd);\n\treq->epoll.op = READ_ONCE(sqe->len);\n\treq->epoll.fd = READ_ONCE(sqe->off);\n\n\tif (ep_op_has_event(req->epoll.op)) {\n\t\tstruct epoll_event __user *ev;\n\n\t\tev = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\t\tif (copy_from_user(&req->epoll.event, ev, sizeof(*ev)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_epoll_ctl(struct io_kiocb *req, unsigned int issue_flags)\n{\n#if defined(CONFIG_EPOLL)\n\tstruct io_epoll *ie = &req->epoll;\n\tint ret;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tret = do_epoll_ctl(ie->epfd, ie->op, ie->fd, &ie->event, force_nonblock);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tif (sqe->ioprio || sqe->buf_index || sqe->off)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->madvise.addr = READ_ONCE(sqe->addr);\n\treq->madvise.len = READ_ONCE(sqe->len);\n\treq->madvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_madvise(struct io_kiocb *req, unsigned int issue_flags)\n{\n#if defined(CONFIG_ADVISE_SYSCALLS) && defined(CONFIG_MMU)\n\tstruct io_madvise *ma = &req->madvise;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = do_madvise(current->mm, ma->addr, ma->len, ma->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic int io_fadvise_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (sqe->ioprio || sqe->buf_index || sqe->addr)\n\t\treturn -EINVAL;\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\treq->fadvise.offset = READ_ONCE(sqe->off);\n\treq->fadvise.len = READ_ONCE(sqe->len);\n\treq->fadvise.advice = READ_ONCE(sqe->fadvise_advice);\n\treturn 0;\n}\n\nstatic int io_fadvise(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_fadvise *fa = &req->fadvise;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK) {\n\t\tswitch (fa->advice) {\n\t\tcase POSIX_FADV_NORMAL:\n\t\tcase POSIX_FADV_RANDOM:\n\t\tcase POSIX_FADV_SEQUENTIAL:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tret = vfs_fadvise(req->file, fa->offset, fa->len, fa->advice);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_statx_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->statx.dfd = READ_ONCE(sqe->fd);\n\treq->statx.mask = READ_ONCE(sqe->len);\n\treq->statx.filename = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\treq->statx.buffer = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\treq->statx.flags = READ_ONCE(sqe->statx_flags);\n\n\treturn 0;\n}\n\nstatic int io_statx(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_statx *ctx = &req->statx;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = do_statx(ctx->dfd, ctx->filename, ctx->flags, ctx->mask,\n\t\t       ctx->buffer);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\treturn 0;\n}\n\nstatic int io_close(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct files_struct *files = current->files;\n\tstruct io_close *close = &req->close;\n\tstruct fdtable *fdt;\n\tstruct file *file;\n\tint ret;\n\n\tfile = NULL;\n\tret = -EBADF;\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (close->fd >= fdt->max_fds) {\n\t\tspin_unlock(&files->file_lock);\n\t\tgoto err;\n\t}\n\tfile = fdt->fd[close->fd];\n\tif (!file) {\n\t\tspin_unlock(&files->file_lock);\n\t\tgoto err;\n\t}\n\n\tif (file->f_op == &io_uring_fops) {\n\t\tspin_unlock(&files->file_lock);\n\t\tfile = NULL;\n\t\tgoto err;\n\t}\n\n\t/* if the file has a flush method, be safe and punt to async */\n\tif (file->f_op->flush && (issue_flags & IO_URING_F_NONBLOCK)) {\n\t\tspin_unlock(&files->file_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tret = __close_fd_get_file(close->fd, &file);\n\tspin_unlock(&files->file_lock);\n\tif (ret < 0) {\n\t\tif (ret == -ENOENT)\n\t\t\tret = -EBADF;\n\t\tgoto err;\n\t}\n\n\t/* No ->flush() or already async, safely close from here */\n\tret = filp_close(file, current->files);\nerr:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tif (file)\n\t\tfput(file);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_sfr_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (unlikely(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(sqe->addr || sqe->ioprio || sqe->buf_index))\n\t\treturn -EINVAL;\n\n\treq->sync.off = READ_ONCE(sqe->off);\n\treq->sync.len = READ_ONCE(sqe->len);\n\treq->sync.flags = READ_ONCE(sqe->sync_range_flags);\n\treturn 0;\n}\n\nstatic int io_sync_file_range(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tint ret;\n\n\t/* sync_file_range always requires a blocking context */\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tret = sync_file_range(req->file, req->sync.off, req->sync.len,\n\t\t\t\treq->sync.flags);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\n#if defined(CONFIG_NET)\nstatic int io_setup_async_msg(struct io_kiocb *req,\n\t\t\t      struct io_async_msghdr *kmsg)\n{\n\tstruct io_async_msghdr *async_msg = req->async_data;\n\n\tif (async_msg)\n\t\treturn -EAGAIN;\n\tif (io_alloc_async_data(req)) {\n\t\tkfree(kmsg->free_iov);\n\t\treturn -ENOMEM;\n\t}\n\tasync_msg = req->async_data;\n\treq->flags |= REQ_F_NEED_CLEANUP;\n\tmemcpy(async_msg, kmsg, sizeof(*kmsg));\n\tasync_msg->msg.msg_name = &async_msg->addr;\n\t/* if were using fast_iov, set it to the new one */\n\tif (!async_msg->free_iov)\n\t\tasync_msg->msg.msg_iter.iov = async_msg->fast_iov;\n\n\treturn -EAGAIN;\n}\n\nstatic int io_sendmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->msg.msg_name = &iomsg->addr;\n\tiomsg->free_iov = iomsg->fast_iov;\n\treturn sendmsg_copy_msghdr(&iomsg->msg, req->sr_msg.umsg,\n\t\t\t\t   req->sr_msg.msg_flags, &iomsg->free_iov);\n}\n\nstatic int io_sendmsg_prep_async(struct io_kiocb *req)\n{\n\tint ret;\n\n\tret = io_sendmsg_copy_hdr(req, req->async_data);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_sendmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\treturn 0;\n}\n\nstatic int io_sendmsg(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint min_ret = 0;\n\tint ret;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tkmsg = req->async_data;\n\tif (!kmsg) {\n\t\tret = io_sendmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tflags = req->sr_msg.msg_flags | MSG_NOSIGNAL;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (issue_flags & IO_URING_F_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\n\tif (flags & MSG_WAITALL)\n\t\tmin_ret = iov_iter_count(&kmsg->msg.msg_iter);\n\n\tret = __sys_sendmsg_sock(sock, &kmsg->msg, flags);\n\tif ((issue_flags & IO_URING_F_NONBLOCK) && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\t/* fast path, check for non-NULL to avoid function call */\n\tif (kmsg->free_iov)\n\t\tkfree(kmsg->free_iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < min_ret)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_send(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tstruct iovec iov;\n\tstruct socket *sock;\n\tunsigned flags;\n\tint min_ret = 0;\n\tint ret;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tret = import_single_range(WRITE, sr->buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\n\tflags = req->sr_msg.msg_flags | MSG_NOSIGNAL;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (issue_flags & IO_URING_F_NONBLOCK)\n\t\tflags |= MSG_DONTWAIT;\n\n\tif (flags & MSG_WAITALL)\n\t\tmin_ret = iov_iter_count(&msg.msg_iter);\n\n\tmsg.msg_flags = flags;\n\tret = sock_sendmsg(sock, &msg);\n\tif ((issue_flags & IO_URING_F_NONBLOCK) && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (ret < min_ret)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int __io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t struct io_async_msghdr *iomsg)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct iovec __user *uiov;\n\tsize_t iov_len;\n\tint ret;\n\n\tret = __copy_msghdr_from_user(&iomsg->msg, sr->umsg,\n\t\t\t\t\t&iomsg->uaddr, &uiov, &iov_len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tif (iov_len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(iomsg->fast_iov, uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tsr->len = iomsg->fast_iov[0].iov_len;\n\t\tiomsg->free_iov = NULL;\n\t} else {\n\t\tiomsg->free_iov = iomsg->fast_iov;\n\t\tret = __import_iovec(READ, uiov, iov_len, UIO_FASTIOV,\n\t\t\t\t     &iomsg->free_iov, &iomsg->msg.msg_iter,\n\t\t\t\t     false);\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int __io_compat_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t\t\tstruct io_async_msghdr *iomsg)\n{\n\tstruct compat_msghdr __user *msg_compat;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct compat_iovec __user *uiov;\n\tcompat_uptr_t ptr;\n\tcompat_size_t len;\n\tint ret;\n\n\tmsg_compat = (struct compat_msghdr __user *) sr->umsg;\n\tret = __get_compat_msghdr(&iomsg->msg, msg_compat, &iomsg->uaddr,\n\t\t\t\t\t&ptr, &len);\n\tif (ret)\n\t\treturn ret;\n\n\tuiov = compat_ptr(ptr);\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tcompat_ssize_t clen;\n\n\t\tif (len > 1)\n\t\t\treturn -EINVAL;\n\t\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\t\treturn -EFAULT;\n\t\tif (__get_user(clen, &uiov->iov_len))\n\t\t\treturn -EFAULT;\n\t\tif (clen < 0)\n\t\t\treturn -EINVAL;\n\t\tsr->len = clen;\n\t\tiomsg->free_iov = NULL;\n\t} else {\n\t\tiomsg->free_iov = iomsg->fast_iov;\n\t\tret = __import_iovec(READ, (struct iovec __user *)uiov, len,\n\t\t\t\t   UIO_FASTIOV, &iomsg->free_iov,\n\t\t\t\t   &iomsg->msg.msg_iter, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int io_recvmsg_copy_hdr(struct io_kiocb *req,\n\t\t\t       struct io_async_msghdr *iomsg)\n{\n\tiomsg->msg.msg_name = &iomsg->addr;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn __io_compat_recvmsg_copy_hdr(req, iomsg);\n#endif\n\n\treturn __io_recvmsg_copy_hdr(req, iomsg);\n}\n\nstatic struct io_buffer *io_recv_buffer_select(struct io_kiocb *req,\n\t\t\t\t\t       bool needs_lock)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct io_buffer *kbuf;\n\n\tkbuf = io_buffer_select(req, &sr->len, sr->bgid, sr->kbuf, needs_lock);\n\tif (IS_ERR(kbuf))\n\t\treturn kbuf;\n\n\tsr->kbuf = kbuf;\n\treq->flags |= REQ_F_BUFFER_SELECTED;\n\treturn kbuf;\n}\n\nstatic inline unsigned int io_put_recv_kbuf(struct io_kiocb *req)\n{\n\treturn io_put_kbuf(req, req->sr_msg.kbuf);\n}\n\nstatic int io_recvmsg_prep_async(struct io_kiocb *req)\n{\n\tint ret;\n\n\tret = io_recvmsg_copy_hdr(req, req->async_data);\n\tif (!ret)\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\treturn ret;\n}\n\nstatic int io_recvmsg_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tsr->msg_flags = READ_ONCE(sqe->msg_flags);\n\tsr->umsg = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tsr->len = READ_ONCE(sqe->len);\n\tsr->bgid = READ_ONCE(sqe->buf_group);\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\tsr->msg_flags |= MSG_CMSG_COMPAT;\n#endif\n\treturn 0;\n}\n\nstatic int io_recvmsg(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_async_msghdr iomsg, *kmsg;\n\tstruct socket *sock;\n\tstruct io_buffer *kbuf;\n\tunsigned flags;\n\tint min_ret = 0;\n\tint ret, cflags = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tkmsg = req->async_data;\n\tif (!kmsg) {\n\t\tret = io_recvmsg_copy_hdr(req, &iomsg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tkmsg = &iomsg;\n\t}\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tkmsg->fast_iov[0].iov_base = u64_to_user_ptr(kbuf->addr);\n\t\tkmsg->fast_iov[0].iov_len = req->sr_msg.len;\n\t\tiov_iter_init(&kmsg->msg.msg_iter, READ, kmsg->fast_iov,\n\t\t\t\t1, req->sr_msg.len);\n\t}\n\n\tflags = req->sr_msg.msg_flags | MSG_NOSIGNAL;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tif (flags & MSG_WAITALL)\n\t\tmin_ret = iov_iter_count(&kmsg->msg.msg_iter);\n\n\tret = __sys_recvmsg_sock(sock, &kmsg->msg, req->sr_msg.umsg,\n\t\t\t\t\tkmsg->uaddr, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn io_setup_async_msg(req, kmsg);\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\n\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\t/* fast path, check for non-NULL to avoid function call */\n\tif (kmsg->free_iov)\n\t\tkfree(kmsg->free_iov);\n\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\tif (ret < min_ret || ((flags & MSG_WAITALL) && (kmsg->msg.msg_flags & (MSG_TRUNC | MSG_CTRUNC))))\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, cflags);\n\treturn 0;\n}\n\nstatic int io_recv(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_buffer *kbuf;\n\tstruct io_sr_msg *sr = &req->sr_msg;\n\tstruct msghdr msg;\n\tvoid __user *buf = sr->buf;\n\tstruct socket *sock;\n\tstruct iovec iov;\n\tunsigned flags;\n\tint min_ret = 0;\n\tint ret, cflags = 0;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tsock = sock_from_file(req->file);\n\tif (unlikely(!sock))\n\t\treturn -ENOTSOCK;\n\n\tif (req->flags & REQ_F_BUFFER_SELECT) {\n\t\tkbuf = io_recv_buffer_select(req, !force_nonblock);\n\t\tif (IS_ERR(kbuf))\n\t\t\treturn PTR_ERR(kbuf);\n\t\tbuf = u64_to_user_ptr(kbuf->addr);\n\t}\n\n\tret = import_single_range(READ, buf, sr->len, &iov, &msg.msg_iter);\n\tif (unlikely(ret))\n\t\tgoto out_free;\n\n\tmsg.msg_name = NULL;\n\tmsg.msg_control = NULL;\n\tmsg.msg_controllen = 0;\n\tmsg.msg_namelen = 0;\n\tmsg.msg_iocb = NULL;\n\tmsg.msg_flags = 0;\n\n\tflags = req->sr_msg.msg_flags | MSG_NOSIGNAL;\n\tif (flags & MSG_DONTWAIT)\n\t\treq->flags |= REQ_F_NOWAIT;\n\telse if (force_nonblock)\n\t\tflags |= MSG_DONTWAIT;\n\n\tif (flags & MSG_WAITALL)\n\t\tmin_ret = iov_iter_count(&msg.msg_iter);\n\n\tret = sock_recvmsg(sock, &msg, flags);\n\tif (force_nonblock && ret == -EAGAIN)\n\t\treturn -EAGAIN;\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout_free:\n\tif (req->flags & REQ_F_BUFFER_SELECTED)\n\t\tcflags = io_put_recv_kbuf(req);\n\tif (ret < min_ret || ((flags & MSG_WAITALL) && (msg.msg_flags & (MSG_TRUNC | MSG_CTRUNC))))\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, cflags);\n\treturn 0;\n}\n\nstatic int io_accept_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_accept *accept = &req->accept;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index)\n\t\treturn -EINVAL;\n\n\taccept->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\taccept->addr_len = u64_to_user_ptr(READ_ONCE(sqe->addr2));\n\taccept->flags = READ_ONCE(sqe->accept_flags);\n\taccept->nofile = rlimit(RLIMIT_NOFILE);\n\treturn 0;\n}\n\nstatic int io_accept(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_accept *accept = &req->accept;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\tunsigned int file_flags = force_nonblock ? O_NONBLOCK : 0;\n\tint ret;\n\n\tif (req->file->f_flags & O_NONBLOCK)\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tret = __sys_accept4_file(req->file, file_flags, accept->addr,\n\t\t\t\t\taccept->addr_len, accept->flags,\n\t\t\t\t\taccept->nofile);\n\tif (ret == -EAGAIN && force_nonblock)\n\t\treturn -EAGAIN;\n\tif (ret < 0) {\n\t\tif (ret == -ERESTARTSYS)\n\t\t\tret = -EINTR;\n\t\treq_set_fail_links(req);\n\t}\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_connect_prep_async(struct io_kiocb *req)\n{\n\tstruct io_async_connect *io = req->async_data;\n\tstruct io_connect *conn = &req->connect;\n\n\treturn move_addr_to_kernel(conn->addr, conn->addr_len, &io->address);\n}\n\nstatic int io_connect_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_connect *conn = &req->connect;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->len || sqe->buf_index || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\tconn->addr = u64_to_user_ptr(READ_ONCE(sqe->addr));\n\tconn->addr_len =  READ_ONCE(sqe->addr2);\n\treturn 0;\n}\n\nstatic int io_connect(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_async_connect __io, *io;\n\tunsigned file_flags;\n\tint ret;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\n\tif (req->async_data) {\n\t\tio = req->async_data;\n\t} else {\n\t\tret = move_addr_to_kernel(req->connect.addr,\n\t\t\t\t\t\treq->connect.addr_len,\n\t\t\t\t\t\t&__io.address);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tio = &__io;\n\t}\n\n\tfile_flags = force_nonblock ? O_NONBLOCK : 0;\n\n\tret = __sys_connect_file(req->file, &io->address,\n\t\t\t\t\treq->connect.addr_len, file_flags);\n\tif ((ret == -EAGAIN || ret == -EINPROGRESS) && force_nonblock) {\n\t\tif (req->async_data)\n\t\t\treturn -EAGAIN;\n\t\tif (io_alloc_async_data(req)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(req->async_data, &__io, sizeof(__io));\n\t\treturn -EAGAIN;\n\t}\n\tif (ret == -ERESTARTSYS)\n\t\tret = -EINTR;\nout:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n#else /* !CONFIG_NET */\n#define IO_NETOP_FN(op)\t\t\t\t\t\t\t\\\nstatic int io_##op(struct io_kiocb *req, unsigned int issue_flags)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn -EOPNOTSUPP;\t\t\t\t\t\t\\\n}\n\n#define IO_NETOP_PREP(op)\t\t\t\t\t\t\\\nIO_NETOP_FN(op)\t\t\t\t\t\t\t\t\\\nstatic int io_##op##_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe) \\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn -EOPNOTSUPP;\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\n#define IO_NETOP_PREP_ASYNC(op)\t\t\t\t\t\t\\\nIO_NETOP_PREP(op)\t\t\t\t\t\t\t\\\nstatic int io_##op##_prep_async(struct io_kiocb *req)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn -EOPNOTSUPP;\t\t\t\t\t\t\\\n}\n\nIO_NETOP_PREP_ASYNC(sendmsg);\nIO_NETOP_PREP_ASYNC(recvmsg);\nIO_NETOP_PREP_ASYNC(connect);\nIO_NETOP_PREP(accept);\nIO_NETOP_FN(send);\nIO_NETOP_FN(recv);\n#endif /* CONFIG_NET */\n\nstruct io_poll_table {\n\tstruct poll_table_struct pt;\n\tstruct io_kiocb *req;\n\tint error;\n};\n\nstatic int __io_async_wake(struct io_kiocb *req, struct io_poll_iocb *poll,\n\t\t\t   __poll_t mask, task_work_func_t func)\n{\n\tint ret;\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\n\ttrace_io_uring_task_add(req->ctx, req->opcode, req->user_data, mask);\n\n\tlist_del_init(&poll->wait.entry);\n\n\treq->result = mask;\n\treq->task_work.func = func;\n\n\t/*\n\t * If this fails, then the task is exiting. When a task exits, the\n\t * work gets canceled, so just cancel this request as well instead\n\t * of executing it. We can't safely execute it anyway, as we may not\n\t * have the needed state needed for it anyway.\n\t */\n\tret = io_req_task_work_add(req);\n\tif (unlikely(ret)) {\n\t\tWRITE_ONCE(poll->canceled, true);\n\t\tio_req_task_work_add_fallback(req, func);\n\t}\n\treturn 1;\n}\n\nstatic bool io_poll_rewait(struct io_kiocb *req, struct io_poll_iocb *poll)\n\t__acquires(&req->ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tstruct poll_table_struct pt = { ._key = poll->events };\n\n\t\treq->result = vfs_poll(req->file, &pt) & poll->events;\n\t}\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req->result && !READ_ONCE(poll->canceled)) {\n\t\tadd_wait_queue(poll->head, &poll->wait);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic struct io_poll_iocb *io_poll_get_double(struct io_kiocb *req)\n{\n\t/* pure poll stashes this in ->async_data, poll driven retry elsewhere */\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn req->async_data;\n\treturn req->apoll->double_poll;\n}\n\nstatic struct io_poll_iocb *io_poll_get_single(struct io_kiocb *req)\n{\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn &req->poll;\n\treturn &req->apoll->poll;\n}\n\nstatic void io_poll_remove_double(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = io_poll_get_double(req);\n\n\tlockdep_assert_held(&req->ctx->completion_lock);\n\n\tif (poll && poll->head) {\n\t\tstruct wait_queue_head *head = poll->head;\n\n\t\tspin_lock(&head->lock);\n\t\tlist_del_init(&poll->wait.entry);\n\t\tif (poll->wait.private)\n\t\t\treq_ref_put(req);\n\t\tpoll->head = NULL;\n\t\tspin_unlock(&head->lock);\n\t}\n}\n\nstatic bool io_poll_complete(struct io_kiocb *req, __poll_t mask, int error)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned flags = IORING_CQE_F_MORE;\n\n\tif (!error && req->poll.canceled) {\n\t\terror = -ECANCELED;\n\t\treq->poll.events |= EPOLLONESHOT;\n\t}\n\tif (!error)\n\t\terror = mangle_poll(mask);\n\tif (req->poll.events & EPOLLONESHOT)\n\t\tflags = 0;\n\tif (!__io_cqring_fill_event(req, error, flags)) {\n\t\tio_poll_remove_waitqs(req);\n\t\treq->poll.done = true;\n\t\tflags = 0;\n\t}\n\tio_commit_cqring(ctx);\n\treturn !(flags & IORING_CQE_F_MORE);\n}\n\nstatic void io_poll_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *nxt;\n\n\tif (io_poll_rewait(req, &req->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t} else {\n\t\tbool done, post_ev;\n\n\t\tpost_ev = done = io_poll_complete(req, req->result, 0);\n\t\tif (done) {\n\t\t\thash_del(&req->hash_node);\n\t\t} else if (!(req->poll.events & EPOLLONESHOT)) {\n\t\t\tpost_ev = true;\n\t\t\treq->result = 0;\n\t\t\tadd_wait_queue(req->poll.head, &req->poll.wait);\n\t\t}\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\n\t\tif (post_ev)\n\t\t\tio_cqring_ev_posted(ctx);\n\t\tif (done) {\n\t\t\tnxt = io_put_req_find_next(req);\n\t\t\tif (nxt)\n\t\t\t\t__io_req_task_submit(nxt);\n\t\t}\n\t}\n}\n\nstatic int io_poll_double_wake(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t       int sync, void *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = io_poll_get_single(req);\n\t__poll_t mask = key_to_poll(key);\n\n\t/* for instances that support it check for an event match first: */\n\tif (mask && !(mask & poll->events))\n\t\treturn 0;\n\tif (!(poll->events & EPOLLONESHOT))\n\t\treturn poll->wait.func(&poll->wait, mode, sync, key);\n\n\tlist_del_init(&wait->entry);\n\n\tif (poll && poll->head) {\n\t\tbool done;\n\n\t\tspin_lock(&poll->head->lock);\n\t\tdone = list_empty(&poll->wait.entry);\n\t\tif (!done)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t/* make sure double remove sees this as being gone */\n\t\twait->private = NULL;\n\t\tspin_unlock(&poll->head->lock);\n\t\tif (!done) {\n\t\t\t/* use wait func handler, so it matches the rq type */\n\t\t\tpoll->wait.func(&poll->wait, mode, sync, key);\n\t\t}\n\t}\n\treq_ref_put(req);\n\treturn 1;\n}\n\nstatic void io_init_poll_iocb(struct io_poll_iocb *poll, __poll_t events,\n\t\t\t      wait_queue_func_t wake_func)\n{\n\tpoll->head = NULL;\n\tpoll->done = false;\n\tpoll->canceled = false;\n\tpoll->update_events = poll->update_user_data = false;\n#define IO_POLL_UNMASK\t(EPOLLERR|EPOLLHUP|EPOLLNVAL|EPOLLRDHUP)\n\t/* mask in events that we always want/need */\n\tpoll->events = events | IO_POLL_UNMASK;\n\tINIT_LIST_HEAD(&poll->wait.entry);\n\tinit_waitqueue_func_entry(&poll->wait, wake_func);\n}\n\nstatic void __io_queue_proc(struct io_poll_iocb *poll, struct io_poll_table *pt,\n\t\t\t    struct wait_queue_head *head,\n\t\t\t    struct io_poll_iocb **poll_ptr)\n{\n\tstruct io_kiocb *req = pt->req;\n\n\t/*\n\t * If poll->head is already set, it's because the file being polled\n\t * uses multiple waitqueues for poll handling (eg one for read, one\n\t * for write). Setup a separate io_poll_iocb if this happens.\n\t */\n\tif (unlikely(poll->head)) {\n\t\tstruct io_poll_iocb *poll_one = poll;\n\n\t\t/* already have a 2nd entry, fail a third attempt */\n\t\tif (*poll_ptr) {\n\t\t\tpt->error = -EINVAL;\n\t\t\treturn;\n\t\t}\n\t\t/* double add on the same waitqueue head, ignore */\n\t\tif (poll->head == head)\n\t\t\treturn;\n\t\tpoll = kmalloc(sizeof(*poll), GFP_ATOMIC);\n\t\tif (!poll) {\n\t\t\tpt->error = -ENOMEM;\n\t\t\treturn;\n\t\t}\n\t\tio_init_poll_iocb(poll, poll_one->events, io_poll_double_wake);\n\t\treq_ref_get(req);\n\t\tpoll->wait.private = req;\n\t\t*poll_ptr = poll;\n\t}\n\n\tpt->error = 0;\n\tpoll->head = head;\n\n\tif (poll->events & EPOLLEXCLUSIVE)\n\t\tadd_wait_queue_exclusive(head, &poll->wait);\n\telse\n\t\tadd_wait_queue(head, &poll->wait);\n}\n\nstatic void io_async_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\tstruct async_poll *apoll = pt->req->apoll;\n\n\t__io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);\n}\n\nstatic void io_async_task_func(struct callback_head *cb)\n{\n\tstruct io_kiocb *req = container_of(cb, struct io_kiocb, task_work);\n\tstruct async_poll *apoll = req->apoll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\t\thash_del(&req->hash_node);\n\n\tio_poll_remove_double(req);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\tio_req_complete_failed(req, -ECANCELED);\n\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}\n\nstatic int io_async_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->apoll->poll;\n\n\ttrace_io_uring_poll_wake(req->ctx, req->opcode, req->user_data,\n\t\t\t\t\tkey_to_poll(key));\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_async_task_func);\n}\n\nstatic void io_poll_req_insert(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct hlist_head *list;\n\n\tlist = &ctx->cancel_hash[hash_long(req->user_data, ctx->cancel_hash_bits)];\n\thlist_add_head(&req->hash_node, list);\n}\n\nstatic __poll_t __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t      struct io_poll_iocb *poll,\n\t\t\t\t      struct io_poll_table *ipt, __poll_t mask,\n\t\t\t\t      wait_queue_func_t wake_func)\n\t__acquires(&ctx->completion_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tbool cancel = false;\n\n\tINIT_HLIST_NODE(&req->hash_node);\n\tio_init_poll_iocb(poll, mask, wake_func);\n\tpoll->file = req->file;\n\tpoll->wait.private = req;\n\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = -EINVAL;\n\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (likely(poll->head)) {\n\t\tspin_lock(&poll->head->lock);\n\t\tif (unlikely(list_empty(&poll->wait.entry))) {\n\t\t\tif (ipt->error)\n\t\t\t\tcancel = true;\n\t\t\tipt->error = 0;\n\t\t\tmask = 0;\n\t\t}\n\t\tif ((mask && (poll->events & EPOLLONESHOT)) || ipt->error)\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\telse if (cancel)\n\t\t\tWRITE_ONCE(poll->canceled, true);\n\t\telse if (!poll->done) /* actually waiting for an event */\n\t\t\tio_poll_req_insert(req);\n\t\tspin_unlock(&poll->head->lock);\n\t}\n\n\treturn mask;\n}\n\nstatic bool io_arm_poll_handler(struct io_kiocb *req)\n{\n\tconst struct io_op_def *def = &io_op_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask, ret;\n\tint rw;\n\n\tif (!req->file || !file_can_poll(req->file))\n\t\treturn false;\n\tif (req->flags & REQ_F_POLLED)\n\t\treturn false;\n\tif (def->pollin)\n\t\trw = READ;\n\telse if (def->pollout)\n\t\trw = WRITE;\n\telse\n\t\treturn false;\n\t/* if we can't nonblock try, then no point in arming a poll handler */\n\tif (!io_file_supports_async(req, rw))\n\t\treturn false;\n\n\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\tif (unlikely(!apoll))\n\t\treturn false;\n\tapoll->double_poll = NULL;\n\n\treq->flags |= REQ_F_POLLED;\n\treq->apoll = apoll;\n\n\tmask = EPOLLONESHOT;\n\tif (def->pollin)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (def->pollout)\n\t\tmask |= POLLOUT | POLLWRNORM;\n\n\t/* If reading from MSG_ERRQUEUE using recvmsg, ignore POLLIN */\n\tif ((req->opcode == IORING_OP_RECVMSG) &&\n\t    (req->sr_msg.msg_flags & MSG_ERRQUEUE))\n\t\tmask &= ~POLLIN;\n\n\tmask |= POLLERR | POLLPRI;\n\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask,\n\t\t\t\t\tio_async_wake);\n\tif (ret || ipt.error) {\n\t\tio_poll_remove_double(req);\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(apoll->double_poll);\n\t\tkfree(apoll);\n\t\treturn false;\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\ttrace_io_uring_poll_arm(ctx, req->opcode, req->user_data, mask,\n\t\t\t\t\tapoll->poll.events);\n\treturn true;\n}\n\nstatic bool __io_poll_remove_one(struct io_kiocb *req,\n\t\t\t\t struct io_poll_iocb *poll)\n{\n\tbool do_complete = false;\n\n\tif (!poll->head)\n\t\treturn false;\n\tspin_lock(&poll->head->lock);\n\tWRITE_ONCE(poll->canceled, true);\n\tif (!list_empty(&poll->wait.entry)) {\n\t\tlist_del_init(&poll->wait.entry);\n\t\tdo_complete = true;\n\t}\n\tspin_unlock(&poll->head->lock);\n\thash_del(&req->hash_node);\n\treturn do_complete;\n}\n\nstatic bool io_poll_remove_waitqs(struct io_kiocb *req)\n{\n\tbool do_complete;\n\n\tio_poll_remove_double(req);\n\n\tif (req->opcode == IORING_OP_POLL_ADD) {\n\t\tdo_complete = __io_poll_remove_one(req, &req->poll);\n\t} else {\n\t\tstruct async_poll *apoll = req->apoll;\n\n\t\t/* non-poll requests have submit ref still */\n\t\tdo_complete = __io_poll_remove_one(req, &apoll->poll);\n\t\tif (do_complete) {\n\t\t\tio_put_req(req);\n\t\t\tkfree(apoll->double_poll);\n\t\t\tkfree(apoll);\n\t\t}\n\t}\n\n\treturn do_complete;\n}\n\nstatic bool io_poll_remove_one(struct io_kiocb *req)\n{\n\tbool do_complete;\n\n\tdo_complete = io_poll_remove_waitqs(req);\n\tif (do_complete) {\n\t\tio_cqring_fill_event(req, -ECANCELED);\n\t\tio_commit_cqring(req->ctx);\n\t\treq_set_fail_links(req);\n\t\tio_put_req_deferred(req, 1);\n\t}\n\n\treturn do_complete;\n}\n\n/*\n * Returns true if we found and killed one or more poll requests\n */\nstatic bool io_poll_remove_all(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t       struct files_struct *files)\n{\n\tstruct hlist_node *tmp;\n\tstruct io_kiocb *req;\n\tint posted = 0, i;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list;\n\n\t\tlist = &ctx->cancel_hash[i];\n\t\thlist_for_each_entry_safe(req, tmp, list, hash_node) {\n\t\t\tif (io_match_task(req, tsk, files))\n\t\t\t\tposted += io_poll_remove_one(req);\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (posted)\n\t\tio_cqring_ev_posted(ctx);\n\n\treturn posted != 0;\n}\n\nstatic struct io_kiocb *io_poll_find(struct io_ring_ctx *ctx, __u64 sqe_addr)\n{\n\tstruct hlist_head *list;\n\tstruct io_kiocb *req;\n\n\tlist = &ctx->cancel_hash[hash_long(sqe_addr, ctx->cancel_hash_bits)];\n\thlist_for_each_entry(req, list, hash_node) {\n\t\tif (sqe_addr != req->user_data)\n\t\t\tcontinue;\n\t\treturn req;\n\t}\n\n\treturn NULL;\n}\n\nstatic int io_poll_cancel(struct io_ring_ctx *ctx, __u64 sqe_addr)\n{\n\tstruct io_kiocb *req;\n\n\treq = io_poll_find(ctx, sqe_addr);\n\tif (!req)\n\t\treturn -ENOENT;\n\tif (io_poll_remove_one(req))\n\t\treturn 0;\n\n\treturn -EALREADY;\n}\n\nstatic int io_poll_remove_prep(struct io_kiocb *req,\n\t\t\t       const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->buf_index ||\n\t    sqe->poll_events)\n\t\treturn -EINVAL;\n\n\treq->poll_remove.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\n/*\n * Find a running poll command that matches one specified in sqe->addr,\n * and remove it if found.\n */\nstatic int io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tret = io_poll_cancel(ctx, req->poll_remove.addr);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_req_complete(req, ret);\n\treturn 0;\n}\n\nstatic int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_poll_iocb *poll = &req->poll;\n\n\treturn __io_async_wake(req, poll, key_to_poll(key), io_poll_task_func);\n}\n\nstatic void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\n\t__io_queue_proc(&pt->req->poll, pt, head, (struct io_poll_iocb **) &pt->req->async_data);\n}\n\nstatic int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tu32 events, flags;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->len);\n\tif (flags & ~(IORING_POLL_ADD_MULTI | IORING_POLL_UPDATE_EVENTS |\n\t\t\tIORING_POLL_UPDATE_USER_DATA))\n\t\treturn -EINVAL;\n\tevents = READ_ONCE(sqe->poll32_events);\n#ifdef __BIG_ENDIAN\n\tevents = swahw32(events);\n#endif\n\tif (!(flags & IORING_POLL_ADD_MULTI))\n\t\tevents |= EPOLLONESHOT;\n\tpoll->update_events = poll->update_user_data = false;\n\tif (flags & IORING_POLL_UPDATE_EVENTS) {\n\t\tpoll->update_events = true;\n\t\tpoll->old_user_data = READ_ONCE(sqe->addr);\n\t}\n\tif (flags & IORING_POLL_UPDATE_USER_DATA) {\n\t\tpoll->update_user_data = true;\n\t\tpoll->new_user_data = READ_ONCE(sqe->off);\n\t}\n\tif (!(poll->update_events || poll->update_user_data) &&\n\t     (sqe->off || sqe->addr))\n\t\treturn -EINVAL;\n\tpoll->events = demangle_poll(events) |\n\t\t\t\t(events & (EPOLLEXCLUSIVE|EPOLLONESHOT));\n\treturn 0;\n}\n\nstatic int __io_poll_add(struct io_kiocb *req)\n{\n\tstruct io_poll_iocb *poll = &req->poll;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_poll_table ipt;\n\t__poll_t mask;\n\n\tipt.pt._qproc = io_poll_queue_proc;\n\n\tmask = __io_arm_poll_handler(req, &req->poll, &ipt, poll->events,\n\t\t\t\t\tio_poll_wake);\n\n\tif (mask) { /* no async, we'd stolen it */\n\t\tipt.error = 0;\n\t\tio_poll_complete(req, mask, 0);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tif (mask) {\n\t\tio_cqring_ev_posted(ctx);\n\t\tif (poll->events & EPOLLONESHOT)\n\t\t\tio_put_req(req);\n\t}\n\treturn ipt.error;\n}\n\nstatic int io_poll_update(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *preq;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tpreq = io_poll_find(ctx, req->poll.old_user_data);\n\tif (!preq) {\n\t\tret = -ENOENT;\n\t\tgoto err;\n\t} else if (preq->opcode != IORING_OP_POLL_ADD) {\n\t\t/* don't allow internal poll updates */\n\t\tret = -EACCES;\n\t\tgoto err;\n\t}\n\tif (!__io_poll_remove_one(preq, &preq->poll)) {\n\t\t/* in process of completing/removal */\n\t\tret = -EALREADY;\n\t\tgoto err;\n\t}\n\t/* we now have a detached poll request. reissue. */\n\tret = 0;\nerr:\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (ret < 0) {\n\t\treq_set_fail_links(req);\n\t\tio_req_complete(req, ret);\n\t\treturn 0;\n\t}\n\t/* only mask one event flags, keep behavior flags */\n\tif (req->poll.update_events) {\n\t\tpreq->poll.events &= ~0xffff;\n\t\tpreq->poll.events |= req->poll.events & 0xffff;\n\t\tpreq->poll.events |= IO_POLL_UNMASK;\n\t}\n\tif (req->poll.update_user_data)\n\t\tpreq->user_data = req->poll.new_user_data;\n\n\t/* complete update request, we're done with it */\n\tio_req_complete(req, ret);\n\n\tret = __io_poll_add(preq);\n\tif (ret < 0) {\n\t\treq_set_fail_links(preq);\n\t\tio_req_complete(preq, ret);\n\t}\n\treturn 0;\n}\n\nstatic int io_poll_add(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tif (!req->poll.update_events && !req->poll.update_user_data)\n\t\treturn __io_poll_add(req);\n\treturn io_poll_update(req);\n}\n\nstatic enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tlist_del_init(&req->timeout.list);\n\tatomic_set(&req->ctx->cq_timeouts,\n\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\n\tio_cqring_fill_event(req, -ETIME);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tio_cqring_ev_posted(ctx);\n\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic struct io_kiocb *io_timeout_extract(struct io_ring_ctx *ctx,\n\t\t\t\t\t   __u64 user_data)\n{\n\tstruct io_timeout_data *io;\n\tstruct io_kiocb *req;\n\tint ret = -ENOENT;\n\n\tlist_for_each_entry(req, &ctx->timeout_list, timeout.list) {\n\t\tif (user_data == req->user_data) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret == -ENOENT)\n\t\treturn ERR_PTR(ret);\n\n\tio = req->async_data;\n\tret = hrtimer_try_to_cancel(&io->timer);\n\tif (ret == -1)\n\t\treturn ERR_PTR(-EALREADY);\n\tlist_del_init(&req->timeout.list);\n\treturn req;\n}\n\nstatic int io_timeout_cancel(struct io_ring_ctx *ctx, __u64 user_data)\n{\n\tstruct io_kiocb *req = io_timeout_extract(ctx, user_data);\n\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq_set_fail_links(req);\n\tio_cqring_fill_event(req, -ECANCELED);\n\tio_put_req_deferred(req, 1);\n\treturn 0;\n}\n\nstatic int io_timeout_update(struct io_ring_ctx *ctx, __u64 user_data,\n\t\t\t     struct timespec64 *ts, enum hrtimer_mode mode)\n{\n\tstruct io_kiocb *req = io_timeout_extract(ctx, user_data);\n\tstruct io_timeout_data *data;\n\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq->timeout.off = 0; /* noseq */\n\tdata = req->async_data;\n\tlist_add_tail(&req->timeout.list, &ctx->timeout_list);\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, mode);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(*ts), mode);\n\treturn 0;\n}\n\nstatic int io_timeout_remove_prep(struct io_kiocb *req,\n\t\t\t\t  const struct io_uring_sqe *sqe)\n{\n\tstruct io_timeout_rem *tr = &req->timeout_rem;\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len)\n\t\treturn -EINVAL;\n\n\ttr->addr = READ_ONCE(sqe->addr);\n\ttr->flags = READ_ONCE(sqe->timeout_flags);\n\tif (tr->flags & IORING_TIMEOUT_UPDATE) {\n\t\tif (tr->flags & ~(IORING_TIMEOUT_UPDATE|IORING_TIMEOUT_ABS))\n\t\t\treturn -EINVAL;\n\t\tif (get_timespec64(&tr->ts, u64_to_user_ptr(sqe->addr2)))\n\t\t\treturn -EFAULT;\n\t} else if (tr->flags) {\n\t\t/* timeout removal doesn't support flags */\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic inline enum hrtimer_mode io_translate_timeout_mode(unsigned int flags)\n{\n\treturn (flags & IORING_TIMEOUT_ABS) ? HRTIMER_MODE_ABS\n\t\t\t\t\t    : HRTIMER_MODE_REL;\n}\n\n/*\n * Remove or update an existing timeout command\n */\nstatic int io_timeout_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_timeout_rem *tr = &req->timeout_rem;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!(req->timeout_rem.flags & IORING_TIMEOUT_UPDATE))\n\t\tret = io_timeout_cancel(ctx, tr->addr);\n\telse\n\t\tret = io_timeout_update(ctx, tr->addr, &tr->ts,\n\t\t\t\t\tio_translate_timeout_mode(tr->flags));\n\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn 0;\n}\n\nstatic int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\t   bool is_timeout_link)\n{\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->buf_index || sqe->len != 1)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~IORING_TIMEOUT_ABS)\n\t\treturn -EINVAL;\n\n\treq->timeout.off = off;\n\n\tif (!req->async_data && io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, data->mode);\n\tif (is_timeout_link)\n\t\tio_req_track_inflight(req);\n\treturn 0;\n}\n\nstatic int io_timeout(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_timeout_data *data = req->async_data;\n\tstruct list_head *entry;\n\tu32 tail, off = req->timeout.off;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\n\t/*\n\t * sqe->off holds how many events that need to occur for this\n\t * timeout event to be satisfied. If it isn't set, then this is\n\t * a pure timeout request, sequence isn't used.\n\t */\n\tif (io_is_timeout_noseq(req)) {\n\t\tentry = ctx->timeout_list.prev;\n\t\tgoto add;\n\t}\n\n\ttail = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\treq->timeout.target_seq = tail + off;\n\n\t/* Update the last seq here in case io_flush_timeouts() hasn't.\n\t * This is safe because ->completion_lock is held, and submissions\n\t * and completions are never mixed in the same ->completion_lock section.\n\t */\n\tctx->cq_last_tm_flush = tail;\n\n\t/*\n\t * Insertion sort, ensuring the first entry in the list is always\n\t * the one we need first.\n\t */\n\tlist_for_each_prev(entry, &ctx->timeout_list) {\n\t\tstruct io_kiocb *nxt = list_entry(entry, struct io_kiocb,\n\t\t\t\t\t\t  timeout.list);\n\n\t\tif (io_is_timeout_noseq(nxt))\n\t\t\tcontinue;\n\t\t/* nxt.seq is behind @tail, otherwise would've been completed */\n\t\tif (off >= nxt->timeout.target_seq - tail)\n\t\t\tbreak;\n\t}\nadd:\n\tlist_add(&req->timeout.list, entry);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn 0;\n}\n\nstruct io_cancel_data {\n\tstruct io_ring_ctx *ctx;\n\tu64 user_data;\n};\n\nstatic bool io_cancel_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_cancel_data *cd = data;\n\n\treturn req->ctx == cd->ctx && req->user_data == cd->user_data;\n}\n\nstatic int io_async_cancel_one(struct io_uring_task *tctx, u64 user_data,\n\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_cancel_data data = { .ctx = ctx, .user_data = user_data, };\n\tenum io_wq_cancel cancel_ret;\n\tint ret = 0;\n\n\tif (!tctx || !tctx->io_wq)\n\t\treturn -ENOENT;\n\n\tcancel_ret = io_wq_cancel_cb(tctx->io_wq, io_cancel_cb, &data, false);\n\tswitch (cancel_ret) {\n\tcase IO_WQ_CANCEL_OK:\n\t\tret = 0;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_RUNNING:\n\t\tret = -EALREADY;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_NOTFOUND:\n\t\tret = -ENOENT;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void io_async_find_and_cancel(struct io_ring_ctx *ctx,\n\t\t\t\t     struct io_kiocb *req, __u64 sqe_addr,\n\t\t\t\t     int success_ret)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tret = io_async_cancel_one(req->task->io_uring, sqe_addr, ctx);\n\tif (ret != -ENOENT) {\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tgoto done;\n\t}\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tret = io_timeout_cancel(ctx, sqe_addr);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tret = io_poll_cancel(ctx, sqe_addr);\ndone:\n\tif (!ret)\n\t\tret = success_ret;\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\tio_cqring_ev_posted(ctx);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n}\n\nstatic int io_async_cancel_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->len || sqe->cancel_flags)\n\t\treturn -EINVAL;\n\n\treq->cancel.addr = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_async_cancel(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tu64 sqe_addr = req->cancel.addr;\n\tstruct io_tctx_node *node;\n\tint ret;\n\n\t/* tasks should wait for their io-wq threads, so safe w/o sync */\n\tret = io_async_cancel_one(req->task->io_uring, sqe_addr, ctx);\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tret = io_timeout_cancel(ctx, sqe_addr);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tret = io_poll_cancel(ctx, sqe_addr);\n\tif (ret != -ENOENT)\n\t\tgoto done;\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\t/* slow path, try all io-wq's */\n\tio_ring_submit_lock(ctx, !(issue_flags & IO_URING_F_NONBLOCK));\n\tret = -ENOENT;\n\tlist_for_each_entry(node, &ctx->tctx_list, ctx_node) {\n\t\tstruct io_uring_task *tctx = node->task->io_uring;\n\n\t\tif (!tctx || !tctx->io_wq)\n\t\t\tcontinue;\n\t\tret = io_async_cancel_one(tctx, req->cancel.addr, ctx);\n\t\tif (ret != -ENOENT)\n\t\t\tbreak;\n\t}\n\tio_ring_submit_unlock(ctx, !(issue_flags & IO_URING_F_NONBLOCK));\n\n\tspin_lock_irq(&ctx->completion_lock);\ndone:\n\tio_cqring_fill_event(req, ret);\n\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\tio_cqring_ev_posted(ctx);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tio_put_req(req);\n\treturn 0;\n}\n\nstatic int io_rsrc_update_prep(struct io_kiocb *req,\n\t\t\t\tconst struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_SQPOLL))\n\t\treturn -EINVAL;\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->rw_flags)\n\t\treturn -EINVAL;\n\n\treq->rsrc_update.offset = READ_ONCE(sqe->off);\n\treq->rsrc_update.nr_args = READ_ONCE(sqe->len);\n\tif (!req->rsrc_update.nr_args)\n\t\treturn -EINVAL;\n\treq->rsrc_update.arg = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_files_update(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_rsrc_update up;\n\tint ret;\n\n\tif (issue_flags & IO_URING_F_NONBLOCK)\n\t\treturn -EAGAIN;\n\n\tup.offset = req->rsrc_update.offset;\n\tup.data = req->rsrc_update.arg;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_sqe_files_update(ctx, &up, req->rsrc_update.nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\t__io_req_complete(req, issue_flags, ret, 0);\n\treturn 0;\n}\n\nstatic int io_req_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\treturn 0;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\treturn io_read_prep(req, sqe);\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\treturn io_write_prep(req, sqe);\n\tcase IORING_OP_POLL_ADD:\n\t\treturn io_poll_add_prep(req, sqe);\n\tcase IORING_OP_POLL_REMOVE:\n\t\treturn io_poll_remove_prep(req, sqe);\n\tcase IORING_OP_FSYNC:\n\t\treturn io_fsync_prep(req, sqe);\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\treturn io_sfr_prep(req, sqe);\n\tcase IORING_OP_SENDMSG:\n\tcase IORING_OP_SEND:\n\t\treturn io_sendmsg_prep(req, sqe);\n\tcase IORING_OP_RECVMSG:\n\tcase IORING_OP_RECV:\n\t\treturn io_recvmsg_prep(req, sqe);\n\tcase IORING_OP_CONNECT:\n\t\treturn io_connect_prep(req, sqe);\n\tcase IORING_OP_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, false);\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\treturn io_timeout_remove_prep(req, sqe);\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\treturn io_async_cancel_prep(req, sqe);\n\tcase IORING_OP_LINK_TIMEOUT:\n\t\treturn io_timeout_prep(req, sqe, true);\n\tcase IORING_OP_ACCEPT:\n\t\treturn io_accept_prep(req, sqe);\n\tcase IORING_OP_FALLOCATE:\n\t\treturn io_fallocate_prep(req, sqe);\n\tcase IORING_OP_OPENAT:\n\t\treturn io_openat_prep(req, sqe);\n\tcase IORING_OP_CLOSE:\n\t\treturn io_close_prep(req, sqe);\n\tcase IORING_OP_FILES_UPDATE:\n\t\treturn io_rsrc_update_prep(req, sqe);\n\tcase IORING_OP_STATX:\n\t\treturn io_statx_prep(req, sqe);\n\tcase IORING_OP_FADVISE:\n\t\treturn io_fadvise_prep(req, sqe);\n\tcase IORING_OP_MADVISE:\n\t\treturn io_madvise_prep(req, sqe);\n\tcase IORING_OP_OPENAT2:\n\t\treturn io_openat2_prep(req, sqe);\n\tcase IORING_OP_EPOLL_CTL:\n\t\treturn io_epoll_ctl_prep(req, sqe);\n\tcase IORING_OP_SPLICE:\n\t\treturn io_splice_prep(req, sqe);\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\treturn io_provide_buffers_prep(req, sqe);\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\treturn io_remove_buffers_prep(req, sqe);\n\tcase IORING_OP_TEE:\n\t\treturn io_tee_prep(req, sqe);\n\tcase IORING_OP_SHUTDOWN:\n\t\treturn io_shutdown_prep(req, sqe);\n\tcase IORING_OP_RENAMEAT:\n\t\treturn io_renameat_prep(req, sqe);\n\tcase IORING_OP_UNLINKAT:\n\t\treturn io_unlinkat_prep(req, sqe);\n\t}\n\n\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n\t\t\treq->opcode);\n\treturn-EINVAL;\n}\n\nstatic int io_req_prep_async(struct io_kiocb *req)\n{\n\tif (!io_op_defs[req->opcode].needs_async_setup)\n\t\treturn 0;\n\tif (WARN_ON_ONCE(req->async_data))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -EAGAIN;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_READV:\n\t\treturn io_rw_prep_async(req, READ);\n\tcase IORING_OP_WRITEV:\n\t\treturn io_rw_prep_async(req, WRITE);\n\tcase IORING_OP_SENDMSG:\n\t\treturn io_sendmsg_prep_async(req);\n\tcase IORING_OP_RECVMSG:\n\t\treturn io_recvmsg_prep_async(req);\n\tcase IORING_OP_CONNECT:\n\t\treturn io_connect_prep_async(req);\n\t}\n\tprintk_once(KERN_WARNING \"io_uring: prep_async() bad opcode %d\\n\",\n\t\t    req->opcode);\n\treturn -EFAULT;\n}\n\nstatic u32 io_get_sequence(struct io_kiocb *req)\n{\n\tstruct io_kiocb *pos;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tu32 total_submitted, nr_reqs = 0;\n\n\tio_for_each_link(pos, req)\n\t\tnr_reqs++;\n\n\ttotal_submitted = ctx->cached_sq_head - ctx->cached_sq_dropped;\n\treturn total_submitted - nr_reqs;\n}\n\nstatic int io_req_defer(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_defer_entry *de;\n\tint ret;\n\tu32 seq;\n\n\t/* Still need defer if there is pending req in defer list. */\n\tif (likely(list_empty_careful(&ctx->defer_list) &&\n\t\t!(req->flags & REQ_F_IO_DRAIN)))\n\t\treturn 0;\n\n\tseq = io_get_sequence(req);\n\t/* Still a chance to pass the sequence check */\n\tif (!req_need_defer(req, seq) && list_empty_careful(&ctx->defer_list))\n\t\treturn 0;\n\n\tret = io_req_prep_async(req);\n\tif (ret)\n\t\treturn ret;\n\tio_prep_async_link(req);\n\tde = kmalloc(sizeof(*de), GFP_KERNEL);\n\tif (!de)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (!req_need_defer(req, seq) && list_empty(&ctx->defer_list)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tkfree(de);\n\t\tio_queue_async_work(req);\n\t\treturn -EIOCBQUEUED;\n\t}\n\n\ttrace_io_uring_defer(ctx, req, req->user_data);\n\tde->req = req;\n\tde->seq = seq;\n\tlist_add_tail(&de->list, &ctx->defer_list);\n\tspin_unlock_irq(&ctx->completion_lock);\n\treturn -EIOCBQUEUED;\n}\n\nstatic void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\t\tkfree((void *)(unsigned long)req->rw.addr);\n\t\t\tbreak;\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_RECV:\n\t\t\tkfree(req->sr_msg.kbuf);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tswitch (req->opcode) {\n\t\tcase IORING_OP_READV:\n\t\tcase IORING_OP_READ_FIXED:\n\t\tcase IORING_OP_READ:\n\t\tcase IORING_OP_WRITEV:\n\t\tcase IORING_OP_WRITE_FIXED:\n\t\tcase IORING_OP_WRITE: {\n\t\t\tstruct io_async_rw *io = req->async_data;\n\t\t\tif (io->free_iovec)\n\t\t\t\tkfree(io->free_iovec);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_RECVMSG:\n\t\tcase IORING_OP_SENDMSG: {\n\t\t\tstruct io_async_msghdr *io = req->async_data;\n\n\t\t\tkfree(io->free_iov);\n\t\t\tbreak;\n\t\t\t}\n\t\tcase IORING_OP_SPLICE:\n\t\tcase IORING_OP_TEE:\n\t\t\tif (!(req->splice.flags & SPLICE_F_FD_IN_FIXED))\n\t\t\t\tio_put_file(req->splice.file_in);\n\t\t\tbreak;\n\t\tcase IORING_OP_OPENAT:\n\t\tcase IORING_OP_OPENAT2:\n\t\t\tif (req->open.filename)\n\t\t\t\tputname(req->open.filename);\n\t\t\tbreak;\n\t\tcase IORING_OP_RENAMEAT:\n\t\t\tputname(req->rename.oldpath);\n\t\t\tputname(req->rename.newpath);\n\t\t\tbreak;\n\t\tcase IORING_OP_UNLINKAT:\n\t\t\tputname(req->unlink.filename);\n\t\t\tbreak;\n\t\t}\n\t\treq->flags &= ~REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic int io_issue_sqe(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tconst struct cred *creds = NULL;\n\tint ret;\n\n\tif (req->work.creds && req->work.creds != current_cred())\n\t\tcreds = override_creds(req->work.creds);\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tret = io_nop(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\tcase IORING_OP_READ:\n\t\tret = io_read(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\tcase IORING_OP_WRITE:\n\t\tret = io_write(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tret = io_fsync(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tret = io_poll_add(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tret = io_poll_remove(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tret = io_sync_file_range(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tret = io_sendmsg(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SEND:\n\t\tret = io_send(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tret = io_recvmsg(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_RECV:\n\t\tret = io_recv(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tret = io_timeout(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tret = io_timeout_remove(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tret = io_accept(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tret = io_connect(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tret = io_async_cancel(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tret = io_fallocate(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tret = io_openat(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_CLOSE:\n\t\tret = io_close(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FILES_UPDATE:\n\t\tret = io_files_update(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_STATX:\n\t\tret = io_statx(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_FADVISE:\n\t\tret = io_fadvise(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_MADVISE:\n\t\tret = io_madvise(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_OPENAT2:\n\t\tret = io_openat2(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_EPOLL_CTL:\n\t\tret = io_epoll_ctl(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SPLICE:\n\t\tret = io_splice(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_PROVIDE_BUFFERS:\n\t\tret = io_provide_buffers(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_REMOVE_BUFFERS:\n\t\tret = io_remove_buffers(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_TEE:\n\t\tret = io_tee(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_SHUTDOWN:\n\t\tret = io_shutdown(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_RENAMEAT:\n\t\tret = io_renameat(req, issue_flags);\n\t\tbreak;\n\tcase IORING_OP_UNLINKAT:\n\t\tret = io_unlinkat(req, issue_flags);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (creds)\n\t\trevert_creds(creds);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/* If the op doesn't have a file, we're not polling for it */\n\tif ((ctx->flags & IORING_SETUP_IOPOLL) && req->file) {\n\t\tconst bool in_async = io_wq_current_is_worker();\n\n\t\t/* workqueue context doesn't hold uring_lock, grab it now */\n\t\tif (in_async)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tio_iopoll_req_issued(req, in_async);\n\n\t\tif (in_async)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic void io_wq_submit_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_kiocb *timeout;\n\tint ret = 0;\n\n\ttimeout = io_prep_linked_timeout(req);\n\tif (timeout)\n\t\tio_queue_linked_timeout(timeout);\n\n\tif (work->flags & IO_WQ_WORK_CANCEL)\n\t\tret = -ECANCELED;\n\n\tif (!ret) {\n\t\tdo {\n\t\t\tret = io_issue_sqe(req, 0);\n\t\t\t/*\n\t\t\t * We can get EAGAIN for polled IO even though we're\n\t\t\t * forcing a sync submission from here, since we can't\n\t\t\t * wait for request slots on the block side.\n\t\t\t */\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tbreak;\n\t\t\tcond_resched();\n\t\t} while (1);\n\t}\n\n\t/* avoid locking problems by failing it from a clean context */\n\tif (ret) {\n\t\t/* io-wq is going to take one down */\n\t\treq_ref_get(req);\n\t\tio_req_task_queue_fail(req, ret);\n\t}\n}\n\n#define FFS_ASYNC_READ\t\t0x1UL\n#define FFS_ASYNC_WRITE\t\t0x2UL\n#ifdef CONFIG_64BIT\n#define FFS_ISREG\t\t0x4UL\n#else\n#define FFS_ISREG\t\t0x0UL\n#endif\n#define FFS_MASK\t\t~(FFS_ASYNC_READ|FFS_ASYNC_WRITE|FFS_ISREG)\n\nstatic inline struct file **io_fixed_file_slot(struct fixed_rsrc_data *file_data,\n\t\t\t\t\t       unsigned i)\n{\n\tstruct fixed_rsrc_table *table;\n\n\ttable = &file_data->table[i >> IORING_FILE_TABLE_SHIFT];\n\treturn &table->files[i & IORING_FILE_TABLE_MASK];\n}\n\nstatic inline struct file *io_file_from_index(struct io_ring_ctx *ctx,\n\t\t\t\t\t      int index)\n{\n\tstruct file **file_slot = io_fixed_file_slot(ctx->file_data, index);\n\n\treturn (struct file *) ((unsigned long) *file_slot & FFS_MASK);\n}\n\nstatic struct file *io_file_get(struct io_submit_state *state,\n\t\t\t\tstruct io_kiocb *req, int fd, bool fixed)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct file *file;\n\n\tif (fixed) {\n\t\tunsigned long file_ptr;\n\n\t\tif (unlikely((unsigned int)fd >= ctx->nr_user_files))\n\t\t\treturn NULL;\n\t\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\t\tfile_ptr = (unsigned long) *io_fixed_file_slot(ctx->file_data, fd);\n\t\tfile = (struct file *) (file_ptr & FFS_MASK);\n\t\tfile_ptr &= ~FFS_MASK;\n\t\t/* mask in overlapping REQ_F and FFS bits */\n\t\treq->flags |= (file_ptr << REQ_F_ASYNC_READ_BIT);\n\t\tio_set_resource_node(req);\n\t} else {\n\t\ttrace_io_uring_file_get(ctx, fd);\n\t\tfile = __io_file_get(state, fd);\n\n\t\t/* we don't allow fixed io_uring files */\n\t\tif (file && unlikely(file->f_op == &io_uring_fops))\n\t\t\tio_req_track_inflight(req);\n\t}\n\n\treturn file;\n}\n\nstatic enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *prev, *req = data->req;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tprev = req->timeout.head;\n\treq->timeout.head = NULL;\n\n\t/*\n\t * We don't expect the list to be empty, that will only happen if we\n\t * race with the completion of the linked work.\n\t */\n\tif (prev && req_ref_inc_not_zero(prev))\n\t\tio_remove_next_linked(prev);\n\telse\n\t\tprev = NULL;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tif (prev) {\n\t\tio_async_find_and_cancel(ctx, req, prev->user_data, -ETIME);\n\t\tio_put_req_deferred(prev, 1);\n\t} else {\n\t\tio_req_complete_post(req, -ETIME, 0);\n\t\tio_put_req_deferred(req, 1);\n\t}\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void io_queue_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\t/*\n\t * If the back reference is NULL, then our linked request finished\n\t * before we got a chance to setup the timer\n\t */\n\tif (req->timeout.head) {\n\t\tstruct io_timeout_data *data = req->async_data;\n\n\t\tdata->timer.function = io_link_timeout_fn;\n\t\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts),\n\t\t\t\tdata->mode);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\t/* drop submission reference */\n\tio_put_req(req);\n}\n\nstatic struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\tif (!nxt || (req->flags & REQ_F_LINK_TIMEOUT) ||\n\t    nxt->opcode != IORING_OP_LINK_TIMEOUT)\n\t\treturn NULL;\n\n\tnxt->timeout.head = req;\n\tnxt->flags |= REQ_F_LTIMEOUT_ACTIVE;\n\treq->flags |= REQ_F_LINK_TIMEOUT;\n\treturn nxt;\n}\n\nstatic void __io_queue_sqe(struct io_kiocb *req)\n{\n\tstruct io_kiocb *linked_timeout = io_prep_linked_timeout(req);\n\tint ret;\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\t/*\n\t * We async punt it if the file wasn't marked NOWAIT, or if the file\n\t * doesn't support non-blocking read/write attempts\n\t */\n\tif (likely(!ret)) {\n\t\t/* drop submission reference */\n\t\tif (req->flags & REQ_F_COMPLETE_INLINE) {\n\t\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\t\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\t\t\tcs->reqs[cs->nr++] = req;\n\t\t\tif (cs->nr == ARRAY_SIZE(cs->reqs))\n\t\t\t\tio_submit_flush_completions(cs, ctx);\n\t\t} else {\n\t\t\tio_put_req(req);\n\t\t}\n\t} else if (ret == -EAGAIN && !(req->flags & REQ_F_NOWAIT)) {\n\t\tif (!io_arm_poll_handler(req)) {\n\t\t\t/*\n\t\t\t * Queued up for async execution, worker will release\n\t\t\t * submit reference when the iocb is actually submitted.\n\t\t\t */\n\t\t\tio_queue_async_work(req);\n\t\t}\n\t} else {\n\t\tio_req_complete_failed(req, ret);\n\t}\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}\n\nstatic void io_queue_sqe(struct io_kiocb *req)\n{\n\tint ret;\n\n\tret = io_req_defer(req);\n\tif (ret) {\n\t\tif (ret != -EIOCBQUEUED) {\nfail_req:\n\t\t\tio_req_complete_failed(req, ret);\n\t\t}\n\t} else if (req->flags & REQ_F_FORCE_ASYNC) {\n\t\tret = io_req_prep_async(req);\n\t\tif (unlikely(ret))\n\t\t\tgoto fail_req;\n\t\tio_queue_async_work(req);\n\t} else {\n\t\t__io_queue_sqe(req);\n\t}\n}\n\n/*\n * Check SQE restrictions (opcode and flags).\n *\n * Returns 'true' if SQE is allowed, 'false' otherwise.\n */\nstatic inline bool io_check_restriction(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_kiocb *req,\n\t\t\t\t\tunsigned int sqe_flags)\n{\n\tif (!ctx->restricted)\n\t\treturn true;\n\n\tif (!test_bit(req->opcode, ctx->restrictions.sqe_op))\n\t\treturn false;\n\n\tif ((sqe_flags & ctx->restrictions.sqe_flags_required) !=\n\t    ctx->restrictions.sqe_flags_required)\n\t\treturn false;\n\n\tif (sqe_flags & ~(ctx->restrictions.sqe_flags_allowed |\n\t\t\t  ctx->restrictions.sqe_flags_required))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_state *state;\n\tunsigned int sqe_flags;\n\tint personality, ret = 0;\n\n\treq->opcode = READ_ONCE(sqe->opcode);\n\t/* same numerical values with corresponding REQ_F_*, safe to copy */\n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->user_data = READ_ONCE(sqe->user_data);\n\treq->async_data = NULL;\n\treq->file = NULL;\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->fixed_rsrc_refs = NULL;\n\t/* one is dropped after submission, the other at completion */\n\tatomic_set(&req->refs, 2);\n\treq->task = current;\n\treq->result = 0;\n\treq->work.creds = NULL;\n\n\t/* enforce forwards compatibility on users */\n\tif (unlikely(sqe_flags & ~SQE_VALID_FLAGS)) {\n\t\treq->flags = 0;\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(req->opcode >= IORING_OP_LAST))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!io_check_restriction(ctx, req, sqe_flags)))\n\t\treturn -EACCES;\n\n\tif ((sqe_flags & IOSQE_BUFFER_SELECT) &&\n\t    !io_op_defs[req->opcode].buffer_select)\n\t\treturn -EOPNOTSUPP;\n\n\tpersonality = READ_ONCE(sqe->personality);\n\tif (personality) {\n\t\treq->work.creds = xa_load(&ctx->personalities, personality);\n\t\tif (!req->work.creds)\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->work.creds);\n\t}\n\tstate = &ctx->submit_state;\n\n\t/*\n\t * Plug now if we have more than 1 IO left after this, and the target\n\t * is potentially a read/write to block based storage.\n\t */\n\tif (!state->plug_started && state->ios_left > 1 &&\n\t    io_op_defs[req->opcode].plug) {\n\t\tblk_start_plug(&state->plug);\n\t\tstate->plug_started = true;\n\t}\n\n\tif (io_op_defs[req->opcode].needs_file) {\n\t\tbool fixed = req->flags & REQ_F_FIXED_FILE;\n\n\t\treq->file = io_file_get(state, req, READ_ONCE(sqe->fd), fixed);\n\t\tif (unlikely(!req->file))\n\t\t\tret = -EBADF;\n\t}\n\n\tstate->ios_left--;\n\treturn ret;\n}\n\nstatic int io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t\t const struct io_uring_sqe *sqe)\n{\n\tstruct io_submit_link *link = &ctx->submit_state.link;\n\tint ret;\n\n\tret = io_init_req(ctx, req, sqe);\n\tif (unlikely(ret)) {\nfail_req:\n\t\tif (link->head) {\n\t\t\t/* fail even hard links since we don't submit */\n\t\t\tlink->head->flags |= REQ_F_FAIL_LINK;\n\t\t\tio_req_complete_failed(link->head, -ECANCELED);\n\t\t\tlink->head = NULL;\n\t\t}\n\t\tio_req_complete_failed(req, ret);\n\t\treturn ret;\n\t}\n\tret = io_req_prep(req, sqe);\n\tif (unlikely(ret))\n\t\tgoto fail_req;\n\n\t/* don't need @sqe from now on */\n\ttrace_io_uring_submit_sqe(ctx, req->opcode, req->user_data,\n\t\t\t\ttrue, ctx->flags & IORING_SETUP_SQPOLL);\n\n\t/*\n\t * If we already have a head request, queue this one for async\n\t * submittal once the head completes. If we don't have a head but\n\t * IOSQE_IO_LINK is set in the sqe, start a new head. This one will be\n\t * submitted sync once the chain is complete. If none of those\n\t * conditions are true (normal request), then just queue it.\n\t */\n\tif (link->head) {\n\t\tstruct io_kiocb *head = link->head;\n\n\t\t/*\n\t\t * Taking sequential execution of a link, draining both sides\n\t\t * of the link also fullfils IOSQE_IO_DRAIN semantics for all\n\t\t * requests in the link. So, it drains the head and the\n\t\t * next after the link request. The last one is done via\n\t\t * drain_next flag to persist the effect across calls.\n\t\t */\n\t\tif (req->flags & REQ_F_IO_DRAIN) {\n\t\t\thead->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 1;\n\t\t}\n\t\tret = io_req_prep_async(req);\n\t\tif (unlikely(ret))\n\t\t\tgoto fail_req;\n\t\ttrace_io_uring_link(ctx, req, head);\n\t\tlink->last->link = req;\n\t\tlink->last = req;\n\n\t\t/* last request of a link, enqueue the link */\n\t\tif (!(req->flags & (REQ_F_LINK | REQ_F_HARDLINK))) {\n\t\t\tio_queue_sqe(head);\n\t\t\tlink->head = NULL;\n\t\t}\n\t} else {\n\t\tif (unlikely(ctx->drain_next)) {\n\t\t\treq->flags |= REQ_F_IO_DRAIN;\n\t\t\tctx->drain_next = 0;\n\t\t}\n\t\tif (req->flags & (REQ_F_LINK | REQ_F_HARDLINK)) {\n\t\t\tlink->head = req;\n\t\t\tlink->last = req;\n\t\t} else {\n\t\t\tio_queue_sqe(req);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * Batched submission is done, ensure local IO is flushed out.\n */\nstatic void io_submit_state_end(struct io_submit_state *state,\n\t\t\t\tstruct io_ring_ctx *ctx)\n{\n\tif (state->link.head)\n\t\tio_queue_sqe(state->link.head);\n\tif (state->comp.nr)\n\t\tio_submit_flush_completions(&state->comp, ctx);\n\tif (state->plug_started)\n\t\tblk_finish_plug(&state->plug);\n\tio_state_file_put(state);\n}\n\n/*\n * Start submission side cache.\n */\nstatic void io_submit_state_start(struct io_submit_state *state,\n\t\t\t\t  unsigned int max_ios)\n{\n\tstate->plug_started = false;\n\tstate->ios_left = max_ios;\n\t/* set only head, no need to init link_last in advance */\n\tstate->link.head = NULL;\n}\n\nstatic void io_commit_sqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t/*\n\t * Ensure any loads from the SQEs are done at this point,\n\t * since once we write the new head, the application could\n\t * write new data to them.\n\t */\n\tsmp_store_release(&rings->sq.head, ctx->cached_sq_head);\n}\n\n/*\n * Fetch an sqe, if one is available. Note that sqe_ptr will point to memory\n * that is mapped by userspace. This means that care needs to be taken to\n * ensure that reads are stable, as we cannot rely on userspace always\n * being a good citizen. If members of the sqe are validated and then later\n * used, it's important that those reads are done through READ_ONCE() to\n * prevent a re-load down the line.\n */\nstatic const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)\n{\n\tu32 *sq_array = ctx->sq_array;\n\tunsigned head;\n\n\t/*\n\t * The cached sq head (or cq tail) serves two purposes:\n\t *\n\t * 1) allows us to batch the cost of updating the user visible\n\t *    head updates.\n\t * 2) allows the kernel side to track the head on its own, even\n\t *    though the application is the one updating it.\n\t */\n\thead = READ_ONCE(sq_array[ctx->cached_sq_head++ & ctx->sq_mask]);\n\tif (likely(head < ctx->sq_entries))\n\t\treturn &ctx->sq_sqes[head];\n\n\t/* drop invalid entries */\n\tctx->cached_sq_dropped++;\n\tWRITE_ONCE(ctx->rings->sq_dropped, ctx->cached_sq_dropped);\n\treturn NULL;\n}\n\nstatic int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr)\n{\n\tint submitted = 0;\n\n\t/* if we have a backlog and couldn't flush it all, return BUSY */\n\tif (test_bit(0, &ctx->sq_check_overflow)) {\n\t\tif (!__io_cqring_overflow_flush(ctx, false))\n\t\t\treturn -EBUSY;\n\t}\n\n\t/* make sure SQ entry isn't read before tail */\n\tnr = min3(nr, ctx->sq_entries, io_sqring_entries(ctx));\n\n\tif (!percpu_ref_tryget_many(&ctx->refs, nr))\n\t\treturn -EAGAIN;\n\n\tpercpu_counter_add(&current->io_uring->inflight, nr);\n\trefcount_add(nr, &current->usage);\n\tio_submit_state_start(&ctx->submit_state, nr);\n\n\twhile (submitted < nr) {\n\t\tconst struct io_uring_sqe *sqe;\n\t\tstruct io_kiocb *req;\n\n\t\treq = io_alloc_req(ctx);\n\t\tif (unlikely(!req)) {\n\t\t\tif (!submitted)\n\t\t\t\tsubmitted = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tsqe = io_get_sqe(ctx);\n\t\tif (unlikely(!sqe)) {\n\t\t\tkmem_cache_free(req_cachep, req);\n\t\t\tbreak;\n\t\t}\n\t\t/* will complete beyond this point, count as submitted */\n\t\tsubmitted++;\n\t\tif (io_submit_sqe(ctx, req, sqe))\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(submitted != nr)) {\n\t\tint ref_used = (submitted == -EAGAIN) ? 0 : submitted;\n\t\tstruct io_uring_task *tctx = current->io_uring;\n\t\tint unused = nr - ref_used;\n\n\t\tpercpu_ref_put_many(&ctx->refs, unused);\n\t\tpercpu_counter_sub(&tctx->inflight, unused);\n\t\tput_task_struct_many(current, unused);\n\t}\n\n\tio_submit_state_end(&ctx->submit_state, ctx);\n\t /* Commit SQ ring head once we've consumed and submitted all SQEs */\n\tio_commit_sqring(ctx);\n\n\treturn submitted;\n}\n\nstatic inline void io_ring_set_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\t/* Tell userspace we may need a wakeup call */\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags |= IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic inline void io_ring_clear_wakeup_flag(struct io_ring_ctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tctx->rings->sq_flags &= ~IORING_SQ_NEED_WAKEUP;\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > 8)\n\t\tto_submit = 8;\n\n\tif (!list_empty(&ctx->iopoll_list) || to_submit) {\n\t\tunsigned nr_events = 0;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, &nr_events, 0);\n\n\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)) &&\n\t\t    !(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (!io_sqring_full(ctx) && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\twake_up(&ctx->sqo_sq_wait);\n\n\treturn ret;\n}\n\nstatic void io_sqd_update_thread_idle(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\tunsigned sq_thread_idle = 0;\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\tsq_thread_idle = max(sq_thread_idle, ctx->sq_thread_idle);\n\tsqd->sq_thread_idle = sq_thread_idle;\n}\n\nstatic int io_sq_thread(void *data)\n{\n\tstruct io_sq_data *sqd = data;\n\tstruct io_ring_ctx *ctx;\n\tunsigned long timeout = 0;\n\tchar buf[TASK_COMM_LEN];\n\tDEFINE_WAIT(wait);\n\n\tsprintf(buf, \"iou-sqp-%d\", sqd->task_pid);\n\tset_task_comm(current, buf);\n\tcurrent->pf_io_worker = NULL;\n\n\tif (sqd->sq_cpu != -1)\n\t\tset_cpus_allowed_ptr(current, cpumask_of(sqd->sq_cpu));\n\telse\n\t\tset_cpus_allowed_ptr(current, cpu_online_mask);\n\tcurrent->flags |= PF_NO_SETAFFINITY;\n\n\tmutex_lock(&sqd->lock);\n\twhile (!test_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state)) {\n\t\tint ret;\n\t\tbool cap_entries, sqt_spin, needs_sched;\n\n\t\tif (test_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state) ||\n\t\t    signal_pending(current)) {\n\t\t\tbool did_sig = false;\n\n\t\t\tmutex_unlock(&sqd->lock);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tstruct ksignal ksig;\n\n\t\t\t\tdid_sig = get_signal(&ksig);\n\t\t\t}\n\t\t\tmutex_lock(&sqd->lock);\n\t\t\tif (did_sig)\n\t\t\t\tbreak;\n\t\t\tio_run_task_work();\n\t\t\tio_run_task_work_head(&sqd->park_task_work);\n\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t\tcontinue;\n\t\t}\n\t\tsqt_spin = false;\n\t\tcap_entries = !list_is_singular(&sqd->ctx_list);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tconst struct cred *creds = NULL;\n\n\t\t\tif (ctx->sq_creds != current_cred())\n\t\t\t\tcreds = override_creds(ctx->sq_creds);\n\t\t\tret = __io_sq_thread(ctx, cap_entries);\n\t\t\tif (creds)\n\t\t\t\trevert_creds(creds);\n\t\t\tif (!sqt_spin && (ret > 0 || !list_empty(&ctx->iopoll_list)))\n\t\t\t\tsqt_spin = true;\n\t\t}\n\n\t\tif (sqt_spin || !time_after(jiffies, timeout)) {\n\t\t\tio_run_task_work();\n\t\t\tcond_resched();\n\t\t\tif (sqt_spin)\n\t\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t\tcontinue;\n\t\t}\n\n\t\tneeds_sched = true;\n\t\tprepare_to_wait(&sqd->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t    !list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (io_sqring_entries(ctx)) {\n\t\t\t\tneeds_sched = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (needs_sched && !test_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state)) {\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_set_wakeup_flag(ctx);\n\n\t\t\tmutex_unlock(&sqd->lock);\n\t\t\tschedule();\n\t\t\tmutex_lock(&sqd->lock);\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tio_ring_clear_wakeup_flag(ctx);\n\t\t}\n\n\t\tfinish_wait(&sqd->wait, &wait);\n\t\tio_run_task_work_head(&sqd->park_task_work);\n\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t}\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\tio_uring_cancel_sqpoll(ctx);\n\tsqd->thread = NULL;\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\tio_ring_set_wakeup_flag(ctx);\n\tmutex_unlock(&sqd->lock);\n\n\tio_run_task_work();\n\tio_run_task_work_head(&sqd->park_task_work);\n\tcomplete(&sqd->exited);\n\tdo_exit(0);\n}\n\nstruct io_wait_queue {\n\tstruct wait_queue_entry wq;\n\tstruct io_ring_ctx *ctx;\n\tunsigned to_wait;\n\tunsigned nr_timeouts;\n};\n\nstatic inline bool io_should_wake(struct io_wait_queue *iowq)\n{\n\tstruct io_ring_ctx *ctx = iowq->ctx;\n\n\t/*\n\t * Wake up if we have enough events, or if a timeout occurred since we\n\t * started waiting. For timeouts, we always want to return to userspace,\n\t * regardless of event count.\n\t */\n\treturn io_cqring_events(ctx) >= iowq->to_wait ||\n\t\t\tatomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;\n}\n\nstatic int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,\n\t\t\t    int wake_flags, void *key)\n{\n\tstruct io_wait_queue *iowq = container_of(curr, struct io_wait_queue,\n\t\t\t\t\t\t\twq);\n\n\t/*\n\t * Cannot safely flush overflowed CQEs from here, ensure we wake up\n\t * the task, and the next invocation will do it.\n\t */\n\tif (io_should_wake(iowq) || test_bit(0, &iowq->ctx->cq_check_overflow))\n\t\treturn autoremove_wake_function(curr, mode, wake_flags, key);\n\treturn -1;\n}\n\nstatic int io_run_task_work_sig(void)\n{\n\tif (io_run_task_work())\n\t\treturn 1;\n\tif (!signal_pending(current))\n\t\treturn 0;\n\tif (test_thread_flag(TIF_NOTIFY_SIGNAL))\n\t\treturn -ERESTARTSYS;\n\treturn -EINTR;\n}\n\n/* when returns >0, the caller should retry */\nstatic inline int io_cqring_wait_schedule(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct io_wait_queue *iowq,\n\t\t\t\t\t  signed long *timeout)\n{\n\tint ret;\n\n\t/* make sure we run task_work before checking for signals */\n\tret = io_run_task_work_sig();\n\tif (ret || io_should_wake(iowq))\n\t\treturn ret;\n\t/* let the caller flush overflows, retry */\n\tif (test_bit(0, &ctx->cq_check_overflow))\n\t\treturn 1;\n\n\t*timeout = schedule_timeout(*timeout);\n\treturn !*timeout ? -ETIME : 1;\n}\n\n/*\n * Wait until events become available, if we don't already have some. The\n * application must reap them itself, as they reside on the shared cq ring.\n */\nstatic int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,\n\t\t\t  const sigset_t __user *sig, size_t sigsz,\n\t\t\t  struct __kernel_timespec __user *uts)\n{\n\tstruct io_wait_queue iowq = {\n\t\t.wq = {\n\t\t\t.private\t= current,\n\t\t\t.func\t\t= io_wake_function,\n\t\t\t.entry\t\t= LIST_HEAD_INIT(iowq.wq.entry),\n\t\t},\n\t\t.ctx\t\t= ctx,\n\t\t.to_wait\t= min_events,\n\t};\n\tstruct io_rings *rings = ctx->rings;\n\tsigned long timeout = MAX_SCHEDULE_TIMEOUT;\n\tint ret;\n\n\tdo {\n\t\tio_cqring_overflow_flush(ctx, false);\n\t\tif (io_cqring_events(ctx) >= min_events)\n\t\t\treturn 0;\n\t\tif (!io_run_task_work())\n\t\t\tbreak;\n\t} while (1);\n\n\tif (sig) {\n#ifdef CONFIG_COMPAT\n\t\tif (in_compat_syscall())\n\t\t\tret = set_compat_user_sigmask((const compat_sigset_t __user *)sig,\n\t\t\t\t\t\t      sigsz);\n\t\telse\n#endif\n\t\t\tret = set_user_sigmask(sig, sigsz);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (uts) {\n\t\tstruct timespec64 ts;\n\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t\ttimeout = timespec64_to_jiffies(&ts);\n\t}\n\n\tiowq.nr_timeouts = atomic_read(&ctx->cq_timeouts);\n\ttrace_io_uring_cqring_wait(ctx, min_events);\n\tdo {\n\t\t/* if we can't even flush overflow, don't wait for more */\n\t\tif (!io_cqring_overflow_flush(ctx, false)) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t\tprepare_to_wait_exclusive(&ctx->wait, &iowq.wq,\n\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\tret = io_cqring_wait_schedule(ctx, &iowq, &timeout);\n\t\tfinish_wait(&ctx->wait, &iowq.wq);\n\t\tcond_resched();\n\t} while (ret > 0);\n\n\trestore_saved_sigmask_unless(ret == -EINTR);\n\n\treturn READ_ONCE(rings->cq.head) == READ_ONCE(rings->cq.tail) ? ret : 0;\n}\n\nstatic void __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#else\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file;\n\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n#endif\n}\n\nstatic void io_rsrc_data_ref_zero(struct percpu_ref *ref)\n{\n\tstruct fixed_rsrc_data *data;\n\n\tdata = container_of(ref, struct fixed_rsrc_data, refs);\n\tcomplete(&data->done);\n}\n\nstatic inline void io_rsrc_ref_lock(struct io_ring_ctx *ctx)\n{\n\tspin_lock_bh(&ctx->rsrc_ref_lock);\n}\n\nstatic inline void io_rsrc_ref_unlock(struct io_ring_ctx *ctx)\n{\n\tspin_unlock_bh(&ctx->rsrc_ref_lock);\n}\n\nstatic void io_sqe_rsrc_set_node(struct io_ring_ctx *ctx,\n\t\t\t\t struct fixed_rsrc_data *rsrc_data,\n\t\t\t\t struct fixed_rsrc_ref_node *ref_node)\n{\n\tio_rsrc_ref_lock(ctx);\n\trsrc_data->node = ref_node;\n\tlist_add_tail(&ref_node->node, &ctx->rsrc_ref_list);\n\tio_rsrc_ref_unlock(ctx);\n\tpercpu_ref_get(&rsrc_data->refs);\n}\n\nstatic void io_sqe_rsrc_kill_node(struct io_ring_ctx *ctx, struct fixed_rsrc_data *data)\n{\n\tstruct fixed_rsrc_ref_node *ref_node = NULL;\n\n\tio_rsrc_ref_lock(ctx);\n\tref_node = data->node;\n\tdata->node = NULL;\n\tio_rsrc_ref_unlock(ctx);\n\tif (ref_node)\n\t\tpercpu_ref_kill(&ref_node->refs);\n}\n\nstatic int io_rsrc_refnode_prealloc(struct io_ring_ctx *ctx)\n{\n\tif (ctx->rsrc_backup_node)\n\t\treturn 0;\n\tctx->rsrc_backup_node = alloc_fixed_rsrc_ref_node(ctx);\n\treturn ctx->rsrc_backup_node ? 0 : -ENOMEM;\n}\n\nstatic struct fixed_rsrc_ref_node *\nio_rsrc_refnode_get(struct io_ring_ctx *ctx,\n\t\t    struct fixed_rsrc_data *rsrc_data,\n\t\t    void (*rsrc_put)(struct io_ring_ctx *ctx,\n\t\t                     struct io_rsrc_put *prsrc))\n{\n\tstruct fixed_rsrc_ref_node *node = ctx->rsrc_backup_node;\n\n\tWARN_ON_ONCE(!node);\n\n\tctx->rsrc_backup_node = NULL;\n\tnode->rsrc_data = rsrc_data;\n\tnode->rsrc_put = rsrc_put;\n\treturn node;\n}\n\nstatic int io_rsrc_ref_quiesce(struct fixed_rsrc_data *data,\n\t\t\t       struct io_ring_ctx *ctx,\n\t\t\t       void (*rsrc_put)(struct io_ring_ctx *ctx,\n\t\t\t                        struct io_rsrc_put *prsrc))\n{\n\tstruct fixed_rsrc_ref_node *node;\n\tint ret;\n\n\tif (data->quiesce)\n\t\treturn -ENXIO;\n\n\tdata->quiesce = true;\n\tdo {\n\t\tret = io_rsrc_refnode_prealloc(ctx);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tio_sqe_rsrc_kill_node(ctx, data);\n\t\tpercpu_ref_kill(&data->refs);\n\t\tflush_delayed_work(&ctx->rsrc_put_work);\n\n\t\tret = wait_for_completion_interruptible(&data->done);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\tpercpu_ref_resurrect(&data->refs);\n\t\tnode = io_rsrc_refnode_get(ctx, data, rsrc_put);\n\t\tio_sqe_rsrc_set_node(ctx, data, node);\n\t\treinit_completion(&data->done);\n\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tret = io_run_task_work_sig();\n\t\tmutex_lock(&ctx->uring_lock);\n\t} while (ret >= 0);\n\tdata->quiesce = false;\n\n\treturn ret;\n}\n\nstatic struct fixed_rsrc_data *alloc_fixed_rsrc_data(struct io_ring_ctx *ctx)\n{\n\tstruct fixed_rsrc_data *data;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn NULL;\n\n\tif (percpu_ref_init(&data->refs, io_rsrc_data_ref_zero,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL)) {\n\t\tkfree(data);\n\t\treturn NULL;\n\t}\n\tdata->ctx = ctx;\n\tinit_completion(&data->done);\n\treturn data;\n}\n\nstatic void free_fixed_rsrc_data(struct fixed_rsrc_data *data)\n{\n\tpercpu_ref_exit(&data->refs);\n\tkfree(data->table);\n\tkfree(data);\n}\n\nstatic int io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tstruct fixed_rsrc_data *data = ctx->file_data;\n\tunsigned nr_tables, i;\n\tint ret;\n\n\t/*\n\t * percpu_ref_is_dying() is to stop parallel files unregister\n\t * Since we possibly drop uring lock later in this function to\n\t * run task work.\n\t */\n\tif (!data || percpu_ref_is_dying(&data->refs))\n\t\treturn -ENXIO;\n\tret = io_rsrc_ref_quiesce(data, ctx, io_ring_file_put);\n\tif (ret)\n\t\treturn ret;\n\n\t__io_sqe_files_unregister(ctx);\n\tnr_tables = DIV_ROUND_UP(ctx->nr_user_files, IORING_MAX_FILES_TABLE);\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(data->table[i].files);\n\tfree_fixed_rsrc_data(data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n\treturn 0;\n}\n\nstatic void io_sq_thread_unpark(struct io_sq_data *sqd)\n\t__releases(&sqd->lock)\n{\n\tWARN_ON_ONCE(sqd->thread == current);\n\n\t/*\n\t * Do the dance but not conditional clear_bit() because it'd race with\n\t * other threads incrementing park_pending and setting the bit.\n\t */\n\tclear_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\tif (atomic_dec_return(&sqd->park_pending))\n\t\tset_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\tmutex_unlock(&sqd->lock);\n}\n\nstatic void io_sq_thread_park(struct io_sq_data *sqd)\n\t__acquires(&sqd->lock)\n{\n\tWARN_ON_ONCE(sqd->thread == current);\n\n\tatomic_inc(&sqd->park_pending);\n\tset_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\tmutex_lock(&sqd->lock);\n\tif (sqd->thread)\n\t\twake_up_process(sqd->thread);\n}\n\nstatic void io_sq_thread_stop(struct io_sq_data *sqd)\n{\n\tWARN_ON_ONCE(sqd->thread == current);\n\n\tmutex_lock(&sqd->lock);\n\tset_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\tif (sqd->thread)\n\t\twake_up_process(sqd->thread);\n\tmutex_unlock(&sqd->lock);\n\twait_for_completion(&sqd->exited);\n}\n\nstatic void io_put_sq_data(struct io_sq_data *sqd)\n{\n\tif (refcount_dec_and_test(&sqd->refs)) {\n\t\tWARN_ON_ONCE(atomic_read(&sqd->park_pending));\n\n\t\tio_sq_thread_stop(sqd);\n\t\tkfree(sqd);\n\t}\n}\n\nstatic void io_sq_thread_finish(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (sqd) {\n\t\tio_sq_thread_park(sqd);\n\t\tlist_del_init(&ctx->sqd_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tio_put_sq_data(sqd);\n\t\tctx->sq_data = NULL;\n\t\tif (ctx->sq_creds)\n\t\t\tput_cred(ctx->sq_creds);\n\t}\n}\n\nstatic struct io_sq_data *io_attach_sq_data(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx_attach;\n\tstruct io_sq_data *sqd;\n\tstruct fd f;\n\n\tf = fdget(p->wq_fd);\n\tif (!f.file)\n\t\treturn ERR_PTR(-ENXIO);\n\tif (f.file->f_op != &io_uring_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx_attach = f.file->private_data;\n\tsqd = ctx_attach->sq_data;\n\tif (!sqd) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (sqd->task_tgid != current->tgid) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EPERM);\n\t}\n\n\trefcount_inc(&sqd->refs);\n\tfdput(f);\n\treturn sqd;\n}\n\nstatic struct io_sq_data *io_get_sq_data(struct io_uring_params *p,\n\t\t\t\t\t bool *attached)\n{\n\tstruct io_sq_data *sqd;\n\n\t*attached = false;\n\tif (p->flags & IORING_SETUP_ATTACH_WQ) {\n\t\tsqd = io_attach_sq_data(p);\n\t\tif (!IS_ERR(sqd)) {\n\t\t\t*attached = true;\n\t\t\treturn sqd;\n\t\t}\n\t\t/* fall through for EPERM case, setup new sqd/task */\n\t\tif (PTR_ERR(sqd) != -EPERM)\n\t\t\treturn sqd;\n\t}\n\n\tsqd = kzalloc(sizeof(*sqd), GFP_KERNEL);\n\tif (!sqd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tatomic_set(&sqd->park_pending, 0);\n\trefcount_set(&sqd->refs, 1);\n\tINIT_LIST_HEAD(&sqd->ctx_list);\n\tmutex_init(&sqd->lock);\n\tinit_waitqueue_head(&sqd->wait);\n\tinit_completion(&sqd->exited);\n\treturn sqd;\n}\n\n#if defined(CONFIG_UNIX)\n/*\n * Ensure the UNIX gc is aware of our file set, so we are certain that\n * the io_uring can be safely unregistered on process exit, even if we have\n * loops in the file referencing.\n */\nstatic int __io_sqe_files_scm(struct io_ring_ctx *ctx, int nr, int offset)\n{\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\tint i, nr_files;\n\n\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\tif (!fpl)\n\t\treturn -ENOMEM;\n\n\tskb = alloc_skb(0, GFP_KERNEL);\n\tif (!skb) {\n\t\tkfree(fpl);\n\t\treturn -ENOMEM;\n\t}\n\n\tskb->sk = sk;\n\n\tnr_files = 0;\n\tfpl->user = get_uid(current_user());\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct file *file = io_file_from_index(ctx, i + offset);\n\n\t\tif (!file)\n\t\t\tcontinue;\n\t\tfpl->fp[nr_files] = get_file(file);\n\t\tunix_inflight(fpl->user, fpl->fp[nr_files]);\n\t\tnr_files++;\n\t}\n\n\tif (nr_files) {\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = nr_files;\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->destructor = unix_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t\tskb_queue_head(&sk->sk_receive_queue, skb);\n\n\t\tfor (i = 0; i < nr_files; i++)\n\t\t\tfput(fpl->fp[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t\tkfree(fpl);\n\t}\n\n\treturn 0;\n}\n\n/*\n * If UNIX sockets are enabled, fd passing can cause a reference cycle which\n * causes regular reference counting to break down. We rely on the UNIX\n * garbage collection to take care of this problem for us.\n */\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\tunsigned left, total;\n\tint ret = 0;\n\n\ttotal = 0;\n\tleft = ctx->nr_user_files;\n\twhile (left) {\n\t\tunsigned this_files = min_t(unsigned, left, SCM_MAX_FD);\n\n\t\tret = __io_sqe_files_scm(ctx, this_files, total);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tleft -= this_files;\n\t\ttotal += this_files;\n\t}\n\n\tif (!ret)\n\t\treturn 0;\n\n\twhile (total < ctx->nr_user_files) {\n\t\tstruct file *file = io_file_from_index(ctx, total);\n\n\t\tif (file)\n\t\t\tfput(file);\n\t\ttotal++;\n\t}\n\n\treturn ret;\n}\n#else\nstatic int io_sqe_files_scm(struct io_ring_ctx *ctx)\n{\n\treturn 0;\n}\n#endif\n\nstatic int io_sqe_alloc_file_tables(struct fixed_rsrc_data *file_data,\n\t\t\t\t    unsigned nr_tables, unsigned nr_files)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_rsrc_table *table = &file_data->table[i];\n\t\tunsigned this_files;\n\n\t\tthis_files = min(nr_files, IORING_MAX_FILES_TABLE);\n\t\ttable->files = kcalloc(this_files, sizeof(struct file *),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!table->files)\n\t\t\tbreak;\n\t\tnr_files -= this_files;\n\t}\n\n\tif (i == nr_tables)\n\t\treturn 0;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tstruct fixed_rsrc_table *table = &file_data->table[i];\n\t\tkfree(table->files);\n\t}\n\treturn 1;\n}\n\nstatic void io_ring_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc)\n{\n\tstruct file *file = prsrc->file;\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head list, *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tint i;\n\n\t__skb_queue_head_init(&list);\n\n\t/*\n\t * Find the skb that holds this file in its SCM_RIGHTS. When found,\n\t * remove this entry and rearrange the file array.\n\t */\n\tskb = skb_dequeue(head);\n\twhile (skb) {\n\t\tstruct scm_fp_list *fp;\n\n\t\tfp = UNIXCB(skb).fp;\n\t\tfor (i = 0; i < fp->count; i++) {\n\t\t\tint left;\n\n\t\t\tif (fp->fp[i] != file)\n\t\t\t\tcontinue;\n\n\t\t\tunix_notinflight(fp->user, fp->fp[i]);\n\t\t\tleft = fp->count - 1 - i;\n\t\t\tif (left) {\n\t\t\t\tmemmove(&fp->fp[i], &fp->fp[i + 1],\n\t\t\t\t\t\tleft * sizeof(struct file *));\n\t\t\t}\n\t\t\tfp->count--;\n\t\t\tif (!fp->count) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t} else {\n\t\t\t\t__skb_queue_tail(&list, skb);\n\t\t\t}\n\t\t\tfput(file);\n\t\t\tfile = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!file)\n\t\t\tbreak;\n\n\t\t__skb_queue_tail(&list, skb);\n\n\t\tskb = skb_dequeue(head);\n\t}\n\n\tif (skb_peek(&list)) {\n\t\tspin_lock_irq(&head->lock);\n\t\twhile ((skb = __skb_dequeue(&list)) != NULL)\n\t\t\t__skb_queue_tail(head, skb);\n\t\tspin_unlock_irq(&head->lock);\n\t}\n#else\n\tfput(file);\n#endif\n}\n\nstatic void __io_rsrc_put_work(struct fixed_rsrc_ref_node *ref_node)\n{\n\tstruct fixed_rsrc_data *rsrc_data = ref_node->rsrc_data;\n\tstruct io_ring_ctx *ctx = rsrc_data->ctx;\n\tstruct io_rsrc_put *prsrc, *tmp;\n\n\tlist_for_each_entry_safe(prsrc, tmp, &ref_node->rsrc_list, list) {\n\t\tlist_del(&prsrc->list);\n\t\tref_node->rsrc_put(ctx, prsrc);\n\t\tkfree(prsrc);\n\t}\n\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n\tpercpu_ref_put(&rsrc_data->refs);\n}\n\nstatic void io_rsrc_put_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct llist_node *node;\n\n\tctx = container_of(work, struct io_ring_ctx, rsrc_put_work.work);\n\tnode = llist_del_all(&ctx->rsrc_put_llist);\n\n\twhile (node) {\n\t\tstruct fixed_rsrc_ref_node *ref_node;\n\t\tstruct llist_node *next = node->next;\n\n\t\tref_node = llist_entry(node, struct fixed_rsrc_ref_node, llist);\n\t\t__io_rsrc_put_work(ref_node);\n\t\tnode = next;\n\t}\n}\n\nstatic void io_rsrc_node_ref_zero(struct percpu_ref *ref)\n{\n\tstruct fixed_rsrc_ref_node *ref_node;\n\tstruct fixed_rsrc_data *data;\n\tstruct io_ring_ctx *ctx;\n\tbool first_add = false;\n\tint delay = HZ;\n\n\tref_node = container_of(ref, struct fixed_rsrc_ref_node, refs);\n\tdata = ref_node->rsrc_data;\n\tctx = data->ctx;\n\n\tio_rsrc_ref_lock(ctx);\n\tref_node->done = true;\n\n\twhile (!list_empty(&ctx->rsrc_ref_list)) {\n\t\tref_node = list_first_entry(&ctx->rsrc_ref_list,\n\t\t\t\t\tstruct fixed_rsrc_ref_node, node);\n\t\t/* recycle ref nodes in order */\n\t\tif (!ref_node->done)\n\t\t\tbreak;\n\t\tlist_del(&ref_node->node);\n\t\tfirst_add |= llist_add(&ref_node->llist, &ctx->rsrc_put_llist);\n\t}\n\tio_rsrc_ref_unlock(ctx);\n\n\tif (percpu_ref_is_dying(&data->refs))\n\t\tdelay = 0;\n\n\tif (!delay)\n\t\tmod_delayed_work(system_wq, &ctx->rsrc_put_work, 0);\n\telse if (first_add)\n\t\tqueue_delayed_work(system_wq, &ctx->rsrc_put_work, delay);\n}\n\nstatic struct fixed_rsrc_ref_node *alloc_fixed_rsrc_ref_node(\n\t\t\tstruct io_ring_ctx *ctx)\n{\n\tstruct fixed_rsrc_ref_node *ref_node;\n\n\tref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);\n\tif (!ref_node)\n\t\treturn NULL;\n\n\tif (percpu_ref_init(&ref_node->refs, io_rsrc_node_ref_zero,\n\t\t\t    0, GFP_KERNEL)) {\n\t\tkfree(ref_node);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&ref_node->node);\n\tINIT_LIST_HEAD(&ref_node->rsrc_list);\n\tref_node->done = false;\n\treturn ref_node;\n}\n\nstatic void init_fixed_file_ref_node(struct io_ring_ctx *ctx,\n\t\t\t\t     struct fixed_rsrc_ref_node *ref_node)\n{\n\tref_node->rsrc_data = ctx->file_data;\n\tref_node->rsrc_put = io_ring_file_put;\n}\n\nstatic void destroy_fixed_rsrc_ref_node(struct fixed_rsrc_ref_node *ref_node)\n{\n\tpercpu_ref_exit(&ref_node->refs);\n\tkfree(ref_node);\n}\n\n\nstatic int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t unsigned nr_args)\n{\n\t__s32 __user *fds = (__s32 __user *) arg;\n\tunsigned nr_tables, i;\n\tstruct file *file;\n\tint fd, ret = -ENOMEM;\n\tstruct fixed_rsrc_ref_node *ref_node;\n\tstruct fixed_rsrc_data *file_data;\n\n\tif (ctx->file_data)\n\t\treturn -EBUSY;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (nr_args > IORING_MAX_FIXED_FILES)\n\t\treturn -EMFILE;\n\n\tfile_data = alloc_fixed_rsrc_data(ctx);\n\tif (!file_data)\n\t\treturn -ENOMEM;\n\tctx->file_data = file_data;\n\n\tnr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);\n\tfile_data->table = kcalloc(nr_tables, sizeof(*file_data->table),\n\t\t\t\t   GFP_KERNEL);\n\tif (!file_data->table)\n\t\tgoto out_free;\n\n\tif (io_sqe_alloc_file_tables(file_data, nr_tables, nr_args))\n\t\tgoto out_free;\n\n\tfor (i = 0; i < nr_args; i++, ctx->nr_user_files++) {\n\t\tunsigned long file_ptr;\n\n\t\tif (copy_from_user(&fd, &fds[i], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_fput;\n\t\t}\n\t\t/* allow sparse sets */\n\t\tif (fd == -1)\n\t\t\tcontinue;\n\n\t\tfile = fget(fd);\n\t\tret = -EBADF;\n\t\tif (!file)\n\t\t\tgoto out_fput;\n\n\t\t/*\n\t\t * Don't allow io_uring instances to be registered. If UNIX\n\t\t * isn't enabled, then this causes a reference cycle and this\n\t\t * instance can never get freed. If UNIX is enabled we'll\n\t\t * handle it just fine, but there's still no point in allowing\n\t\t * a ring fd as it doesn't support regular read/write anyway.\n\t\t */\n\t\tif (file->f_op == &io_uring_fops) {\n\t\t\tfput(file);\n\t\t\tgoto out_fput;\n\t\t}\n\t\tfile_ptr = (unsigned long) file;\n\t\tif (__io_file_supports_async(file, READ))\n\t\t\tfile_ptr |= FFS_ASYNC_READ;\n\t\tif (__io_file_supports_async(file, WRITE))\n\t\t\tfile_ptr |= FFS_ASYNC_WRITE;\n\t\tif (S_ISREG(file_inode(file)->i_mode))\n\t\t\tfile_ptr |= FFS_ISREG;\n\t\t*io_fixed_file_slot(file_data, i) = (struct file *) file_ptr;\n\t}\n\n\tret = io_sqe_files_scm(ctx);\n\tif (ret) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn ret;\n\t}\n\n\tref_node = alloc_fixed_rsrc_ref_node(ctx);\n\tif (!ref_node) {\n\t\tio_sqe_files_unregister(ctx);\n\t\treturn -ENOMEM;\n\t}\n\tinit_fixed_file_ref_node(ctx, ref_node);\n\n\tio_sqe_rsrc_set_node(ctx, file_data, ref_node);\n\treturn ret;\nout_fput:\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tfile = io_file_from_index(ctx, i);\n\t\tif (file)\n\t\t\tfput(file);\n\t}\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(file_data->table[i].files);\n\tctx->nr_user_files = 0;\nout_free:\n\tfree_fixed_rsrc_data(ctx->file_data);\n\tctx->file_data = NULL;\n\treturn ret;\n}\n\nstatic int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,\n\t\t\t\tint index)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\n\t/*\n\t * See if we can merge this file into an existing skb SCM_RIGHTS\n\t * file set. If there's no room, fall back to allocating a new skb\n\t * and filling it in.\n\t */\n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb) {\n\t\tstruct scm_fp_list *fpl = UNIXCB(skb).fp;\n\n\t\tif (fpl->count < SCM_MAX_FD) {\n\t\t\t__skb_unlink(skb, head);\n\t\t\tspin_unlock_irq(&head->lock);\n\t\t\tfpl->fp[fpl->count] = get_file(file);\n\t\t\tunix_inflight(fpl->user, fpl->fp[fpl->count]);\n\t\t\tfpl->count++;\n\t\t\tspin_lock_irq(&head->lock);\n\t\t\t__skb_queue_head(head, skb);\n\t\t} else {\n\t\t\tskb = NULL;\n\t\t}\n\t}\n\tspin_unlock_irq(&head->lock);\n\n\tif (skb) {\n\t\tfput(file);\n\t\treturn 0;\n\t}\n\n\treturn __io_sqe_files_scm(ctx, 1, index);\n#else\n\treturn 0;\n#endif\n}\n\nstatic int io_queue_rsrc_removal(struct fixed_rsrc_data *data, void *rsrc)\n{\n\tstruct io_rsrc_put *prsrc;\n\tstruct fixed_rsrc_ref_node *ref_node = data->node;\n\n\tprsrc = kzalloc(sizeof(*prsrc), GFP_KERNEL);\n\tif (!prsrc)\n\t\treturn -ENOMEM;\n\n\tprsrc->rsrc = rsrc;\n\tlist_add(&prsrc->list, &ref_node->rsrc_list);\n\n\treturn 0;\n}\n\nstatic inline int io_queue_file_removal(struct fixed_rsrc_data *data,\n\t\t\t\t\tstruct file *file)\n{\n\treturn io_queue_rsrc_removal(data, (void *)file);\n}\n\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_rsrc_update *up,\n\t\t\t\t unsigned nr_args)\n{\n\tstruct fixed_rsrc_data *data = ctx->file_data;\n\tstruct fixed_rsrc_ref_node *ref_node;\n\tstruct file *file, **file_slot;\n\t__s32 __user *fds;\n\tint fd, i, err;\n\t__u32 done;\n\tbool needs_switch = false;\n\n\tif (check_add_overflow(up->offset, nr_args, &done))\n\t\treturn -EOVERFLOW;\n\tif (done > ctx->nr_user_files)\n\t\treturn -EINVAL;\n\terr = io_rsrc_refnode_prealloc(ctx);\n\tif (err)\n\t\treturn err;\n\n\tfds = u64_to_user_ptr(up->data);\n\tfor (done = 0; done < nr_args; done++) {\n\t\terr = 0;\n\t\tif (copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (fd == IORING_REGISTER_FILES_SKIP)\n\t\t\tcontinue;\n\n\t\ti = array_index_nospec(up->offset + done, ctx->nr_user_files);\n\t\tfile_slot = io_fixed_file_slot(ctx->file_data, i);\n\n\t\tif (*file_slot) {\n\t\t\tfile = (struct file *) ((unsigned long) *file_slot & FFS_MASK);\n\t\t\terr = io_queue_file_removal(data, file);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\t*file_slot = NULL;\n\t\t\tneeds_switch = true;\n\t\t}\n\t\tif (fd != -1) {\n\t\t\tfile = fget(fd);\n\t\t\tif (!file) {\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Don't allow io_uring instances to be registered. If\n\t\t\t * UNIX isn't enabled, then this causes a reference\n\t\t\t * cycle and this instance can never get freed. If UNIX\n\t\t\t * is enabled we'll handle it just fine, but there's\n\t\t\t * still no point in allowing a ring fd as it doesn't\n\t\t\t * support regular read/write anyway.\n\t\t\t */\n\t\t\tif (file->f_op == &io_uring_fops) {\n\t\t\t\tfput(file);\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*file_slot = file;\n\t\t\terr = io_sqe_file_register(ctx, file, i);\n\t\t\tif (err) {\n\t\t\t\t*file_slot = NULL;\n\t\t\t\tfput(file);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (needs_switch) {\n\t\tpercpu_ref_kill(&data->node->refs);\n\t\tref_node = io_rsrc_refnode_get(ctx, data, io_ring_file_put);\n\t\tio_sqe_rsrc_set_node(ctx, data, ref_node);\n\t}\n\treturn done ? done : err;\n}\n\nstatic int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t       unsigned nr_args)\n{\n\tstruct io_uring_rsrc_update up;\n\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&up, arg, sizeof(up)))\n\t\treturn -EFAULT;\n\tif (up.resv)\n\t\treturn -EINVAL;\n\n\treturn __io_sqe_files_update(ctx, &up, nr_args);\n}\n\nstatic struct io_wq_work *io_free_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treq = io_put_req_find_next(req);\n\treturn req ? &req->work : NULL;\n}\n\nstatic struct io_wq *io_init_wq_offload(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tstruct io_wq_hash *hash;\n\tstruct io_wq_data data;\n\tunsigned int concurrency;\n\n\thash = ctx->hash_map;\n\tif (!hash) {\n\t\thash = kzalloc(sizeof(*hash), GFP_KERNEL);\n\t\tif (!hash)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\trefcount_set(&hash->refs, 1);\n\t\tinit_waitqueue_head(&hash->wait);\n\t\tctx->hash_map = hash;\n\t}\n\n\tdata.hash = hash;\n\tdata.task = task;\n\tdata.free_work = io_free_work;\n\tdata.do_work = io_wq_submit_work;\n\n\t/* Do QD, or 4 * CPUS, whatever is smallest */\n\tconcurrency = min(ctx->sq_entries, 4 * num_online_cpus());\n\n\treturn io_wq_create(concurrency, &data);\n}\n\nstatic int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kmalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx, task);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\ttctx->last = NULL;\n\tatomic_set(&tctx->in_idle, 0);\n\ttask->io_uring = tctx;\n\tspin_lock_init(&tctx->task_lock);\n\tINIT_WQ_LIST(&tctx->task_list);\n\ttctx->task_state = 0;\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}\n\nvoid __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(tctx->io_wq);\n\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}\n\nstatic int io_sq_offload_create(struct io_ring_ctx *ctx,\n\t\t\t\tstruct io_uring_params *p)\n{\n\tint ret;\n\n\t/* Retain compatibility with failing for an invalid attach attempt */\n\tif ((ctx->flags & (IORING_SETUP_ATTACH_WQ | IORING_SETUP_SQPOLL)) ==\n\t\t\t\tIORING_SETUP_ATTACH_WQ) {\n\t\tstruct fd f;\n\n\t\tf = fdget(p->wq_fd);\n\t\tif (!f.file)\n\t\t\treturn -ENXIO;\n\t\tif (f.file->f_op != &io_uring_fops) {\n\t\t\tfdput(f);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfdput(f);\n\t}\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tstruct task_struct *tsk;\n\t\tstruct io_sq_data *sqd;\n\t\tbool attached;\n\n\t\tsqd = io_get_sq_data(p, &attached);\n\t\tif (IS_ERR(sqd)) {\n\t\t\tret = PTR_ERR(sqd);\n\t\t\tgoto err;\n\t\t}\n\n\t\tctx->sq_creds = get_current_cred();\n\t\tctx->sq_data = sqd;\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tret = 0;\n\t\tio_sq_thread_park(sqd);\n\t\tlist_add(&ctx->sqd_list, &sqd->ctx_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\t/* don't attach to a dying SQPOLL thread, would be racy */\n\t\tif (attached && !sqd->thread)\n\t\t\tret = -ENXIO;\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t\tif (attached)\n\t\t\treturn 0;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids)\n\t\t\t\tgoto err_sqpoll;\n\t\t\tif (!cpu_online(cpu))\n\t\t\t\tgoto err_sqpoll;\n\n\t\t\tsqd->sq_cpu = cpu;\n\t\t} else {\n\t\t\tsqd->sq_cpu = -1;\n\t\t}\n\n\t\tsqd->task_pid = current->pid;\n\t\tsqd->task_tgid = current->tgid;\n\t\ttsk = create_io_thread(io_sq_thread, sqd, NUMA_NO_NODE);\n\t\tif (IS_ERR(tsk)) {\n\t\t\tret = PTR_ERR(tsk);\n\t\t\tgoto err_sqpoll;\n\t\t}\n\n\t\tsqd->thread = tsk;\n\t\tret = io_uring_alloc_task_context(tsk, ctx);\n\t\twake_up_new_task(tsk);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tio_sq_thread_finish(ctx);\n\treturn ret;\nerr_sqpoll:\n\tcomplete(&ctx->sq_data->exited);\n\tgoto err;\n}\n\nstatic inline void __io_unaccount_mem(struct user_struct *user,\n\t\t\t\t      unsigned long nr_pages)\n{\n\tatomic_long_sub(nr_pages, &user->locked_vm);\n}\n\nstatic inline int __io_account_mem(struct user_struct *user,\n\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long page_limit, cur_pages, new_pages;\n\n\t/* Don't allow more pages than we can safely lock */\n\tpage_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tdo {\n\t\tcur_pages = atomic_long_read(&user->locked_vm);\n\t\tnew_pages = cur_pages + nr_pages;\n\t\tif (new_pages > page_limit)\n\t\t\treturn -ENOMEM;\n\t} while (atomic_long_cmpxchg(&user->locked_vm, cur_pages,\n\t\t\t\t\tnew_pages) != cur_pages);\n\n\treturn 0;\n}\n\nstatic void io_unaccount_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)\n{\n\tif (ctx->user)\n\t\t__io_unaccount_mem(ctx->user, nr_pages);\n\n\tif (ctx->mm_account)\n\t\tatomic64_sub(nr_pages, &ctx->mm_account->pinned_vm);\n}\n\nstatic int io_account_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)\n{\n\tint ret;\n\n\tif (ctx->user) {\n\t\tret = __io_account_mem(ctx->user, nr_pages);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ctx->mm_account)\n\t\tatomic64_add(nr_pages, &ctx->mm_account->pinned_vm);\n\n\treturn 0;\n}\n\nstatic void io_mem_free(void *ptr)\n{\n\tstruct page *page;\n\n\tif (!ptr)\n\t\treturn;\n\n\tpage = virt_to_head_page(ptr);\n\tif (put_page_testzero(page))\n\t\tfree_compound_page(page);\n}\n\nstatic void *io_mem_alloc(size_t size)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP |\n\t\t\t\t__GFP_NORETRY | __GFP_ACCOUNT;\n\n\treturn (void *) __get_free_pages(gfp_flags, get_order(size));\n}\n\nstatic unsigned long rings_size(unsigned sq_entries, unsigned cq_entries,\n\t\t\t\tsize_t *sq_offset)\n{\n\tstruct io_rings *rings;\n\tsize_t off, sq_array_size;\n\n\toff = struct_size(rings, cqes, cq_entries);\n\tif (off == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n#ifdef CONFIG_SMP\n\toff = ALIGN(off, SMP_CACHE_BYTES);\n\tif (off == 0)\n\t\treturn SIZE_MAX;\n#endif\n\n\tif (sq_offset)\n\t\t*sq_offset = off;\n\n\tsq_array_size = array_size(sizeof(u32), sq_entries);\n\tif (sq_array_size == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n\tif (check_add_overflow(off, sq_array_size, &off))\n\t\treturn SIZE_MAX;\n\n\treturn off;\n}\n\nstatic int io_sqe_buffers_unregister(struct io_ring_ctx *ctx)\n{\n\tint i, j;\n\n\tif (!ctx->user_bufs)\n\t\treturn -ENXIO;\n\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++)\n\t\t\tunpin_user_page(imu->bvec[j].bv_page);\n\n\t\tif (imu->acct_pages)\n\t\t\tio_unaccount_mem(ctx, imu->acct_pages);\n\t\tkvfree(imu->bvec);\n\t\timu->nr_bvecs = 0;\n\t}\n\n\tkfree(ctx->user_bufs);\n\tctx->user_bufs = NULL;\n\tctx->nr_user_bufs = 0;\n\treturn 0;\n}\n\nstatic int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,\n\t\t       void __user *arg, unsigned index)\n{\n\tstruct iovec __user *src;\n\n#ifdef CONFIG_COMPAT\n\tif (ctx->compat) {\n\t\tstruct compat_iovec __user *ciovs;\n\t\tstruct compat_iovec ciov;\n\n\t\tciovs = (struct compat_iovec __user *) arg;\n\t\tif (copy_from_user(&ciov, &ciovs[index], sizeof(ciov)))\n\t\t\treturn -EFAULT;\n\n\t\tdst->iov_base = u64_to_user_ptr((u64)ciov.iov_base);\n\t\tdst->iov_len = ciov.iov_len;\n\t\treturn 0;\n\t}\n#endif\n\tsrc = (struct iovec __user *) arg;\n\tif (copy_from_user(dst, &src[index], sizeof(*dst)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Not super efficient, but this is just a registration time. And we do cache\n * the last compound head, so generally we'll only do a full search if we don't\n * match that one.\n *\n * We check if the given compound head page has already been accounted, to\n * avoid double accounting it. This allows us to account the full size of the\n * page, not just the constituent pages of a huge page.\n */\nstatic bool headpage_already_acct(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t  int nr_pages, struct page *hpage)\n{\n\tint i, j;\n\n\t/* check current page array */\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i]))\n\t\t\tcontinue;\n\t\tif (compound_head(pages[i]) == hpage)\n\t\t\treturn true;\n\t}\n\n\t/* check previously registered pages */\n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++) {\n\t\t\tif (!PageCompound(imu->bvec[j].bv_page))\n\t\t\t\tcontinue;\n\t\t\tif (compound_head(imu->bvec[j].bv_page) == hpage)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int io_buffer_account_pin(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t int nr_pages, struct io_mapped_ubuf *imu,\n\t\t\t\t struct page **last_hpage)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i])) {\n\t\t\timu->acct_pages++;\n\t\t} else {\n\t\t\tstruct page *hpage;\n\n\t\t\thpage = compound_head(pages[i]);\n\t\t\tif (hpage == *last_hpage)\n\t\t\t\tcontinue;\n\t\t\t*last_hpage = hpage;\n\t\t\tif (headpage_already_acct(ctx, pages, i, hpage))\n\t\t\t\tcontinue;\n\t\t\timu->acct_pages += page_size(hpage) >> PAGE_SHIFT;\n\t\t}\n\t}\n\n\tif (!imu->acct_pages)\n\t\treturn 0;\n\n\tret = io_account_mem(ctx, imu->acct_pages);\n\tif (ret)\n\t\timu->acct_pages = 0;\n\treturn ret;\n}\n\nstatic int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\n\t\t\t\t  struct io_mapped_ubuf *imu,\n\t\t\t\t  struct page **last_hpage)\n{\n\tstruct vm_area_struct **vmas = NULL;\n\tstruct page **pages = NULL;\n\tunsigned long off, start, end, ubuf;\n\tsize_t size;\n\tint ret, pret, nr_pages, i;\n\n\tubuf = (unsigned long) iov->iov_base;\n\tend = (ubuf + iov->iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstart = ubuf >> PAGE_SHIFT;\n\tnr_pages = end - start;\n\n\tret = -ENOMEM;\n\n\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto done;\n\n\tvmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),\n\t\t\t      GFP_KERNEL);\n\tif (!vmas)\n\t\tgoto done;\n\n\timu->bvec = kvmalloc_array(nr_pages, sizeof(struct bio_vec),\n\t\t\t\t   GFP_KERNEL);\n\tif (!imu->bvec)\n\t\tgoto done;\n\n\tret = 0;\n\tmmap_read_lock(current->mm);\n\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t      pages, vmas);\n\tif (pret == nr_pages) {\n\t\t/* don't support file backed memory */\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct vm_area_struct *vma = vmas[i];\n\n\t\t\tif (vma->vm_file &&\n\t\t\t    !is_file_hugepages(vma->vm_file)) {\n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tret = pret < 0 ? pret : -EFAULT;\n\t}\n\tmmap_read_unlock(current->mm);\n\tif (ret) {\n\t\t/*\n\t\t * if we did partial map, or found file backed vmas,\n\t\t * release any pages we did get\n\t\t */\n\t\tif (pret > 0)\n\t\t\tunpin_user_pages(pages, pret);\n\t\tkvfree(imu->bvec);\n\t\tgoto done;\n\t}\n\n\tret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);\n\tif (ret) {\n\t\tunpin_user_pages(pages, pret);\n\t\tkvfree(imu->bvec);\n\t\tgoto done;\n\t}\n\n\toff = ubuf & ~PAGE_MASK;\n\tsize = iov->iov_len;\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t vec_len;\n\n\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\timu->bvec[i].bv_page = pages[i];\n\t\timu->bvec[i].bv_len = vec_len;\n\t\timu->bvec[i].bv_offset = off;\n\t\toff = 0;\n\t\tsize -= vec_len;\n\t}\n\t/* store original address for later verification */\n\timu->ubuf = ubuf;\n\timu->len = iov->iov_len;\n\timu->nr_bvecs = nr_pages;\n\tret = 0;\ndone:\n\tkvfree(pages);\n\tkvfree(vmas);\n\treturn ret;\n}\n\nstatic int io_buffers_map_alloc(struct io_ring_ctx *ctx, unsigned int nr_args)\n{\n\tif (ctx->user_bufs)\n\t\treturn -EBUSY;\n\tif (!nr_args || nr_args > UIO_MAXIOV)\n\t\treturn -EINVAL;\n\n\tctx->user_bufs = kcalloc(nr_args, sizeof(struct io_mapped_ubuf),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!ctx->user_bufs)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int io_buffer_validate(struct iovec *iov)\n{\n\tunsigned long tmp, acct_len = iov->iov_len + (PAGE_SIZE - 1);\n\n\t/*\n\t * Don't impose further limits on the size and buffer\n\t * constraints here, we'll -EINVAL later when IO is\n\t * submitted if they are wrong.\n\t */\n\tif (!iov->iov_base || !iov->iov_len)\n\t\treturn -EFAULT;\n\n\t/* arbitrary limit, but we need something */\n\tif (iov->iov_len > SZ_1G)\n\t\treturn -EFAULT;\n\n\tif (check_add_overflow((unsigned long)iov->iov_base, acct_len, &tmp))\n\t\treturn -EOVERFLOW;\n\n\treturn 0;\n}\n\nstatic int io_sqe_buffers_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t   unsigned int nr_args)\n{\n\tint i, ret;\n\tstruct iovec iov;\n\tstruct page *last_hpage = NULL;\n\n\tret = io_buffers_map_alloc(ctx, nr_args);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tstruct io_mapped_ubuf *imu = &ctx->user_bufs[i];\n\n\t\tret = io_copy_iov(ctx, &iov, arg, i);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = io_buffer_validate(&iov);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = io_sqe_buffer_register(ctx, &iov, imu, &last_hpage);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tctx->nr_user_bufs++;\n\t}\n\n\tif (ret)\n\t\tio_sqe_buffers_unregister(ctx);\n\n\treturn ret;\n}\n\nstatic int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)\n{\n\t__s32 __user *fds = arg;\n\tint fd;\n\n\tif (ctx->cq_ev_fd)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&fd, fds, sizeof(*fds)))\n\t\treturn -EFAULT;\n\n\tctx->cq_ev_fd = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(ctx->cq_ev_fd)) {\n\t\tint ret = PTR_ERR(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_eventfd_unregister(struct io_ring_ctx *ctx)\n{\n\tif (ctx->cq_ev_fd) {\n\t\teventfd_ctx_put(ctx->cq_ev_fd);\n\t\tctx->cq_ev_fd = NULL;\n\t\treturn 0;\n\t}\n\n\treturn -ENXIO;\n}\n\nstatic void io_destroy_buffers(struct io_ring_ctx *ctx)\n{\n\tstruct io_buffer *buf;\n\tunsigned long index;\n\n\txa_for_each(&ctx->io_buffers, index, buf)\n\t\t__io_remove_buffers(ctx, buf, index, -1U);\n}\n\nstatic void io_req_cache_free(struct list_head *list, struct task_struct *tsk)\n{\n\tstruct io_kiocb *req, *nxt;\n\n\tlist_for_each_entry_safe(req, nxt, list, compl.list) {\n\t\tif (tsk && req->task != tsk)\n\t\t\tcontinue;\n\t\tlist_del(&req->compl.list);\n\t\tkmem_cache_free(req_cachep, req);\n\t}\n}\n\nstatic void io_req_caches_free(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *submit_state = &ctx->submit_state;\n\tstruct io_comp_state *cs = &ctx->submit_state.comp;\n\n\tmutex_lock(&ctx->uring_lock);\n\n\tif (submit_state->free_reqs) {\n\t\tkmem_cache_free_bulk(req_cachep, submit_state->free_reqs,\n\t\t\t\t     submit_state->reqs);\n\t\tsubmit_state->free_reqs = 0;\n\t}\n\n\tio_flush_cached_locked_reqs(ctx, cs);\n\tio_req_cache_free(&cs->free_list, NULL);\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\t/*\n\t * Some may use context even when all refs and requests have been put,\n\t * and they are free to do so while still holding uring_lock or\n\t * completion_lock, see __io_req_task_submit(). Wait for them to finish.\n\t */\n\tmutex_lock(&ctx->uring_lock);\n\tmutex_unlock(&ctx->uring_lock);\n\tspin_lock_irq(&ctx->completion_lock);\n\tspin_unlock_irq(&ctx->completion_lock);\n\n\tio_sq_thread_finish(ctx);\n\tio_sqe_buffers_unregister(ctx);\n\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\n\tmutex_lock(&ctx->uring_lock);\n\tio_sqe_files_unregister(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tio_eventfd_unregister(ctx);\n\tio_destroy_buffers(ctx);\n\n\tif (ctx->rsrc_backup_node)\n\t\tdestroy_fixed_rsrc_ref_node(ctx->rsrc_backup_node);\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL; /* so that iput() is called */\n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\n\tio_mem_free(ctx->rings);\n\tio_mem_free(ctx->sq_sqes);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_hash);\n\tkfree(ctx);\n}\n\nstatic __poll_t io_uring_poll(struct file *file, poll_table *wait)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\t__poll_t mask = 0;\n\n\tpoll_wait(file, &ctx->cq_wait, wait);\n\t/*\n\t * synchronizes with barrier from wq_has_sleeper call in\n\t * io_commit_cqring\n\t */\n\tsmp_rmb();\n\tif (!io_sqring_full(ctx))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t/*\n\t * Don't flush cqring overflow list here, just do a simple check.\n\t * Otherwise there could possible be ABBA deadlock:\n\t *      CPU0                    CPU1\n\t *      ----                    ----\n\t * lock(&ctx->uring_lock);\n\t *                              lock(&ep->mtx);\n\t *                              lock(&ctx->uring_lock);\n\t * lock(&ep->mtx);\n\t *\n\t * Users may get EPOLLIN meanwhile seeing nothing in cqring, this\n\t * pushs them to do the flush.\n\t */\n\tif (io_cqring_events(ctx) || test_bit(0, &ctx->cq_check_overflow))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\treturn mask;\n}\n\nstatic int io_uring_fasync(int fd, struct file *file, int on)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\treturn fasync_helper(fd, file, on, &ctx->cq_fasync);\n}\n\nstatic int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tconst struct cred *creds;\n\n\tcreds = xa_erase(&ctx->personalities, id);\n\tif (creds) {\n\t\tput_cred(creds);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic inline bool io_run_ctx_fallback(struct io_ring_ctx *ctx)\n{\n\treturn io_run_task_work_head(&ctx->exit_task_work);\n}\n\nstruct io_tctx_exit {\n\tstruct callback_head\t\ttask_work;\n\tstruct completion\t\tcompletion;\n\tstruct io_ring_ctx\t\t*ctx;\n};\n\nstatic void io_tctx_exit_cb(struct callback_head *cb)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_exit *work;\n\n\twork = container_of(cb, struct io_tctx_exit, task_work);\n\t/*\n\t * When @in_idle, we're in cancellation and it's racy to remove the\n\t * node. It'll be removed by the end of cancellation, just ignore it.\n\t */\n\tif (!atomic_read(&tctx->in_idle))\n\t\tio_uring_del_task_file((unsigned long)work->ctx);\n\tcomplete(&work->completion);\n}\n\nstatic void io_ring_exit_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx, exit_work);\n\tunsigned long timeout = jiffies + HZ * 60 * 5;\n\tstruct io_tctx_exit exit;\n\tstruct io_tctx_node *node;\n\tint ret;\n\n\t/* prevent SQPOLL from submitting new requests */\n\tif (ctx->sq_data) {\n\t\tio_sq_thread_park(ctx->sq_data);\n\t\tlist_del_init(&ctx->sqd_list);\n\t\tio_sqd_update_thread_idle(ctx->sq_data);\n\t\tio_sq_thread_unpark(ctx->sq_data);\n\t}\n\n\t/*\n\t * If we're doing polled IO and end up having requests being\n\t * submitted async (out-of-line), then completions can come in while\n\t * we're waiting for refs to drop. We need to reap these manually,\n\t * as nobody else will be looking for them.\n\t */\n\tdo {\n\t\tio_uring_try_cancel_requests(ctx, NULL, NULL);\n\n\t\tWARN_ON_ONCE(time_after(jiffies, timeout));\n\t} while (!wait_for_completion_timeout(&ctx->ref_comp, HZ/20));\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!list_empty(&ctx->tctx_list)) {\n\t\tWARN_ON_ONCE(time_after(jiffies, timeout));\n\n\t\tnode = list_first_entry(&ctx->tctx_list, struct io_tctx_node,\n\t\t\t\t\tctx_node);\n\t\texit.ctx = ctx;\n\t\tinit_completion(&exit.completion);\n\t\tinit_task_work(&exit.task_work, io_tctx_exit_cb);\n\t\tret = task_work_add(node->task, &exit.task_work, TWA_SIGNAL);\n\t\tif (WARN_ON_ONCE(ret))\n\t\t\tcontinue;\n\t\twake_up_process(node->task);\n\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\twait_for_completion(&exit.completion);\n\t\tcond_resched();\n\t\tmutex_lock(&ctx->uring_lock);\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_ring_ctx_free(ctx);\n}\n\n/* Returns true if we found and killed one or more timeouts */\nstatic bool io_kill_timeouts(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t     struct files_struct *files)\n{\n\tstruct io_kiocb *req, *tmp;\n\tint canceled = 0;\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_safe(req, tmp, &ctx->timeout_list, timeout.list) {\n\t\tif (io_match_task(req, tsk, files)) {\n\t\t\tio_kill_timeout(req, -ECANCELED);\n\t\t\tcanceled++;\n\t\t}\n\t}\n\tif (canceled != 0)\n\t\tio_commit_cqring(ctx);\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (canceled != 0)\n\t\tio_cqring_ev_posted(ctx);\n\treturn canceled != 0;\n}\n\nstatic void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tunsigned long index;\n\tstruct creds *creds;\n\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\t/* if force is set, the ring is going away. always drop after that */\n\tctx->cq_overflow_flushed = 1;\n\tif (ctx->rings)\n\t\t__io_cqring_overflow_flush(ctx, true);\n\txa_for_each(&ctx->personalities, index, creds)\n\t\tio_unregister_personality(ctx, index);\n\tmutex_unlock(&ctx->uring_lock);\n\n\tio_kill_timeouts(ctx, NULL, NULL);\n\tio_poll_remove_all(ctx, NULL, NULL);\n\n\t/* if we failed setting up the ctx, we might not have any rings */\n\tio_iopoll_try_reap_events(ctx);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t/*\n\t * Use system_unbound_wq to avoid spawning tons of event kworkers\n\t * if we're exiting a ton of rings at the same time. It just adds\n\t * noise and overhead, there's no discernable change in runtime\n\t * over using system_wq.\n\t */\n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}\n\nstatic int io_uring_release(struct inode *inode, struct file *file)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\tfile->private_data = NULL;\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn 0;\n}\n\nstruct io_task_cancel {\n\tstruct task_struct *task;\n\tstruct files_struct *files;\n};\n\nstatic bool io_cancel_task_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_task_cancel *cancel = data;\n\tbool ret;\n\n\tif (cancel->files && (req->flags & REQ_F_LINK_TIMEOUT)) {\n\t\tunsigned long flags;\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\t/* protect against races with linked timeouts */\n\t\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\t} else {\n\t\tret = io_match_task(req, cancel->task, cancel->files);\n\t}\n\treturn ret;\n}\n\nstatic bool io_cancel_defer_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\tstruct io_defer_entry *de;\n\tLIST_HEAD(list);\n\n\tspin_lock_irq(&ctx->completion_lock);\n\tlist_for_each_entry_reverse(de, &ctx->defer_list, list) {\n\t\tif (io_match_task(de->req, task, files)) {\n\t\t\tlist_cut_position(&list, &ctx->defer_list, &de->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (list_empty(&list))\n\t\treturn false;\n\n\twhile (!list_empty(&list)) {\n\t\tde = list_first_entry(&list, struct io_defer_entry, list);\n\t\tlist_del_init(&de->list);\n\t\tio_req_complete_failed(de->req, -ECANCELED);\n\t\tkfree(de);\n\t}\n\treturn true;\n}\n\nstatic bool io_cancel_ctx_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treturn req->ctx == data;\n}\n\nstatic bool io_uring_try_cancel_iowq(struct io_ring_ctx *ctx)\n{\n\tstruct io_tctx_node *node;\n\tenum io_wq_cancel cret;\n\tbool ret = false;\n\n\tmutex_lock(&ctx->uring_lock);\n\tlist_for_each_entry(node, &ctx->tctx_list, ctx_node) {\n\t\tstruct io_uring_task *tctx = node->task->io_uring;\n\n\t\t/*\n\t\t * io_wq will stay alive while we hold uring_lock, because it's\n\t\t * killed after ctx nodes, which requires to take the lock.\n\t\t */\n\t\tif (!tctx || !tctx->io_wq)\n\t\t\tcontinue;\n\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_ctx_cb, ctx, true);\n\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n\n\treturn ret;\n}\n\nstatic void io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct task_struct *task,\n\t\t\t\t\t struct files_struct *files)\n{\n\tstruct io_task_cancel cancel = { .task = task, .files = files, };\n\tstruct io_uring_task *tctx = task ? task->io_uring : NULL;\n\n\twhile (1) {\n\t\tenum io_wq_cancel cret;\n\t\tbool ret = false;\n\n\t\tif (!task) {\n\t\t\tret |= io_uring_try_cancel_iowq(ctx);\n\t\t} else if (tctx && tctx->io_wq) {\n\t\t\t/*\n\t\t\t * Cancels requests of all rings, not only @ctx, but\n\t\t\t * it's fine as the task is in exit/exec.\n\t\t\t */\n\t\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t\t       &cancel, true);\n\t\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t\t}\n\n\t\t/* SQPOLL thread does its own polling */\n\t\tif ((!(ctx->flags & IORING_SETUP_SQPOLL) && !files) ||\n\t\t    (ctx->sq_data && ctx->sq_data->thread == current)) {\n\t\t\twhile (!list_empty_careful(&ctx->iopoll_list)) {\n\t\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\t\tret = true;\n\t\t\t}\n\t\t}\n\n\t\tret |= io_cancel_defer_files(ctx, task, files);\n\t\tret |= io_poll_remove_all(ctx, task, files);\n\t\tret |= io_kill_timeouts(ctx, task, files);\n\t\tret |= io_run_task_work();\n\t\tret |= io_run_ctx_fallback(ctx);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n}\n\nstatic int io_uring_count_inflight(struct io_ring_ctx *ctx,\n\t\t\t\t   struct task_struct *task,\n\t\t\t\t   struct files_struct *files)\n{\n\tstruct io_kiocb *req;\n\tint cnt = 0;\n\n\tspin_lock_irq(&ctx->inflight_lock);\n\tlist_for_each_entry(req, &ctx->inflight_list, inflight_entry)\n\t\tcnt += io_match_task(req, task, files);\n\tspin_unlock_irq(&ctx->inflight_lock);\n\treturn cnt;\n}\n\nstatic void io_uring_cancel_files(struct io_ring_ctx *ctx,\n\t\t\t\t  struct task_struct *task,\n\t\t\t\t  struct files_struct *files)\n{\n\twhile (!list_empty_careful(&ctx->inflight_list)) {\n\t\tDEFINE_WAIT(wait);\n\t\tint inflight;\n\n\t\tinflight = io_uring_count_inflight(ctx, task, files);\n\t\tif (!inflight)\n\t\t\tbreak;\n\n\t\tio_uring_try_cancel_requests(ctx, task, files);\n\n\t\tprepare_to_wait(&task->io_uring->wait, &wait,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tif (inflight == io_uring_count_inflight(ctx, task, files))\n\t\t\tschedule();\n\t\tfinish_wait(&task->io_uring->wait, &wait);\n\t}\n}\n\nstatic int __io_uring_add_task_file(struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_node *node;\n\tint ret;\n\n\tif (unlikely(!tctx)) {\n\t\tret = io_uring_alloc_task_context(current, ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\ttctx = current->io_uring;\n\t}\n\tif (!xa_load(&tctx->xa, (unsigned long)ctx)) {\n\t\tnode = kmalloc(sizeof(*node), GFP_KERNEL);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\t\tnode->ctx = ctx;\n\t\tnode->task = current;\n\n\t\tret = xa_err(xa_store(&tctx->xa, (unsigned long)ctx,\n\t\t\t\t\tnode, GFP_KERNEL));\n\t\tif (ret) {\n\t\t\tkfree(node);\n\t\t\treturn ret;\n\t\t}\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tlist_add(&node->ctx_node, &ctx->tctx_list);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\ttctx->last = ctx;\n\treturn 0;\n}\n\n/*\n * Note that this task has used io_uring. We use it for cancelation purposes.\n */\nstatic inline int io_uring_add_task_file(struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (likely(tctx && tctx->last == ctx))\n\t\treturn 0;\n\treturn __io_uring_add_task_file(ctx);\n}\n\n/*\n * Remove this io_uring_file -> task mapping.\n */\nstatic void io_uring_del_task_file(unsigned long index)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_node *node;\n\n\tif (!tctx)\n\t\treturn;\n\tnode = xa_erase(&tctx->xa, index);\n\tif (!node)\n\t\treturn;\n\n\tWARN_ON_ONCE(current != node->task);\n\tWARN_ON_ONCE(list_empty(&node->ctx_node));\n\n\tmutex_lock(&node->ctx->uring_lock);\n\tlist_del(&node->ctx_node);\n\tmutex_unlock(&node->ctx->uring_lock);\n\n\tif (tctx->last == node->ctx)\n\t\ttctx->last = NULL;\n\tkfree(node);\n}\n\nstatic void io_uring_clean_tctx(struct io_uring_task *tctx)\n{\n\tstruct io_tctx_node *node;\n\tunsigned long index;\n\n\txa_for_each(&tctx->xa, index, node)\n\t\tio_uring_del_task_file(index);\n\tif (tctx->io_wq) {\n\t\tio_wq_put_and_exit(tctx->io_wq);\n\t\ttctx->io_wq = NULL;\n\t}\n}\n\nstatic s64 tctx_inflight(struct io_uring_task *tctx)\n{\n\treturn percpu_counter_sum(&tctx->inflight);\n}\n\nstatic void io_sqpoll_cancel_cb(struct callback_head *cb)\n{\n\tstruct io_tctx_exit *work = container_of(cb, struct io_tctx_exit, task_work);\n\tstruct io_ring_ctx *ctx = work->ctx;\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (sqd->thread)\n\t\tio_uring_cancel_sqpoll(ctx);\n\tcomplete(&work->completion);\n}\n\nstatic void io_sqpoll_cancel_sync(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_tctx_exit work = { .ctx = ctx, };\n\tstruct task_struct *task;\n\n\tio_sq_thread_park(sqd);\n\tlist_del_init(&ctx->sqd_list);\n\tio_sqd_update_thread_idle(sqd);\n\ttask = sqd->thread;\n\tif (task) {\n\t\tinit_completion(&work.completion);\n\t\tinit_task_work(&work.task_work, io_sqpoll_cancel_cb);\n\t\tio_task_work_add_head(&sqd->park_task_work, &work.task_work);\n\t\twake_up_process(task);\n\t}\n\tio_sq_thread_unpark(sqd);\n\n\tif (task)\n\t\twait_for_completion(&work.completion);\n}\n\nvoid __io_uring_files_cancel(struct files_struct *files)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_node *node;\n\tunsigned long index;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\txa_for_each(&tctx->xa, index, node) {\n\t\tstruct io_ring_ctx *ctx = node->ctx;\n\n\t\tif (ctx->sq_data) {\n\t\t\tio_sqpoll_cancel_sync(ctx);\n\t\t\tcontinue;\n\t\t}\n\t\tio_uring_cancel_files(ctx, current, files);\n\t\tif (!files)\n\t\t\tio_uring_try_cancel_requests(ctx, current, NULL);\n\t}\n\tatomic_dec(&tctx->in_idle);\n\n\tif (files)\n\t\tio_uring_clean_tctx(tctx);\n}\n\n/* should only be called by SQPOLL task */\nstatic void io_uring_cancel_sqpoll(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tstruct io_uring_task *tctx = current->io_uring;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tWARN_ON_ONCE(!sqd || ctx->sq_data->thread != current);\n\n\tatomic_inc(&tctx->in_idle);\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\tio_uring_try_cancel_requests(ctx, current, NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\tatomic_dec(&tctx->in_idle);\n}\n\n/*\n * Find any io_uring fd that this task has registered or done IO on, and cancel\n * requests.\n */\nvoid __io_uring_task_cancel(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tDEFINE_WAIT(wait);\n\ts64 inflight;\n\n\t/* make sure overflow events are dropped */\n\tatomic_inc(&tctx->in_idle);\n\t__io_uring_files_cancel(NULL);\n\n\tdo {\n\t\t/* read completions before cancelations */\n\t\tinflight = tctx_inflight(tctx);\n\t\tif (!inflight)\n\t\t\tbreak;\n\t\t__io_uring_files_cancel(NULL);\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\t/*\n\t\t * If we've seen completions, retry without waiting. This\n\t\t * avoids a race where a completion comes in before we did\n\t\t * prepare_to_wait().\n\t\t */\n\t\tif (inflight == tctx_inflight(tctx))\n\t\t\tschedule();\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\n\tatomic_dec(&tctx->in_idle);\n\n\tio_uring_clean_tctx(tctx);\n\t/* all current's requests should be gone, we can kill tctx */\n\t__io_uring_free(current);\n}\n\nstatic void *io_uring_validate_mmap_request(struct file *file,\n\t\t\t\t\t    loff_t pgoff, size_t sz)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\tloff_t offset = pgoff << PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tswitch (offset) {\n\tcase IORING_OFF_SQ_RING:\n\tcase IORING_OFF_CQ_RING:\n\t\tptr = ctx->rings;\n\t\tbreak;\n\tcase IORING_OFF_SQES:\n\t\tptr = ctx->sq_sqes;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tpage = virt_to_head_page(ptr);\n\tif (sz > page_size(page))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ptr;\n}\n\n#ifdef CONFIG_MMU\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t sz = vma->vm_end - vma->vm_start;\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tpfn = virt_to_phys(ptr) >> PAGE_SHIFT;\n\treturn remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);\n}\n\n#else /* !CONFIG_MMU */\n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & (VM_SHARED | VM_MAYSHARE) ? 0 : -EINVAL;\n}\n\nstatic unsigned int io_uring_nommu_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT | NOMMU_MAP_READ | NOMMU_MAP_WRITE;\n}\n\nstatic unsigned long io_uring_nommu_get_unmapped_area(struct file *file,\n\tunsigned long addr, unsigned long len,\n\tunsigned long pgoff, unsigned long flags)\n{\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, pgoff, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\treturn (unsigned long) ptr;\n}\n\n#endif /* !CONFIG_MMU */\n\nstatic int io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n\treturn 0;\n}\n\nstatic int io_get_ext_arg(unsigned flags, const void __user *argp, size_t *argsz,\n\t\t\t  struct __kernel_timespec __user **ts,\n\t\t\t  const sigset_t __user **sig)\n{\n\tstruct io_uring_getevents_arg arg;\n\n\t/*\n\t * If EXT_ARG isn't set, then we have no timespec and the argp pointer\n\t * is just a pointer to the sigset_t.\n\t */\n\tif (!(flags & IORING_ENTER_EXT_ARG)) {\n\t\t*sig = (const sigset_t __user *) argp;\n\t\t*ts = NULL;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * EXT_ARG is set - ensure we agree on the size of it and copy in our\n\t * timespec and sigset_t pointers if good.\n\t */\n\tif (*argsz != sizeof(arg))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&arg, argp, sizeof(arg)))\n\t\treturn -EFAULT;\n\t*sig = u64_to_user_ptr(arg.sigmask);\n\t*argsz = arg.sigmask_sz;\n\t*ts = u64_to_user_ptr(arg.ts);\n\treturn 0;\n}\n\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tint submitted = 0;\n\tstruct fd f;\n\tlong ret;\n\n\tio_run_task_work();\n\n\tif (unlikely(flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\t       IORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG)))\n\t\treturn -EINVAL;\n\n\tf = fdget(fd);\n\tif (unlikely(!f.file))\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (unlikely(f.file->f_op != &io_uring_fops))\n\t\tgoto out_fput;\n\n\tret = -ENXIO;\n\tctx = f.file->private_data;\n\tif (unlikely(!percpu_ref_tryget(&ctx->refs)))\n\t\tgoto out_fput;\n\n\tret = -EBADFD;\n\tif (unlikely(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\tgoto out;\n\n\t/*\n\t * For SQ polling, the thread will do all submissions and completions.\n\t * Just return the requested submit count, and wake the thread if\n\t * we were asked to.\n\t */\n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx, false);\n\n\t\tret = -EOWNERDEAD;\n\t\tif (unlikely(ctx->sq_data->thread == NULL)) {\n\t\t\tgoto out;\n\t\t}\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT) {\n\t\t\tret = io_sqpoll_wait_sq(ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tsubmitted = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_task_file(ctx);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tsubmitted = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (submitted != to_submit)\n\t\t\tgoto out;\n\t}\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tconst sigset_t __user *sig;\n\t\tstruct __kernel_timespec __user *ts;\n\n\t\tret = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmin_complete = min(min_complete, ctx->cq_entries);\n\n\t\t/*\n\t\t * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user\n\t\t * space applications don't need to do io completion events\n\t\t * polling again, they can rely on io_sq_thread to do polling\n\t\t * work, which can reduce cpu usage and uring_lock contention.\n\t\t */\n\t\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t\t    !(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\t\tret = io_iopoll_check(ctx, min_complete);\n\t\t} else {\n\t\t\tret = io_cqring_wait(ctx, min_complete, sig, argsz, ts);\n\t\t}\n\t}\n\nout:\n\tpercpu_ref_put(&ctx->refs);\nout_fput:\n\tfdput(f);\n\treturn submitted ? submitted : ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int io_uring_show_cred(struct seq_file *m, unsigned int id,\n\t\tconst struct cred *cred)\n{\n\tstruct user_namespace *uns = seq_user_ns(m);\n\tstruct group_info *gi;\n\tkernel_cap_t cap;\n\tunsigned __capi;\n\tint g;\n\n\tseq_printf(m, \"%5d\\n\", id);\n\tseq_put_decimal_ull(m, \"\\tUid:\\t\", from_kuid_munged(uns, cred->uid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->euid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->suid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kuid_munged(uns, cred->fsuid));\n\tseq_put_decimal_ull(m, \"\\n\\tGid:\\t\", from_kgid_munged(uns, cred->gid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->egid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->sgid));\n\tseq_put_decimal_ull(m, \"\\t\\t\", from_kgid_munged(uns, cred->fsgid));\n\tseq_puts(m, \"\\n\\tGroups:\\t\");\n\tgi = cred->group_info;\n\tfor (g = 0; g < gi->ngroups; g++) {\n\t\tseq_put_decimal_ull(m, g ? \" \" : \"\",\n\t\t\t\t\tfrom_kgid_munged(uns, gi->gid[g]));\n\t}\n\tseq_puts(m, \"\\n\\tCapEff:\\t\");\n\tcap = cred->cap_effective;\n\tCAP_FOR_EACH_U32(__capi)\n\t\tseq_put_hex_ll(m, NULL, cap.cap[CAP_LAST_U32 - __capi], 8);\n\tseq_putc(m, '\\n');\n\treturn 0;\n}\n\nstatic void __io_uring_show_fdinfo(struct io_ring_ctx *ctx, struct seq_file *m)\n{\n\tstruct io_sq_data *sq = NULL;\n\tbool has_lock;\n\tint i;\n\n\t/*\n\t * Avoid ABBA deadlock between the seq lock and the io_uring mutex,\n\t * since fdinfo case grabs it in the opposite direction of normal use\n\t * cases. If we fail to get the lock, we just don't iterate any\n\t * structures that could be going away outside the io_uring mutex.\n\t */\n\thas_lock = mutex_trylock(&ctx->uring_lock);\n\n\tif (has_lock && (ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tsq = ctx->sq_data;\n\t\tif (!sq->thread)\n\t\t\tsq = NULL;\n\t}\n\n\tseq_printf(m, \"SqThread:\\t%d\\n\", sq ? task_pid_nr(sq->thread) : -1);\n\tseq_printf(m, \"SqThreadCpu:\\t%d\\n\", sq ? task_cpu(sq->thread) : -1);\n\tseq_printf(m, \"UserFiles:\\t%u\\n\", ctx->nr_user_files);\n\tfor (i = 0; has_lock && i < ctx->nr_user_files; i++) {\n\t\tstruct file *f = io_file_from_index(ctx, i);\n\n\t\tif (f)\n\t\t\tseq_printf(m, \"%5u: %s\\n\", i, file_dentry(f)->d_iname);\n\t\telse\n\t\t\tseq_printf(m, \"%5u: <none>\\n\", i);\n\t}\n\tseq_printf(m, \"UserBufs:\\t%u\\n\", ctx->nr_user_bufs);\n\tfor (i = 0; has_lock && i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *buf = &ctx->user_bufs[i];\n\n\t\tseq_printf(m, \"%5u: 0x%llx/%u\\n\", i, buf->ubuf,\n\t\t\t\t\t\t(unsigned int) buf->len);\n\t}\n\tif (has_lock && !xa_empty(&ctx->personalities)) {\n\t\tunsigned long index;\n\t\tconst struct cred *cred;\n\n\t\tseq_printf(m, \"Personalities:\\n\");\n\t\txa_for_each(&ctx->personalities, index, cred)\n\t\t\tio_uring_show_cred(m, index, cred);\n\t}\n\tseq_printf(m, \"PollList:\\n\");\n\tspin_lock_irq(&ctx->completion_lock);\n\tfor (i = 0; i < (1U << ctx->cancel_hash_bits); i++) {\n\t\tstruct hlist_head *list = &ctx->cancel_hash[i];\n\t\tstruct io_kiocb *req;\n\n\t\thlist_for_each_entry(req, list, hash_node)\n\t\t\tseq_printf(m, \"  op=%d, task_works=%d\\n\", req->opcode,\n\t\t\t\t\treq->task->task_works != NULL);\n\t}\n\tspin_unlock_irq(&ctx->completion_lock);\n\tif (has_lock)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_uring_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct io_ring_ctx *ctx = f->private_data;\n\n\tif (percpu_ref_tryget(&ctx->refs)) {\n\t\t__io_uring_show_fdinfo(ctx, m);\n\t\tpercpu_ref_put(&ctx->refs);\n\t}\n}\n#endif\n\nstatic const struct file_operations io_uring_fops = {\n\t.release\t= io_uring_release,\n\t.mmap\t\t= io_uring_mmap,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = io_uring_nommu_get_unmapped_area,\n\t.mmap_capabilities = io_uring_nommu_mmap_capabilities,\n#endif\n\t.poll\t\t= io_uring_poll,\n\t.fasync\t\t= io_uring_fasync,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= io_uring_show_fdinfo,\n#endif\n};\n\nstatic int io_allocate_scq_urings(struct io_ring_ctx *ctx,\n\t\t\t\t  struct io_uring_params *p)\n{\n\tstruct io_rings *rings;\n\tsize_t size, sq_array_offset;\n\n\t/* make sure these are sane, as we already accounted them */\n\tctx->sq_entries = p->sq_entries;\n\tctx->cq_entries = p->cq_entries;\n\n\tsize = rings_size(p->sq_entries, p->cq_entries, &sq_array_offset);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\trings = io_mem_alloc(size);\n\tif (!rings)\n\t\treturn -ENOMEM;\n\n\tctx->rings = rings;\n\tctx->sq_array = (u32 *)((char *)rings + sq_array_offset);\n\trings->sq_ring_mask = p->sq_entries - 1;\n\trings->cq_ring_mask = p->cq_entries - 1;\n\trings->sq_ring_entries = p->sq_entries;\n\trings->cq_ring_entries = p->cq_entries;\n\tctx->sq_mask = rings->sq_ring_mask;\n\tctx->cq_mask = rings->cq_ring_mask;\n\n\tsize = array_size(sizeof(struct io_uring_sqe), p->sq_entries);\n\tif (size == SIZE_MAX) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tctx->sq_sqes = io_mem_alloc(size);\n\tif (!ctx->sq_sqes) {\n\t\tio_mem_free(ctx->rings);\n\t\tctx->rings = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_uring_install_fd(struct io_ring_ctx *ctx, struct file *file)\n{\n\tint ret, fd;\n\n\tfd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (fd < 0)\n\t\treturn fd;\n\n\tret = io_uring_add_task_file(ctx);\n\tif (ret) {\n\t\tput_unused_fd(fd);\n\t\treturn ret;\n\t}\n\tfd_install(fd, file);\n\treturn fd;\n}\n\n/*\n * Allocate an anonymous fd, this is what constitutes the application\n * visible backing of an io_uring instance. The application mmaps this\n * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,\n * we have to tie this fd to a socket for file garbage collection purposes.\n */\nstatic struct file *io_uring_get_file(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n#if defined(CONFIG_UNIX)\n\tint ret;\n\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n#endif\n\n\tfile = anon_inode_getfile(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\tO_RDWR | O_CLOEXEC);\n#if defined(CONFIG_UNIX)\n\tif (IS_ERR(file)) {\n\t\tsock_release(ctx->ring_sock);\n\t\tctx->ring_sock = NULL;\n\t} else {\n\t\tctx->ring_sock->file = file;\n\t}\n#endif\n\treturn file;\n}\n\nstatic int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t   struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\t/*\n\t * Use twice as many entries for the CQ ring. It's possible for the\n\t * application to drive a higher depth than the size of the SQ ring,\n\t * since the sqes are only used at submission time. This allows for\n\t * some flexibility in overcommitting a bit. If the application has\n\t * set IORING_SETUP_CQSIZE, it will have passed in the desired number\n\t * of CQ ring entries manually.\n\t */\n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t/*\n\t\t * If IORING_SETUP_CQSIZE is set, we do the same roundup\n\t\t * to a power-of-two, if it isn't already. We do NOT impose\n\t\t * any cq vs sq ring sizing.\n\t\t */\n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->compat = in_compat_syscall();\n\tif (!capable(CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\n\t/*\n\t * This is just grabbed for accounting purposes. When a process exits,\n\t * the mm is exited and dropped before the files, hence we need to hang\n\t * on to this mm purely for the purposes of being able to unaccount\n\t * memory (locked/pinned vm). It's not used for anything else.\n\t */\n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tmemset(&p->sq_off, 0, sizeof(p->sq_off));\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\n\tmemset(&p->cq_off, 0, sizeof(p->cq_off));\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\t/*\n\t * Install ring fd as the very last thing, so we don't risk someone\n\t * having closed it before we finish setup\n\t */\n\tret = io_uring_install_fd(ctx, file);\n\tif (ret < 0) {\n\t\t/* fput will clean it up */\n\t\tfput(file);\n\t\treturn ret;\n\t}\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\n}\n\n/*\n * Sets up an aio uring context, and returns the fd. Applications asks for a\n * ring size, we return the actual sq/cq ring sizes (among other things) in the\n * params structure passed in.\n */\nstatic long io_uring_setup(u32 entries, struct io_uring_params __user *params)\n{\n\tstruct io_uring_params p;\n\tint i;\n\n\tif (copy_from_user(&p, params, sizeof(p)))\n\t\treturn -EFAULT;\n\tfor (i = 0; i < ARRAY_SIZE(p.resv); i++) {\n\t\tif (p.resv[i])\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |\n\t\t\tIORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |\n\t\t\tIORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |\n\t\t\tIORING_SETUP_R_DISABLED))\n\t\treturn -EINVAL;\n\n\treturn  io_uring_create(entries, &p, params);\n}\n\nSYSCALL_DEFINE2(io_uring_setup, u32, entries,\n\t\tstruct io_uring_params __user *, params)\n{\n\treturn io_uring_setup(entries, params);\n}\n\nstatic int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)\n{\n\tstruct io_uring_probe *p;\n\tsize_t size;\n\tint i, ret;\n\n\tsize = struct_size(p, ops, nr_args);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\tp = kzalloc(size, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tret = -EFAULT;\n\tif (copy_from_user(p, arg, size))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (memchr_inv(p, 0, size))\n\t\tgoto out;\n\n\tp->last_op = IORING_OP_LAST - 1;\n\tif (nr_args > IORING_OP_LAST)\n\t\tnr_args = IORING_OP_LAST;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tp->ops[i].op = i;\n\t\tif (!io_op_defs[i].not_supported)\n\t\t\tp->ops[i].flags = IO_URING_OP_SUPPORTED;\n\t}\n\tp->ops_len = i;\n\n\tret = 0;\n\tif (copy_to_user(arg, p, size))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tu32 id;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = xa_alloc_cyclic(&ctx->personalities, &id, (void *)creds,\n\t\t\tXA_LIMIT(0, USHRT_MAX), &ctx->pers_next, GFP_KERNEL);\n\tif (!ret)\n\t\treturn id;\n\tput_cred(creds);\n\treturn ret;\n}\n\nstatic int io_register_restrictions(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t\t    unsigned int nr_args)\n{\n\tstruct io_uring_restriction *res;\n\tsize_t size;\n\tint i, ret;\n\n\t/* Restrictions allowed only if rings started disabled */\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\t/* We allow only a single restrictions registration */\n\tif (ctx->restrictions.registered)\n\t\treturn -EBUSY;\n\n\tif (!arg || nr_args > IORING_MAX_RESTRICTIONS)\n\t\treturn -EINVAL;\n\n\tsize = array_size(nr_args, sizeof(*res));\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\tres = memdup_user(arg, size);\n\tif (IS_ERR(res))\n\t\treturn PTR_ERR(res);\n\n\tret = 0;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tswitch (res[i].opcode) {\n\t\tcase IORING_RESTRICTION_REGISTER_OP:\n\t\t\tif (res[i].register_op >= IORING_REGISTER_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].register_op,\n\t\t\t\t  ctx->restrictions.register_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_OP:\n\t\t\tif (res[i].sqe_op >= IORING_OP_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].sqe_op, ctx->restrictions.sqe_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_ALLOWED:\n\t\t\tctx->restrictions.sqe_flags_allowed = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_REQUIRED:\n\t\t\tctx->restrictions.sqe_flags_required = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t/* Reset all restrictions if an error happened */\n\tif (ret != 0)\n\t\tmemset(&ctx->restrictions, 0, sizeof(ctx->restrictions));\n\telse\n\t\tctx->restrictions.registered = true;\n\n\tkfree(res);\n\treturn ret;\n}\n\nstatic int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\tif (ctx->sq_data && wq_has_sleeper(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n\treturn 0;\n}\n\nstatic bool io_register_op_must_quiesce(int op)\n{\n\tswitch (op) {\n\tcase IORING_UNREGISTER_FILES:\n\tcase IORING_REGISTER_FILES_UPDATE:\n\tcase IORING_REGISTER_PROBE:\n\tcase IORING_REGISTER_PERSONALITY:\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n\t\t\t       void __user *arg, unsigned nr_args)\n\t__releases(ctx->uring_lock)\n\t__acquires(ctx->uring_lock)\n{\n\tint ret;\n\n\t/*\n\t * We're inside the ring mutex, if the ref is already dying, then\n\t * someone else killed the ctx or is already going through\n\t * io_uring_register().\n\t */\n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn -ENXIO;\n\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\tpercpu_ref_kill(&ctx->refs);\n\n\t\t/*\n\t\t * Drop uring mutex before waiting for references to exit. If\n\t\t * another thread is currently inside io_uring_enter() it might\n\t\t * need to grab the uring_lock to make progress. If we hold it\n\t\t * here across the drain wait, then we can deadlock. It's safe\n\t\t * to drop the mutex here, since no new references will come in\n\t\t * after we've killed the percpu ref.\n\t\t */\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tdo {\n\t\t\tret = wait_for_completion_interruptible(&ctx->ref_comp);\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t\tret = io_run_task_work_sig();\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tif (ret) {\n\t\t\tpercpu_ref_resurrect(&ctx->refs);\n\t\t\tgoto out_quiesce;\n\t\t}\n\t}\n\n\tif (ctx->restricted) {\n\t\tif (opcode >= IORING_REGISTER_LAST) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!test_bit(opcode, ctx->restrictions.register_op)) {\n\t\t\tret = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (opcode) {\n\tcase IORING_REGISTER_BUFFERS:\n\t\tret = io_sqe_buffers_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_BUFFERS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_buffers_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES:\n\t\tret = io_sqe_files_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_FILES:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_files_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES_UPDATE:\n\t\tret = io_sqe_files_update(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_EVENTFD:\n\tcase IORING_REGISTER_EVENTFD_ASYNC:\n\t\tret = -EINVAL;\n\t\tif (nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_eventfd_register(ctx, arg);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (opcode == IORING_REGISTER_EVENTFD_ASYNC)\n\t\t\tctx->eventfd_async = 1;\n\t\telse\n\t\t\tctx->eventfd_async = 0;\n\t\tbreak;\n\tcase IORING_UNREGISTER_EVENTFD:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_eventfd_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_PROBE:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args > 256)\n\t\t\tbreak;\n\t\tret = io_probe(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_personality(ctx);\n\t\tbreak;\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg)\n\t\t\tbreak;\n\t\tret = io_unregister_personality(ctx, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_ENABLE_RINGS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_enable_rings(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_RESTRICTIONS:\n\t\tret = io_register_restrictions(ctx, arg, nr_args);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\tif (io_register_op_must_quiesce(opcode)) {\n\t\t/* bring the ctx back to life */\n\t\tpercpu_ref_reinit(&ctx->refs);\nout_quiesce:\n\t\treinit_completion(&ctx->ref_comp);\n\t}\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,\n\t\tvoid __user *, arg, unsigned int, nr_args)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tstruct fd f;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EOPNOTSUPP;\n\tif (f.file->f_op != &io_uring_fops)\n\t\tgoto out_fput;\n\n\tctx = f.file->private_data;\n\n\tio_run_task_work();\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_uring_register(ctx, opcode, arg, nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,\n\t\t\t\t\t\t\tctx->cq_ev_fd != NULL, ret);\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int __init io_uring_init(void)\n{\n#define __BUILD_BUG_VERIFY_ELEMENT(stype, eoffset, etype, ename) do { \\\n\tBUILD_BUG_ON(offsetof(stype, ename) != eoffset); \\\n\tBUILD_BUG_ON(sizeof(etype) != sizeof_field(stype, ename)); \\\n} while (0)\n\n#define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \\\n\t__BUILD_BUG_VERIFY_ELEMENT(struct io_uring_sqe, eoffset, etype, ename)\n\tBUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);\n\tBUILD_BUG_SQE_ELEM(0,  __u8,   opcode);\n\tBUILD_BUG_SQE_ELEM(1,  __u8,   flags);\n\tBUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);\n\tBUILD_BUG_SQE_ELEM(4,  __s32,  fd);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  off);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  addr2);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  addr);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);\n\tBUILD_BUG_SQE_ELEM(24, __u32,  len);\n\tBUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */   int, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u32, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);\n\tBUILD_BUG_SQE_ELEM(28, /* compat */ __u16,  poll_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  poll32_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  open_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);\n\tBUILD_BUG_SQE_ELEM(32, __u64,  user_data);\n\tBUILD_BUG_SQE_ELEM(40, __u16,  buf_index);\n\tBUILD_BUG_SQE_ELEM(42, __u16,  personality);\n\tBUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);\n\n\tBUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);\n\tBUILD_BUG_ON(__REQ_F_LAST_BIT >= 8 * sizeof(int));\n\treq_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC |\n\t\t\t\tSLAB_ACCOUNT);\n\treturn 0;\n};\n__initcall(io_uring_init);\n"}}, "reports": [{"events": [{"location": {"col": 4, "file": 0, "line": 6059}, "message": "WARNING: NULL check before some freeing functions is not needed."}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "2102868a147247e660b80a4113475e24", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 1735}, "message": "WARNING: NULL check before some freeing functions is not needed."}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "026351162aca779337750407b273c68d", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 3335}, "message": "WARNING: NULL check before some freeing functions is not needed."}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "aee732d26acf553880465ffaa98d4696", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 4610}, "message": "WARNING: NULL check before some freeing functions is not needed."}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "5e3532493192e785b1a9b25d23842681", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 4367}, "message": "WARNING: NULL check before some freeing functions is not needed."}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "5e3532493192e785b1a9b25d23842681", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}, {"events": [{"location": {"col": 2, "file": 0, "line": 3431}, "message": "WARNING: NULL check before some freeing functions is not needed."}], "macros": [], "notes": [], "path": "/src/fs/io_uring.c", "reportHash": "aee732d26acf553880465ffaa98d4696", "checkerName": "coccinelle", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
