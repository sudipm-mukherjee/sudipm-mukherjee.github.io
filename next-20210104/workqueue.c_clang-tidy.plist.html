<!DOCTYPE html>
<html>
  <head>
    <title>Plist HTML Viewer</title>

    <meta charset="UTF-8">

    <style type="text/css">
      .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.CodeMirror-lines{padding:4px 0}.CodeMirror pre{padding:0 4px}.CodeMirror-gutter-filler,.CodeMirror-scrollbar-filler{background-color:#fff}.CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.CodeMirror-guttermarker{color:#000}.CodeMirror-guttermarker-subtle{color:#999}.CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.cm-fat-cursor .CodeMirror-cursor{width:auto;border:0!important;background:#7e7}.cm-fat-cursor div.CodeMirror-cursors{z-index:1}.cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.cm-tab{display:inline-block;text-decoration:inherit}.CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:-20px;overflow:hidden}.CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.cm-s-default .cm-header{color:#00f}.cm-s-default .cm-quote{color:#090}.cm-negative{color:#d44}.cm-positive{color:#292}.cm-header,.cm-strong{font-weight:700}.cm-em{font-style:italic}.cm-link{text-decoration:underline}.cm-strikethrough{text-decoration:line-through}.cm-s-default .cm-keyword{color:#708}.cm-s-default .cm-atom{color:#219}.cm-s-default .cm-number{color:#164}.cm-s-default .cm-def{color:#00f}.cm-s-default .cm-variable-2{color:#05a}.cm-s-default .cm-type,.cm-s-default .cm-variable-3{color:#085}.cm-s-default .cm-comment{color:#a50}.cm-s-default .cm-string{color:#a11}.cm-s-default .cm-string-2{color:#f50}.cm-s-default .cm-meta{color:#555}.cm-s-default .cm-qualifier{color:#555}.cm-s-default .cm-builtin{color:#30a}.cm-s-default .cm-bracket{color:#997}.cm-s-default .cm-tag{color:#170}.cm-s-default .cm-attribute{color:#00c}.cm-s-default .cm-hr{color:#999}.cm-s-default .cm-link{color:#00c}.cm-s-default .cm-error{color:red}.cm-invalidchar{color:red}.CodeMirror-composing{border-bottom:2px solid}div.CodeMirror span.CodeMirror-matchingbracket{color:#0f0}div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#f22}.CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.CodeMirror-activeline-background{background:#e8f2ff}.CodeMirror{position:relative;overflow:hidden;background:#fff}.CodeMirror-scroll{overflow:scroll!important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:0;position:relative}.CodeMirror-sizer{position:relative;border-right:30px solid transparent}.CodeMirror-gutter-filler,.CodeMirror-hscrollbar,.CodeMirror-scrollbar-filler,.CodeMirror-vscrollbar{position:absolute;z-index:6;display:none}.CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.CodeMirror-scrollbar-filler{right:0;bottom:0}.CodeMirror-gutter-filler{left:0;bottom:0}.CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:0 0!important;border:none!important}.CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.CodeMirror-gutter-wrapper ::selection{background-color:transparent}.CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.CodeMirror-lines{cursor:text;min-height:1px}.CodeMirror pre{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:0 0;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.CodeMirror-wrap pre{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.CodeMirror-linewidget{position:relative;z-index:2;overflow:auto}.CodeMirror-rtl pre{direction:rtl}.CodeMirror-code{outline:0}.CodeMirror-gutter,.CodeMirror-gutters,.CodeMirror-linenumber,.CodeMirror-scroll,.CodeMirror-sizer{-moz-box-sizing:content-box;box-sizing:content-box}.CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.CodeMirror-cursor{position:absolute;pointer-events:none}.CodeMirror-measure pre{position:static}div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}div.CodeMirror-dragcursors{visibility:visible}.CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.CodeMirror-selected{background:#d9d9d9}.CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.CodeMirror-crosshair{cursor:crosshair}.CodeMirror-line::selection,.CodeMirror-line>span::selection,.CodeMirror-line>span>span::selection{background:#d7d4f0}.CodeMirror-line::-moz-selection,.CodeMirror-line>span::-moz-selection,.CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.cm-force-border{padding-right:.1px}@media print{.CodeMirror div.CodeMirror-cursors{visibility:hidden}}.cm-tab-wrap-hack:after{content:''}span.CodeMirror-selectedtext{background:0 0}
/*# sourceMappingURL=codemirror.min.css.map */

      .severity-low {
  background-color: #669603;
}

.severity-low:after {
  content : 'L';
}

.severity-unspecified {
  background-color: #666666;
}

.severity-unspecified:after {
  content : 'U';
}

.severity-style {
  background-color: #9932cc;
}

.severity-style:after {
  content : 'S';
}

.severity-medium {
  background-color: #a9d323;
  color: black;
}

.severity-medium:after {
  content : 'M';
}

.severity-high {
  background-color: #ffa800;
}

.severity-high:after {
  content : 'H';
}

.severity-critical {
  background-color: #e92625;
}

.severity-critical:after {
  content : 'C';
}

i[class*="severity-"] {
  line-height: normal;
  text-transform: capitalize;
  font-size: 0.8em;
  font-weight: bold;
  color: white;
  display: inline-block;
  width: 16px;
  height: 16px;
  text-align: center;
  font-family: sans-serif;
}

      html, body {
  width: 100%;
  height: 100%;
  padding: 0px;
  margin: 0px;
}

div.container {
  padding: 10px;
}

#content {
  height: 100%;
  display: block;
  overflow: hidden;
}

#content > div {
  margin: 10px;
  overflow: hidden;
  border: 1px solid #ddd;
  border-radius: 3px;
  overflow: hidden;
  height: 97%;
}

.button {
  background-color: #f1f1f1;
  text-decoration: none;
  display: inline-block;
  padding: 8px 16px;
  color: black;
  cursor: pointer;
}

.button:hover {
  background-color: #ddd;
  color: black;
}

.review-status {
  color: white;
  text-align: center;
}

.review-status-confirmed {
  background-color: #e92625;
}

.review-status-false-positive {
  background-color: grey;
}

.review-status-intentional {
  background-color: #669603;
}

      div.container {
  width: 100%;
  height: 100%;
  padding: 0px;
}

#editor-wrapper {
  margin: 10px;
}

#side-bar {
  float: left;
  width: 260px;
  margin: 0px;
}

#report-nav ul {
  list-style-type: none;
  padding: 0;
  margin: 0;
  overflow-y: auto;
  height: 100%;
}

#report-nav ul > li {
  padding: .4em;
  background-color: #fff;
  border-bottom: 1px solid rgba(0,0,0,.125);
  text-align: left;
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

#report-nav ul > li.active {
  background-color: #427ea9;
  color: white;
}

#report-nav ul > li:hover {
  background-color: #427ea9;
  color: white;
  cursor: pointer;
}

#report-nav ul a {
  text-decoration: none;
}

#report-nav i[class*="severity-"] {
  margin-right: 5px;
}

.header {
  border-bottom: 1px solid lightgrey;
  font-family: monospace;
  padding: 10px;
  background-color: #fafbfc;
  border-bottom: 1px solid #e1e4e8;
  border-top-left-radius: 2px;
  border-top-right-radius: 2px;
}

#report-nav .header {
  font-weight: bold;
}

#editor-wrapper .header > div {
  padding-top: 2px;
}

#file-path,
#checker-name {
  color: #195ea2;
}

#review-status {
  padding: 0px 5px;
}

#file-path {
  font-family: monospace;
}

.check-msg {
  display: inline-block;
  padding: 3px 6px;
  margin: 1px;
  -webkit-border-radius: 5px;
  -moz-border-radius: 5px;
  border-radius: 5px;
}

.check-msg.info {
  color: #00546f;
  background-color: #bfdfe9;
  border: 1px solid #87a8b3;
}

.check-msg.error {
  background-color: #f2dede;
  color: #a94442;
  border: 1px solid #ebcccc;
}

.check-msg.macro {
  background-color: #d7dac2;
  color: #4f5c6d;
  border: 1px solid #d7dac2;
}

.check-msg.note {
  background-color: #d7d7d7;
  color: #4f5c6d;
  border: 1px solid #bfbfbf;
}

.check-msg.current {
  border: 2px dashed #3692ff;
}

.check-msg .tag {
  padding: 1px 5px;
  text-align: center;
  border-radius: 2px;
  margin-right: 5px;
  text-decoration: inherit;
}

.check-msg .tag.macro {
  background-color: #83876a;
  color: white;
  text-transform: capitalize;
}

.check-msg .tag.note {
  background-color: #9299a1;
  color: white;
  text-transform: capitalize;
}

.checker-enum {
  color: white;
  padding: 1px 5px;
  text-align: center;
  border-radius: 25px;
  margin-right: 5px;
  text-decoration: inherit;
}

.checker-enum.info {
  background-color: #427ea9;
}

.checker-enum.error {
  background-color: #a94442;
}

.arrow {
  border: solid black;
  border-width: 0 3px 3px 0;
  display: inline-block;
  padding: 3px;
  cursor: pointer;
  margin: 0px 5px;
}

.arrow:hover {
  border: solid #437ea8;
  border-width: 0 3px 3px 0;
}

.left-arrow {
  transform: rotate(135deg);
  -webkit-transform: rotate(135deg);
}

.right-arrow {
  transform: rotate(-45deg);
  -webkit-transform: rotate(-45deg);
}

    </style>

    <script type="text/javascript">
      function setNonCompatibleBrowserMessage() {
  document.body.innerHTML =
    '<h2 style="margin-left: 20px;">Your browser is not compatible with CodeChecker Viewer!</h2> \
     <p style="margin-left: 20px;">The version required for the following browsers are:</p> \
     <ul style="margin-left: 20px;"> \
     <li>Internet Explorer: version 9 or newer</li> \
     <li>Firefox: version 22.0 or newer</li> \
     </ul>';
}

// http://stackoverflow.com/questions/5916900/how-can-you-detect-the-version-of-a-browser
var browserVersion = (function(){
  var ua = navigator.userAgent, tem,
    M = ua.match(/(opera|chrome|safari|firefox|msie|trident(?=\/))\/?\s*(\d+)/i) || [];

  if (/trident/i.test(M[1])) {
    tem = /\brv[ :]+(\d+)/g.exec(ua) || [];
    return 'IE ' + (tem[1] || '');
  }

  if (M[1] === 'Chrome') {
    tem = ua.match(/\b(OPR|Edge)\/(\d+)/);
    if (tem != null) return tem.slice(1).join(' ').replace('OPR', 'Opera');
  }

  M = M[2] ? [M[1], M[2]] : [navigator.appName, navigator.appVersion, '-?'];
  if ((tem = ua.match(/version\/(\d+)/i)) != null) M.splice(1, 1, tem[1]);
    return M.join(' ');
})();

var pos = browserVersion.indexOf(' ');
var browser = browserVersion.substr(0, pos);
var version = parseInt(browserVersion.substr(pos + 1));

var browserCompatible
  = browser === 'Firefox'
  ? version >= 22
  : browser === 'IE'
  ? version >= 9
  : true;


      /* MIT License

Copyright (C) 2017 by Marijn Haverbeke <marijnh@gmail.com> and others

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.
 */
      !function(e,t){"object"==typeof exports&&"undefined"!=typeof module?module.exports=t():"function"==typeof define&&define.amd?define(t):e.CodeMirror=t()}(this,function(){"use strict";function e(e){return new RegExp("(^|\\s)"+e+"(?:$|\\s)\\s*")}function t(e){for(var t=e.childNodes.length;t>0;--t)e.removeChild(e.firstChild);return e}function r(e,r){return t(e).appendChild(r)}function n(e,t,r,n){var i=document.createElement(e);if(r&&(i.className=r),n&&(i.style.cssText=n),"string"==typeof t)i.appendChild(document.createTextNode(t));else if(t)for(var o=0;o<t.length;++o)i.appendChild(t[o]);return i}function i(e,t,r,i){var o=n(e,t,r,i);return o.setAttribute("role","presentation"),o}function o(e,t){if(3==t.nodeType&&(t=t.parentNode),e.contains)return e.contains(t);do{if(11==t.nodeType&&(t=t.host),t==e)return!0}while(t=t.parentNode)}function l(){var e;try{e=document.activeElement}catch(t){e=document.body||null}for(;e&&e.shadowRoot&&e.shadowRoot.activeElement;)e=e.shadowRoot.activeElement;return e}function s(t,r){var n=t.className;e(r).test(n)||(t.className+=(n?" ":"")+r)}function a(t,r){for(var n=t.split(" "),i=0;i<n.length;i++)n[i]&&!e(n[i]).test(r)&&(r+=" "+n[i]);return r}function u(e){var t=Array.prototype.slice.call(arguments,1);return function(){return e.apply(null,t)}}function c(e,t,r){t||(t={});for(var n in e)!e.hasOwnProperty(n)||!1===r&&t.hasOwnProperty(n)||(t[n]=e[n]);return t}function f(e,t,r,n,i){null==t&&-1==(t=e.search(/[^\s\u00a0]/))&&(t=e.length);for(var o=n||0,l=i||0;;){var s=e.indexOf("\t",o);if(s<0||s>=t)return l+(t-o);l+=s-o,l+=r-l%r,o=s+1}}function h(e,t){for(var r=0;r<e.length;++r)if(e[r]==t)return r;return-1}function d(e,t,r){for(var n=0,i=0;;){var o=e.indexOf("\t",n);-1==o&&(o=e.length);var l=o-n;if(o==e.length||i+l>=t)return n+Math.min(l,t-i);if(i+=o-n,i+=r-i%r,n=o+1,i>=t)return n}}function p(e){for(;Kl.length<=e;)Kl.push(g(Kl)+" ");return Kl[e]}function g(e){return e[e.length-1]}function v(e,t){for(var r=[],n=0;n<e.length;n++)r[n]=t(e[n],n);return r}function m(e,t,r){for(var n=0,i=r(t);n<e.length&&r(e[n])<=i;)n++;e.splice(n,0,t)}function y(){}function b(e,t){var r;return Object.create?r=Object.create(e):(y.prototype=e,r=new y),t&&c(t,r),r}function w(e){return/\w/.test(e)||e>""&&(e.toUpperCase()!=e.toLowerCase()||jl.test(e))}function x(e,t){return t?!!(t.source.indexOf("\\w")>-1&&w(e))||t.test(e):w(e)}function C(e){for(var t in e)if(e.hasOwnProperty(t)&&e[t])return!1;return!0}function S(e){return e.charCodeAt(0)>=768&&Xl.test(e)}function L(e,t,r){for(;(r<0?t>0:t<e.length)&&S(e.charAt(t));)t+=r;return t}function k(e,t,r){for(var n=t>r?-1:1;;){if(t==r)return t;var i=(t+r)/2,o=n<0?Math.ceil(i):Math.floor(i);if(o==t)return e(o)?t:r;e(o)?r=o:t=o+n}}function T(e,t,r){var o=this;this.input=r,o.scrollbarFiller=n("div",null,"CodeMirror-scrollbar-filler"),o.scrollbarFiller.setAttribute("cm-not-content","true"),o.gutterFiller=n("div",null,"CodeMirror-gutter-filler"),o.gutterFiller.setAttribute("cm-not-content","true"),o.lineDiv=i("div",null,"CodeMirror-code"),o.selectionDiv=n("div",null,null,"position: relative; z-index: 1"),o.cursorDiv=n("div",null,"CodeMirror-cursors"),o.measure=n("div",null,"CodeMirror-measure"),o.lineMeasure=n("div",null,"CodeMirror-measure"),o.lineSpace=i("div",[o.measure,o.lineMeasure,o.selectionDiv,o.cursorDiv,o.lineDiv],null,"position: relative; outline: none");var l=i("div",[o.lineSpace],"CodeMirror-lines");o.mover=n("div",[l],null,"position: relative"),o.sizer=n("div",[o.mover],"CodeMirror-sizer"),o.sizerWidth=null,o.heightForcer=n("div",null,null,"position: absolute; height: "+Rl+"px; width: 1px;"),o.gutters=n("div",null,"CodeMirror-gutters"),o.lineGutter=null,o.scroller=n("div",[o.sizer,o.heightForcer,o.gutters],"CodeMirror-scroll"),o.scroller.setAttribute("tabIndex","-1"),o.wrapper=n("div",[o.scrollbarFiller,o.gutterFiller,o.scroller],"CodeMirror"),gl&&vl<8&&(o.gutters.style.zIndex=-1,o.scroller.style.paddingRight=0),ml||fl&&Tl||(o.scroller.draggable=!0),e&&(e.appendChild?e.appendChild(o.wrapper):e(o.wrapper)),o.viewFrom=o.viewTo=t.first,o.reportedViewFrom=o.reportedViewTo=t.first,o.view=[],o.renderedView=null,o.externalMeasured=null,o.viewOffset=0,o.lastWrapHeight=o.lastWrapWidth=0,o.updateLineNumbers=null,o.nativeBarWidth=o.barHeight=o.barWidth=0,o.scrollbarsClipped=!1,o.lineNumWidth=o.lineNumInnerWidth=o.lineNumChars=null,o.alignWidgets=!1,o.cachedCharWidth=o.cachedTextHeight=o.cachedPaddingH=null,o.maxLine=null,o.maxLineLength=0,o.maxLineChanged=!1,o.wheelDX=o.wheelDY=o.wheelStartX=o.wheelStartY=null,o.shift=!1,o.selForContextMenu=null,o.activeTouch=null,r.init(o)}function M(e,t){if((t-=e.first)<0||t>=e.size)throw new Error("There is no line "+(t+e.first)+" in the document.");for(var r=e;!r.lines;)for(var n=0;;++n){var i=r.children[n],o=i.chunkSize();if(t<o){r=i;break}t-=o}return r.lines[t]}function N(e,t,r){var n=[],i=t.line;return e.iter(t.line,r.line+1,function(e){var o=e.text;i==r.line&&(o=o.slice(0,r.ch)),i==t.line&&(o=o.slice(t.ch)),n.push(o),++i}),n}function O(e,t,r){var n=[];return e.iter(t,r,function(e){n.push(e.text)}),n}function A(e,t){var r=t-e.height;if(r)for(var n=e;n;n=n.parent)n.height+=r}function W(e){if(null==e.parent)return null;for(var t=e.parent,r=h(t.lines,e),n=t.parent;n;t=n,n=n.parent)for(var i=0;n.children[i]!=t;++i)r+=n.children[i].chunkSize();return r+t.first}function D(e,t){var r=e.first;e:do{for(var n=0;n<e.children.length;++n){var i=e.children[n],o=i.height;if(t<o){e=i;continue e}t-=o,r+=i.chunkSize()}return r}while(!e.lines);for(var l=0;l<e.lines.length;++l){var s=e.lines[l].height;if(t<s)break;t-=s}return r+l}function H(e,t){return t>=e.first&&t<e.first+e.size}function F(e,t){return String(e.lineNumberFormatter(t+e.firstLineNumber))}function E(e,t,r){if(void 0===r&&(r=null),!(this instanceof E))return new E(e,t,r);this.line=e,this.ch=t,this.sticky=r}function P(e,t){return e.line-t.line||e.ch-t.ch}function I(e,t){return e.sticky==t.sticky&&0==P(e,t)}function z(e){return E(e.line,e.ch)}function R(e,t){return P(e,t)<0?t:e}function B(e,t){return P(e,t)<0?e:t}function G(e,t){return Math.max(e.first,Math.min(t,e.first+e.size-1))}function U(e,t){if(t.line<e.first)return E(e.first,0);var r=e.first+e.size-1;return t.line>r?E(r,M(e,r).text.length):V(t,M(e,t.line).text.length)}function V(e,t){var r=e.ch;return null==r||r>t?E(e.line,t):r<0?E(e.line,0):e}function K(e,t){for(var r=[],n=0;n<t.length;n++)r[n]=U(e,t[n]);return r}function j(){Yl=!0}function X(){_l=!0}function Y(e,t,r){this.marker=e,this.from=t,this.to=r}function _(e,t){if(e)for(var r=0;r<e.length;++r){var n=e[r];if(n.marker==t)return n}}function $(e,t){for(var r,n=0;n<e.length;++n)e[n]!=t&&(r||(r=[])).push(e[n]);return r}function q(e,t){e.markedSpans=e.markedSpans?e.markedSpans.concat([t]):[t],t.marker.attachLine(e)}function Z(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t)||o.from==t&&"bookmark"==l.type&&(!r||!o.marker.insertLeft)){var s=null==o.to||(l.inclusiveRight?o.to>=t:o.to>t);(n||(n=[])).push(new Y(l,o.from,s?null:o.to))}}return n}function Q(e,t,r){var n;if(e)for(var i=0;i<e.length;++i){var o=e[i],l=o.marker;if(null==o.to||(l.inclusiveRight?o.to>=t:o.to>t)||o.from==t&&"bookmark"==l.type&&(!r||o.marker.insertLeft)){var s=null==o.from||(l.inclusiveLeft?o.from<=t:o.from<t);(n||(n=[])).push(new Y(l,s?null:o.from-t,null==o.to?null:o.to-t))}}return n}function J(e,t){if(t.full)return null;var r=H(e,t.from.line)&&M(e,t.from.line).markedSpans,n=H(e,t.to.line)&&M(e,t.to.line).markedSpans;if(!r&&!n)return null;var i=t.from.ch,o=t.to.ch,l=0==P(t.from,t.to),s=Z(r,i,l),a=Q(n,o,l),u=1==t.text.length,c=g(t.text).length+(u?i:0);if(s)for(var f=0;f<s.length;++f){var h=s[f];if(null==h.to){var d=_(a,h.marker);d?u&&(h.to=null==d.to?null:d.to+c):h.to=i}}if(a)for(var p=0;p<a.length;++p){var v=a[p];null!=v.to&&(v.to+=c),null==v.from?_(s,v.marker)||(v.from=c,u&&(s||(s=[])).push(v)):(v.from+=c,u&&(s||(s=[])).push(v))}s&&(s=ee(s)),a&&a!=s&&(a=ee(a));var m=[s];if(!u){var y,b=t.text.length-2;if(b>0&&s)for(var w=0;w<s.length;++w)null==s[w].to&&(y||(y=[])).push(new Y(s[w].marker,null,null));for(var x=0;x<b;++x)m.push(y);m.push(a)}return m}function ee(e){for(var t=0;t<e.length;++t){var r=e[t];null!=r.from&&r.from==r.to&&!1!==r.marker.clearWhenEmpty&&e.splice(t--,1)}return e.length?e:null}function te(e,t,r){var n=null;if(e.iter(t.line,r.line+1,function(e){if(e.markedSpans)for(var t=0;t<e.markedSpans.length;++t){var r=e.markedSpans[t].marker;!r.readOnly||n&&-1!=h(n,r)||(n||(n=[])).push(r)}}),!n)return null;for(var i=[{from:t,to:r}],o=0;o<n.length;++o)for(var l=n[o],s=l.find(0),a=0;a<i.length;++a){var u=i[a];if(!(P(u.to,s.from)<0||P(u.from,s.to)>0)){var c=[a,1],f=P(u.from,s.from),d=P(u.to,s.to);(f<0||!l.inclusiveLeft&&!f)&&c.push({from:u.from,to:s.from}),(d>0||!l.inclusiveRight&&!d)&&c.push({from:s.to,to:u.to}),i.splice.apply(i,c),a+=c.length-3}}return i}function re(e){var t=e.markedSpans;if(t){for(var r=0;r<t.length;++r)t[r].marker.detachLine(e);e.markedSpans=null}}function ne(e,t){if(t){for(var r=0;r<t.length;++r)t[r].marker.attachLine(e);e.markedSpans=t}}function ie(e){return e.inclusiveLeft?-1:0}function oe(e){return e.inclusiveRight?1:0}function le(e,t){var r=e.lines.length-t.lines.length;if(0!=r)return r;var n=e.find(),i=t.find(),o=P(n.from,i.from)||ie(e)-ie(t);if(o)return-o;var l=P(n.to,i.to)||oe(e)-oe(t);return l||t.id-e.id}function se(e,t){var r,n=_l&&e.markedSpans;if(n)for(var i=void 0,o=0;o<n.length;++o)(i=n[o]).marker.collapsed&&null==(t?i.from:i.to)&&(!r||le(r,i.marker)<0)&&(r=i.marker);return r}function ae(e){return se(e,!0)}function ue(e){return se(e,!1)}function ce(e,t,r,n,i){var o=M(e,t),l=_l&&o.markedSpans;if(l)for(var s=0;s<l.length;++s){var a=l[s];if(a.marker.collapsed){var u=a.marker.find(0),c=P(u.from,r)||ie(a.marker)-ie(i),f=P(u.to,n)||oe(a.marker)-oe(i);if(!(c>=0&&f<=0||c<=0&&f>=0)&&(c<=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.to,r)>=0:P(u.to,r)>0)||c>=0&&(a.marker.inclusiveRight&&i.inclusiveLeft?P(u.from,n)<=0:P(u.from,n)<0)))return!0}}}function fe(e){for(var t;t=ae(e);)e=t.find(-1,!0).line;return e}function he(e){for(var t;t=ue(e);)e=t.find(1,!0).line;return e}function de(e){for(var t,r;t=ue(e);)e=t.find(1,!0).line,(r||(r=[])).push(e);return r}function pe(e,t){var r=M(e,t),n=fe(r);return r==n?t:W(n)}function ge(e,t){if(t>e.lastLine())return t;var r,n=M(e,t);if(!ve(e,n))return t;for(;r=ue(n);)n=r.find(1,!0).line;return W(n)+1}function ve(e,t){var r=_l&&t.markedSpans;if(r)for(var n=void 0,i=0;i<r.length;++i)if((n=r[i]).marker.collapsed){if(null==n.from)return!0;if(!n.marker.widgetNode&&0==n.from&&n.marker.inclusiveLeft&&me(e,t,n))return!0}}function me(e,t,r){if(null==r.to){var n=r.marker.find(1,!0);return me(e,n.line,_(n.line.markedSpans,r.marker))}if(r.marker.inclusiveRight&&r.to==t.text.length)return!0;for(var i=void 0,o=0;o<t.markedSpans.length;++o)if((i=t.markedSpans[o]).marker.collapsed&&!i.marker.widgetNode&&i.from==r.to&&(null==i.to||i.to!=r.from)&&(i.marker.inclusiveLeft||r.marker.inclusiveRight)&&me(e,t,i))return!0}function ye(e){for(var t=0,r=(e=fe(e)).parent,n=0;n<r.lines.length;++n){var i=r.lines[n];if(i==e)break;t+=i.height}for(var o=r.parent;o;r=o,o=r.parent)for(var l=0;l<o.children.length;++l){var s=o.children[l];if(s==r)break;t+=s.height}return t}function be(e){if(0==e.height)return 0;for(var t,r=e.text.length,n=e;t=ae(n);){var i=t.find(0,!0);n=i.from.line,r+=i.from.ch-i.to.ch}for(n=e;t=ue(n);){var o=t.find(0,!0);r-=n.text.length-o.from.ch,r+=(n=o.to.line).text.length-o.to.ch}return r}function we(e){var t=e.display,r=e.doc;t.maxLine=M(r,r.first),t.maxLineLength=be(t.maxLine),t.maxLineChanged=!0,r.iter(function(e){var r=be(e);r>t.maxLineLength&&(t.maxLineLength=r,t.maxLine=e)})}function xe(e,t,r,n){if(!e)return n(t,r,"ltr",0);for(var i=!1,o=0;o<e.length;++o){var l=e[o];(l.from<r&&l.to>t||t==r&&l.to==t)&&(n(Math.max(l.from,t),Math.min(l.to,r),1==l.level?"rtl":"ltr",o),i=!0)}i||n(t,r,"ltr")}function Ce(e,t,r){var n;$l=null;for(var i=0;i<e.length;++i){var o=e[i];if(o.from<t&&o.to>t)return i;o.to==t&&(o.from!=o.to&&"before"==r?n=i:$l=i),o.from==t&&(o.from!=o.to&&"before"!=r?n=i:$l=i)}return null!=n?n:$l}function Se(e,t){var r=e.order;return null==r&&(r=e.order=ql(e.text,t)),r}function Le(e,t){return e._handlers&&e._handlers[t]||Zl}function ke(e,t,r){if(e.removeEventListener)e.removeEventListener(t,r,!1);else if(e.detachEvent)e.detachEvent("on"+t,r);else{var n=e._handlers,i=n&&n[t];if(i){var o=h(i,r);o>-1&&(n[t]=i.slice(0,o).concat(i.slice(o+1)))}}}function Te(e,t){var r=Le(e,t);if(r.length)for(var n=Array.prototype.slice.call(arguments,2),i=0;i<r.length;++i)r[i].apply(null,n)}function Me(e,t,r){return"string"==typeof t&&(t={type:t,preventDefault:function(){this.defaultPrevented=!0}}),Te(e,r||t.type,e,t),He(t)||t.codemirrorIgnore}function Ne(e){var t=e._handlers&&e._handlers.cursorActivity;if(t)for(var r=e.curOp.cursorActivityHandlers||(e.curOp.cursorActivityHandlers=[]),n=0;n<t.length;++n)-1==h(r,t[n])&&r.push(t[n])}function Oe(e,t){return Le(e,t).length>0}function Ae(e){e.prototype.on=function(e,t){Ql(this,e,t)},e.prototype.off=function(e,t){ke(this,e,t)}}function We(e){e.preventDefault?e.preventDefault():e.returnValue=!1}function De(e){e.stopPropagation?e.stopPropagation():e.cancelBubble=!0}function He(e){return null!=e.defaultPrevented?e.defaultPrevented:0==e.returnValue}function Fe(e){We(e),De(e)}function Ee(e){return e.target||e.srcElement}function Pe(e){var t=e.which;return null==t&&(1&e.button?t=1:2&e.button?t=3:4&e.button&&(t=2)),Ml&&e.ctrlKey&&1==t&&(t=3),t}function Ie(e){if(null==Il){var t=n("span","​");r(e,n("span",[t,document.createTextNode("x")])),0!=e.firstChild.offsetHeight&&(Il=t.offsetWidth<=1&&t.offsetHeight>2&&!(gl&&vl<8))}var i=Il?n("span","​"):n("span"," ",null,"display: inline-block; width: 1px; margin-right: -1px");return i.setAttribute("cm-text",""),i}function ze(e){if(null!=zl)return zl;var n=r(e,document.createTextNode("AخA")),i=Wl(n,0,1).getBoundingClientRect(),o=Wl(n,1,2).getBoundingClientRect();return t(e),!(!i||i.left==i.right)&&(zl=o.right-i.right<3)}function Re(e){if(null!=ns)return ns;var t=r(e,n("span","x")),i=t.getBoundingClientRect(),o=Wl(t,0,1).getBoundingClientRect();return ns=Math.abs(i.left-o.left)>1}function Be(e,t){arguments.length>2&&(t.dependencies=Array.prototype.slice.call(arguments,2)),is[e]=t}function Ge(e){if("string"==typeof e&&os.hasOwnProperty(e))e=os[e];else if(e&&"string"==typeof e.name&&os.hasOwnProperty(e.name)){var t=os[e.name];"string"==typeof t&&(t={name:t}),(e=b(t,e)).name=t.name}else{if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+xml$/.test(e))return Ge("application/xml");if("string"==typeof e&&/^[\w\-]+\/[\w\-]+\+json$/.test(e))return Ge("application/json")}return"string"==typeof e?{name:e}:e||{name:"null"}}function Ue(e,t){t=Ge(t);var r=is[t.name];if(!r)return Ue(e,"text/plain");var n=r(e,t);if(ls.hasOwnProperty(t.name)){var i=ls[t.name];for(var o in i)i.hasOwnProperty(o)&&(n.hasOwnProperty(o)&&(n["_"+o]=n[o]),n[o]=i[o])}if(n.name=t.name,t.helperType&&(n.helperType=t.helperType),t.modeProps)for(var l in t.modeProps)n[l]=t.modeProps[l];return n}function Ve(e,t){c(t,ls.hasOwnProperty(e)?ls[e]:ls[e]={})}function Ke(e,t){if(!0===t)return t;if(e.copyState)return e.copyState(t);var r={};for(var n in t){var i=t[n];i instanceof Array&&(i=i.concat([])),r[n]=i}return r}function je(e,t){for(var r;e.innerMode&&(r=e.innerMode(t))&&r.mode!=e;)t=r.state,e=r.mode;return r||{mode:e,state:t}}function Xe(e,t,r){return!e.startState||e.startState(t,r)}function Ye(e,t,r,n){var i=[e.state.modeGen],o={};tt(e,t.text,e.doc.mode,r,function(e,t){return i.push(e,t)},o,n);for(var l=r.state,s=0;s<e.state.overlays.length;++s)!function(n){var l=e.state.overlays[n],s=1,a=0;r.state=!0,tt(e,t.text,l.mode,r,function(e,t){for(var r=s;a<e;){var n=i[s];n>e&&i.splice(s,1,e,i[s+1],n),s+=2,a=Math.min(e,n)}if(t)if(l.opaque)i.splice(r,s-r,e,"overlay "+t),s=r+2;else for(;r<s;r+=2){var o=i[r+1];i[r+1]=(o?o+" ":"")+"overlay "+t}},o)}(s);return r.state=l,{styles:i,classes:o.bgClass||o.textClass?o:null}}function _e(e,t,r){if(!t.styles||t.styles[0]!=e.state.modeGen){var n=$e(e,W(t)),i=t.text.length>e.options.maxHighlightLength&&Ke(e.doc.mode,n.state),o=Ye(e,t,n);i&&(n.state=i),t.stateAfter=n.save(!i),t.styles=o.styles,o.classes?t.styleClasses=o.classes:t.styleClasses&&(t.styleClasses=null),r===e.doc.highlightFrontier&&(e.doc.modeFrontier=Math.max(e.doc.modeFrontier,++e.doc.highlightFrontier))}return t.styles}function $e(e,t,r){var n=e.doc,i=e.display;if(!n.mode.startState)return new us(n,!0,t);var o=rt(e,t,r),l=o>n.first&&M(n,o-1).stateAfter,s=l?us.fromSaved(n,l,o):new us(n,Xe(n.mode),o);return n.iter(o,t,function(r){qe(e,r.text,s);var n=s.line;r.stateAfter=n==t-1||n%5==0||n>=i.viewFrom&&n<i.viewTo?s.save():null,s.nextLine()}),r&&(n.modeFrontier=s.line),s}function qe(e,t,r,n){var i=e.doc.mode,o=new ss(t,e.options.tabSize,r);for(o.start=o.pos=n||0,""==t&&Ze(i,r.state);!o.eol();)Qe(i,o,r.state),o.start=o.pos}function Ze(e,t){if(e.blankLine)return e.blankLine(t);if(e.innerMode){var r=je(e,t);return r.mode.blankLine?r.mode.blankLine(r.state):void 0}}function Qe(e,t,r,n){for(var i=0;i<10;i++){n&&(n[0]=je(e,r).mode);var o=e.token(t,r);if(t.pos>t.start)return o}throw new Error("Mode "+e.name+" failed to advance stream.")}function Je(e,t,r,n){var i,o,l=e.doc,s=l.mode,a=M(l,(t=U(l,t)).line),u=$e(e,t.line,r),c=new ss(a.text,e.options.tabSize,u);for(n&&(o=[]);(n||c.pos<t.ch)&&!c.eol();)c.start=c.pos,i=Qe(s,c,u.state),n&&o.push(new cs(c,i,Ke(l.mode,u.state)));return n?o:new cs(c,i,u.state)}function et(e,t){if(e)for(;;){var r=e.match(/(?:^|\s+)line-(background-)?(\S+)/);if(!r)break;e=e.slice(0,r.index)+e.slice(r.index+r[0].length);var n=r[1]?"bgClass":"textClass";null==t[n]?t[n]=r[2]:new RegExp("(?:^|s)"+r[2]+"(?:$|s)").test(t[n])||(t[n]+=" "+r[2])}return e}function tt(e,t,r,n,i,o,l){var s=r.flattenSpans;null==s&&(s=e.options.flattenSpans);var a,u=0,c=null,f=new ss(t,e.options.tabSize,n),h=e.options.addModeClass&&[null];for(""==t&&et(Ze(r,n.state),o);!f.eol();){if(f.pos>e.options.maxHighlightLength?(s=!1,l&&qe(e,t,n,f.pos),f.pos=t.length,a=null):a=et(Qe(r,f,n.state,h),o),h){var d=h[0].name;d&&(a="m-"+(a?d+" "+a:d))}if(!s||c!=a){for(;u<f.start;)i(u=Math.min(f.start,u+5e3),c);c=a}f.start=f.pos}for(;u<f.pos;){var p=Math.min(f.pos,u+5e3);i(p,c),u=p}}function rt(e,t,r){for(var n,i,o=e.doc,l=r?-1:t-(e.doc.mode.innerMode?1e3:100),s=t;s>l;--s){if(s<=o.first)return o.first;var a=M(o,s-1),u=a.stateAfter;if(u&&(!r||s+(u instanceof as?u.lookAhead:0)<=o.modeFrontier))return s;var c=f(a.text,null,e.options.tabSize);(null==i||n>c)&&(i=s-1,n=c)}return i}function nt(e,t){if(e.modeFrontier=Math.min(e.modeFrontier,t),!(e.highlightFrontier<t-10)){for(var r=e.first,n=t-1;n>r;n--){var i=M(e,n).stateAfter;if(i&&(!(i instanceof as)||n+i.lookAhead<t)){r=n+1;break}}e.highlightFrontier=Math.min(e.highlightFrontier,r)}}function it(e,t,r,n){e.text=t,e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null),null!=e.order&&(e.order=null),re(e),ne(e,r);var i=n?n(e):1;i!=e.height&&A(e,i)}function ot(e){e.parent=null,re(e)}function lt(e,t){if(!e||/^\s*$/.test(e))return null;var r=t.addModeClass?ps:ds;return r[e]||(r[e]=e.replace(/\S+/g,"cm-$&"))}function st(e,t){var r=i("span",null,null,ml?"padding-right: .1px":null),n={pre:i("pre",[r],"CodeMirror-line"),content:r,col:0,pos:0,cm:e,trailingSpace:!1,splitSpaces:(gl||ml)&&e.getOption("lineWrapping")};t.measure={};for(var o=0;o<=(t.rest?t.rest.length:0);o++){var l=o?t.rest[o-1]:t.line,s=void 0;n.pos=0,n.addToken=ut,ze(e.display.measure)&&(s=Se(l,e.doc.direction))&&(n.addToken=ft(n.addToken,s)),n.map=[],dt(l,n,_e(e,l,t!=e.display.externalMeasured&&W(l))),l.styleClasses&&(l.styleClasses.bgClass&&(n.bgClass=a(l.styleClasses.bgClass,n.bgClass||"")),l.styleClasses.textClass&&(n.textClass=a(l.styleClasses.textClass,n.textClass||""))),0==n.map.length&&n.map.push(0,0,n.content.appendChild(Ie(e.display.measure))),0==o?(t.measure.map=n.map,t.measure.cache={}):((t.measure.maps||(t.measure.maps=[])).push(n.map),(t.measure.caches||(t.measure.caches=[])).push({}))}if(ml){var u=n.content.lastChild;(/\bcm-tab\b/.test(u.className)||u.querySelector&&u.querySelector(".cm-tab"))&&(n.content.className="cm-tab-wrap-hack")}return Te(e,"renderLine",e,t.line,n.pre),n.pre.className&&(n.textClass=a(n.pre.className,n.textClass||"")),n}function at(e){var t=n("span","•","cm-invalidchar");return t.title="\\u"+e.charCodeAt(0).toString(16),t.setAttribute("aria-label",t.title),t}function ut(e,t,r,i,o,l,s){if(t){var a,u=e.splitSpaces?ct(t,e.trailingSpace):t,c=e.cm.state.specialChars,f=!1;if(c.test(t)){a=document.createDocumentFragment();for(var h=0;;){c.lastIndex=h;var d=c.exec(t),g=d?d.index-h:t.length-h;if(g){var v=document.createTextNode(u.slice(h,h+g));gl&&vl<9?a.appendChild(n("span",[v])):a.appendChild(v),e.map.push(e.pos,e.pos+g,v),e.col+=g,e.pos+=g}if(!d)break;h+=g+1;var m=void 0;if("\t"==d[0]){var y=e.cm.options.tabSize,b=y-e.col%y;(m=a.appendChild(n("span",p(b),"cm-tab"))).setAttribute("role","presentation"),m.setAttribute("cm-text","\t"),e.col+=b}else"\r"==d[0]||"\n"==d[0]?((m=a.appendChild(n("span","\r"==d[0]?"␍":"␤","cm-invalidchar"))).setAttribute("cm-text",d[0]),e.col+=1):((m=e.cm.options.specialCharPlaceholder(d[0])).setAttribute("cm-text",d[0]),gl&&vl<9?a.appendChild(n("span",[m])):a.appendChild(m),e.col+=1);e.map.push(e.pos,e.pos+1,m),e.pos++}}else e.col+=t.length,a=document.createTextNode(u),e.map.push(e.pos,e.pos+t.length,a),gl&&vl<9&&(f=!0),e.pos+=t.length;if(e.trailingSpace=32==u.charCodeAt(t.length-1),r||i||o||f||s){var w=r||"";i&&(w+=i),o&&(w+=o);var x=n("span",[a],w,s);return l&&(x.title=l),e.content.appendChild(x)}e.content.appendChild(a)}}function ct(e,t){if(e.length>1&&!/  /.test(e))return e;for(var r=t,n="",i=0;i<e.length;i++){var o=e.charAt(i);" "!=o||!r||i!=e.length-1&&32!=e.charCodeAt(i+1)||(o=" "),n+=o,r=" "==o}return n}function ft(e,t){return function(r,n,i,o,l,s,a){i=i?i+" cm-force-border":"cm-force-border";for(var u=r.pos,c=u+n.length;;){for(var f=void 0,h=0;h<t.length&&!((f=t[h]).to>u&&f.from<=u);h++);if(f.to>=c)return e(r,n,i,o,l,s,a);e(r,n.slice(0,f.to-u),i,o,null,s,a),o=null,n=n.slice(f.to-u),u=f.to}}}function ht(e,t,r,n){var i=!n&&r.widgetNode;i&&e.map.push(e.pos,e.pos+t,i),!n&&e.cm.display.input.needsContentAttribute&&(i||(i=e.content.appendChild(document.createElement("span"))),i.setAttribute("cm-marker",r.id)),i&&(e.cm.display.input.setUneditable(i),e.content.appendChild(i)),e.pos+=t,e.trailingSpace=!1}function dt(e,t,r){var n=e.markedSpans,i=e.text,o=0;if(n)for(var l,s,a,u,c,f,h,d=i.length,p=0,g=1,v="",m=0;;){if(m==p){a=u=c=f=s="",h=null,m=1/0;for(var y=[],b=void 0,w=0;w<n.length;++w){var x=n[w],C=x.marker;"bookmark"==C.type&&x.from==p&&C.widgetNode?y.push(C):x.from<=p&&(null==x.to||x.to>p||C.collapsed&&x.to==p&&x.from==p)?(null!=x.to&&x.to!=p&&m>x.to&&(m=x.to,u=""),C.className&&(a+=" "+C.className),C.css&&(s=(s?s+";":"")+C.css),C.startStyle&&x.from==p&&(c+=" "+C.startStyle),C.endStyle&&x.to==m&&(b||(b=[])).push(C.endStyle,x.to),C.title&&!f&&(f=C.title),C.collapsed&&(!h||le(h.marker,C)<0)&&(h=x)):x.from>p&&m>x.from&&(m=x.from)}if(b)for(var S=0;S<b.length;S+=2)b[S+1]==m&&(u+=" "+b[S]);if(!h||h.from==p)for(var L=0;L<y.length;++L)ht(t,0,y[L]);if(h&&(h.from||0)==p){if(ht(t,(null==h.to?d+1:h.to)-p,h.marker,null==h.from),null==h.to)return;h.to==p&&(h=!1)}}if(p>=d)break;for(var k=Math.min(d,m);;){if(v){var T=p+v.length;if(!h){var M=T>k?v.slice(0,k-p):v;t.addToken(t,M,l?l+a:a,c,p+M.length==m?u:"",f,s)}if(T>=k){v=v.slice(k-p),p=k;break}p=T,c=""}v=i.slice(o,o=r[g++]),l=lt(r[g++],t.cm.options)}}else for(var N=1;N<r.length;N+=2)t.addToken(t,i.slice(o,o=r[N]),lt(r[N+1],t.cm.options))}function pt(e,t,r){this.line=t,this.rest=de(t),this.size=this.rest?W(g(this.rest))-r+1:1,this.node=this.text=null,this.hidden=ve(e,t)}function gt(e,t,r){for(var n,i=[],o=t;o<r;o=n){var l=new pt(e.doc,M(e.doc,o),o);n=o+l.size,i.push(l)}return i}function vt(e){gs?gs.ops.push(e):e.ownsGroup=gs={ops:[e],delayedCallbacks:[]}}function mt(e){var t=e.delayedCallbacks,r=0;do{for(;r<t.length;r++)t[r].call(null);for(var n=0;n<e.ops.length;n++){var i=e.ops[n];if(i.cursorActivityHandlers)for(;i.cursorActivityCalled<i.cursorActivityHandlers.length;)i.cursorActivityHandlers[i.cursorActivityCalled++].call(null,i.cm)}}while(r<t.length)}function yt(e,t){var r=e.ownsGroup;if(r)try{mt(r)}finally{gs=null,t(r)}}function bt(e,t){var r=Le(e,t);if(r.length){var n,i=Array.prototype.slice.call(arguments,2);gs?n=gs.delayedCallbacks:vs?n=vs:(n=vs=[],setTimeout(wt,0));for(var o=0;o<r.length;++o)!function(e){n.push(function(){return r[e].apply(null,i)})}(o)}}function wt(){var e=vs;vs=null;for(var t=0;t<e.length;++t)e[t]()}function xt(e,t,r,n){for(var i=0;i<t.changes.length;i++){var o=t.changes[i];"text"==o?kt(e,t):"gutter"==o?Mt(e,t,r,n):"class"==o?Tt(e,t):"widget"==o&&Nt(e,t,n)}t.changes=null}function Ct(e){return e.node==e.text&&(e.node=n("div",null,null,"position: relative"),e.text.parentNode&&e.text.parentNode.replaceChild(e.node,e.text),e.node.appendChild(e.text),gl&&vl<8&&(e.node.style.zIndex=2)),e.node}function St(e,t){var r=t.bgClass?t.bgClass+" "+(t.line.bgClass||""):t.line.bgClass;if(r&&(r+=" CodeMirror-linebackground"),t.background)r?t.background.className=r:(t.background.parentNode.removeChild(t.background),t.background=null);else if(r){var i=Ct(t);t.background=i.insertBefore(n("div",null,r),i.firstChild),e.display.input.setUneditable(t.background)}}function Lt(e,t){var r=e.display.externalMeasured;return r&&r.line==t.line?(e.display.externalMeasured=null,t.measure=r.measure,r.built):st(e,t)}function kt(e,t){var r=t.text.className,n=Lt(e,t);t.text==t.node&&(t.node=n.pre),t.text.parentNode.replaceChild(n.pre,t.text),t.text=n.pre,n.bgClass!=t.bgClass||n.textClass!=t.textClass?(t.bgClass=n.bgClass,t.textClass=n.textClass,Tt(e,t)):r&&(t.text.className=r)}function Tt(e,t){St(e,t),t.line.wrapClass?Ct(t).className=t.line.wrapClass:t.node!=t.text&&(t.node.className="");var r=t.textClass?t.textClass+" "+(t.line.textClass||""):t.line.textClass;t.text.className=r||""}function Mt(e,t,r,i){if(t.gutter&&(t.node.removeChild(t.gutter),t.gutter=null),t.gutterBackground&&(t.node.removeChild(t.gutterBackground),t.gutterBackground=null),t.line.gutterClass){var o=Ct(t);t.gutterBackground=n("div",null,"CodeMirror-gutter-background "+t.line.gutterClass,"left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px; width: "+i.gutterTotalWidth+"px"),e.display.input.setUneditable(t.gutterBackground),o.insertBefore(t.gutterBackground,t.text)}var l=t.line.gutterMarkers;if(e.options.lineNumbers||l){var s=Ct(t),a=t.gutter=n("div",null,"CodeMirror-gutter-wrapper","left: "+(e.options.fixedGutter?i.fixedPos:-i.gutterTotalWidth)+"px");if(e.display.input.setUneditable(a),s.insertBefore(a,t.text),t.line.gutterClass&&(a.className+=" "+t.line.gutterClass),!e.options.lineNumbers||l&&l["CodeMirror-linenumbers"]||(t.lineNumber=a.appendChild(n("div",F(e.options,r),"CodeMirror-linenumber CodeMirror-gutter-elt","left: "+i.gutterLeft["CodeMirror-linenumbers"]+"px; width: "+e.display.lineNumInnerWidth+"px"))),l)for(var u=0;u<e.options.gutters.length;++u){var c=e.options.gutters[u],f=l.hasOwnProperty(c)&&l[c];f&&a.appendChild(n("div",[f],"CodeMirror-gutter-elt","left: "+i.gutterLeft[c]+"px; width: "+i.gutterWidth[c]+"px"))}}}function Nt(e,t,r){t.alignable&&(t.alignable=null);for(var n=t.node.firstChild,i=void 0;n;n=i)i=n.nextSibling,"CodeMirror-linewidget"==n.className&&t.node.removeChild(n);At(e,t,r)}function Ot(e,t,r,n){var i=Lt(e,t);return t.text=t.node=i.pre,i.bgClass&&(t.bgClass=i.bgClass),i.textClass&&(t.textClass=i.textClass),Tt(e,t),Mt(e,t,r,n),At(e,t,n),t.node}function At(e,t,r){if(Wt(e,t.line,t,r,!0),t.rest)for(var n=0;n<t.rest.length;n++)Wt(e,t.rest[n],t,r,!1)}function Wt(e,t,r,i,o){if(t.widgets)for(var l=Ct(r),s=0,a=t.widgets;s<a.length;++s){var u=a[s],c=n("div",[u.node],"CodeMirror-linewidget");u.handleMouseEvents||c.setAttribute("cm-ignore-events","true"),Dt(u,c,r,i),e.display.input.setUneditable(c),o&&u.above?l.insertBefore(c,r.gutter||r.text):l.appendChild(c),bt(u,"redraw")}}function Dt(e,t,r,n){if(e.noHScroll){(r.alignable||(r.alignable=[])).push(t);var i=n.wrapperWidth;t.style.left=n.fixedPos+"px",e.coverGutter||(i-=n.gutterTotalWidth,t.style.paddingLeft=n.gutterTotalWidth+"px"),t.style.width=i+"px"}e.coverGutter&&(t.style.zIndex=5,t.style.position="relative",e.noHScroll||(t.style.marginLeft=-n.gutterTotalWidth+"px"))}function Ht(e){if(null!=e.height)return e.height;var t=e.doc.cm;if(!t)return 0;if(!o(document.body,e.node)){var i="position: relative;";e.coverGutter&&(i+="margin-left: -"+t.display.gutters.offsetWidth+"px;"),e.noHScroll&&(i+="width: "+t.display.wrapper.clientWidth+"px;"),r(t.display.measure,n("div",[e.node],null,i))}return e.height=e.node.parentNode.offsetHeight}function Ft(e,t){for(var r=Ee(t);r!=e.wrapper;r=r.parentNode)if(!r||1==r.nodeType&&"true"==r.getAttribute("cm-ignore-events")||r.parentNode==e.sizer&&r!=e.mover)return!0}function Et(e){return e.lineSpace.offsetTop}function Pt(e){return e.mover.offsetHeight-e.lineSpace.offsetHeight}function It(e){if(e.cachedPaddingH)return e.cachedPaddingH;var t=r(e.measure,n("pre","x")),i=window.getComputedStyle?window.getComputedStyle(t):t.currentStyle,o={left:parseInt(i.paddingLeft),right:parseInt(i.paddingRight)};return isNaN(o.left)||isNaN(o.right)||(e.cachedPaddingH=o),o}function zt(e){return Rl-e.display.nativeBarWidth}function Rt(e){return e.display.scroller.clientWidth-zt(e)-e.display.barWidth}function Bt(e){return e.display.scroller.clientHeight-zt(e)-e.display.barHeight}function Gt(e,t,r){var n=e.options.lineWrapping,i=n&&Rt(e);if(!t.measure.heights||n&&t.measure.width!=i){var o=t.measure.heights=[];if(n){t.measure.width=i;for(var l=t.text.firstChild.getClientRects(),s=0;s<l.length-1;s++){var a=l[s],u=l[s+1];Math.abs(a.bottom-u.bottom)>2&&o.push((a.bottom+u.top)/2-r.top)}}o.push(r.bottom-r.top)}}function Ut(e,t,r){if(e.line==t)return{map:e.measure.map,cache:e.measure.cache};for(var n=0;n<e.rest.length;n++)if(e.rest[n]==t)return{map:e.measure.maps[n],cache:e.measure.caches[n]};for(var i=0;i<e.rest.length;i++)if(W(e.rest[i])>r)return{map:e.measure.maps[i],cache:e.measure.caches[i],before:!0}}function Vt(e,t){var n=W(t=fe(t)),i=e.display.externalMeasured=new pt(e.doc,t,n);i.lineN=n;var o=i.built=st(e,i);return i.text=o.pre,r(e.display.lineMeasure,o.pre),i}function Kt(e,t,r,n){return Yt(e,Xt(e,t),r,n)}function jt(e,t){if(t>=e.display.viewFrom&&t<e.display.viewTo)return e.display.view[Lr(e,t)];var r=e.display.externalMeasured;return r&&t>=r.lineN&&t<r.lineN+r.size?r:void 0}function Xt(e,t){var r=W(t),n=jt(e,r);n&&!n.text?n=null:n&&n.changes&&(xt(e,n,r,br(e)),e.curOp.forceUpdate=!0),n||(n=Vt(e,t));var i=Ut(n,t,r);return{line:t,view:n,rect:null,map:i.map,cache:i.cache,before:i.before,hasHeights:!1}}function Yt(e,t,r,n,i){t.before&&(r=-1);var o,l=r+(n||"");return t.cache.hasOwnProperty(l)?o=t.cache[l]:(t.rect||(t.rect=t.view.text.getBoundingClientRect()),t.hasHeights||(Gt(e,t.view,t.rect),t.hasHeights=!0),(o=qt(e,t,r,n)).bogus||(t.cache[l]=o)),{left:o.left,right:o.right,top:i?o.rtop:o.top,bottom:i?o.rbottom:o.bottom}}function _t(e,t,r){for(var n,i,o,l,s,a,u=0;u<e.length;u+=3)if(s=e[u],a=e[u+1],t<s?(i=0,o=1,l="left"):t<a?o=(i=t-s)+1:(u==e.length-3||t==a&&e[u+3]>t)&&(i=(o=a-s)-1,t>=a&&(l="right")),null!=i){if(n=e[u+2],s==a&&r==(n.insertLeft?"left":"right")&&(l=r),"left"==r&&0==i)for(;u&&e[u-2]==e[u-3]&&e[u-1].insertLeft;)n=e[2+(u-=3)],l="left";if("right"==r&&i==a-s)for(;u<e.length-3&&e[u+3]==e[u+4]&&!e[u+5].insertLeft;)n=e[(u+=3)+2],l="right";break}return{node:n,start:i,end:o,collapse:l,coverStart:s,coverEnd:a}}function $t(e,t){var r=ms;if("left"==t)for(var n=0;n<e.length&&(r=e[n]).left==r.right;n++);else for(var i=e.length-1;i>=0&&(r=e[i]).left==r.right;i--);return r}function qt(e,t,r,n){var i,o=_t(t.map,r,n),l=o.node,s=o.start,a=o.end,u=o.collapse;if(3==l.nodeType){for(var c=0;c<4;c++){for(;s&&S(t.line.text.charAt(o.coverStart+s));)--s;for(;o.coverStart+a<o.coverEnd&&S(t.line.text.charAt(o.coverStart+a));)++a;if((i=gl&&vl<9&&0==s&&a==o.coverEnd-o.coverStart?l.parentNode.getBoundingClientRect():$t(Wl(l,s,a).getClientRects(),n)).left||i.right||0==s)break;a=s,s-=1,u="right"}gl&&vl<11&&(i=Zt(e.display.measure,i))}else{s>0&&(u=n="right");var f;i=e.options.lineWrapping&&(f=l.getClientRects()).length>1?f["right"==n?f.length-1:0]:l.getBoundingClientRect()}if(gl&&vl<9&&!s&&(!i||!i.left&&!i.right)){var h=l.parentNode.getClientRects()[0];i=h?{left:h.left,right:h.left+yr(e.display),top:h.top,bottom:h.bottom}:ms}for(var d=i.top-t.rect.top,p=i.bottom-t.rect.top,g=(d+p)/2,v=t.view.measure.heights,m=0;m<v.length-1&&!(g<v[m]);m++);var y=m?v[m-1]:0,b=v[m],w={left:("right"==u?i.right:i.left)-t.rect.left,right:("left"==u?i.left:i.right)-t.rect.left,top:y,bottom:b};return i.left||i.right||(w.bogus=!0),e.options.singleCursorHeightPerLine||(w.rtop=d,w.rbottom=p),w}function Zt(e,t){if(!window.screen||null==screen.logicalXDPI||screen.logicalXDPI==screen.deviceXDPI||!Re(e))return t;var r=screen.logicalXDPI/screen.deviceXDPI,n=screen.logicalYDPI/screen.deviceYDPI;return{left:t.left*r,right:t.right*r,top:t.top*n,bottom:t.bottom*n}}function Qt(e){if(e.measure&&(e.measure.cache={},e.measure.heights=null,e.rest))for(var t=0;t<e.rest.length;t++)e.measure.caches[t]={}}function Jt(e){e.display.externalMeasure=null,t(e.display.lineMeasure);for(var r=0;r<e.display.view.length;r++)Qt(e.display.view[r])}function er(e){Jt(e),e.display.cachedCharWidth=e.display.cachedTextHeight=e.display.cachedPaddingH=null,e.options.lineWrapping||(e.display.maxLineChanged=!0),e.display.lineNumChars=null}function tr(){return bl&&kl?-(document.body.getBoundingClientRect().left-parseInt(getComputedStyle(document.body).marginLeft)):window.pageXOffset||(document.documentElement||document.body).scrollLeft}function rr(){return bl&&kl?-(document.body.getBoundingClientRect().top-parseInt(getComputedStyle(document.body).marginTop)):window.pageYOffset||(document.documentElement||document.body).scrollTop}function nr(e){var t=0;if(e.widgets)for(var r=0;r<e.widgets.length;++r)e.widgets[r].above&&(t+=Ht(e.widgets[r]));return t}function ir(e,t,r,n,i){if(!i){var o=nr(t);r.top+=o,r.bottom+=o}if("line"==n)return r;n||(n="local");var l=ye(t);if("local"==n?l+=Et(e.display):l-=e.display.viewOffset,"page"==n||"window"==n){var s=e.display.lineSpace.getBoundingClientRect();l+=s.top+("window"==n?0:rr());var a=s.left+("window"==n?0:tr());r.left+=a,r.right+=a}return r.top+=l,r.bottom+=l,r}function or(e,t,r){if("div"==r)return t;var n=t.left,i=t.top;if("page"==r)n-=tr(),i-=rr();else if("local"==r||!r){var o=e.display.sizer.getBoundingClientRect();n+=o.left,i+=o.top}var l=e.display.lineSpace.getBoundingClientRect();return{left:n-l.left,top:i-l.top}}function lr(e,t,r,n,i){return n||(n=M(e.doc,t.line)),ir(e,n,Kt(e,n,t.ch,i),r)}function sr(e,t,r,n,i,o){function l(t,l){var s=Yt(e,i,t,l?"right":"left",o);return l?s.left=s.right:s.right=s.left,ir(e,n,s,r)}function s(e,t,r){var n=1==a[t].level;return l(r?e-1:e,n!=r)}n=n||M(e.doc,t.line),i||(i=Xt(e,n));var a=Se(n,e.doc.direction),u=t.ch,c=t.sticky;if(u>=n.text.length?(u=n.text.length,c="before"):u<=0&&(u=0,c="after"),!a)return l("before"==c?u-1:u,"before"==c);var f=Ce(a,u,c),h=$l,d=s(u,f,"before"==c);return null!=h&&(d.other=s(u,h,"before"!=c)),d}function ar(e,t){var r=0;t=U(e.doc,t),e.options.lineWrapping||(r=yr(e.display)*t.ch);var n=M(e.doc,t.line),i=ye(n)+Et(e.display);return{left:r,right:r,top:i,bottom:i+n.height}}function ur(e,t,r,n,i){var o=E(e,t,r);return o.xRel=i,n&&(o.outside=!0),o}function cr(e,t,r){var n=e.doc;if((r+=e.display.viewOffset)<0)return ur(n.first,0,null,!0,-1);var i=D(n,r),o=n.first+n.size-1;if(i>o)return ur(n.first+n.size-1,M(n,o).text.length,null,!0,1);t<0&&(t=0);for(var l=M(n,i);;){var s=pr(e,l,i,t,r),a=ue(l),u=a&&a.find(0,!0);if(!a||!(s.ch>u.from.ch||s.ch==u.from.ch&&s.xRel>0))return s;i=W(l=u.to.line)}}function fr(e,t,r,n){n-=nr(t);var i=t.text.length,o=k(function(t){return Yt(e,r,t-1).bottom<=n},i,0);return i=k(function(t){return Yt(e,r,t).top>n},o,i),{begin:o,end:i}}function hr(e,t,r,n){return r||(r=Xt(e,t)),fr(e,t,r,ir(e,t,Yt(e,r,n),"line").top)}function dr(e,t,r,n){return!(e.bottom<=r)&&(e.top>r||(n?e.left:e.right)>t)}function pr(e,t,r,n,i){i-=ye(t);var o=Xt(e,t),l=nr(t),s=0,a=t.text.length,u=!0,c=Se(t,e.doc.direction);if(c){var f=(e.options.lineWrapping?vr:gr)(e,t,r,o,c,n,i);s=(u=1!=f.level)?f.from:f.to-1,a=u?f.to:f.from-1}var h,d,p=null,g=null,v=k(function(t){var r=Yt(e,o,t);return r.top+=l,r.bottom+=l,!!dr(r,n,i,!1)&&(r.top<=i&&r.left<=n&&(p=t,g=r),!0)},s,a),m=!1;if(g){var y=n-g.left<g.right-n,b=y==u;v=p+(b?0:1),d=b?"after":"before",h=y?g.left:g.right}else{u||v!=a&&v!=s||v++,d=0==v?"after":v==t.text.length?"before":Yt(e,o,v-(u?1:0)).bottom+l<=i==u?"after":"before";var w=sr(e,E(r,v,d),"line",t,o);h=w.left,m=i<w.top||i>=w.bottom}return v=L(t.text,v,1),ur(r,v,d,m,n-h)}function gr(e,t,r,n,i,o,l){var s=k(function(s){var a=i[s],u=1!=a.level;return dr(sr(e,E(r,u?a.to:a.from,u?"before":"after"),"line",t,n),o,l,!0)},0,i.length-1),a=i[s];if(s>0){var u=1!=a.level,c=sr(e,E(r,u?a.from:a.to,u?"after":"before"),"line",t,n);dr(c,o,l,!0)&&c.top>l&&(a=i[s-1])}return a}function vr(e,t,r,n,i,o,l){for(var s=fr(e,t,n,l),a=s.begin,u=s.end,c=null,f=null,h=0;h<i.length;h++){var d=i[h];if(!(d.from>=u||d.to<=a)){var p=Yt(e,n,1!=d.level?Math.min(u,d.to)-1:Math.max(a,d.from)).right,g=p<o?o-p+1e9:p-o;(!c||f>g)&&(c=d,f=g)}}return c||(c=i[i.length-1]),c.from<a&&(c={from:a,to:c.to,level:c.level}),c.to>u&&(c={from:c.from,to:u,level:c.level}),c}function mr(e){if(null!=e.cachedTextHeight)return e.cachedTextHeight;if(null==hs){hs=n("pre");for(var i=0;i<49;++i)hs.appendChild(document.createTextNode("x")),hs.appendChild(n("br"));hs.appendChild(document.createTextNode("x"))}r(e.measure,hs);var o=hs.offsetHeight/50;return o>3&&(e.cachedTextHeight=o),t(e.measure),o||1}function yr(e){if(null!=e.cachedCharWidth)return e.cachedCharWidth;var t=n("span","xxxxxxxxxx"),i=n("pre",[t]);r(e.measure,i);var o=t.getBoundingClientRect(),l=(o.right-o.left)/10;return l>2&&(e.cachedCharWidth=l),l||10}function br(e){for(var t=e.display,r={},n={},i=t.gutters.clientLeft,o=t.gutters.firstChild,l=0;o;o=o.nextSibling,++l)r[e.options.gutters[l]]=o.offsetLeft+o.clientLeft+i,n[e.options.gutters[l]]=o.clientWidth;return{fixedPos:wr(t),gutterTotalWidth:t.gutters.offsetWidth,gutterLeft:r,gutterWidth:n,wrapperWidth:t.wrapper.clientWidth}}function wr(e){return e.scroller.getBoundingClientRect().left-e.sizer.getBoundingClientRect().left}function xr(e){var t=mr(e.display),r=e.options.lineWrapping,n=r&&Math.max(5,e.display.scroller.clientWidth/yr(e.display)-3);return function(i){if(ve(e.doc,i))return 0;var o=0;if(i.widgets)for(var l=0;l<i.widgets.length;l++)i.widgets[l].height&&(o+=i.widgets[l].height);return r?o+(Math.ceil(i.text.length/n)||1)*t:o+t}}function Cr(e){var t=e.doc,r=xr(e);t.iter(function(e){var t=r(e);t!=e.height&&A(e,t)})}function Sr(e,t,r,n){var i=e.display;if(!r&&"true"==Ee(t).getAttribute("cm-not-content"))return null;var o,l,s=i.lineSpace.getBoundingClientRect();try{o=t.clientX-s.left,l=t.clientY-s.top}catch(t){return null}var a,u=cr(e,o,l);if(n&&1==u.xRel&&(a=M(e.doc,u.line).text).length==u.ch){var c=f(a,a.length,e.options.tabSize)-a.length;u=E(u.line,Math.max(0,Math.round((o-It(e.display).left)/yr(e.display))-c))}return u}function Lr(e,t){if(t>=e.display.viewTo)return null;if((t-=e.display.viewFrom)<0)return null;for(var r=e.display.view,n=0;n<r.length;n++)if((t-=r[n].size)<0)return n}function kr(e){e.display.input.showSelection(e.display.input.prepareSelection())}function Tr(e,t){void 0===t&&(t=!0);for(var r=e.doc,n={},i=n.cursors=document.createDocumentFragment(),o=n.selection=document.createDocumentFragment(),l=0;l<r.sel.ranges.length;l++)if(t||l!=r.sel.primIndex){var s=r.sel.ranges[l];if(!(s.from().line>=e.display.viewTo||s.to().line<e.display.viewFrom)){var a=s.empty();(a||e.options.showCursorWhenSelecting)&&Mr(e,s.head,i),a||Or(e,s,o)}}return n}function Mr(e,t,r){var i=sr(e,t,"div",null,null,!e.options.singleCursorHeightPerLine),o=r.appendChild(n("div"," ","CodeMirror-cursor"));if(o.style.left=i.left+"px",o.style.top=i.top+"px",o.style.height=Math.max(0,i.bottom-i.top)*e.options.cursorHeight+"px",i.other){var l=r.appendChild(n("div"," ","CodeMirror-cursor CodeMirror-secondarycursor"));l.style.display="",l.style.left=i.other.left+"px",l.style.top=i.other.top+"px",l.style.height=.85*(i.other.bottom-i.other.top)+"px"}}function Nr(e,t){return e.top-t.top||e.left-t.left}function Or(e,t,r){function i(e,t,r,i){t<0&&(t=0),t=Math.round(t),i=Math.round(i),a.appendChild(n("div",null,"CodeMirror-selected","position: absolute; left: "+e+"px;\n                             top: "+t+"px; width: "+(null==r?f-e:r)+"px;\n                             height: "+(i-t)+"px"))}function o(t,r,n){function o(r,n){return lr(e,E(t,r),"div",u,n)}var l,a,u=M(s,t),h=u.text.length,d=Se(u,s.direction);return xe(d,r||0,null==n?h:n,function(t,s,p,g){var v=o(t,"ltr"==p?"left":"right"),m=o(s-1,"ltr"==p?"right":"left");if("ltr"==p){var y=null==r&&0==t?c:v.left,b=null==n&&s==h?f:m.right;m.top-v.top<=3?i(y,m.top,b-y,m.bottom):(i(y,v.top,null,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top),i(c,m.top,m.right,m.bottom))}else if(t<s){var w=null==r&&0==t?f:v.right,x=null==n&&s==h?c:m.left;if(m.top-v.top<=3)i(x,m.top,w-x,m.bottom);else{var C=c;if(g){var S=hr(e,u,null,t).end;C=o(S-(/\s/.test(u.text.charAt(S-1))?2:1),"left").left}i(C,v.top,w-C,v.bottom),v.bottom<m.top&&i(c,v.bottom,null,m.top);var L=null;d.length,L=o(hr(e,u,null,s).begin,"right").right-x,i(x,m.top,L,m.bottom)}}(!l||Nr(v,l)<0)&&(l=v),Nr(m,l)<0&&(l=m),(!a||Nr(v,a)<0)&&(a=v),Nr(m,a)<0&&(a=m)}),{start:l,end:a}}var l=e.display,s=e.doc,a=document.createDocumentFragment(),u=It(e.display),c=u.left,f=Math.max(l.sizerWidth,Rt(e)-l.sizer.offsetLeft)-u.right,h=t.from(),d=t.to();if(h.line==d.line)o(h.line,h.ch,d.ch);else{var p=M(s,h.line),g=M(s,d.line),v=fe(p)==fe(g),m=o(h.line,h.ch,v?p.text.length+1:null).end,y=o(d.line,v?0:null,d.ch).start;v&&(m.top<y.top-2?(i(m.right,m.top,null,m.bottom),i(c,y.top,y.left,y.bottom)):i(m.right,m.top,y.left-m.right,m.bottom)),m.bottom<y.top&&i(c,m.bottom,null,y.top)}r.appendChild(a)}function Ar(e){if(e.state.focused){var t=e.display;clearInterval(t.blinker);var r=!0;t.cursorDiv.style.visibility="",e.options.cursorBlinkRate>0?t.blinker=setInterval(function(){return t.cursorDiv.style.visibility=(r=!r)?"":"hidden"},e.options.cursorBlinkRate):e.options.cursorBlinkRate<0&&(t.cursorDiv.style.visibility="hidden")}}function Wr(e){e.state.focused||(e.display.input.focus(),Hr(e))}function Dr(e){e.state.delayingBlurEvent=!0,setTimeout(function(){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1,Fr(e))},100)}function Hr(e,t){e.state.delayingBlurEvent&&(e.state.delayingBlurEvent=!1),"nocursor"!=e.options.readOnly&&(e.state.focused||(Te(e,"focus",e,t),e.state.focused=!0,s(e.display.wrapper,"CodeMirror-focused"),e.curOp||e.display.selForContextMenu==e.doc.sel||(e.display.input.reset(),ml&&setTimeout(function(){return e.display.input.reset(!0)},20)),e.display.input.receivedFocus()),Ar(e))}function Fr(e,t){e.state.delayingBlurEvent||(e.state.focused&&(Te(e,"blur",e,t),e.state.focused=!1,Fl(e.display.wrapper,"CodeMirror-focused")),clearInterval(e.display.blinker),setTimeout(function(){e.state.focused||(e.display.shift=!1)},150))}function Er(e){for(var t=e.display,r=t.lineDiv.offsetTop,n=0;n<t.view.length;n++){var i=t.view[n],o=void 0;if(!i.hidden){if(gl&&vl<8){var l=i.node.offsetTop+i.node.offsetHeight;o=l-r,r=l}else{var s=i.node.getBoundingClientRect();o=s.bottom-s.top}var a=i.line.height-o;if(o<2&&(o=mr(t)),(a>.005||a<-.005)&&(A(i.line,o),Pr(i.line),i.rest))for(var u=0;u<i.rest.length;u++)Pr(i.rest[u])}}}function Pr(e){if(e.widgets)for(var t=0;t<e.widgets.length;++t)e.widgets[t].height=e.widgets[t].node.parentNode.offsetHeight}function Ir(e,t,r){var n=r&&null!=r.top?Math.max(0,r.top):e.scroller.scrollTop;n=Math.floor(n-Et(e));var i=r&&null!=r.bottom?r.bottom:n+e.wrapper.clientHeight,o=D(t,n),l=D(t,i);if(r&&r.ensure){var s=r.ensure.from.line,a=r.ensure.to.line;s<o?(o=s,l=D(t,ye(M(t,s))+e.wrapper.clientHeight)):Math.min(a,t.lastLine())>=l&&(o=D(t,ye(M(t,a))-e.wrapper.clientHeight),l=a)}return{from:o,to:Math.max(l,o+1)}}function zr(e){var t=e.display,r=t.view;if(t.alignWidgets||t.gutters.firstChild&&e.options.fixedGutter){for(var n=wr(t)-t.scroller.scrollLeft+e.doc.scrollLeft,i=t.gutters.offsetWidth,o=n+"px",l=0;l<r.length;l++)if(!r[l].hidden){e.options.fixedGutter&&(r[l].gutter&&(r[l].gutter.style.left=o),r[l].gutterBackground&&(r[l].gutterBackground.style.left=o));var s=r[l].alignable;if(s)for(var a=0;a<s.length;a++)s[a].style.left=o}e.options.fixedGutter&&(t.gutters.style.left=n+i+"px")}}function Rr(e){if(!e.options.lineNumbers)return!1;var t=e.doc,r=F(e.options,t.first+t.size-1),i=e.display;if(r.length!=i.lineNumChars){var o=i.measure.appendChild(n("div",[n("div",r)],"CodeMirror-linenumber CodeMirror-gutter-elt")),l=o.firstChild.offsetWidth,s=o.offsetWidth-l;return i.lineGutter.style.width="",i.lineNumInnerWidth=Math.max(l,i.lineGutter.offsetWidth-s)+1,i.lineNumWidth=i.lineNumInnerWidth+s,i.lineNumChars=i.lineNumInnerWidth?r.length:-1,i.lineGutter.style.width=i.lineNumWidth+"px",Wn(e),!0}return!1}function Br(e,t){if(!Me(e,"scrollCursorIntoView")){var r=e.display,i=r.sizer.getBoundingClientRect(),o=null;if(t.top+i.top<0?o=!0:t.bottom+i.top>(window.innerHeight||document.documentElement.clientHeight)&&(o=!1),null!=o&&!Sl){var l=n("div","​",null,"position: absolute;\n                         top: "+(t.top-r.viewOffset-Et(e.display))+"px;\n                         height: "+(t.bottom-t.top+zt(e)+r.barHeight)+"px;\n                         left: "+t.left+"px; width: "+Math.max(2,t.right-t.left)+"px;");e.display.lineSpace.appendChild(l),l.scrollIntoView(o),e.display.lineSpace.removeChild(l)}}}function Gr(e,t,r,n){null==n&&(n=0);var i;e.options.lineWrapping||t!=r||(r="before"==(t=t.ch?E(t.line,"before"==t.sticky?t.ch-1:t.ch,"after"):t).sticky?E(t.line,t.ch+1,"before"):t);for(var o=0;o<5;o++){var l=!1,s=sr(e,t),a=r&&r!=t?sr(e,r):s,u=Vr(e,i={left:Math.min(s.left,a.left),top:Math.min(s.top,a.top)-n,right:Math.max(s.left,a.left),bottom:Math.max(s.bottom,a.bottom)+n}),c=e.doc.scrollTop,f=e.doc.scrollLeft;if(null!=u.scrollTop&&(qr(e,u.scrollTop),Math.abs(e.doc.scrollTop-c)>1&&(l=!0)),null!=u.scrollLeft&&(Qr(e,u.scrollLeft),Math.abs(e.doc.scrollLeft-f)>1&&(l=!0)),!l)break}return i}function Ur(e,t){var r=Vr(e,t);null!=r.scrollTop&&qr(e,r.scrollTop),null!=r.scrollLeft&&Qr(e,r.scrollLeft)}function Vr(e,t){var r=e.display,n=mr(e.display);t.top<0&&(t.top=0);var i=e.curOp&&null!=e.curOp.scrollTop?e.curOp.scrollTop:r.scroller.scrollTop,o=Bt(e),l={};t.bottom-t.top>o&&(t.bottom=t.top+o);var s=e.doc.height+Pt(r),a=t.top<n,u=t.bottom>s-n;if(t.top<i)l.scrollTop=a?0:t.top;else if(t.bottom>i+o){var c=Math.min(t.top,(u?s:t.bottom)-o);c!=i&&(l.scrollTop=c)}var f=e.curOp&&null!=e.curOp.scrollLeft?e.curOp.scrollLeft:r.scroller.scrollLeft,h=Rt(e)-(e.options.fixedGutter?r.gutters.offsetWidth:0),d=t.right-t.left>h;return d&&(t.right=t.left+h),t.left<10?l.scrollLeft=0:t.left<f?l.scrollLeft=Math.max(0,t.left-(d?0:10)):t.right>h+f-3&&(l.scrollLeft=t.right+(d?0:10)-h),l}function Kr(e,t){null!=t&&(_r(e),e.curOp.scrollTop=(null==e.curOp.scrollTop?e.doc.scrollTop:e.curOp.scrollTop)+t)}function jr(e){_r(e);var t=e.getCursor();e.curOp.scrollToPos={from:t,to:t,margin:e.options.cursorScrollMargin}}function Xr(e,t,r){null==t&&null==r||_r(e),null!=t&&(e.curOp.scrollLeft=t),null!=r&&(e.curOp.scrollTop=r)}function Yr(e,t){_r(e),e.curOp.scrollToPos=t}function _r(e){var t=e.curOp.scrollToPos;t&&(e.curOp.scrollToPos=null,$r(e,ar(e,t.from),ar(e,t.to),t.margin))}function $r(e,t,r,n){var i=Vr(e,{left:Math.min(t.left,r.left),top:Math.min(t.top,r.top)-n,right:Math.max(t.right,r.right),bottom:Math.max(t.bottom,r.bottom)+n});Xr(e,i.scrollLeft,i.scrollTop)}function qr(e,t){Math.abs(e.doc.scrollTop-t)<2||(fl||On(e,{top:t}),Zr(e,t,!0),fl&&On(e),Cn(e,100))}function Zr(e,t,r){t=Math.min(e.display.scroller.scrollHeight-e.display.scroller.clientHeight,t),(e.display.scroller.scrollTop!=t||r)&&(e.doc.scrollTop=t,e.display.scrollbars.setScrollTop(t),e.display.scroller.scrollTop!=t&&(e.display.scroller.scrollTop=t))}function Qr(e,t,r,n){t=Math.min(t,e.display.scroller.scrollWidth-e.display.scroller.clientWidth),(r?t==e.doc.scrollLeft:Math.abs(e.doc.scrollLeft-t)<2)&&!n||(e.doc.scrollLeft=t,zr(e),e.display.scroller.scrollLeft!=t&&(e.display.scroller.scrollLeft=t),e.display.scrollbars.setScrollLeft(t))}function Jr(e){var t=e.display,r=t.gutters.offsetWidth,n=Math.round(e.doc.height+Pt(e.display));return{clientHeight:t.scroller.clientHeight,viewHeight:t.wrapper.clientHeight,scrollWidth:t.scroller.scrollWidth,clientWidth:t.scroller.clientWidth,viewWidth:t.wrapper.clientWidth,barLeft:e.options.fixedGutter?r:0,docHeight:n,scrollHeight:n+zt(e)+t.barHeight,nativeBarWidth:t.nativeBarWidth,gutterWidth:r}}function en(e,t){t||(t=Jr(e));var r=e.display.barWidth,n=e.display.barHeight;tn(e,t);for(var i=0;i<4&&r!=e.display.barWidth||n!=e.display.barHeight;i++)r!=e.display.barWidth&&e.options.lineWrapping&&Er(e),tn(e,Jr(e)),r=e.display.barWidth,n=e.display.barHeight}function tn(e,t){var r=e.display,n=r.scrollbars.update(t);r.sizer.style.paddingRight=(r.barWidth=n.right)+"px",r.sizer.style.paddingBottom=(r.barHeight=n.bottom)+"px",r.heightForcer.style.borderBottom=n.bottom+"px solid transparent",n.right&&n.bottom?(r.scrollbarFiller.style.display="block",r.scrollbarFiller.style.height=n.bottom+"px",r.scrollbarFiller.style.width=n.right+"px"):r.scrollbarFiller.style.display="",n.bottom&&e.options.coverGutterNextToScrollbar&&e.options.fixedGutter?(r.gutterFiller.style.display="block",r.gutterFiller.style.height=n.bottom+"px",r.gutterFiller.style.width=t.gutterWidth+"px"):r.gutterFiller.style.display=""}function rn(e){e.display.scrollbars&&(e.display.scrollbars.clear(),e.display.scrollbars.addClass&&Fl(e.display.wrapper,e.display.scrollbars.addClass)),e.display.scrollbars=new ws[e.options.scrollbarStyle](function(t){e.display.wrapper.insertBefore(t,e.display.scrollbarFiller),Ql(t,"mousedown",function(){e.state.focused&&setTimeout(function(){return e.display.input.focus()},0)}),t.setAttribute("cm-not-content","true")},function(t,r){"horizontal"==r?Qr(e,t):qr(e,t)},e),e.display.scrollbars.addClass&&s(e.display.wrapper,e.display.scrollbars.addClass)}function nn(e){e.curOp={cm:e,viewChanged:!1,startHeight:e.doc.height,forceUpdate:!1,updateInput:null,typing:!1,changeObjs:null,cursorActivityHandlers:null,cursorActivityCalled:0,selectionChanged:!1,updateMaxLine:!1,scrollLeft:null,scrollTop:null,scrollToPos:null,focus:!1,id:++xs},vt(e.curOp)}function on(e){yt(e.curOp,function(e){for(var t=0;t<e.ops.length;t++)e.ops[t].cm.curOp=null;ln(e)})}function ln(e){for(var t=e.ops,r=0;r<t.length;r++)sn(t[r]);for(var n=0;n<t.length;n++)an(t[n]);for(var i=0;i<t.length;i++)un(t[i]);for(var o=0;o<t.length;o++)cn(t[o]);for(var l=0;l<t.length;l++)fn(t[l])}function sn(e){var t=e.cm,r=t.display;Ln(t),e.updateMaxLine&&we(t),e.mustUpdate=e.viewChanged||e.forceUpdate||null!=e.scrollTop||e.scrollToPos&&(e.scrollToPos.from.line<r.viewFrom||e.scrollToPos.to.line>=r.viewTo)||r.maxLineChanged&&t.options.lineWrapping,e.update=e.mustUpdate&&new Cs(t,e.mustUpdate&&{top:e.scrollTop,ensure:e.scrollToPos},e.forceUpdate)}function an(e){e.updatedDisplay=e.mustUpdate&&Mn(e.cm,e.update)}function un(e){var t=e.cm,r=t.display;e.updatedDisplay&&Er(t),e.barMeasure=Jr(t),r.maxLineChanged&&!t.options.lineWrapping&&(e.adjustWidthTo=Kt(t,r.maxLine,r.maxLine.text.length).left+3,t.display.sizerWidth=e.adjustWidthTo,e.barMeasure.scrollWidth=Math.max(r.scroller.clientWidth,r.sizer.offsetLeft+e.adjustWidthTo+zt(t)+t.display.barWidth),e.maxScrollLeft=Math.max(0,r.sizer.offsetLeft+e.adjustWidthTo-Rt(t))),(e.updatedDisplay||e.selectionChanged)&&(e.preparedSelection=r.input.prepareSelection())}function cn(e){var t=e.cm;null!=e.adjustWidthTo&&(t.display.sizer.style.minWidth=e.adjustWidthTo+"px",e.maxScrollLeft<t.doc.scrollLeft&&Qr(t,Math.min(t.display.scroller.scrollLeft,e.maxScrollLeft),!0),t.display.maxLineChanged=!1);var r=e.focus&&e.focus==l();e.preparedSelection&&t.display.input.showSelection(e.preparedSelection,r),(e.updatedDisplay||e.startHeight!=t.doc.height)&&en(t,e.barMeasure),e.updatedDisplay&&Dn(t,e.barMeasure),e.selectionChanged&&Ar(t),t.state.focused&&e.updateInput&&t.display.input.reset(e.typing),r&&Wr(e.cm)}function fn(e){var t=e.cm,r=t.display,n=t.doc;e.updatedDisplay&&Nn(t,e.update),null==r.wheelStartX||null==e.scrollTop&&null==e.scrollLeft&&!e.scrollToPos||(r.wheelStartX=r.wheelStartY=null),null!=e.scrollTop&&Zr(t,e.scrollTop,e.forceScroll),null!=e.scrollLeft&&Qr(t,e.scrollLeft,!0,!0),e.scrollToPos&&Br(t,Gr(t,U(n,e.scrollToPos.from),U(n,e.scrollToPos.to),e.scrollToPos.margin));var i=e.maybeHiddenMarkers,o=e.maybeUnhiddenMarkers;if(i)for(var l=0;l<i.length;++l)i[l].lines.length||Te(i[l],"hide");if(o)for(var s=0;s<o.length;++s)o[s].lines.length&&Te(o[s],"unhide");r.wrapper.offsetHeight&&(n.scrollTop=t.display.scroller.scrollTop),e.changeObjs&&Te(t,"changes",t,e.changeObjs),e.update&&e.update.finish()}function hn(e,t){if(e.curOp)return t();nn(e);try{return t()}finally{on(e)}}function dn(e,t){return function(){if(e.curOp)return t.apply(e,arguments);nn(e);try{return t.apply(e,arguments)}finally{on(e)}}}function pn(e){return function(){if(this.curOp)return e.apply(this,arguments);nn(this);try{return e.apply(this,arguments)}finally{on(this)}}}function gn(e){return function(){var t=this.cm;if(!t||t.curOp)return e.apply(this,arguments);nn(t);try{return e.apply(this,arguments)}finally{on(t)}}}function vn(e,t,r,n){null==t&&(t=e.doc.first),null==r&&(r=e.doc.first+e.doc.size),n||(n=0);var i=e.display;if(n&&r<i.viewTo&&(null==i.updateLineNumbers||i.updateLineNumbers>t)&&(i.updateLineNumbers=t),e.curOp.viewChanged=!0,t>=i.viewTo)_l&&pe(e.doc,t)<i.viewTo&&yn(e);else if(r<=i.viewFrom)_l&&ge(e.doc,r+n)>i.viewFrom?yn(e):(i.viewFrom+=n,i.viewTo+=n);else if(t<=i.viewFrom&&r>=i.viewTo)yn(e);else if(t<=i.viewFrom){var o=bn(e,r,r+n,1);o?(i.view=i.view.slice(o.index),i.viewFrom=o.lineN,i.viewTo+=n):yn(e)}else if(r>=i.viewTo){var l=bn(e,t,t,-1);l?(i.view=i.view.slice(0,l.index),i.viewTo=l.lineN):yn(e)}else{var s=bn(e,t,t,-1),a=bn(e,r,r+n,1);s&&a?(i.view=i.view.slice(0,s.index).concat(gt(e,s.lineN,a.lineN)).concat(i.view.slice(a.index)),i.viewTo+=n):yn(e)}var u=i.externalMeasured;u&&(r<u.lineN?u.lineN+=n:t<u.lineN+u.size&&(i.externalMeasured=null))}function mn(e,t,r){e.curOp.viewChanged=!0;var n=e.display,i=e.display.externalMeasured;if(i&&t>=i.lineN&&t<i.lineN+i.size&&(n.externalMeasured=null),!(t<n.viewFrom||t>=n.viewTo)){var o=n.view[Lr(e,t)];if(null!=o.node){var l=o.changes||(o.changes=[]);-1==h(l,r)&&l.push(r)}}}function yn(e){e.display.viewFrom=e.display.viewTo=e.doc.first,e.display.view=[],e.display.viewOffset=0}function bn(e,t,r,n){var i,o=Lr(e,t),l=e.display.view;if(!_l||r==e.doc.first+e.doc.size)return{index:o,lineN:r};for(var s=e.display.viewFrom,a=0;a<o;a++)s+=l[a].size;if(s!=t){if(n>0){if(o==l.length-1)return null;i=s+l[o].size-t,o++}else i=s-t;t+=i,r+=i}for(;pe(e.doc,r)!=r;){if(o==(n<0?0:l.length-1))return null;r+=n*l[o-(n<0?1:0)].size,o+=n}return{index:o,lineN:r}}function wn(e,t,r){var n=e.display;0==n.view.length||t>=n.viewTo||r<=n.viewFrom?(n.view=gt(e,t,r),n.viewFrom=t):(n.viewFrom>t?n.view=gt(e,t,n.viewFrom).concat(n.view):n.viewFrom<t&&(n.view=n.view.slice(Lr(e,t))),n.viewFrom=t,n.viewTo<r?n.view=n.view.concat(gt(e,n.viewTo,r)):n.viewTo>r&&(n.view=n.view.slice(0,Lr(e,r)))),n.viewTo=r}function xn(e){for(var t=e.display.view,r=0,n=0;n<t.length;n++){var i=t[n];i.hidden||i.node&&!i.changes||++r}return r}function Cn(e,t){e.doc.highlightFrontier<e.display.viewTo&&e.state.highlight.set(t,u(Sn,e))}function Sn(e){var t=e.doc;if(!(t.highlightFrontier>=e.display.viewTo)){var r=+new Date+e.options.workTime,n=$e(e,t.highlightFrontier),i=[];t.iter(n.line,Math.min(t.first+t.size,e.display.viewTo+500),function(o){if(n.line>=e.display.viewFrom){var l=o.styles,s=o.text.length>e.options.maxHighlightLength?Ke(t.mode,n.state):null,a=Ye(e,o,n,!0);s&&(n.state=s),o.styles=a.styles;var u=o.styleClasses,c=a.classes;c?o.styleClasses=c:u&&(o.styleClasses=null);for(var f=!l||l.length!=o.styles.length||u!=c&&(!u||!c||u.bgClass!=c.bgClass||u.textClass!=c.textClass),h=0;!f&&h<l.length;++h)f=l[h]!=o.styles[h];f&&i.push(n.line),o.stateAfter=n.save(),n.nextLine()}else o.text.length<=e.options.maxHighlightLength&&qe(e,o.text,n),o.stateAfter=n.line%5==0?n.save():null,n.nextLine();if(+new Date>r)return Cn(e,e.options.workDelay),!0}),t.highlightFrontier=n.line,t.modeFrontier=Math.max(t.modeFrontier,n.line),i.length&&hn(e,function(){for(var t=0;t<i.length;t++)mn(e,i[t],"text")})}}function Ln(e){var t=e.display;!t.scrollbarsClipped&&t.scroller.offsetWidth&&(t.nativeBarWidth=t.scroller.offsetWidth-t.scroller.clientWidth,t.heightForcer.style.height=zt(e)+"px",t.sizer.style.marginBottom=-t.nativeBarWidth+"px",t.sizer.style.borderRightWidth=zt(e)+"px",t.scrollbarsClipped=!0)}function kn(e){if(e.hasFocus())return null;var t=l();if(!t||!o(e.display.lineDiv,t))return null;var r={activeElt:t};if(window.getSelection){var n=window.getSelection();n.anchorNode&&n.extend&&o(e.display.lineDiv,n.anchorNode)&&(r.anchorNode=n.anchorNode,r.anchorOffset=n.anchorOffset,r.focusNode=n.focusNode,r.focusOffset=n.focusOffset)}return r}function Tn(e){if(e&&e.activeElt&&e.activeElt!=l()&&(e.activeElt.focus(),e.anchorNode&&o(document.body,e.anchorNode)&&o(document.body,e.focusNode))){var t=window.getSelection(),r=document.createRange();r.setEnd(e.anchorNode,e.anchorOffset),r.collapse(!1),t.removeAllRanges(),t.addRange(r),t.extend(e.focusNode,e.focusOffset)}}function Mn(e,r){var n=e.display,i=e.doc;if(r.editorIsHidden)return yn(e),!1;if(!r.force&&r.visible.from>=n.viewFrom&&r.visible.to<=n.viewTo&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo)&&n.renderedView==n.view&&0==xn(e))return!1;Rr(e)&&(yn(e),r.dims=br(e));var o=i.first+i.size,l=Math.max(r.visible.from-e.options.viewportMargin,i.first),s=Math.min(o,r.visible.to+e.options.viewportMargin);n.viewFrom<l&&l-n.viewFrom<20&&(l=Math.max(i.first,n.viewFrom)),n.viewTo>s&&n.viewTo-s<20&&(s=Math.min(o,n.viewTo)),_l&&(l=pe(e.doc,l),s=ge(e.doc,s));var a=l!=n.viewFrom||s!=n.viewTo||n.lastWrapHeight!=r.wrapperHeight||n.lastWrapWidth!=r.wrapperWidth;wn(e,l,s),n.viewOffset=ye(M(e.doc,n.viewFrom)),e.display.mover.style.top=n.viewOffset+"px";var u=xn(e);if(!a&&0==u&&!r.force&&n.renderedView==n.view&&(null==n.updateLineNumbers||n.updateLineNumbers>=n.viewTo))return!1;var c=kn(e);return u>4&&(n.lineDiv.style.display="none"),An(e,n.updateLineNumbers,r.dims),u>4&&(n.lineDiv.style.display=""),n.renderedView=n.view,Tn(c),t(n.cursorDiv),t(n.selectionDiv),n.gutters.style.height=n.sizer.style.minHeight=0,a&&(n.lastWrapHeight=r.wrapperHeight,n.lastWrapWidth=r.wrapperWidth,Cn(e,400)),n.updateLineNumbers=null,!0}function Nn(e,t){for(var r=t.viewport,n=!0;(n&&e.options.lineWrapping&&t.oldDisplayWidth!=Rt(e)||(r&&null!=r.top&&(r={top:Math.min(e.doc.height+Pt(e.display)-Bt(e),r.top)}),t.visible=Ir(e.display,e.doc,r),!(t.visible.from>=e.display.viewFrom&&t.visible.to<=e.display.viewTo)))&&Mn(e,t);n=!1){Er(e);var i=Jr(e);kr(e),en(e,i),Dn(e,i),t.force=!1}t.signal(e,"update",e),e.display.viewFrom==e.display.reportedViewFrom&&e.display.viewTo==e.display.reportedViewTo||(t.signal(e,"viewportChange",e,e.display.viewFrom,e.display.viewTo),e.display.reportedViewFrom=e.display.viewFrom,e.display.reportedViewTo=e.display.viewTo)}function On(e,t){var r=new Cs(e,t);if(Mn(e,r)){Er(e),Nn(e,r);var n=Jr(e);kr(e),en(e,n),Dn(e,n),r.finish()}}function An(e,r,n){function i(t){var r=t.nextSibling;return ml&&Ml&&e.display.currentWheelTarget==t?t.style.display="none":t.parentNode.removeChild(t),r}for(var o=e.display,l=e.options.lineNumbers,s=o.lineDiv,a=s.firstChild,u=o.view,c=o.viewFrom,f=0;f<u.length;f++){var d=u[f];if(d.hidden);else if(d.node&&d.node.parentNode==s){for(;a!=d.node;)a=i(a);var p=l&&null!=r&&r<=c&&d.lineNumber;d.changes&&(h(d.changes,"gutter")>-1&&(p=!1),xt(e,d,c,n)),p&&(t(d.lineNumber),d.lineNumber.appendChild(document.createTextNode(F(e.options,c)))),a=d.node.nextSibling}else{var g=Ot(e,d,c,n);s.insertBefore(g,a)}c+=d.size}for(;a;)a=i(a)}function Wn(e){var t=e.display.gutters.offsetWidth;e.display.sizer.style.marginLeft=t+"px"}function Dn(e,t){e.display.sizer.style.minHeight=t.docHeight+"px",e.display.heightForcer.style.top=t.docHeight+"px",e.display.gutters.style.height=t.docHeight+e.display.barHeight+zt(e)+"px"}function Hn(e){var r=e.display.gutters,i=e.options.gutters;t(r);for(var o=0;o<i.length;++o){var l=i[o],s=r.appendChild(n("div",null,"CodeMirror-gutter "+l));"CodeMirror-linenumbers"==l&&(e.display.lineGutter=s,s.style.width=(e.display.lineNumWidth||1)+"px")}r.style.display=o?"":"none",Wn(e)}function Fn(e){var t=h(e.gutters,"CodeMirror-linenumbers");-1==t&&e.lineNumbers?e.gutters=e.gutters.concat(["CodeMirror-linenumbers"]):t>-1&&!e.lineNumbers&&(e.gutters=e.gutters.slice(0),e.gutters.splice(t,1))}function En(e){var t=e.wheelDeltaX,r=e.wheelDeltaY;return null==t&&e.detail&&e.axis==e.HORIZONTAL_AXIS&&(t=e.detail),null==r&&e.detail&&e.axis==e.VERTICAL_AXIS?r=e.detail:null==r&&(r=e.wheelDelta),{x:t,y:r}}function Pn(e){var t=En(e);return t.x*=Ls,t.y*=Ls,t}function In(e,t){var r=En(t),n=r.x,i=r.y,o=e.display,l=o.scroller,s=l.scrollWidth>l.clientWidth,a=l.scrollHeight>l.clientHeight;if(n&&s||i&&a){if(i&&Ml&&ml)e:for(var u=t.target,c=o.view;u!=l;u=u.parentNode)for(var f=0;f<c.length;f++)if(c[f].node==u){e.display.currentWheelTarget=u;break e}if(n&&!fl&&!wl&&null!=Ls)return i&&a&&qr(e,Math.max(0,l.scrollTop+i*Ls)),Qr(e,Math.max(0,l.scrollLeft+n*Ls)),(!i||i&&a)&&We(t),void(o.wheelStartX=null);if(i&&null!=Ls){var h=i*Ls,d=e.doc.scrollTop,p=d+o.wrapper.clientHeight;h<0?d=Math.max(0,d+h-50):p=Math.min(e.doc.height,p+h+50),On(e,{top:d,bottom:p})}Ss<20&&(null==o.wheelStartX?(o.wheelStartX=l.scrollLeft,o.wheelStartY=l.scrollTop,o.wheelDX=n,o.wheelDY=i,setTimeout(function(){if(null!=o.wheelStartX){var e=l.scrollLeft-o.wheelStartX,t=l.scrollTop-o.wheelStartY,r=t&&o.wheelDY&&t/o.wheelDY||e&&o.wheelDX&&e/o.wheelDX;o.wheelStartX=o.wheelStartY=null,r&&(Ls=(Ls*Ss+r)/(Ss+1),++Ss)}},200)):(o.wheelDX+=n,o.wheelDY+=i))}}function zn(e,t){var r=e[t];e.sort(function(e,t){return P(e.from(),t.from())}),t=h(e,r);for(var n=1;n<e.length;n++){var i=e[n],o=e[n-1];if(P(o.to(),i.from())>=0){var l=B(o.from(),i.from()),s=R(o.to(),i.to()),a=o.empty()?i.from()==i.head:o.from()==o.head;n<=t&&--t,e.splice(--n,2,new Ts(a?s:l,a?l:s))}}return new ks(e,t)}function Rn(e,t){return new ks([new Ts(e,t||e)],0)}function Bn(e){return e.text?E(e.from.line+e.text.length-1,g(e.text).length+(1==e.text.length?e.from.ch:0)):e.to}function Gn(e,t){if(P(e,t.from)<0)return e;if(P(e,t.to)<=0)return Bn(t);var r=e.line+t.text.length-(t.to.line-t.from.line)-1,n=e.ch;return e.line==t.to.line&&(n+=Bn(t).ch-t.to.ch),E(r,n)}function Un(e,t){for(var r=[],n=0;n<e.sel.ranges.length;n++){var i=e.sel.ranges[n];r.push(new Ts(Gn(i.anchor,t),Gn(i.head,t)))}return zn(r,e.sel.primIndex)}function Vn(e,t,r){return e.line==t.line?E(r.line,e.ch-t.ch+r.ch):E(r.line+(e.line-t.line),e.ch)}function Kn(e,t,r){for(var n=[],i=E(e.first,0),o=i,l=0;l<t.length;l++){var s=t[l],a=Vn(s.from,i,o),u=Vn(Bn(s),i,o);if(i=s.to,o=u,"around"==r){var c=e.sel.ranges[l],f=P(c.head,c.anchor)<0;n[l]=new Ts(f?u:a,f?a:u)}else n[l]=new Ts(a,a)}return new ks(n,e.sel.primIndex)}function jn(e){e.doc.mode=Ue(e.options,e.doc.modeOption),Xn(e)}function Xn(e){e.doc.iter(function(e){e.stateAfter&&(e.stateAfter=null),e.styles&&(e.styles=null)}),e.doc.modeFrontier=e.doc.highlightFrontier=e.doc.first,Cn(e,100),e.state.modeGen++,e.curOp&&vn(e)}function Yn(e,t){return 0==t.from.ch&&0==t.to.ch&&""==g(t.text)&&(!e.cm||e.cm.options.wholeLineUpdateBefore)}function _n(e,t,r,n){function i(e){return r?r[e]:null}function o(e,r,i){it(e,r,i,n),bt(e,"change",e,t)}function l(e,t){for(var r=[],o=e;o<t;++o)r.push(new fs(u[o],i(o),n));return r}var s=t.from,a=t.to,u=t.text,c=M(e,s.line),f=M(e,a.line),h=g(u),d=i(u.length-1),p=a.line-s.line;if(t.full)e.insert(0,l(0,u.length)),e.remove(u.length,e.size-u.length);else if(Yn(e,t)){var v=l(0,u.length-1);o(f,f.text,d),p&&e.remove(s.line,p),v.length&&e.insert(s.line,v)}else if(c==f)if(1==u.length)o(c,c.text.slice(0,s.ch)+h+c.text.slice(a.ch),d);else{var m=l(1,u.length-1);m.push(new fs(h+c.text.slice(a.ch),d,n)),o(c,c.text.slice(0,s.ch)+u[0],i(0)),e.insert(s.line+1,m)}else if(1==u.length)o(c,c.text.slice(0,s.ch)+u[0]+f.text.slice(a.ch),i(0)),e.remove(s.line+1,p);else{o(c,c.text.slice(0,s.ch)+u[0],i(0)),o(f,h+f.text.slice(a.ch),d);var y=l(1,u.length-1);p>1&&e.remove(s.line+1,p-1),e.insert(s.line+1,y)}bt(e,"change",e,t)}function $n(e,t,r){function n(e,i,o){if(e.linked)for(var l=0;l<e.linked.length;++l){var s=e.linked[l];if(s.doc!=i){var a=o&&s.sharedHist;r&&!a||(t(s.doc,a),n(s.doc,e,a))}}}n(e,null,!0)}function qn(e,t){if(t.cm)throw new Error("This document is already in use.");e.doc=t,t.cm=e,Cr(e),jn(e),Zn(e),e.options.lineWrapping||we(e),e.options.mode=t.modeOption,vn(e)}function Zn(e){("rtl"==e.doc.direction?s:Fl)(e.display.lineDiv,"CodeMirror-rtl")}function Qn(e){hn(e,function(){Zn(e),vn(e)})}function Jn(e){this.done=[],this.undone=[],this.undoDepth=1/0,this.lastModTime=this.lastSelTime=0,this.lastOp=this.lastSelOp=null,this.lastOrigin=this.lastSelOrigin=null,this.generation=this.maxGeneration=e||1}function ei(e,t){var r={from:z(t.from),to:Bn(t),text:N(e,t.from,t.to)};return si(e,r,t.from.line,t.to.line+1),$n(e,function(e){return si(e,r,t.from.line,t.to.line+1)},!0),r}function ti(e){for(;e.length&&g(e).ranges;)e.pop()}function ri(e,t){return t?(ti(e.done),g(e.done)):e.done.length&&!g(e.done).ranges?g(e.done):e.done.length>1&&!e.done[e.done.length-2].ranges?(e.done.pop(),g(e.done)):void 0}function ni(e,t,r,n){var i=e.history;i.undone.length=0;var o,l,s=+new Date;if((i.lastOp==n||i.lastOrigin==t.origin&&t.origin&&("+"==t.origin.charAt(0)&&e.cm&&i.lastModTime>s-e.cm.options.historyEventDelay||"*"==t.origin.charAt(0)))&&(o=ri(i,i.lastOp==n)))l=g(o.changes),0==P(t.from,t.to)&&0==P(t.from,l.to)?l.to=Bn(t):o.changes.push(ei(e,t));else{var a=g(i.done);for(a&&a.ranges||li(e.sel,i.done),o={changes:[ei(e,t)],generation:i.generation},i.done.push(o);i.done.length>i.undoDepth;)i.done.shift(),i.done[0].ranges||i.done.shift()}i.done.push(r),i.generation=++i.maxGeneration,i.lastModTime=i.lastSelTime=s,i.lastOp=i.lastSelOp=n,i.lastOrigin=i.lastSelOrigin=t.origin,l||Te(e,"historyAdded")}function ii(e,t,r,n){var i=t.charAt(0);return"*"==i||"+"==i&&r.ranges.length==n.ranges.length&&r.somethingSelected()==n.somethingSelected()&&new Date-e.history.lastSelTime<=(e.cm?e.cm.options.historyEventDelay:500)}function oi(e,t,r,n){var i=e.history,o=n&&n.origin;r==i.lastSelOp||o&&i.lastSelOrigin==o&&(i.lastModTime==i.lastSelTime&&i.lastOrigin==o||ii(e,o,g(i.done),t))?i.done[i.done.length-1]=t:li(t,i.done),i.lastSelTime=+new Date,i.lastSelOrigin=o,i.lastSelOp=r,n&&!1!==n.clearRedo&&ti(i.undone)}function li(e,t){var r=g(t);r&&r.ranges&&r.equals(e)||t.push(e)}function si(e,t,r,n){var i=t["spans_"+e.id],o=0;e.iter(Math.max(e.first,r),Math.min(e.first+e.size,n),function(r){r.markedSpans&&((i||(i=t["spans_"+e.id]={}))[o]=r.markedSpans),++o})}function ai(e){if(!e)return null;for(var t,r=0;r<e.length;++r)e[r].marker.explicitlyCleared?t||(t=e.slice(0,r)):t&&t.push(e[r]);return t?t.length?t:null:e}function ui(e,t){var r=t["spans_"+e.id];if(!r)return null;for(var n=[],i=0;i<t.text.length;++i)n.push(ai(r[i]));return n}function ci(e,t){var r=ui(e,t),n=J(e,t);if(!r)return n;if(!n)return r;for(var i=0;i<r.length;++i){var o=r[i],l=n[i];if(o&&l)e:for(var s=0;s<l.length;++s){for(var a=l[s],u=0;u<o.length;++u)if(o[u].marker==a.marker)continue e;o.push(a)}else l&&(r[i]=l)}return r}function fi(e,t,r){for(var n=[],i=0;i<e.length;++i){var o=e[i];if(o.ranges)n.push(r?ks.prototype.deepCopy.call(o):o);else{var l=o.changes,s=[];n.push({changes:s});for(var a=0;a<l.length;++a){var u=l[a],c=void 0;if(s.push({from:u.from,to:u.to,text:u.text}),t)for(var f in u)(c=f.match(/^spans_(\d+)$/))&&h(t,Number(c[1]))>-1&&(g(s)[f]=u[f],delete u[f])}}}return n}function hi(e,t,r,n){if(n){var i=e.anchor;if(r){var o=P(t,i)<0;o!=P(r,i)<0?(i=t,t=r):o!=P(t,r)<0&&(t=r)}return new Ts(i,t)}return new Ts(r||t,t)}function di(e,t,r,n,i){null==i&&(i=e.cm&&(e.cm.display.shift||e.extend)),bi(e,new ks([hi(e.sel.primary(),t,r,i)],0),n)}function pi(e,t,r){for(var n=[],i=e.cm&&(e.cm.display.shift||e.extend),o=0;o<e.sel.ranges.length;o++)n[o]=hi(e.sel.ranges[o],t[o],null,i);bi(e,zn(n,e.sel.primIndex),r)}function gi(e,t,r,n){var i=e.sel.ranges.slice(0);i[t]=r,bi(e,zn(i,e.sel.primIndex),n)}function vi(e,t,r,n){bi(e,Rn(t,r),n)}function mi(e,t,r){var n={ranges:t.ranges,update:function(t){var r=this;this.ranges=[];for(var n=0;n<t.length;n++)r.ranges[n]=new Ts(U(e,t[n].anchor),U(e,t[n].head))},origin:r&&r.origin};return Te(e,"beforeSelectionChange",e,n),e.cm&&Te(e.cm,"beforeSelectionChange",e.cm,n),n.ranges!=t.ranges?zn(n.ranges,n.ranges.length-1):t}function yi(e,t,r){var n=e.history.done,i=g(n);i&&i.ranges?(n[n.length-1]=t,wi(e,t,r)):bi(e,t,r)}function bi(e,t,r){wi(e,t,r),oi(e,e.sel,e.cm?e.cm.curOp.id:NaN,r)}function wi(e,t,r){(Oe(e,"beforeSelectionChange")||e.cm&&Oe(e.cm,"beforeSelectionChange"))&&(t=mi(e,t,r)),xi(e,Si(e,t,r&&r.bias||(P(t.primary().head,e.sel.primary().head)<0?-1:1),!0)),r&&!1===r.scroll||!e.cm||jr(e.cm)}function xi(e,t){t.equals(e.sel)||(e.sel=t,e.cm&&(e.cm.curOp.updateInput=e.cm.curOp.selectionChanged=!0,Ne(e.cm)),bt(e,"cursorActivity",e))}function Ci(e){xi(e,Si(e,e.sel,null,!1))}function Si(e,t,r,n){for(var i,o=0;o<t.ranges.length;o++){var l=t.ranges[o],s=t.ranges.length==e.sel.ranges.length&&e.sel.ranges[o],a=ki(e,l.anchor,s&&s.anchor,r,n),u=ki(e,l.head,s&&s.head,r,n);(i||a!=l.anchor||u!=l.head)&&(i||(i=t.ranges.slice(0,o)),i[o]=new Ts(a,u))}return i?zn(i,t.primIndex):t}function Li(e,t,r,n,i){var o=M(e,t.line);if(o.markedSpans)for(var l=0;l<o.markedSpans.length;++l){var s=o.markedSpans[l],a=s.marker;if((null==s.from||(a.inclusiveLeft?s.from<=t.ch:s.from<t.ch))&&(null==s.to||(a.inclusiveRight?s.to>=t.ch:s.to>t.ch))){if(i&&(Te(a,"beforeCursorEnter"),a.explicitlyCleared)){if(o.markedSpans){--l;continue}break}if(!a.atomic)continue;if(r){var u=a.find(n<0?1:-1),c=void 0;if((n<0?a.inclusiveRight:a.inclusiveLeft)&&(u=Ti(e,u,-n,u&&u.line==t.line?o:null)),u&&u.line==t.line&&(c=P(u,r))&&(n<0?c<0:c>0))return Li(e,u,t,n,i)}var f=a.find(n<0?-1:1);return(n<0?a.inclusiveLeft:a.inclusiveRight)&&(f=Ti(e,f,n,f.line==t.line?o:null)),f?Li(e,f,t,n,i):null}}return t}function ki(e,t,r,n,i){var o=n||1,l=Li(e,t,r,o,i)||!i&&Li(e,t,r,o,!0)||Li(e,t,r,-o,i)||!i&&Li(e,t,r,-o,!0);return l||(e.cantEdit=!0,E(e.first,0))}function Ti(e,t,r,n){return r<0&&0==t.ch?t.line>e.first?U(e,E(t.line-1)):null:r>0&&t.ch==(n||M(e,t.line)).text.length?t.line<e.first+e.size-1?E(t.line+1,0):null:new E(t.line,t.ch+r)}function Mi(e){e.setSelection(E(e.firstLine(),0),E(e.lastLine()),Gl)}function Ni(e,t,r){var n={canceled:!1,from:t.from,to:t.to,text:t.text,origin:t.origin,cancel:function(){return n.canceled=!0}};return r&&(n.update=function(t,r,i,o){t&&(n.from=U(e,t)),r&&(n.to=U(e,r)),i&&(n.text=i),void 0!==o&&(n.origin=o)}),Te(e,"beforeChange",e,n),e.cm&&Te(e.cm,"beforeChange",e.cm,n),n.canceled?null:{from:n.from,to:n.to,text:n.text,origin:n.origin}}function Oi(e,t,r){if(e.cm){if(!e.cm.curOp)return dn(e.cm,Oi)(e,t,r);if(e.cm.state.suppressEdits)return}if(!(Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"))||(t=Ni(e,t,!0))){var n=Yl&&!r&&te(e,t.from,t.to);if(n)for(var i=n.length-1;i>=0;--i)Ai(e,{from:n[i].from,to:n[i].to,text:i?[""]:t.text,origin:t.origin});else Ai(e,t)}}function Ai(e,t){if(1!=t.text.length||""!=t.text[0]||0!=P(t.from,t.to)){var r=Un(e,t);ni(e,t,r,e.cm?e.cm.curOp.id:NaN),Hi(e,t,r,J(e,t));var n=[];$n(e,function(e,r){r||-1!=h(n,e.history)||(zi(e.history,t),n.push(e.history)),Hi(e,t,null,J(e,t))})}}function Wi(e,t,r){if(!e.cm||!e.cm.state.suppressEdits||r){for(var n,i=e.history,o=e.sel,l="undo"==t?i.done:i.undone,s="undo"==t?i.undone:i.done,a=0;a<l.length&&(n=l[a],r?!n.ranges||n.equals(e.sel):n.ranges);a++);if(a!=l.length){for(i.lastOrigin=i.lastSelOrigin=null;(n=l.pop()).ranges;){if(li(n,s),r&&!n.equals(e.sel))return void bi(e,n,{clearRedo:!1});o=n}var u=[];li(o,s),s.push({changes:u,generation:i.generation}),i.generation=n.generation||++i.maxGeneration;for(var c=Oe(e,"beforeChange")||e.cm&&Oe(e.cm,"beforeChange"),f=n.changes.length-1;f>=0;--f){var d=function(r){var i=n.changes[r];if(i.origin=t,c&&!Ni(e,i,!1))return l.length=0,{};u.push(ei(e,i));var o=r?Un(e,i):g(l);Hi(e,i,o,ci(e,i)),!r&&e.cm&&e.cm.scrollIntoView({from:i.from,to:Bn(i)});var s=[];$n(e,function(e,t){t||-1!=h(s,e.history)||(zi(e.history,i),s.push(e.history)),Hi(e,i,null,ci(e,i))})}(f);if(d)return d.v}}}}function Di(e,t){if(0!=t&&(e.first+=t,e.sel=new ks(v(e.sel.ranges,function(e){return new Ts(E(e.anchor.line+t,e.anchor.ch),E(e.head.line+t,e.head.ch))}),e.sel.primIndex),e.cm)){vn(e.cm,e.first,e.first-t,t);for(var r=e.cm.display,n=r.viewFrom;n<r.viewTo;n++)mn(e.cm,n,"gutter")}}function Hi(e,t,r,n){if(e.cm&&!e.cm.curOp)return dn(e.cm,Hi)(e,t,r,n);if(t.to.line<e.first)Di(e,t.text.length-1-(t.to.line-t.from.line));else if(!(t.from.line>e.lastLine())){if(t.from.line<e.first){var i=t.text.length-1-(e.first-t.from.line);Di(e,i),t={from:E(e.first,0),to:E(t.to.line+i,t.to.ch),text:[g(t.text)],origin:t.origin}}var o=e.lastLine();t.to.line>o&&(t={from:t.from,to:E(o,M(e,o).text.length),text:[t.text[0]],origin:t.origin}),t.removed=N(e,t.from,t.to),r||(r=Un(e,t)),e.cm?Fi(e.cm,t,n):_n(e,t,n),wi(e,r,Gl)}}function Fi(e,t,r){var n=e.doc,i=e.display,o=t.from,l=t.to,s=!1,a=o.line;e.options.lineWrapping||(a=W(fe(M(n,o.line))),n.iter(a,l.line+1,function(e){if(e==i.maxLine)return s=!0,!0})),n.sel.contains(t.from,t.to)>-1&&Ne(e),_n(n,t,r,xr(e)),e.options.lineWrapping||(n.iter(a,o.line+t.text.length,function(e){var t=be(e);t>i.maxLineLength&&(i.maxLine=e,i.maxLineLength=t,i.maxLineChanged=!0,s=!1)}),s&&(e.curOp.updateMaxLine=!0)),nt(n,o.line),Cn(e,400);var u=t.text.length-(l.line-o.line)-1;t.full?vn(e):o.line!=l.line||1!=t.text.length||Yn(e.doc,t)?vn(e,o.line,l.line+1,u):mn(e,o.line,"text");var c=Oe(e,"changes"),f=Oe(e,"change");if(f||c){var h={from:o,to:l,text:t.text,removed:t.removed,origin:t.origin};f&&bt(e,"change",e,h),c&&(e.curOp.changeObjs||(e.curOp.changeObjs=[])).push(h)}e.display.selForContextMenu=null}function Ei(e,t,r,n,i){if(n||(n=r),P(n,r)<0){var o;r=(o=[n,r])[0],n=o[1]}"string"==typeof t&&(t=e.splitLines(t)),Oi(e,{from:r,to:n,text:t,origin:i})}function Pi(e,t,r,n){r<e.line?e.line+=n:t<e.line&&(e.line=t,e.ch=0)}function Ii(e,t,r,n){for(var i=0;i<e.length;++i){var o=e[i],l=!0;if(o.ranges){o.copied||((o=e[i]=o.deepCopy()).copied=!0);for(var s=0;s<o.ranges.length;s++)Pi(o.ranges[s].anchor,t,r,n),Pi(o.ranges[s].head,t,r,n)}else{for(var a=0;a<o.changes.length;++a){var u=o.changes[a];if(r<u.from.line)u.from=E(u.from.line+n,u.from.ch),u.to=E(u.to.line+n,u.to.ch);else if(t<=u.to.line){l=!1;break}}l||(e.splice(0,i+1),i=0)}}}function zi(e,t){var r=t.from.line,n=t.to.line,i=t.text.length-(n-r)-1;Ii(e.done,r,n,i),Ii(e.undone,r,n,i)}function Ri(e,t,r,n){var i=t,o=t;return"number"==typeof t?o=M(e,G(e,t)):i=W(t),null==i?null:(n(o,i)&&e.cm&&mn(e.cm,i,r),o)}function Bi(e){var t=this;this.lines=e,this.parent=null;for(var r=0,n=0;n<e.length;++n)e[n].parent=t,r+=e[n].height;this.height=r}function Gi(e){var t=this;this.children=e;for(var r=0,n=0,i=0;i<e.length;++i){var o=e[i];r+=o.chunkSize(),n+=o.height,o.parent=t}this.size=r,this.height=n,this.parent=null}function Ui(e,t,r){ye(t)<(e.curOp&&e.curOp.scrollTop||e.doc.scrollTop)&&Kr(e,r)}function Vi(e,t,r,n){var i=new Ms(e,r,n),o=e.cm;return o&&i.noHScroll&&(o.display.alignWidgets=!0),Ri(e,t,"widget",function(t){var r=t.widgets||(t.widgets=[]);if(null==i.insertAt?r.push(i):r.splice(Math.min(r.length-1,Math.max(0,i.insertAt)),0,i),i.line=t,o&&!ve(e,t)){var n=ye(t)<e.scrollTop;A(t,t.height+Ht(i)),n&&Kr(o,i.height),o.curOp.forceUpdate=!0}return!0}),bt(o,"lineWidgetAdded",o,i,"number"==typeof t?t:W(t)),i}function Ki(e,t,r,n,o){if(n&&n.shared)return ji(e,t,r,n,o);if(e.cm&&!e.cm.curOp)return dn(e.cm,Ki)(e,t,r,n,o);var l=new Os(e,o),s=P(t,r);if(n&&c(n,l,!1),s>0||0==s&&!1!==l.clearWhenEmpty)return l;if(l.replacedWith&&(l.collapsed=!0,l.widgetNode=i("span",[l.replacedWith],"CodeMirror-widget"),n.handleMouseEvents||l.widgetNode.setAttribute("cm-ignore-events","true"),n.insertLeft&&(l.widgetNode.insertLeft=!0)),l.collapsed){if(ce(e,t.line,t,r,l)||t.line!=r.line&&ce(e,r.line,t,r,l))throw new Error("Inserting collapsed marker partially overlapping an existing one");X()}l.addToHistory&&ni(e,{from:t,to:r,origin:"markText"},e.sel,NaN);var a,u=t.line,f=e.cm;if(e.iter(u,r.line+1,function(e){f&&l.collapsed&&!f.options.lineWrapping&&fe(e)==f.display.maxLine&&(a=!0),l.collapsed&&u!=t.line&&A(e,0),q(e,new Y(l,u==t.line?t.ch:null,u==r.line?r.ch:null)),++u}),l.collapsed&&e.iter(t.line,r.line+1,function(t){ve(e,t)&&A(t,0)}),l.clearOnEnter&&Ql(l,"beforeCursorEnter",function(){return l.clear()}),l.readOnly&&(j(),(e.history.done.length||e.history.undone.length)&&e.clearHistory()),l.collapsed&&(l.id=++Ns,l.atomic=!0),f){if(a&&(f.curOp.updateMaxLine=!0),l.collapsed)vn(f,t.line,r.line+1);else if(l.className||l.title||l.startStyle||l.endStyle||l.css)for(var h=t.line;h<=r.line;h++)mn(f,h,"text");l.atomic&&Ci(f.doc),bt(f,"markerAdded",f,l)}return l}function ji(e,t,r,n,i){(n=c(n)).shared=!1;var o=[Ki(e,t,r,n,i)],l=o[0],s=n.widgetNode;return $n(e,function(e){s&&(n.widgetNode=s.cloneNode(!0)),o.push(Ki(e,U(e,t),U(e,r),n,i));for(var a=0;a<e.linked.length;++a)if(e.linked[a].isParent)return;l=g(o)}),new As(o,l)}function Xi(e){return e.findMarks(E(e.first,0),e.clipPos(E(e.lastLine())),function(e){return e.parent})}function Yi(e,t){for(var r=0;r<t.length;r++){var n=t[r],i=n.find(),o=e.clipPos(i.from),l=e.clipPos(i.to);if(P(o,l)){var s=Ki(e,o,l,n.primary,n.primary.type);n.markers.push(s),s.parent=n}}}function _i(e){for(var t=0;t<e.length;t++)!function(t){var r=e[t],n=[r.primary.doc];$n(r.primary.doc,function(e){return n.push(e)});for(var i=0;i<r.markers.length;i++){var o=r.markers[i];-1==h(n,o.doc)&&(o.parent=null,r.markers.splice(i--,1))}}(t)}function $i(e){var t=this;if(Qi(t),!Me(t,e)&&!Ft(t.display,e)){We(e),gl&&(Hs=+new Date);var r=Sr(t,e,!0),n=e.dataTransfer.files;if(r&&!t.isReadOnly())if(n&&n.length&&window.FileReader&&window.File)for(var i=n.length,o=Array(i),l=0,s=0;s<i;++s)!function(e,n){if(!t.options.allowDropFileTypes||-1!=h(t.options.allowDropFileTypes,e.type)){var s=new FileReader;s.onload=dn(t,function(){var e=s.result;if(/[\x00-\x08\x0e-\x1f]{2}/.test(e)&&(e=""),o[n]=e,++l==i){var a={from:r=U(t.doc,r),to:r,text:t.doc.splitLines(o.join(t.doc.lineSeparator())),origin:"paste"};Oi(t.doc,a),yi(t.doc,Rn(r,Bn(a)))}}),s.readAsText(e)}}(n[s],s);else{if(t.state.draggingText&&t.doc.sel.contains(r)>-1)return t.state.draggingText(e),void setTimeout(function(){return t.display.input.focus()},20);try{var a=e.dataTransfer.getData("Text");if(a){var u;if(t.state.draggingText&&!t.state.draggingText.copy&&(u=t.listSelections()),wi(t.doc,Rn(r,r)),u)for(var c=0;c<u.length;++c)Ei(t.doc,"",u[c].anchor,u[c].head,"drag");t.replaceSelection(a,"around","paste"),t.display.input.focus()}}catch(e){}}}}function qi(e,t){if(gl&&(!e.state.draggingText||+new Date-Hs<100))Fe(t);else if(!Me(e,t)&&!Ft(e.display,t)&&(t.dataTransfer.setData("Text",e.getSelection()),t.dataTransfer.effectAllowed="copyMove",t.dataTransfer.setDragImage&&!xl)){var r=n("img",null,null,"position: fixed; left: 0; top: 0;");r.src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==",wl&&(r.width=r.height=1,e.display.wrapper.appendChild(r),r._top=r.offsetTop),t.dataTransfer.setDragImage(r,0,0),wl&&r.parentNode.removeChild(r)}}function Zi(e,t){var i=Sr(e,t);if(i){var o=document.createDocumentFragment();Mr(e,i,o),e.display.dragCursor||(e.display.dragCursor=n("div",null,"CodeMirror-cursors CodeMirror-dragcursors"),e.display.lineSpace.insertBefore(e.display.dragCursor,e.display.cursorDiv)),r(e.display.dragCursor,o)}}function Qi(e){e.display.dragCursor&&(e.display.lineSpace.removeChild(e.display.dragCursor),e.display.dragCursor=null)}function Ji(e){if(document.getElementsByClassName)for(var t=document.getElementsByClassName("CodeMirror"),r=0;r<t.length;r++){var n=t[r].CodeMirror;n&&e(n)}}function eo(){Fs||(to(),Fs=!0)}function to(){var e;Ql(window,"resize",function(){null==e&&(e=setTimeout(function(){e=null,Ji(ro)},100))}),Ql(window,"blur",function(){return Ji(Fr)})}function ro(e){var t=e.display;t.lastWrapHeight==t.wrapper.clientHeight&&t.lastWrapWidth==t.wrapper.clientWidth||(t.cachedCharWidth=t.cachedTextHeight=t.cachedPaddingH=null,t.scrollbarsClipped=!1,e.setSize())}function no(e){var t=e.split(/-(?!$)/);e=t[t.length-1];for(var r,n,i,o,l=0;l<t.length-1;l++){var s=t[l];if(/^(cmd|meta|m)$/i.test(s))o=!0;else if(/^a(lt)?$/i.test(s))r=!0;else if(/^(c|ctrl|control)$/i.test(s))n=!0;else{if(!/^s(hift)?$/i.test(s))throw new Error("Unrecognized modifier name: "+s);i=!0}}return r&&(e="Alt-"+e),n&&(e="Ctrl-"+e),o&&(e="Cmd-"+e),i&&(e="Shift-"+e),e}function io(e){var t={};for(var r in e)if(e.hasOwnProperty(r)){var n=e[r];if(/^(name|fallthrough|(de|at)tach)$/.test(r))continue;if("..."==n){delete e[r];continue}for(var i=v(r.split(" "),no),o=0;o<i.length;o++){var l=void 0,s=void 0;o==i.length-1?(s=i.join(" "),l=n):(s=i.slice(0,o+1).join(" "),l="...");var a=t[s];if(a){if(a!=l)throw new Error("Inconsistent bindings for "+s)}else t[s]=l}delete e[r]}for(var u in t)e[u]=t[u];return e}function oo(e,t,r,n){var i=(t=uo(t)).call?t.call(e,n):t[e];if(!1===i)return"nothing";if("..."===i)return"multi";if(null!=i&&r(i))return"handled";if(t.fallthrough){if("[object Array]"!=Object.prototype.toString.call(t.fallthrough))return oo(e,t.fallthrough,r,n);for(var o=0;o<t.fallthrough.length;o++){var l=oo(e,t.fallthrough[o],r,n);if(l)return l}}}function lo(e){var t="string"==typeof e?e:Es[e.keyCode];return"Ctrl"==t||"Alt"==t||"Shift"==t||"Mod"==t}function so(e,t,r){var n=e;return t.altKey&&"Alt"!=n&&(e="Alt-"+e),(Dl?t.metaKey:t.ctrlKey)&&"Ctrl"!=n&&(e="Ctrl-"+e),(Dl?t.ctrlKey:t.metaKey)&&"Cmd"!=n&&(e="Cmd-"+e),!r&&t.shiftKey&&"Shift"!=n&&(e="Shift-"+e),e}function ao(e,t){if(wl&&34==e.keyCode&&e.char)return!1;var r=Es[e.keyCode];return null!=r&&!e.altGraphKey&&so(r,e,t)}function uo(e){return"string"==typeof e?Rs[e]:e}function co(e,t){for(var r=e.doc.sel.ranges,n=[],i=0;i<r.length;i++){for(var o=t(r[i]);n.length&&P(o.from,g(n).to)<=0;){var l=n.pop();if(P(l.from,o.from)<0){o.from=l.from;break}}n.push(o)}hn(e,function(){for(var t=n.length-1;t>=0;t--)Ei(e.doc,"",n[t].from,n[t].to,"+delete");jr(e)})}function fo(e,t,r){var n=L(e.text,t+r,r);return n<0||n>e.text.length?null:n}function ho(e,t,r){var n=fo(e,t.ch,r);return null==n?null:new E(t.line,n,r<0?"after":"before")}function po(e,t,r,n,i){if(e){var o=Se(r,t.doc.direction);if(o){var l,s=i<0?g(o):o[0],a=i<0==(1==s.level)?"after":"before";if(s.level>0){var u=Xt(t,r);l=i<0?r.text.length-1:0;var c=Yt(t,u,l).top;l=k(function(e){return Yt(t,u,e).top==c},i<0==(1==s.level)?s.from:s.to-1,l),"before"==a&&(l=fo(r,l,1))}else l=i<0?s.to:s.from;return new E(n,l,a)}}return new E(n,i<0?r.text.length:0,i<0?"before":"after")}function go(e,t,r,n){var i=Se(t,e.doc.direction);if(!i)return ho(t,r,n);r.ch>=t.text.length?(r.ch=t.text.length,r.sticky="before"):r.ch<=0&&(r.ch=0,r.sticky="after");var o=Ce(i,r.ch,r.sticky),l=i[o];if("ltr"==e.doc.direction&&l.level%2==0&&(n>0?l.to>r.ch:l.from<r.ch))return ho(t,r,n);var s,a=function(e,r){return fo(t,e instanceof E?e.ch:e,r)},u=function(r){return e.options.lineWrapping?(s=s||Xt(e,t),hr(e,t,s,r)):{begin:0,end:t.text.length}},c=u("before"==r.sticky?a(r,-1):r.ch);if("rtl"==e.doc.direction||1==l.level){var f=1==l.level==n<0,h=a(r,f?1:-1);if(null!=h&&(f?h<=l.to&&h<=c.end:h>=l.from&&h>=c.begin)){var d=f?"before":"after";return new E(r.line,h,d)}}var p=function(e,t,n){for(var o=function(e,t){return t?new E(r.line,a(e,1),"before"):new E(r.line,e,"after")};e>=0&&e<i.length;e+=t){var l=i[e],s=t>0==(1!=l.level),u=s?n.begin:a(n.end,-1);if(l.from<=u&&u<l.to)return o(u,s);if(u=s?l.from:a(l.to,-1),n.begin<=u&&u<n.end)return o(u,s)}},g=p(o+n,n,c);if(g)return g;var v=n>0?c.end:a(c.begin,-1);return null==v||n>0&&v==t.text.length||!(g=p(n>0?0:i.length-1,n,u(v)))?null:g}function vo(e,t){var r=M(e.doc,t),n=fe(r);return n!=r&&(t=W(n)),po(!0,e,n,t,1)}function mo(e,t){var r=M(e.doc,t),n=he(r);return n!=r&&(t=W(n)),po(!0,e,r,t,-1)}function yo(e,t){var r=vo(e,t.line),n=M(e.doc,r.line),i=Se(n,e.doc.direction);if(!i||0==i[0].level){var o=Math.max(0,n.text.search(/\S/)),l=t.line==r.line&&t.ch<=o&&t.ch;return E(r.line,l?0:o,r.sticky)}return r}function bo(e,t,r){if("string"==typeof t&&!(t=Bs[t]))return!1;e.display.input.ensurePolled();var n=e.display.shift,i=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),r&&(e.display.shift=!1),i=t(e)!=Bl}finally{e.display.shift=n,e.state.suppressEdits=!1}return i}function wo(e,t,r){for(var n=0;n<e.state.keyMaps.length;n++){var i=oo(t,e.state.keyMaps[n],r,e);if(i)return i}return e.options.extraKeys&&oo(t,e.options.extraKeys,r,e)||oo(t,e.options.keyMap,r,e)}function xo(e,t,r,n){var i=e.state.keySeq;if(i){if(lo(t))return"handled";Gs.set(50,function(){e.state.keySeq==i&&(e.state.keySeq=null,e.display.input.reset())}),t=i+" "+t}var o=wo(e,t,n);return"multi"==o&&(e.state.keySeq=t),"handled"==o&&bt(e,"keyHandled",e,t,r),"handled"!=o&&"multi"!=o||(We(r),Ar(e)),i&&!o&&/\'$/.test(t)?(We(r),!0):!!o}function Co(e,t){var r=ao(t,!0);return!!r&&(t.shiftKey&&!e.state.keySeq?xo(e,"Shift-"+r,t,function(t){return bo(e,t,!0)})||xo(e,r,t,function(t){if("string"==typeof t?/^go[A-Z]/.test(t):t.motion)return bo(e,t)}):xo(e,r,t,function(t){return bo(e,t)}))}function So(e,t,r){return xo(e,"'"+r+"'",t,function(t){return bo(e,t,!0)})}function Lo(e){var t=this;if(t.curOp.focus=l(),!Me(t,e)){gl&&vl<11&&27==e.keyCode&&(e.returnValue=!1);var r=e.keyCode;t.display.shift=16==r||e.shiftKey;var n=Co(t,e);wl&&(Us=n?r:null,!n&&88==r&&!rs&&(Ml?e.metaKey:e.ctrlKey)&&t.replaceSelection("",null,"cut")),18!=r||/\bCodeMirror-crosshair\b/.test(t.display.lineDiv.className)||ko(t)}}function ko(e){function t(e){18!=e.keyCode&&e.altKey||(Fl(r,"CodeMirror-crosshair"),ke(document,"keyup",t),ke(document,"mouseover",t))}var r=e.display.lineDiv;s(r,"CodeMirror-crosshair"),Ql(document,"keyup",t),Ql(document,"mouseover",t)}function To(e){16==e.keyCode&&(this.doc.sel.shift=!1),Me(this,e)}function Mo(e){var t=this;if(!(Ft(t.display,e)||Me(t,e)||e.ctrlKey&&!e.altKey||Ml&&e.metaKey)){var r=e.keyCode,n=e.charCode;if(wl&&r==Us)return Us=null,void We(e);if(!wl||e.which&&!(e.which<10)||!Co(t,e)){var i=String.fromCharCode(null==n?r:n);"\b"!=i&&(So(t,e,i)||t.display.input.onKeyPress(e))}}}function No(e,t){var r=+new Date;return js&&js.compare(r,e,t)?(Ks=js=null,"triple"):Ks&&Ks.compare(r,e,t)?(js=new Vs(r,e,t),Ks=null,"double"):(Ks=new Vs(r,e,t),js=null,"single")}function Oo(e){var t=this,r=t.display;if(!(Me(t,e)||r.activeTouch&&r.input.supportsTouch()))if(r.input.ensurePolled(),r.shift=e.shiftKey,Ft(r,e))ml||(r.scroller.draggable=!1,setTimeout(function(){return r.scroller.draggable=!0},100));else if(!zo(t,e)){var n=Sr(t,e),i=Pe(e),o=n?No(n,i):"single";window.focus(),1==i&&t.state.selectingText&&t.state.selectingText(e),n&&Ao(t,i,n,o,e)||(1==i?n?Do(t,n,o,e):Ee(e)==r.scroller&&We(e):2==i?(n&&di(t.doc,n),setTimeout(function(){return r.input.focus()},20)):3==i&&(Hl?Ro(t,e):Dr(t)))}}function Ao(e,t,r,n,i){var o="Click";return"double"==n?o="Double"+o:"triple"==n&&(o="Triple"+o),o=(1==t?"Left":2==t?"Middle":"Right")+o,xo(e,so(o,i),i,function(t){if("string"==typeof t&&(t=Bs[t]),!t)return!1;var n=!1;try{e.isReadOnly()&&(e.state.suppressEdits=!0),n=t(e,r)!=Bl}finally{e.state.suppressEdits=!1}return n})}function Wo(e,t,r){var n=e.getOption("configureMouse"),i=n?n(e,t,r):{};if(null==i.unit){var o=Nl?r.shiftKey&&r.metaKey:r.altKey;i.unit=o?"rectangle":"single"==t?"char":"double"==t?"word":"line"}return(null==i.extend||e.doc.extend)&&(i.extend=e.doc.extend||r.shiftKey),null==i.addNew&&(i.addNew=Ml?r.metaKey:r.ctrlKey),null==i.moveOnDrag&&(i.moveOnDrag=!(Ml?r.altKey:r.ctrlKey)),i}function Do(e,t,r,n){gl?setTimeout(u(Wr,e),0):e.curOp.focus=l();var i,o=Wo(e,r,n),s=e.doc.sel;e.options.dragDrop&&Jl&&!e.isReadOnly()&&"single"==r&&(i=s.contains(t))>-1&&(P((i=s.ranges[i]).from(),t)<0||t.xRel>0)&&(P(i.to(),t)>0||t.xRel<0)?Ho(e,n,t,o):Eo(e,n,t,o)}function Ho(e,t,r,n){var i=e.display,o=!1,l=dn(e,function(t){ml&&(i.scroller.draggable=!1),e.state.draggingText=!1,ke(document,"mouseup",l),ke(document,"mousemove",s),ke(i.scroller,"dragstart",a),ke(i.scroller,"drop",l),o||(We(t),n.addNew||di(e.doc,r,null,null,n.extend),ml||gl&&9==vl?setTimeout(function(){document.body.focus(),i.input.focus()},20):i.input.focus())}),s=function(e){o=o||Math.abs(t.clientX-e.clientX)+Math.abs(t.clientY-e.clientY)>=10},a=function(){return o=!0};ml&&(i.scroller.draggable=!0),e.state.draggingText=l,l.copy=!n.moveOnDrag,i.scroller.dragDrop&&i.scroller.dragDrop(),Ql(document,"mouseup",l),Ql(document,"mousemove",s),Ql(i.scroller,"dragstart",a),Ql(i.scroller,"drop",l),Dr(e),setTimeout(function(){return i.input.focus()},20)}function Fo(e,t,r){if("char"==r)return new Ts(t,t);if("word"==r)return e.findWordAt(t);if("line"==r)return new Ts(E(t.line,0),U(e.doc,E(t.line+1,0)));var n=r(e,t);return new Ts(n.from,n.to)}function Eo(e,t,r,n){function i(t){if(0!=P(m,t))if(m=t,"rectangle"==n.unit){for(var i=[],o=e.options.tabSize,l=f(M(u,r.line).text,r.ch,o),s=f(M(u,t.line).text,t.ch,o),a=Math.min(l,s),g=Math.max(l,s),v=Math.min(r.line,t.line),y=Math.min(e.lastLine(),Math.max(r.line,t.line));v<=y;v++){var b=M(u,v).text,w=d(b,a,o);a==g?i.push(new Ts(E(v,w),E(v,w))):b.length>w&&i.push(new Ts(E(v,w),E(v,d(b,g,o))))}i.length||i.push(new Ts(r,r)),bi(u,zn(p.ranges.slice(0,h).concat(i),h),{origin:"*mouse",scroll:!1}),e.scrollIntoView(t)}else{var x,C=c,S=Fo(e,t,n.unit),L=C.anchor;P(S.anchor,L)>0?(x=S.head,L=B(C.from(),S.anchor)):(x=S.anchor,L=R(C.to(),S.head));var k=p.ranges.slice(0);k[h]=Po(e,new Ts(U(u,L),x)),bi(u,zn(k,h),Ul)}}function o(t){var r=++b,s=Sr(e,t,!0,"rectangle"==n.unit);if(s)if(0!=P(s,m)){e.curOp.focus=l(),i(s);var c=Ir(a,u);(s.line>=c.to||s.line<c.from)&&setTimeout(dn(e,function(){b==r&&o(t)}),150)}else{var f=t.clientY<y.top?-20:t.clientY>y.bottom?20:0;f&&setTimeout(dn(e,function(){b==r&&(a.scroller.scrollTop+=f,o(t))}),50)}}function s(t){e.state.selectingText=!1,b=1/0,We(t),a.input.focus(),ke(document,"mousemove",w),ke(document,"mouseup",x),u.history.lastSelOrigin=null}var a=e.display,u=e.doc;We(t);var c,h,p=u.sel,g=p.ranges;if(n.addNew&&!n.extend?(h=u.sel.contains(r),c=h>-1?g[h]:new Ts(r,r)):(c=u.sel.primary(),h=u.sel.primIndex),"rectangle"==n.unit)n.addNew||(c=new Ts(r,r)),r=Sr(e,t,!0,!0),h=-1;else{var v=Fo(e,r,n.unit);c=n.extend?hi(c,v.anchor,v.head,n.extend):v}n.addNew?-1==h?(h=g.length,bi(u,zn(g.concat([c]),h),{scroll:!1,origin:"*mouse"})):g.length>1&&g[h].empty()&&"char"==n.unit&&!n.extend?(bi(u,zn(g.slice(0,h).concat(g.slice(h+1)),0),{scroll:!1,origin:"*mouse"}),p=u.sel):gi(u,h,c,Ul):(h=0,bi(u,new ks([c],0),Ul),p=u.sel);var m=r,y=a.wrapper.getBoundingClientRect(),b=0,w=dn(e,function(e){Pe(e)?o(e):s(e)}),x=dn(e,s);e.state.selectingText=x,Ql(document,"mousemove",w),Ql(document,"mouseup",x)}function Po(e,t){var r=t.anchor,n=t.head,i=M(e.doc,r.line);if(0==P(r,n)&&r.sticky==n.sticky)return t;var o=Se(i);if(!o)return t;var l=Ce(o,r.ch,r.sticky),s=o[l];if(s.from!=r.ch&&s.to!=r.ch)return t;var a=l+(s.from==r.ch==(1!=s.level)?0:1);if(0==a||a==o.length)return t;var u;if(n.line!=r.line)u=(n.line-r.line)*("ltr"==e.doc.direction?1:-1)>0;else{var c=Ce(o,n.ch,n.sticky),f=c-l||(n.ch-r.ch)*(1==s.level?-1:1);u=c==a-1||c==a?f<0:f>0}var h=o[a+(u?-1:0)],d=u==(1==h.level),p=d?h.from:h.to,g=d?"after":"before";return r.ch==p&&r.sticky==g?t:new Ts(new E(r.line,p,g),n)}function Io(e,t,r,n){var i,o;if(t.touches)i=t.touches[0].clientX,o=t.touches[0].clientY;else try{i=t.clientX,o=t.clientY}catch(t){return!1}if(i>=Math.floor(e.display.gutters.getBoundingClientRect().right))return!1;n&&We(t);var l=e.display,s=l.lineDiv.getBoundingClientRect();if(o>s.bottom||!Oe(e,r))return He(t);o-=s.top-l.viewOffset;for(var a=0;a<e.options.gutters.length;++a){var u=l.gutters.childNodes[a];if(u&&u.getBoundingClientRect().right>=i)return Te(e,r,e,D(e.doc,o),e.options.gutters[a],t),He(t)}}function zo(e,t){return Io(e,t,"gutterClick",!0)}function Ro(e,t){Ft(e.display,t)||Bo(e,t)||Me(e,t,"contextmenu")||e.display.input.onContextMenu(t)}function Bo(e,t){return!!Oe(e,"gutterContextMenu")&&Io(e,t,"gutterContextMenu",!1)}function Go(e){e.display.wrapper.className=e.display.wrapper.className.replace(/\s*cm-s-\S+/g,"")+e.options.theme.replace(/(^|\s)\s*/g," cm-s-"),er(e)}function Uo(e){Hn(e),vn(e),zr(e)}function Vo(e,t,r){if(!t!=!(r&&r!=Xs)){var n=e.display.dragFunctions,i=t?Ql:ke;i(e.display.scroller,"dragstart",n.start),i(e.display.scroller,"dragenter",n.enter),i(e.display.scroller,"dragover",n.over),i(e.display.scroller,"dragleave",n.leave),i(e.display.scroller,"drop",n.drop)}}function Ko(e){e.options.lineWrapping?(s(e.display.wrapper,"CodeMirror-wrap"),e.display.sizer.style.minWidth="",e.display.sizerWidth=null):(Fl(e.display.wrapper,"CodeMirror-wrap"),we(e)),Cr(e),vn(e),er(e),setTimeout(function(){return en(e)},100)}function jo(e,t){var r=this;if(!(this instanceof jo))return new jo(e,t);this.options=t=t?c(t):{},c(Ys,t,!1),Fn(t);var n=t.value;"string"==typeof n&&(n=new Ds(n,t.mode,null,t.lineSeparator,t.direction)),this.doc=n;var i=new jo.inputStyles[t.inputStyle](this),o=this.display=new T(e,n,i);o.wrapper.CodeMirror=this,Hn(this),Go(this),t.lineWrapping&&(this.display.wrapper.className+=" CodeMirror-wrap"),rn(this),this.state={keyMaps:[],overlays:[],modeGen:0,overwrite:!1,delayingBlurEvent:!1,focused:!1,suppressEdits:!1,pasteIncoming:!1,cutIncoming:!1,selectingText:!1,draggingText:!1,highlight:new Pl,keySeq:null,specialChars:null},t.autofocus&&!Tl&&o.input.focus(),gl&&vl<11&&setTimeout(function(){return r.display.input.reset(!0)},20),Xo(this),eo(),nn(this),this.curOp.forceUpdate=!0,qn(this,n),t.autofocus&&!Tl||this.hasFocus()?setTimeout(u(Hr,this),20):Fr(this);for(var l in _s)_s.hasOwnProperty(l)&&_s[l](r,t[l],Xs);Rr(this),t.finishInit&&t.finishInit(this);for(var s=0;s<$s.length;++s)$s[s](r);on(this),ml&&t.lineWrapping&&"optimizelegibility"==getComputedStyle(o.lineDiv).textRendering&&(o.lineDiv.style.textRendering="auto")}function Xo(e){function t(){i.activeTouch&&(o=setTimeout(function(){return i.activeTouch=null},1e3),(l=i.activeTouch).end=+new Date)}function r(e){if(1!=e.touches.length)return!1;var t=e.touches[0];return t.radiusX<=1&&t.radiusY<=1}function n(e,t){if(null==t.left)return!0;var r=t.left-e.left,n=t.top-e.top;return r*r+n*n>400}var i=e.display;Ql(i.scroller,"mousedown",dn(e,Oo)),gl&&vl<11?Ql(i.scroller,"dblclick",dn(e,function(t){if(!Me(e,t)){var r=Sr(e,t);if(r&&!zo(e,t)&&!Ft(e.display,t)){We(t);var n=e.findWordAt(r);di(e.doc,n.anchor,n.head)}}})):Ql(i.scroller,"dblclick",function(t){return Me(e,t)||We(t)}),Hl||Ql(i.scroller,"contextmenu",function(t){return Ro(e,t)});var o,l={end:0};Ql(i.scroller,"touchstart",function(t){if(!Me(e,t)&&!r(t)&&!zo(e,t)){i.input.ensurePolled(),clearTimeout(o);var n=+new Date;i.activeTouch={start:n,moved:!1,prev:n-l.end<=300?l:null},1==t.touches.length&&(i.activeTouch.left=t.touches[0].pageX,i.activeTouch.top=t.touches[0].pageY)}}),Ql(i.scroller,"touchmove",function(){i.activeTouch&&(i.activeTouch.moved=!0)}),Ql(i.scroller,"touchend",function(r){var o=i.activeTouch;if(o&&!Ft(i,r)&&null!=o.left&&!o.moved&&new Date-o.start<300){var l,s=e.coordsChar(i.activeTouch,"page");l=!o.prev||n(o,o.prev)?new Ts(s,s):!o.prev.prev||n(o,o.prev.prev)?e.findWordAt(s):new Ts(E(s.line,0),U(e.doc,E(s.line+1,0))),e.setSelection(l.anchor,l.head),e.focus(),We(r)}t()}),Ql(i.scroller,"touchcancel",t),Ql(i.scroller,"scroll",function(){i.scroller.clientHeight&&(qr(e,i.scroller.scrollTop),Qr(e,i.scroller.scrollLeft,!0),Te(e,"scroll",e))}),Ql(i.scroller,"mousewheel",function(t){return In(e,t)}),Ql(i.scroller,"DOMMouseScroll",function(t){return In(e,t)}),Ql(i.wrapper,"scroll",function(){return i.wrapper.scrollTop=i.wrapper.scrollLeft=0}),i.dragFunctions={enter:function(t){Me(e,t)||Fe(t)},over:function(t){Me(e,t)||(Zi(e,t),Fe(t))},start:function(t){return qi(e,t)},drop:dn(e,$i),leave:function(t){Me(e,t)||Qi(e)}};var s=i.input.getField();Ql(s,"keyup",function(t){return To.call(e,t)}),Ql(s,"keydown",dn(e,Lo)),Ql(s,"keypress",dn(e,Mo)),Ql(s,"focus",function(t){return Hr(e,t)}),Ql(s,"blur",function(t){return Fr(e,t)})}function Yo(e,t,r,n){var i,o=e.doc;null==r&&(r="add"),"smart"==r&&(o.mode.indent?i=$e(e,t).state:r="prev");var l=e.options.tabSize,s=M(o,t),a=f(s.text,null,l);s.stateAfter&&(s.stateAfter=null);var u,c=s.text.match(/^\s*/)[0];if(n||/\S/.test(s.text)){if("smart"==r&&((u=o.mode.indent(i,s.text.slice(c.length),s.text))==Bl||u>150)){if(!n)return;r="prev"}}else u=0,r="not";"prev"==r?u=t>o.first?f(M(o,t-1).text,null,l):0:"add"==r?u=a+e.options.indentUnit:"subtract"==r?u=a-e.options.indentUnit:"number"==typeof r&&(u=a+r),u=Math.max(0,u);var h="",d=0;if(e.options.indentWithTabs)for(var g=Math.floor(u/l);g;--g)d+=l,h+="\t";if(d<u&&(h+=p(u-d)),h!=c)return Ei(o,h,E(t,0),E(t,c.length),"+input"),s.stateAfter=null,!0;for(var v=0;v<o.sel.ranges.length;v++){var m=o.sel.ranges[v];if(m.head.line==t&&m.head.ch<c.length){var y=E(t,c.length);gi(o,v,new Ts(y,y));break}}}function _o(e){qs=e}function $o(e,t,r,n,i){var o=e.doc;e.display.shift=!1,n||(n=o.sel);var l=e.state.pasteIncoming||"paste"==i,s=es(t),a=null;if(l&&n.ranges.length>1)if(qs&&qs.text.join("\n")==t){if(n.ranges.length%qs.text.length==0){a=[];for(var u=0;u<qs.text.length;u++)a.push(o.splitLines(qs.text[u]))}}else s.length==n.ranges.length&&e.options.pasteLinesPerSelection&&(a=v(s,function(e){return[e]}));for(var c,f=n.ranges.length-1;f>=0;f--){var h=n.ranges[f],d=h.from(),p=h.to();h.empty()&&(r&&r>0?d=E(d.line,d.ch-r):e.state.overwrite&&!l?p=E(p.line,Math.min(M(o,p.line).text.length,p.ch+g(s).length)):qs&&qs.lineWise&&qs.text.join("\n")==t&&(d=p=E(d.line,0))),c=e.curOp.updateInput;var m={from:d,to:p,text:a?a[f%a.length]:s,origin:i||(l?"paste":e.state.cutIncoming?"cut":"+input")};Oi(e.doc,m),bt(e,"inputRead",e,m)}t&&!l&&Zo(e,t),jr(e),e.curOp.updateInput=c,e.curOp.typing=!0,e.state.pasteIncoming=e.state.cutIncoming=!1}function qo(e,t){var r=e.clipboardData&&e.clipboardData.getData("Text");if(r)return e.preventDefault(),t.isReadOnly()||t.options.disableInput||hn(t,function(){return $o(t,r,0,null,"paste")}),!0}function Zo(e,t){if(e.options.electricChars&&e.options.smartIndent)for(var r=e.doc.sel,n=r.ranges.length-1;n>=0;n--){var i=r.ranges[n];if(!(i.head.ch>100||n&&r.ranges[n-1].head.line==i.head.line)){var o=e.getModeAt(i.head),l=!1;if(o.electricChars){for(var s=0;s<o.electricChars.length;s++)if(t.indexOf(o.electricChars.charAt(s))>-1){l=Yo(e,i.head.line,"smart");break}}else o.electricInput&&o.electricInput.test(M(e.doc,i.head.line).text.slice(0,i.head.ch))&&(l=Yo(e,i.head.line,"smart"));l&&bt(e,"electricInput",e,i.head.line)}}}function Qo(e){for(var t=[],r=[],n=0;n<e.doc.sel.ranges.length;n++){var i=e.doc.sel.ranges[n].head.line,o={anchor:E(i,0),head:E(i+1,0)};r.push(o),t.push(e.getRange(o.anchor,o.head))}return{text:t,ranges:r}}function Jo(e,t){e.setAttribute("autocorrect","off"),e.setAttribute("autocapitalize","off"),e.setAttribute("spellcheck",!!t)}function el(){var e=n("textarea",null,null,"position: absolute; bottom: -1em; padding: 0; width: 1px; height: 1em; outline: none"),t=n("div",[e],null,"overflow: hidden; position: relative; width: 3px; height: 0px;");return ml?e.style.width="1000px":e.setAttribute("wrap","off"),Ll&&(e.style.border="1px solid black"),Jo(e),t}function tl(e,t,r,n,i){function o(){var n=t.line+r;return!(n<e.first||n>=e.first+e.size)&&(t=new E(n,t.ch,t.sticky),u=M(e,n))}function l(n){var l;if(null==(l=i?go(e.cm,u,t,r):ho(u,t,r))){if(n||!o())return!1;t=po(i,e.cm,u,t.line,r)}else t=l;return!0}var s=t,a=r,u=M(e,t.line);if("char"==n)l();else if("column"==n)l(!0);else if("word"==n||"group"==n)for(var c=null,f="group"==n,h=e.cm&&e.cm.getHelper(t,"wordChars"),d=!0;!(r<0)||l(!d);d=!1){var p=u.text.charAt(t.ch)||"\n",g=x(p,h)?"w":f&&"\n"==p?"n":!f||/\s/.test(p)?null:"p";if(!f||d||g||(g="s"),c&&c!=g){r<0&&(r=1,l(),t.sticky="after");break}if(g&&(c=g),r>0&&!l(!d))break}var v=ki(e,t,s,a,!0);return I(s,v)&&(v.hitSide=!0),v}function rl(e,t,r,n){var i,o=e.doc,l=t.left;if("page"==n){var s=Math.min(e.display.wrapper.clientHeight,window.innerHeight||document.documentElement.clientHeight),a=Math.max(s-.5*mr(e.display),3);i=(r>0?t.bottom:t.top)+r*a}else"line"==n&&(i=r>0?t.bottom+3:t.top-3);for(var u;(u=cr(e,l,i)).outside;){if(r<0?i<=0:i>=o.height){u.hitSide=!0;break}i+=5*r}return u}function nl(e,t){var r=jt(e,t.line);if(!r||r.hidden)return null;var n=M(e.doc,t.line),i=Ut(r,n,t.line),o=Se(n,e.doc.direction),l="left";o&&(l=Ce(o,t.ch)%2?"right":"left");var s=_t(i.map,t.ch,l);return s.offset="right"==s.collapse?s.end:s.start,s}function il(e){for(var t=e;t;t=t.parentNode)if(/CodeMirror-gutter-wrapper/.test(t.className))return!0;return!1}function ol(e,t){return t&&(e.bad=!0),e}function ll(e,t,r,n,i){function o(e){return function(t){return t.id==e}}function l(){c&&(u+=f,c=!1)}function s(e){e&&(l(),u+=e)}function a(t){if(1==t.nodeType){var r=t.getAttribute("cm-text");if(null!=r)return void s(r||t.textContent.replace(/\u200b/g,""));var u,h=t.getAttribute("cm-marker");if(h){var d=e.findMarks(E(n,0),E(i+1,0),o(+h));return void(d.length&&(u=d[0].find(0))&&s(N(e.doc,u.from,u.to).join(f)))}if("false"==t.getAttribute("contenteditable"))return;var p=/^(pre|div|p)$/i.test(t.nodeName);p&&l();for(var g=0;g<t.childNodes.length;g++)a(t.childNodes[g]);p&&(c=!0)}else 3==t.nodeType&&s(t.nodeValue)}for(var u="",c=!1,f=e.doc.lineSeparator();a(t),t!=r;)t=t.nextSibling;return u}function sl(e,t,r){var n;if(t==e.display.lineDiv){if(!(n=e.display.lineDiv.childNodes[r]))return ol(e.clipPos(E(e.display.viewTo-1)),!0);t=null,r=0}else for(n=t;;n=n.parentNode){if(!n||n==e.display.lineDiv)return null;if(n.parentNode&&n.parentNode==e.display.lineDiv)break}for(var i=0;i<e.display.view.length;i++){var o=e.display.view[i];if(o.node==n)return al(o,t,r)}}function al(e,t,r){function n(t,r,n){for(var i=-1;i<(f?f.length:0);i++)for(var o=i<0?c.map:f[i],l=0;l<o.length;l+=3){var s=o[l+2];if(s==t||s==r){var a=W(i<0?e.line:e.rest[i]),u=o[l]+n;return(n<0||s!=t)&&(u=o[l+(n?1:0)]),E(a,u)}}}var i=e.text.firstChild,l=!1;if(!t||!o(i,t))return ol(E(W(e.line),0),!0);if(t==i&&(l=!0,t=i.childNodes[r],r=0,!t)){var s=e.rest?g(e.rest):e.line;return ol(E(W(s),s.text.length),l)}var a=3==t.nodeType?t:null,u=t;for(a||1!=t.childNodes.length||3!=t.firstChild.nodeType||(a=t.firstChild,r&&(r=a.nodeValue.length));u.parentNode!=i;)u=u.parentNode;var c=e.measure,f=c.maps,h=n(a,u,r);if(h)return ol(h,l);for(var d=u.nextSibling,p=a?a.nodeValue.length-r:0;d;d=d.nextSibling){if(h=n(d,d.firstChild,0))return ol(E(h.line,h.ch-p),l);p+=d.textContent.length}for(var v=u.previousSibling,m=r;v;v=v.previousSibling){if(h=n(v,v.firstChild,-1))return ol(E(h.line,h.ch+m),l);m+=v.textContent.length}}var ul=navigator.userAgent,cl=navigator.platform,fl=/gecko\/\d/i.test(ul),hl=/MSIE \d/.test(ul),dl=/Trident\/(?:[7-9]|\d{2,})\..*rv:(\d+)/.exec(ul),pl=/Edge\/(\d+)/.exec(ul),gl=hl||dl||pl,vl=gl&&(hl?document.documentMode||6:+(pl||dl)[1]),ml=!pl&&/WebKit\//.test(ul),yl=ml&&/Qt\/\d+\.\d+/.test(ul),bl=!pl&&/Chrome\//.test(ul),wl=/Opera\//.test(ul),xl=/Apple Computer/.test(navigator.vendor),Cl=/Mac OS X 1\d\D([8-9]|\d\d)\D/.test(ul),Sl=/PhantomJS/.test(ul),Ll=!pl&&/AppleWebKit/.test(ul)&&/Mobile\/\w+/.test(ul),kl=/Android/.test(ul),Tl=Ll||kl||/webOS|BlackBerry|Opera Mini|Opera Mobi|IEMobile/i.test(ul),Ml=Ll||/Mac/.test(cl),Nl=/\bCrOS\b/.test(ul),Ol=/win/i.test(cl),Al=wl&&ul.match(/Version\/(\d*\.\d*)/);Al&&(Al=Number(Al[1])),Al&&Al>=15&&(wl=!1,ml=!0);var Wl,Dl=Ml&&(yl||wl&&(null==Al||Al<12.11)),Hl=fl||gl&&vl>=9,Fl=function(t,r){var n=t.className,i=e(r).exec(n);if(i){var o=n.slice(i.index+i[0].length);t.className=n.slice(0,i.index)+(o?i[1]+o:"")}};Wl=document.createRange?function(e,t,r,n){var i=document.createRange();return i.setEnd(n||e,r),i.setStart(e,t),i}:function(e,t,r){var n=document.body.createTextRange();try{n.moveToElementText(e.parentNode)}catch(e){return n}return n.collapse(!0),n.moveEnd("character",r),n.moveStart("character",t),n};var El=function(e){e.select()};Ll?El=function(e){e.selectionStart=0,e.selectionEnd=e.value.length}:gl&&(El=function(e){try{e.select()}catch(e){}});var Pl=function(){this.id=null};Pl.prototype.set=function(e,t){clearTimeout(this.id),this.id=setTimeout(t,e)};var Il,zl,Rl=30,Bl={toString:function(){return"CodeMirror.Pass"}},Gl={scroll:!1},Ul={origin:"*mouse"},Vl={origin:"+move"},Kl=[""],jl=/[\u00df\u0587\u0590-\u05f4\u0600-\u06ff\u3040-\u309f\u30a0-\u30ff\u3400-\u4db5\u4e00-\u9fcc\uac00-\ud7af]/,Xl=/[\u0300-\u036f\u0483-\u0489\u0591-\u05bd\u05bf\u05c1\u05c2\u05c4\u05c5\u05c7\u0610-\u061a\u064b-\u065e\u0670\u06d6-\u06dc\u06de-\u06e4\u06e7\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0900-\u0902\u093c\u0941-\u0948\u094d\u0951-\u0955\u0962\u0963\u0981\u09bc\u09be\u09c1-\u09c4\u09cd\u09d7\u09e2\u09e3\u0a01\u0a02\u0a3c\u0a41\u0a42\u0a47\u0a48\u0a4b-\u0a4d\u0a51\u0a70\u0a71\u0a75\u0a81\u0a82\u0abc\u0ac1-\u0ac5\u0ac7\u0ac8\u0acd\u0ae2\u0ae3\u0b01\u0b3c\u0b3e\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b57\u0b62\u0b63\u0b82\u0bbe\u0bc0\u0bcd\u0bd7\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55\u0c56\u0c62\u0c63\u0cbc\u0cbf\u0cc2\u0cc6\u0ccc\u0ccd\u0cd5\u0cd6\u0ce2\u0ce3\u0d3e\u0d41-\u0d44\u0d4d\u0d57\u0d62\u0d63\u0dca\u0dcf\u0dd2-\u0dd4\u0dd6\u0ddf\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb\u0ebc\u0ec8-\u0ecd\u0f18\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86\u0f87\u0f90-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039\u103a\u103d\u103e\u1058\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085\u1086\u108d\u109d\u135f\u1712-\u1714\u1732-\u1734\u1752\u1753\u1772\u1773\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u18a9\u1920-\u1922\u1927\u1928\u1932\u1939-\u193b\u1a17\u1a18\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80\u1b81\u1ba2-\u1ba5\u1ba8\u1ba9\u1c2c-\u1c33\u1c36\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1dc0-\u1de6\u1dfd-\u1dff\u200c\u200d\u20d0-\u20f0\u2cef-\u2cf1\u2de0-\u2dff\u302a-\u302f\u3099\u309a\ua66f-\ua672\ua67c\ua67d\ua6f0\ua6f1\ua802\ua806\ua80b\ua825\ua826\ua8c4\ua8e0-\ua8f1\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\uaa29-\uaa2e\uaa31\uaa32\uaa35\uaa36\uaa43\uaa4c\uaab0\uaab2-\uaab4\uaab7\uaab8\uaabe\uaabf\uaac1\uabe5\uabe8\uabed\udc00-\udfff\ufb1e\ufe00-\ufe0f\ufe20-\ufe26\uff9e\uff9f]/,Yl=!1,_l=!1,$l=null,ql=function(){function e(e){return e<=247?r.charAt(e):1424<=e&&e<=1524?"R":1536<=e&&e<=1785?n.charAt(e-1536):1774<=e&&e<=2220?"r":8192<=e&&e<=8203?"w":8204==e?"b":"L"}function t(e,t,r){this.level=e,this.from=t,this.to=r}var r="bbbbbbbbbtstwsbbbbbbbbbbbbbbssstwNN%%%NNNNNN,N,N1111111111NNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNNNLLLLLLLLLLLLLLLLLLLLLLLLLLNNNNbbbbbbsbbbbbbbbbbbbbbbbbbbbbbbbbb,N%%%%NNNNLNNNNN%%11NLNNN1LNNNNNLLLLLLLLLLLLLLLLLLLLLLLNLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLN",n="nnnnnnNNr%%r,rNNmmmmmmmmmmmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmmmmmmmmmmmmmmmnnnnnnnnnn%nnrrrmrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrmmmmmmmnNmmmmmmrrmmNmmmmrr1111111111",i=/[\u0590-\u05f4\u0600-\u06ff\u0700-\u08ac]/,o=/[stwN]/,l=/[LRr]/,s=/[Lb1n]/,a=/[1n]/;return function(r,n){var u="ltr"==n?"L":"R";if(0==r.length||"ltr"==n&&!i.test(r))return!1;for(var c=r.length,f=[],h=0;h<c;++h)f.push(e(r.charCodeAt(h)));for(var d=0,p=u;d<c;++d){var v=f[d];"m"==v?f[d]=p:p=v}for(var m=0,y=u;m<c;++m){var b=f[m];"1"==b&&"r"==y?f[m]="n":l.test(b)&&(y=b,"r"==b&&(f[m]="R"))}for(var w=1,x=f[0];w<c-1;++w){var C=f[w];"+"==C&&"1"==x&&"1"==f[w+1]?f[w]="1":","!=C||x!=f[w+1]||"1"!=x&&"n"!=x||(f[w]=x),x=C}for(var S=0;S<c;++S){var L=f[S];if(","==L)f[S]="N";else if("%"==L){var k=void 0;for(k=S+1;k<c&&"%"==f[k];++k);for(var T=S&&"!"==f[S-1]||k<c&&"1"==f[k]?"1":"N",M=S;M<k;++M)f[M]=T;S=k-1}}for(var N=0,O=u;N<c;++N){var A=f[N];"L"==O&&"1"==A?f[N]="L":l.test(A)&&(O=A)}for(var W=0;W<c;++W)if(o.test(f[W])){var D=void 0;for(D=W+1;D<c&&o.test(f[D]);++D);for(var H="L"==(W?f[W-1]:u),F=H==("L"==(D<c?f[D]:u))?H?"L":"R":u,E=W;E<D;++E)f[E]=F;W=D-1}for(var P,I=[],z=0;z<c;)if(s.test(f[z])){var R=z;for(++z;z<c&&s.test(f[z]);++z);I.push(new t(0,R,z))}else{var B=z,G=I.length;for(++z;z<c&&"L"!=f[z];++z);for(var U=B;U<z;)if(a.test(f[U])){B<U&&I.splice(G,0,new t(1,B,U));var V=U;for(++U;U<z&&a.test(f[U]);++U);I.splice(G,0,new t(2,V,U)),B=U}else++U;B<z&&I.splice(G,0,new t(1,B,z))}return 1==I[0].level&&(P=r.match(/^\s+/))&&(I[0].from=P[0].length,I.unshift(new t(0,0,P[0].length))),1==g(I).level&&(P=r.match(/\s+$/))&&(g(I).to-=P[0].length,I.push(new t(0,c-P[0].length,c))),"rtl"==n?I.reverse():I}}(),Zl=[],Ql=function(e,t,r){if(e.addEventListener)e.addEventListener(t,r,!1);else if(e.attachEvent)e.attachEvent("on"+t,r);else{var n=e._handlers||(e._handlers={});n[t]=(n[t]||Zl).concat(r)}},Jl=function(){if(gl&&vl<9)return!1;var e=n("div");return"draggable"in e||"dragDrop"in e}(),es=3!="\n\nb".split(/\n/).length?function(e){for(var t=0,r=[],n=e.length;t<=n;){var i=e.indexOf("\n",t);-1==i&&(i=e.length);var o=e.slice(t,"\r"==e.charAt(i-1)?i-1:i),l=o.indexOf("\r");-1!=l?(r.push(o.slice(0,l)),t+=l+1):(r.push(o),t=i+1)}return r}:function(e){return e.split(/\r\n?|\n/)},ts=window.getSelection?function(e){try{return e.selectionStart!=e.selectionEnd}catch(e){return!1}}:function(e){var t;try{t=e.ownerDocument.selection.createRange()}catch(e){}return!(!t||t.parentElement()!=e)&&0!=t.compareEndPoints("StartToEnd",t)},rs=function(){var e=n("div");return"oncopy"in e||(e.setAttribute("oncopy","return;"),"function"==typeof e.oncopy)}(),ns=null,is={},os={},ls={},ss=function(e,t,r){this.pos=this.start=0,this.string=e,this.tabSize=t||8,this.lastColumnPos=this.lastColumnValue=0,this.lineStart=0,this.lineOracle=r};ss.prototype.eol=function(){return this.pos>=this.string.length},ss.prototype.sol=function(){return this.pos==this.lineStart},ss.prototype.peek=function(){return this.string.charAt(this.pos)||void 0},ss.prototype.next=function(){if(this.pos<this.string.length)return this.string.charAt(this.pos++)},ss.prototype.eat=function(e){var t=this.string.charAt(this.pos);if("string"==typeof e?t==e:t&&(e.test?e.test(t):e(t)))return++this.pos,t},ss.prototype.eatWhile=function(e){for(var t=this.pos;this.eat(e););return this.pos>t},ss.prototype.eatSpace=function(){for(var e=this,t=this.pos;/[\s\u00a0]/.test(this.string.charAt(this.pos));)++e.pos;return this.pos>t},ss.prototype.skipToEnd=function(){this.pos=this.string.length},ss.prototype.skipTo=function(e){var t=this.string.indexOf(e,this.pos);if(t>-1)return this.pos=t,!0},ss.prototype.backUp=function(e){this.pos-=e},ss.prototype.column=function(){return this.lastColumnPos<this.start&&(this.lastColumnValue=f(this.string,this.start,this.tabSize,this.lastColumnPos,this.lastColumnValue),this.lastColumnPos=this.start),this.lastColumnValue-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.indentation=function(){return f(this.string,null,this.tabSize)-(this.lineStart?f(this.string,this.lineStart,this.tabSize):0)},ss.prototype.match=function(e,t,r){if("string"!=typeof e){var n=this.string.slice(this.pos).match(e);return n&&n.index>0?null:(n&&!1!==t&&(this.pos+=n[0].length),n)}var i=function(e){return r?e.toLowerCase():e};if(i(this.string.substr(this.pos,e.length))==i(e))return!1!==t&&(this.pos+=e.length),!0},ss.prototype.current=function(){return this.string.slice(this.start,this.pos)},ss.prototype.hideFirstChars=function(e,t){this.lineStart+=e;try{return t()}finally{this.lineStart-=e}},ss.prototype.lookAhead=function(e){var t=this.lineOracle;return t&&t.lookAhead(e)};var as=function(e,t){this.state=e,this.lookAhead=t},us=function(e,t,r,n){this.state=t,this.doc=e,this.line=r,this.maxLookAhead=n||0};us.prototype.lookAhead=function(e){var t=this.doc.getLine(this.line+e);return null!=t&&e>this.maxLookAhead&&(this.maxLookAhead=e),t},us.prototype.nextLine=function(){this.line++,this.maxLookAhead>0&&this.maxLookAhead--},us.fromSaved=function(e,t,r){return t instanceof as?new us(e,Ke(e.mode,t.state),r,t.lookAhead):new us(e,Ke(e.mode,t),r)},us.prototype.save=function(e){var t=!1!==e?Ke(this.doc.mode,this.state):this.state;return this.maxLookAhead>0?new as(t,this.maxLookAhead):t};var cs=function(e,t,r){this.start=e.start,this.end=e.pos,this.string=e.current(),this.type=t||null,this.state=r},fs=function(e,t,r){this.text=e,ne(this,t),this.height=r?r(this):1};fs.prototype.lineNo=function(){return W(this)},Ae(fs);var hs,ds={},ps={},gs=null,vs=null,ms={left:0,right:0,top:0,bottom:0},ys=function(e,t,r){this.cm=r;var i=this.vert=n("div",[n("div",null,null,"min-width: 1px")],"CodeMirror-vscrollbar"),o=this.horiz=n("div",[n("div",null,null,"height: 100%; min-height: 1px")],"CodeMirror-hscrollbar");e(i),e(o),Ql(i,"scroll",function(){i.clientHeight&&t(i.scrollTop,"vertical")}),Ql(o,"scroll",function(){o.clientWidth&&t(o.scrollLeft,"horizontal")}),this.checkedZeroWidth=!1,gl&&vl<8&&(this.horiz.style.minHeight=this.vert.style.minWidth="18px")};ys.prototype.update=function(e){var t=e.scrollWidth>e.clientWidth+1,r=e.scrollHeight>e.clientHeight+1,n=e.nativeBarWidth;if(r){this.vert.style.display="block",this.vert.style.bottom=t?n+"px":"0";var i=e.viewHeight-(t?n:0);this.vert.firstChild.style.height=Math.max(0,e.scrollHeight-e.clientHeight+i)+"px"}else this.vert.style.display="",this.vert.firstChild.style.height="0";if(t){this.horiz.style.display="block",this.horiz.style.right=r?n+"px":"0",this.horiz.style.left=e.barLeft+"px";var o=e.viewWidth-e.barLeft-(r?n:0);this.horiz.firstChild.style.width=Math.max(0,e.scrollWidth-e.clientWidth+o)+"px"}else this.horiz.style.display="",this.horiz.firstChild.style.width="0";return!this.checkedZeroWidth&&e.clientHeight>0&&(0==n&&this.zeroWidthHack(),this.checkedZeroWidth=!0),{right:r?n:0,bottom:t?n:0}},ys.prototype.setScrollLeft=function(e){this.horiz.scrollLeft!=e&&(this.horiz.scrollLeft=e),this.disableHoriz&&this.enableZeroWidthBar(this.horiz,this.disableHoriz,"horiz")},ys.prototype.setScrollTop=function(e){this.vert.scrollTop!=e&&(this.vert.scrollTop=e),this.disableVert&&this.enableZeroWidthBar(this.vert,this.disableVert,"vert")},ys.prototype.zeroWidthHack=function(){var e=Ml&&!Cl?"12px":"18px";this.horiz.style.height=this.vert.style.width=e,this.horiz.style.pointerEvents=this.vert.style.pointerEvents="none",this.disableHoriz=new Pl,this.disableVert=new Pl},ys.prototype.enableZeroWidthBar=function(e,t,r){function n(){var i=e.getBoundingClientRect();("vert"==r?document.elementFromPoint(i.right-1,(i.top+i.bottom)/2):document.elementFromPoint((i.right+i.left)/2,i.bottom-1))!=e?e.style.pointerEvents="none":t.set(1e3,n)}e.style.pointerEvents="auto",t.set(1e3,n)},ys.prototype.clear=function(){var e=this.horiz.parentNode;e.removeChild(this.horiz),e.removeChild(this.vert)};var bs=function(){};bs.prototype.update=function(){return{bottom:0,right:0}},bs.prototype.setScrollLeft=function(){},bs.prototype.setScrollTop=function(){},bs.prototype.clear=function(){};var ws={native:ys,null:bs},xs=0,Cs=function(e,t,r){var n=e.display;this.viewport=t,this.visible=Ir(n,e.doc,t),this.editorIsHidden=!n.wrapper.offsetWidth,this.wrapperHeight=n.wrapper.clientHeight,this.wrapperWidth=n.wrapper.clientWidth,this.oldDisplayWidth=Rt(e),this.force=r,this.dims=br(e),this.events=[]};Cs.prototype.signal=function(e,t){Oe(e,t)&&this.events.push(arguments)},Cs.prototype.finish=function(){for(var e=this,t=0;t<this.events.length;t++)Te.apply(null,e.events[t])};var Ss=0,Ls=null;gl?Ls=-.53:fl?Ls=15:bl?Ls=-.7:xl&&(Ls=-1/3);var ks=function(e,t){this.ranges=e,this.primIndex=t};ks.prototype.primary=function(){return this.ranges[this.primIndex]},ks.prototype.equals=function(e){var t=this;if(e==this)return!0;if(e.primIndex!=this.primIndex||e.ranges.length!=this.ranges.length)return!1;for(var r=0;r<this.ranges.length;r++){var n=t.ranges[r],i=e.ranges[r];if(!I(n.anchor,i.anchor)||!I(n.head,i.head))return!1}return!0},ks.prototype.deepCopy=function(){for(var e=this,t=[],r=0;r<this.ranges.length;r++)t[r]=new Ts(z(e.ranges[r].anchor),z(e.ranges[r].head));return new ks(t,this.primIndex)},ks.prototype.somethingSelected=function(){for(var e=this,t=0;t<this.ranges.length;t++)if(!e.ranges[t].empty())return!0;return!1},ks.prototype.contains=function(e,t){var r=this;t||(t=e);for(var n=0;n<this.ranges.length;n++){var i=r.ranges[n];if(P(t,i.from())>=0&&P(e,i.to())<=0)return n}return-1};var Ts=function(e,t){this.anchor=e,this.head=t};Ts.prototype.from=function(){return B(this.anchor,this.head)},Ts.prototype.to=function(){return R(this.anchor,this.head)},Ts.prototype.empty=function(){return this.head.line==this.anchor.line&&this.head.ch==this.anchor.ch},Bi.prototype={chunkSize:function(){return this.lines.length},removeInner:function(e,t){for(var r=this,n=e,i=e+t;n<i;++n){var o=r.lines[n];r.height-=o.height,ot(o),bt(o,"delete")}this.lines.splice(e,t)},collapse:function(e){e.push.apply(e,this.lines)},insertInner:function(e,t,r){var n=this;this.height+=r,this.lines=this.lines.slice(0,e).concat(t).concat(this.lines.slice(e));for(var i=0;i<t.length;++i)t[i].parent=n},iterN:function(e,t,r){for(var n=this,i=e+t;e<i;++e)if(r(n.lines[e]))return!0}},Gi.prototype={chunkSize:function(){return this.size},removeInner:function(e,t){var r=this;this.size-=t;for(var n=0;n<this.children.length;++n){var i=r.children[n],o=i.chunkSize();if(e<o){var l=Math.min(t,o-e),s=i.height;if(i.removeInner(e,l),r.height-=s-i.height,o==l&&(r.children.splice(n--,1),i.parent=null),0==(t-=l))break;e=0}else e-=o}if(this.size-t<25&&(this.children.length>1||!(this.children[0]instanceof Bi))){var a=[];this.collapse(a),this.children=[new Bi(a)],this.children[0].parent=this}},collapse:function(e){for(var t=this,r=0;r<this.children.length;++r)t.children[r].collapse(e)},insertInner:function(e,t,r){var n=this;this.size+=t.length,this.height+=r;for(var i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<=l){if(o.insertInner(e,t,r),o.lines&&o.lines.length>50){for(var s=o.lines.length%25+25,a=s;a<o.lines.length;){var u=new Bi(o.lines.slice(a,a+=25));o.height-=u.height,n.children.splice(++i,0,u),u.parent=n}o.lines=o.lines.slice(0,s),n.maybeSpill()}break}e-=l}},maybeSpill:function(){if(!(this.children.length<=10)){var e=this;do{var t=new Gi(e.children.splice(e.children.length-5,5));if(e.parent){e.size-=t.size,e.height-=t.height;var r=h(e.parent.children,e);e.parent.children.splice(r+1,0,t)}else{var n=new Gi(e.children);n.parent=e,e.children=[n,t],e=n}t.parent=e.parent}while(e.children.length>10);e.parent.maybeSpill()}},iterN:function(e,t,r){for(var n=this,i=0;i<this.children.length;++i){var o=n.children[i],l=o.chunkSize();if(e<l){var s=Math.min(t,l-e);if(o.iterN(e,s,r))return!0;if(0==(t-=s))break;e=0}else e-=l}}};var Ms=function(e,t,r){var n=this;if(r)for(var i in r)r.hasOwnProperty(i)&&(n[i]=r[i]);this.doc=e,this.node=t};Ms.prototype.clear=function(){var e=this,t=this.doc.cm,r=this.line.widgets,n=this.line,i=W(n);if(null!=i&&r){for(var o=0;o<r.length;++o)r[o]==e&&r.splice(o--,1);r.length||(n.widgets=null);var l=Ht(this);A(n,Math.max(0,n.height-l)),t&&(hn(t,function(){Ui(t,n,-l),mn(t,i,"widget")}),bt(t,"lineWidgetCleared",t,this,i))}},Ms.prototype.changed=function(){var e=this,t=this.height,r=this.doc.cm,n=this.line;this.height=null;var i=Ht(this)-t;i&&(A(n,n.height+i),r&&hn(r,function(){r.curOp.forceUpdate=!0,Ui(r,n,i),bt(r,"lineWidgetChanged",r,e,W(n))}))},Ae(Ms);var Ns=0,Os=function(e,t){this.lines=[],this.type=t,this.doc=e,this.id=++Ns};Os.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){var t=this.doc.cm,r=t&&!t.curOp;if(r&&nn(t),Oe(this,"clear")){var n=this.find();n&&bt(this,"clear",n.from,n.to)}for(var i=null,o=null,l=0;l<this.lines.length;++l){var s=e.lines[l],a=_(s.markedSpans,e);t&&!e.collapsed?mn(t,W(s),"text"):t&&(null!=a.to&&(o=W(s)),null!=a.from&&(i=W(s))),s.markedSpans=$(s.markedSpans,a),null==a.from&&e.collapsed&&!ve(e.doc,s)&&t&&A(s,mr(t.display))}if(t&&this.collapsed&&!t.options.lineWrapping)for(var u=0;u<this.lines.length;++u){var c=fe(e.lines[u]),f=be(c);f>t.display.maxLineLength&&(t.display.maxLine=c,t.display.maxLineLength=f,t.display.maxLineChanged=!0)}null!=i&&t&&this.collapsed&&vn(t,i,o+1),this.lines.length=0,this.explicitlyCleared=!0,this.atomic&&this.doc.cantEdit&&(this.doc.cantEdit=!1,t&&Ci(t.doc)),t&&bt(t,"markerCleared",t,this,i,o),r&&on(t),this.parent&&this.parent.clear()}},Os.prototype.find=function(e,t){var r=this;null==e&&"bookmark"==this.type&&(e=1);for(var n,i,o=0;o<this.lines.length;++o){var l=r.lines[o],s=_(l.markedSpans,r);if(null!=s.from&&(n=E(t?l:W(l),s.from),-1==e))return n;if(null!=s.to&&(i=E(t?l:W(l),s.to),1==e))return i}return n&&{from:n,to:i}},Os.prototype.changed=function(){var e=this,t=this.find(-1,!0),r=this,n=this.doc.cm;t&&n&&hn(n,function(){var i=t.line,o=W(t.line),l=jt(n,o);if(l&&(Qt(l),n.curOp.selectionChanged=n.curOp.forceUpdate=!0),n.curOp.updateMaxLine=!0,!ve(r.doc,i)&&null!=r.height){var s=r.height;r.height=null;var a=Ht(r)-s;a&&A(i,i.height+a)}bt(n,"markerChanged",n,e)})},Os.prototype.attachLine=function(e){if(!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;t.maybeHiddenMarkers&&-1!=h(t.maybeHiddenMarkers,this)||(t.maybeUnhiddenMarkers||(t.maybeUnhiddenMarkers=[])).push(this)}this.lines.push(e)},Os.prototype.detachLine=function(e){if(this.lines.splice(h(this.lines,e),1),!this.lines.length&&this.doc.cm){var t=this.doc.cm.curOp;(t.maybeHiddenMarkers||(t.maybeHiddenMarkers=[])).push(this)}},Ae(Os);var As=function(e,t){var r=this;this.markers=e,this.primary=t;for(var n=0;n<e.length;++n)e[n].parent=r};As.prototype.clear=function(){var e=this;if(!this.explicitlyCleared){this.explicitlyCleared=!0;for(var t=0;t<this.markers.length;++t)e.markers[t].clear();bt(this,"clear")}},As.prototype.find=function(e,t){return this.primary.find(e,t)},Ae(As);var Ws=0,Ds=function(e,t,r,n,i){if(!(this instanceof Ds))return new Ds(e,t,r,n,i);null==r&&(r=0),Gi.call(this,[new Bi([new fs("",null)])]),this.first=r,this.scrollTop=this.scrollLeft=0,this.cantEdit=!1,this.cleanGeneration=1,this.modeFrontier=this.highlightFrontier=r;var o=E(r,0);this.sel=Rn(o),this.history=new Jn(null),this.id=++Ws,this.modeOption=t,this.lineSep=n,this.direction="rtl"==i?"rtl":"ltr",this.extend=!1,"string"==typeof e&&(e=this.splitLines(e)),_n(this,{from:o,to:o,text:e}),bi(this,Rn(o),Gl)};Ds.prototype=b(Gi.prototype,{constructor:Ds,iter:function(e,t,r){r?this.iterN(e-this.first,t-e,r):this.iterN(this.first,this.first+this.size,e)},insert:function(e,t){for(var r=0,n=0;n<t.length;++n)r+=t[n].height;this.insertInner(e-this.first,t,r)},remove:function(e,t){this.removeInner(e-this.first,t)},getValue:function(e){var t=O(this,this.first,this.first+this.size);return!1===e?t:t.join(e||this.lineSeparator())},setValue:gn(function(e){var t=E(this.first,0),r=this.first+this.size-1;Oi(this,{from:t,to:E(r,M(this,r).text.length),text:this.splitLines(e),origin:"setValue",full:!0},!0),this.cm&&Xr(this.cm,0,0),bi(this,Rn(t),Gl)}),replaceRange:function(e,t,r,n){Ei(this,e,t=U(this,t),r=r?U(this,r):t,n)},getRange:function(e,t,r){var n=N(this,U(this,e),U(this,t));return!1===r?n:n.join(r||this.lineSeparator())},getLine:function(e){var t=this.getLineHandle(e);return t&&t.text},getLineHandle:function(e){if(H(this,e))return M(this,e)},getLineNumber:function(e){return W(e)},getLineHandleVisualStart:function(e){return"number"==typeof e&&(e=M(this,e)),fe(e)},lineCount:function(){return this.size},firstLine:function(){return this.first},lastLine:function(){return this.first+this.size-1},clipPos:function(e){return U(this,e)},getCursor:function(e){var t=this.sel.primary();return null==e||"head"==e?t.head:"anchor"==e?t.anchor:"end"==e||"to"==e||!1===e?t.to():t.from()},listSelections:function(){return this.sel.ranges},somethingSelected:function(){return this.sel.somethingSelected()},setCursor:gn(function(e,t,r){vi(this,U(this,"number"==typeof e?E(e,t||0):e),null,r)}),setSelection:gn(function(e,t,r){vi(this,U(this,e),U(this,t||e),r)}),extendSelection:gn(function(e,t,r){di(this,U(this,e),t&&U(this,t),r)}),extendSelections:gn(function(e,t){pi(this,K(this,e),t)}),extendSelectionsBy:gn(function(e,t){pi(this,K(this,v(this.sel.ranges,e)),t)}),setSelections:gn(function(e,t,r){var n=this;if(e.length){for(var i=[],o=0;o<e.length;o++)i[o]=new Ts(U(n,e[o].anchor),U(n,e[o].head));null==t&&(t=Math.min(e.length-1,this.sel.primIndex)),bi(this,zn(i,t),r)}}),addSelection:gn(function(e,t,r){var n=this.sel.ranges.slice(0);n.push(new Ts(U(this,e),U(this,t||e))),bi(this,zn(n,n.length-1),r)}),getSelection:function(e){for(var t,r=this,n=this.sel.ranges,i=0;i<n.length;i++){var o=N(r,n[i].from(),n[i].to());t=t?t.concat(o):o}return!1===e?t:t.join(e||this.lineSeparator())},getSelections:function(e){for(var t=this,r=[],n=this.sel.ranges,i=0;i<n.length;i++){var o=N(t,n[i].from(),n[i].to());!1!==e&&(o=o.join(e||t.lineSeparator())),r[i]=o}return r},replaceSelection:function(e,t,r){for(var n=[],i=0;i<this.sel.ranges.length;i++)n[i]=e;this.replaceSelections(n,t,r||"+input")},replaceSelections:gn(function(e,t,r){for(var n=this,i=[],o=this.sel,l=0;l<o.ranges.length;l++){var s=o.ranges[l];i[l]={from:s.from(),to:s.to(),text:n.splitLines(e[l]),origin:r}}for(var a=t&&"end"!=t&&Kn(this,i,t),u=i.length-1;u>=0;u--)Oi(n,i[u]);a?yi(this,a):this.cm&&jr(this.cm)}),undo:gn(function(){Wi(this,"undo")}),redo:gn(function(){Wi(this,"redo")}),undoSelection:gn(function(){Wi(this,"undo",!0)}),redoSelection:gn(function(){Wi(this,"redo",!0)}),setExtending:function(e){this.extend=e},getExtending:function(){return this.extend},historySize:function(){for(var e=this.history,t=0,r=0,n=0;n<e.done.length;n++)e.done[n].ranges||++t;for(var i=0;i<e.undone.length;i++)e.undone[i].ranges||++r;return{undo:t,redo:r}},clearHistory:function(){this.history=new Jn(this.history.maxGeneration)},markClean:function(){this.cleanGeneration=this.changeGeneration(!0)},changeGeneration:function(e){return e&&(this.history.lastOp=this.history.lastSelOp=this.history.lastOrigin=null),this.history.generation},isClean:function(e){return this.history.generation==(e||this.cleanGeneration)},getHistory:function(){return{done:fi(this.history.done),undone:fi(this.history.undone)}},setHistory:function(e){var t=this.history=new Jn(this.history.maxGeneration);t.done=fi(e.done.slice(0),null,!0),t.undone=fi(e.undone.slice(0),null,!0)},setGutterMarker:gn(function(e,t,r){return Ri(this,e,"gutter",function(e){var n=e.gutterMarkers||(e.gutterMarkers={});return n[t]=r,!r&&C(n)&&(e.gutterMarkers=null),!0})}),clearGutter:gn(function(e){var t=this;this.iter(function(r){r.gutterMarkers&&r.gutterMarkers[e]&&Ri(t,r,"gutter",function(){return r.gutterMarkers[e]=null,C(r.gutterMarkers)&&(r.gutterMarkers=null),!0})})}),lineInfo:function(e){var t;if("number"==typeof e){if(!H(this,e))return null;if(t=e,!(e=M(this,e)))return null}else if(null==(t=W(e)))return null;return{line:t,handle:e,text:e.text,gutterMarkers:e.gutterMarkers,textClass:e.textClass,bgClass:e.bgClass,wrapClass:e.wrapClass,widgets:e.widgets}},addLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass";if(t[i]){if(e(n).test(t[i]))return!1;t[i]+=" "+n}else t[i]=n;return!0})}),removeLineClass:gn(function(t,r,n){return Ri(this,t,"gutter"==r?"gutter":"class",function(t){var i="text"==r?"textClass":"background"==r?"bgClass":"gutter"==r?"gutterClass":"wrapClass",o=t[i];if(!o)return!1;if(null==n)t[i]=null;else{var l=o.match(e(n));if(!l)return!1;var s=l.index+l[0].length;t[i]=o.slice(0,l.index)+(l.index&&s!=o.length?" ":"")+o.slice(s)||null}return!0})}),addLineWidget:gn(function(e,t,r){return Vi(this,e,t,r)}),removeLineWidget:function(e){e.clear()},markText:function(e,t,r){return Ki(this,U(this,e),U(this,t),r,r&&r.type||"range")},setBookmark:function(e,t){var r={replacedWith:t&&(null==t.nodeType?t.widget:t),insertLeft:t&&t.insertLeft,clearWhenEmpty:!1,shared:t&&t.shared,handleMouseEvents:t&&t.handleMouseEvents};return e=U(this,e),Ki(this,e,e,r,"bookmark")},findMarksAt:function(e){var t=[],r=M(this,(e=U(this,e)).line).markedSpans;if(r)for(var n=0;n<r.length;++n){var i=r[n];(null==i.from||i.from<=e.ch)&&(null==i.to||i.to>=e.ch)&&t.push(i.marker.parent||i.marker)}return t},findMarks:function(e,t,r){e=U(this,e),t=U(this,t);var n=[],i=e.line;return this.iter(e.line,t.line+1,function(o){var l=o.markedSpans;if(l)for(var s=0;s<l.length;s++){var a=l[s];null!=a.to&&i==e.line&&e.ch>=a.to||null==a.from&&i!=e.line||null!=a.from&&i==t.line&&a.from>=t.ch||r&&!r(a.marker)||n.push(a.marker.parent||a.marker)}++i}),n},getAllMarks:function(){var e=[];return this.iter(function(t){var r=t.markedSpans;if(r)for(var n=0;n<r.length;++n)null!=r[n].from&&e.push(r[n].marker)}),e},posFromIndex:function(e){var t,r=this.first,n=this.lineSeparator().length;return this.iter(function(i){var o=i.text.length+n;if(o>e)return t=e,!0;e-=o,++r}),U(this,E(r,t))},indexFromPos:function(e){var t=(e=U(this,e)).ch;if(e.line<this.first||e.ch<0)return 0;var r=this.lineSeparator().length;return this.iter(this.first,e.line,function(e){t+=e.text.length+r}),t},copy:function(e){var t=new Ds(O(this,this.first,this.first+this.size),this.modeOption,this.first,this.lineSep,this.direction);return t.scrollTop=this.scrollTop,t.scrollLeft=this.scrollLeft,t.sel=this.sel,t.extend=!1,e&&(t.history.undoDepth=this.history.undoDepth,t.setHistory(this.getHistory())),t},linkedDoc:function(e){e||(e={});var t=this.first,r=this.first+this.size;null!=e.from&&e.from>t&&(t=e.from),null!=e.to&&e.to<r&&(r=e.to);var n=new Ds(O(this,t,r),e.mode||this.modeOption,t,this.lineSep,this.direction);return e.sharedHist&&(n.history=this.history),(this.linked||(this.linked=[])).push({doc:n,sharedHist:e.sharedHist}),n.linked=[{doc:this,isParent:!0,sharedHist:e.sharedHist}],Yi(n,Xi(this)),n},unlinkDoc:function(e){var t=this;if(e instanceof jo&&(e=e.doc),this.linked)for(var r=0;r<this.linked.length;++r)if(t.linked[r].doc==e){t.linked.splice(r,1),e.unlinkDoc(t),_i(Xi(t));break}if(e.history==this.history){var n=[e.id];$n(e,function(e){return n.push(e.id)},!0),e.history=new Jn(null),e.history.done=fi(this.history.done,n),e.history.undone=fi(this.history.undone,n)}},iterLinkedDocs:function(e){$n(this,e)},getMode:function(){return this.mode},getEditor:function(){return this.cm},splitLines:function(e){return this.lineSep?e.split(this.lineSep):es(e)},lineSeparator:function(){return this.lineSep||"\n"},setDirection:gn(function(e){"rtl"!=e&&(e="ltr"),e!=this.direction&&(this.direction=e,this.iter(function(e){return e.order=null}),this.cm&&Qn(this.cm))})}),Ds.prototype.eachLine=Ds.prototype.iter;for(var Hs=0,Fs=!1,Es={3:"Enter",8:"Backspace",9:"Tab",13:"Enter",16:"Shift",17:"Ctrl",18:"Alt",19:"Pause",20:"CapsLock",27:"Esc",32:"Space",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"Left",38:"Up",39:"Right",40:"Down",44:"PrintScrn",45:"Insert",46:"Delete",59:";",61:"=",91:"Mod",92:"Mod",93:"Mod",106:"*",107:"=",109:"-",110:".",111:"/",127:"Delete",173:"-",186:";",187:"=",188:",",189:"-",190:".",191:"/",192:"`",219:"[",220:"\\",221:"]",222:"'",63232:"Up",63233:"Down",63234:"Left",63235:"Right",63272:"Delete",63273:"Home",63275:"End",63276:"PageUp",63277:"PageDown",63302:"Insert"},Ps=0;Ps<10;Ps++)Es[Ps+48]=Es[Ps+96]=String(Ps);for(var Is=65;Is<=90;Is++)Es[Is]=String.fromCharCode(Is);for(var zs=1;zs<=12;zs++)Es[zs+111]=Es[zs+63235]="F"+zs;var Rs={};Rs.basic={Left:"goCharLeft",Right:"goCharRight",Up:"goLineUp",Down:"goLineDown",End:"goLineEnd",Home:"goLineStartSmart",PageUp:"goPageUp",PageDown:"goPageDown",Delete:"delCharAfter",Backspace:"delCharBefore","Shift-Backspace":"delCharBefore",Tab:"defaultTab","Shift-Tab":"indentAuto",Enter:"newlineAndIndent",Insert:"toggleOverwrite",Esc:"singleSelection"},Rs.pcDefault={"Ctrl-A":"selectAll","Ctrl-D":"deleteLine","Ctrl-Z":"undo","Shift-Ctrl-Z":"redo","Ctrl-Y":"redo","Ctrl-Home":"goDocStart","Ctrl-End":"goDocEnd","Ctrl-Up":"goLineUp","Ctrl-Down":"goLineDown","Ctrl-Left":"goGroupLeft","Ctrl-Right":"goGroupRight","Alt-Left":"goLineStart","Alt-Right":"goLineEnd","Ctrl-Backspace":"delGroupBefore","Ctrl-Delete":"delGroupAfter","Ctrl-S":"save","Ctrl-F":"find","Ctrl-G":"findNext","Shift-Ctrl-G":"findPrev","Shift-Ctrl-F":"replace","Shift-Ctrl-R":"replaceAll","Ctrl-[":"indentLess","Ctrl-]":"indentMore","Ctrl-U":"undoSelection","Shift-Ctrl-U":"redoSelection","Alt-U":"redoSelection",fallthrough:"basic"},Rs.emacsy={"Ctrl-F":"goCharRight","Ctrl-B":"goCharLeft","Ctrl-P":"goLineUp","Ctrl-N":"goLineDown","Alt-F":"goWordRight","Alt-B":"goWordLeft","Ctrl-A":"goLineStart","Ctrl-E":"goLineEnd","Ctrl-V":"goPageDown","Shift-Ctrl-V":"goPageUp","Ctrl-D":"delCharAfter","Ctrl-H":"delCharBefore","Alt-D":"delWordAfter","Alt-Backspace":"delWordBefore","Ctrl-K":"killLine","Ctrl-T":"transposeChars","Ctrl-O":"openLine"},Rs.macDefault={"Cmd-A":"selectAll","Cmd-D":"deleteLine","Cmd-Z":"undo","Shift-Cmd-Z":"redo","Cmd-Y":"redo","Cmd-Home":"goDocStart","Cmd-Up":"goDocStart","Cmd-End":"goDocEnd","Cmd-Down":"goDocEnd","Alt-Left":"goGroupLeft","Alt-Right":"goGroupRight","Cmd-Left":"goLineLeft","Cmd-Right":"goLineRight","Alt-Backspace":"delGroupBefore","Ctrl-Alt-Backspace":"delGroupAfter","Alt-Delete":"delGroupAfter","Cmd-S":"save","Cmd-F":"find","Cmd-G":"findNext","Shift-Cmd-G":"findPrev","Cmd-Alt-F":"replace","Shift-Cmd-Alt-F":"replaceAll","Cmd-[":"indentLess","Cmd-]":"indentMore","Cmd-Backspace":"delWrappedLineLeft","Cmd-Delete":"delWrappedLineRight","Cmd-U":"undoSelection","Shift-Cmd-U":"redoSelection","Ctrl-Up":"goDocStart","Ctrl-Down":"goDocEnd",fallthrough:["basic","emacsy"]},Rs.default=Ml?Rs.macDefault:Rs.pcDefault;var Bs={selectAll:Mi,singleSelection:function(e){return e.setSelection(e.getCursor("anchor"),e.getCursor("head"),Gl)},killLine:function(e){return co(e,function(t){if(t.empty()){var r=M(e.doc,t.head.line).text.length;return t.head.ch==r&&t.head.line<e.lastLine()?{from:t.head,to:E(t.head.line+1,0)}:{from:t.head,to:E(t.head.line,r)}}return{from:t.from(),to:t.to()}})},deleteLine:function(e){return co(e,function(t){return{from:E(t.from().line,0),to:U(e.doc,E(t.to().line+1,0))}})},delLineLeft:function(e){return co(e,function(e){return{from:E(e.from().line,0),to:e.from()}})},delWrappedLineLeft:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5;return{from:e.coordsChar({left:0,top:r},"div"),to:t.from()}})},delWrappedLineRight:function(e){return co(e,function(t){var r=e.charCoords(t.head,"div").top+5,n=e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div");return{from:t.from(),to:n}})},undo:function(e){return e.undo()},redo:function(e){return e.redo()},undoSelection:function(e){return e.undoSelection()},redoSelection:function(e){return e.redoSelection()},goDocStart:function(e){return e.extendSelection(E(e.firstLine(),0))},goDocEnd:function(e){return e.extendSelection(E(e.lastLine()))},goLineStart:function(e){return e.extendSelectionsBy(function(t){return vo(e,t.head.line)},{origin:"+move",bias:1})},goLineStartSmart:function(e){return e.extendSelectionsBy(function(t){return yo(e,t.head)},{origin:"+move",bias:1})},goLineEnd:function(e){return e.extendSelectionsBy(function(t){return mo(e,t.head.line)},{origin:"+move",bias:-1})},goLineRight:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:e.display.lineDiv.offsetWidth+100,top:r},"div")},Vl)},goLineLeft:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5;return e.coordsChar({left:0,top:r},"div")},Vl)},goLineLeftSmart:function(e){return e.extendSelectionsBy(function(t){var r=e.cursorCoords(t.head,"div").top+5,n=e.coordsChar({left:0,top:r},"div");return n.ch<e.getLine(n.line).search(/\S/)?yo(e,t.head):n},Vl)},goLineUp:function(e){return e.moveV(-1,"line")},goLineDown:function(e){return e.moveV(1,"line")},goPageUp:function(e){return e.moveV(-1,"page")},goPageDown:function(e){return e.moveV(1,"page")},goCharLeft:function(e){return e.moveH(-1,"char")},goCharRight:function(e){return e.moveH(1,"char")},goColumnLeft:function(e){return e.moveH(-1,"column")},goColumnRight:function(e){return e.moveH(1,"column")},goWordLeft:function(e){return e.moveH(-1,"word")},goGroupRight:function(e){return e.moveH(1,"group")},goGroupLeft:function(e){return e.moveH(-1,"group")},goWordRight:function(e){return e.moveH(1,"word")},delCharBefore:function(e){return e.deleteH(-1,"char")},delCharAfter:function(e){return e.deleteH(1,"char")},delWordBefore:function(e){return e.deleteH(-1,"word")},delWordAfter:function(e){return e.deleteH(1,"word")},delGroupBefore:function(e){return e.deleteH(-1,"group")},delGroupAfter:function(e){return e.deleteH(1,"group")},indentAuto:function(e){return e.indentSelection("smart")},indentMore:function(e){return e.indentSelection("add")},indentLess:function(e){return e.indentSelection("subtract")},insertTab:function(e){return e.replaceSelection("\t")},insertSoftTab:function(e){for(var t=[],r=e.listSelections(),n=e.options.tabSize,i=0;i<r.length;i++){var o=r[i].from(),l=f(e.getLine(o.line),o.ch,n);t.push(p(n-l%n))}e.replaceSelections(t)},defaultTab:function(e){e.somethingSelected()?e.indentSelection("add"):e.execCommand("insertTab")},transposeChars:function(e){return hn(e,function(){for(var t=e.listSelections(),r=[],n=0;n<t.length;n++)if(t[n].empty()){var i=t[n].head,o=M(e.doc,i.line).text;if(o)if(i.ch==o.length&&(i=new E(i.line,i.ch-1)),i.ch>0)i=new E(i.line,i.ch+1),e.replaceRange(o.charAt(i.ch-1)+o.charAt(i.ch-2),E(i.line,i.ch-2),i,"+transpose");else if(i.line>e.doc.first){var l=M(e.doc,i.line-1).text;l&&(i=new E(i.line,1),e.replaceRange(o.charAt(0)+e.doc.lineSeparator()+l.charAt(l.length-1),E(i.line-1,l.length-1),i,"+transpose"))}r.push(new Ts(i,i))}e.setSelections(r)})},newlineAndIndent:function(e){return hn(e,function(){for(var t=e.listSelections(),r=t.length-1;r>=0;r--)e.replaceRange(e.doc.lineSeparator(),t[r].anchor,t[r].head,"+input");t=e.listSelections();for(var n=0;n<t.length;n++)e.indentLine(t[n].from().line,null,!0);jr(e)})},openLine:function(e){return e.replaceSelection("\n","start")},toggleOverwrite:function(e){return e.toggleOverwrite()}},Gs=new Pl,Us=null,Vs=function(e,t,r){this.time=e,this.pos=t,this.button=r};Vs.prototype.compare=function(e,t,r){return this.time+400>e&&0==P(t,this.pos)&&r==this.button};var Ks,js,Xs={toString:function(){return"CodeMirror.Init"}},Ys={},_s={};jo.defaults=Ys,jo.optionHandlers=_s;var $s=[];jo.defineInitHook=function(e){return $s.push(e)};var qs=null,Zs=function(e){this.cm=e,this.lastAnchorNode=this.lastAnchorOffset=this.lastFocusNode=this.lastFocusOffset=null,this.polling=new Pl,this.composing=null,this.gracePeriod=!1,this.readDOMTimeout=null};Zs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()}),"cut"==e.type&&i.replaceSelection("",null,"cut");else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type&&i.operation(function(){i.setSelections(t.ranges,0,Gl),i.replaceSelection("",null,"cut")})}if(e.clipboardData){e.clipboardData.clearData();var r=qs.text.join("\n");if(e.clipboardData.setData("Text",r),e.clipboardData.getData("Text")==r)return void e.preventDefault()}var l=el(),s=l.firstChild;i.display.lineSpace.insertBefore(l,i.display.lineSpace.firstChild),s.value=qs.text.join("\n");var a=document.activeElement;El(s),setTimeout(function(){i.display.lineSpace.removeChild(l),a.focus(),a==o&&n.showPrimarySelection()},50)}}var r=this,n=this,i=n.cm,o=n.div=e.lineDiv;Jo(o,i.options.spellcheck),Ql(o,"paste",function(e){Me(i,e)||qo(e,i)||vl<=11&&setTimeout(dn(i,function(){return r.updateFromDOM()}),20)}),Ql(o,"compositionstart",function(e){r.composing={data:e.data,done:!1}}),Ql(o,"compositionupdate",function(e){r.composing||(r.composing={data:e.data,done:!1})}),Ql(o,"compositionend",function(e){r.composing&&(e.data!=r.composing.data&&r.readFromDOMSoon(),r.composing.done=!0)}),Ql(o,"touchstart",function(){return n.forceCompositionEnd()}),Ql(o,"input",function(){r.composing||r.readFromDOMSoon()}),Ql(o,"copy",t),Ql(o,"cut",t)},Zs.prototype.prepareSelection=function(){var e=Tr(this.cm,!1);return e.focus=this.cm.state.focused,e},Zs.prototype.showSelection=function(e,t){e&&this.cm.display.view.length&&((e.focus||t)&&this.showPrimarySelection(),this.showMultipleSelections(e))},Zs.prototype.showPrimarySelection=function(){var e=window.getSelection(),t=this.cm,r=t.doc.sel.primary(),n=r.from(),i=r.to();if(t.display.viewTo==t.display.viewFrom||n.line>=t.display.viewTo||i.line<t.display.viewFrom)e.removeAllRanges();else{var o=sl(t,e.anchorNode,e.anchorOffset),l=sl(t,e.focusNode,e.focusOffset);if(!o||o.bad||!l||l.bad||0!=P(B(o,l),n)||0!=P(R(o,l),i)){var s=t.display.view,a=n.line>=t.display.viewFrom&&nl(t,n)||{node:s[0].measure.map[2],offset:0},u=i.line<t.display.viewTo&&nl(t,i);if(!u){var c=s[s.length-1].measure,f=c.maps?c.maps[c.maps.length-1]:c.map;u={node:f[f.length-1],offset:f[f.length-2]-f[f.length-3]}}if(a&&u){var h,d=e.rangeCount&&e.getRangeAt(0);try{h=Wl(a.node,a.offset,u.offset,u.node)}catch(e){}h&&(!fl&&t.state.focused?(e.collapse(a.node,a.offset),h.collapsed||(e.removeAllRanges(),e.addRange(h))):(e.removeAllRanges(),e.addRange(h)),d&&null==e.anchorNode?e.addRange(d):fl&&this.startGracePeriod()),this.rememberSelection()}else e.removeAllRanges()}}},Zs.prototype.startGracePeriod=function(){var e=this;clearTimeout(this.gracePeriod),this.gracePeriod=setTimeout(function(){e.gracePeriod=!1,e.selectionChanged()&&e.cm.operation(function(){return e.cm.curOp.selectionChanged=!0})},20)},Zs.prototype.showMultipleSelections=function(e){r(this.cm.display.cursorDiv,e.cursors),r(this.cm.display.selectionDiv,e.selection)},Zs.prototype.rememberSelection=function(){var e=window.getSelection();this.lastAnchorNode=e.anchorNode,this.lastAnchorOffset=e.anchorOffset,this.lastFocusNode=e.focusNode,this.lastFocusOffset=e.focusOffset},Zs.prototype.selectionInEditor=function(){var e=window.getSelection();if(!e.rangeCount)return!1;var t=e.getRangeAt(0).commonAncestorContainer;return o(this.div,t)},Zs.prototype.focus=function(){"nocursor"!=this.cm.options.readOnly&&(this.selectionInEditor()||this.showSelection(this.prepareSelection(),!0),this.div.focus())},Zs.prototype.blur=function(){this.div.blur()},Zs.prototype.getField=function(){return this.div},Zs.prototype.supportsTouch=function(){return!0},Zs.prototype.receivedFocus=function(){function e(){t.cm.state.focused&&(t.pollSelection(),t.polling.set(t.cm.options.pollInterval,e))}var t=this;this.selectionInEditor()?this.pollSelection():hn(this.cm,function(){return t.cm.curOp.selectionChanged=!0}),this.polling.set(this.cm.options.pollInterval,e)},Zs.prototype.selectionChanged=function(){var e=window.getSelection();return e.anchorNode!=this.lastAnchorNode||e.anchorOffset!=this.lastAnchorOffset||e.focusNode!=this.lastFocusNode||e.focusOffset!=this.lastFocusOffset},Zs.prototype.pollSelection=function(){if(null==this.readDOMTimeout&&!this.gracePeriod&&this.selectionChanged()){var e=window.getSelection(),t=this.cm;if(kl&&bl&&this.cm.options.gutters.length&&il(e.anchorNode))return this.cm.triggerOnKeyDown({type:"keydown",keyCode:8,preventDefault:Math.abs}),this.blur(),void this.focus();if(!this.composing){this.rememberSelection();var r=sl(t,e.anchorNode,e.anchorOffset),n=sl(t,e.focusNode,e.focusOffset);r&&n&&hn(t,function(){bi(t.doc,Rn(r,n),Gl),(r.bad||n.bad)&&(t.curOp.selectionChanged=!0)})}}},Zs.prototype.pollContent=function(){null!=this.readDOMTimeout&&(clearTimeout(this.readDOMTimeout),this.readDOMTimeout=null);var e=this.cm,t=e.display,r=e.doc.sel.primary(),n=r.from(),i=r.to();if(0==n.ch&&n.line>e.firstLine()&&(n=E(n.line-1,M(e.doc,n.line-1).length)),i.ch==M(e.doc,i.line).text.length&&i.line<e.lastLine()&&(i=E(i.line+1,0)),n.line<t.viewFrom||i.line>t.viewTo-1)return!1;var o,l,s;n.line==t.viewFrom||0==(o=Lr(e,n.line))?(l=W(t.view[0].line),s=t.view[0].node):(l=W(t.view[o].line),s=t.view[o-1].node.nextSibling);var a,u,c=Lr(e,i.line);if(c==t.view.length-1?(a=t.viewTo-1,u=t.lineDiv.lastChild):(a=W(t.view[c+1].line)-1,u=t.view[c+1].node.previousSibling),!s)return!1;for(var f=e.doc.splitLines(ll(e,s,u,l,a)),h=N(e.doc,E(l,0),E(a,M(e.doc,a).text.length));f.length>1&&h.length>1;)if(g(f)==g(h))f.pop(),h.pop(),a--;else{if(f[0]!=h[0])break;f.shift(),h.shift(),l++}for(var d=0,p=0,v=f[0],m=h[0],y=Math.min(v.length,m.length);d<y&&v.charCodeAt(d)==m.charCodeAt(d);)++d;for(var b=g(f),w=g(h),x=Math.min(b.length-(1==f.length?d:0),w.length-(1==h.length?d:0));p<x&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)++p;if(1==f.length&&1==h.length&&l==n.line)for(;d&&d>n.ch&&b.charCodeAt(b.length-p-1)==w.charCodeAt(w.length-p-1);)d--,p++;f[f.length-1]=b.slice(0,b.length-p).replace(/^\u200b+/,""),f[0]=f[0].slice(d).replace(/\u200b+$/,"");var C=E(l,d),S=E(a,h.length?g(h).length-p:0);return f.length>1||f[0]||P(C,S)?(Ei(e.doc,f,C,S,"+input"),!0):void 0},Zs.prototype.ensurePolled=function(){this.forceCompositionEnd()},Zs.prototype.reset=function(){this.forceCompositionEnd()},Zs.prototype.forceCompositionEnd=function(){this.composing&&(clearTimeout(this.readDOMTimeout),this.composing=null,this.updateFromDOM(),this.div.blur(),this.div.focus())},Zs.prototype.readFromDOMSoon=function(){var e=this;null==this.readDOMTimeout&&(this.readDOMTimeout=setTimeout(function(){if(e.readDOMTimeout=null,e.composing){if(!e.composing.done)return;e.composing=null}e.updateFromDOM()},80))},Zs.prototype.updateFromDOM=function(){var e=this;!this.cm.isReadOnly()&&this.pollContent()||hn(this.cm,function(){return vn(e.cm)})},Zs.prototype.setUneditable=function(e){e.contentEditable="false"},Zs.prototype.onKeyPress=function(e){0!=e.charCode&&(e.preventDefault(),this.cm.isReadOnly()||dn(this.cm,$o)(this.cm,String.fromCharCode(null==e.charCode?e.keyCode:e.charCode),0))},Zs.prototype.readOnlyChanged=function(e){this.div.contentEditable=String("nocursor"!=e)},Zs.prototype.onContextMenu=function(){},Zs.prototype.resetPosition=function(){},Zs.prototype.needsContentAttribute=!0;var Qs=function(e){this.cm=e,this.prevInput="",this.pollingFast=!1,this.polling=new Pl,this.hasSelection=!1,this.composing=null};Qs.prototype.init=function(e){function t(e){if(!Me(i,e)){if(i.somethingSelected())_o({lineWise:!1,text:i.getSelections()});else{if(!i.options.lineWiseCopyCut)return;var t=Qo(i);_o({lineWise:!0,text:t.text}),"cut"==e.type?i.setSelections(t.ranges,null,Gl):(n.prevInput="",l.value=t.text.join("\n"),El(l))}"cut"==e.type&&(i.state.cutIncoming=!0)}}var r=this,n=this,i=this.cm,o=this.wrapper=el(),l=this.textarea=o.firstChild;e.wrapper.insertBefore(o,e.wrapper.firstChild),Ll&&(l.style.width="0px"),Ql(l,"input",function(){gl&&vl>=9&&r.hasSelection&&(r.hasSelection=null),n.poll()}),Ql(l,"paste",function(e){Me(i,e)||qo(e,i)||(i.state.pasteIncoming=!0,n.fastPoll())}),Ql(l,"cut",t),Ql(l,"copy",t),Ql(e.scroller,"paste",function(t){Ft(e,t)||Me(i,t)||(i.state.pasteIncoming=!0,n.focus())}),Ql(e.lineSpace,"selectstart",function(t){Ft(e,t)||We(t)}),Ql(l,"compositionstart",function(){var e=i.getCursor("from");n.composing&&n.composing.range.clear(),n.composing={start:e,range:i.markText(e,i.getCursor("to"),{className:"CodeMirror-composing"})}}),Ql(l,"compositionend",function(){n.composing&&(n.poll(),n.composing.range.clear(),n.composing=null)})},Qs.prototype.prepareSelection=function(){var e=this.cm,t=e.display,r=e.doc,n=Tr(e);if(e.options.moveInputWithCursor){var i=sr(e,r.sel.primary().head,"div"),o=t.wrapper.getBoundingClientRect(),l=t.lineDiv.getBoundingClientRect();n.teTop=Math.max(0,Math.min(t.wrapper.clientHeight-10,i.top+l.top-o.top)),n.teLeft=Math.max(0,Math.min(t.wrapper.clientWidth-10,i.left+l.left-o.left))}return n},Qs.prototype.showSelection=function(e){var t=this.cm.display;r(t.cursorDiv,e.cursors),r(t.selectionDiv,e.selection),null!=e.teTop&&(this.wrapper.style.top=e.teTop+"px",this.wrapper.style.left=e.teLeft+"px")},Qs.prototype.reset=function(e){if(!this.contextMenuPending&&!this.composing){var t=this.cm;if(t.somethingSelected()){this.prevInput="";var r=t.getSelection();this.textarea.value=r,t.state.focused&&El(this.textarea),gl&&vl>=9&&(this.hasSelection=r)}else e||(this.prevInput=this.textarea.value="",gl&&vl>=9&&(this.hasSelection=null))}},Qs.prototype.getField=function(){return this.textarea},Qs.prototype.supportsTouch=function(){return!1},Qs.prototype.focus=function(){if("nocursor"!=this.cm.options.readOnly&&(!Tl||l()!=this.textarea))try{this.textarea.focus()}catch(e){}},Qs.prototype.blur=function(){this.textarea.blur()},Qs.prototype.resetPosition=function(){this.wrapper.style.top=this.wrapper.style.left=0},Qs.prototype.receivedFocus=function(){this.slowPoll()},Qs.prototype.slowPoll=function(){var e=this;this.pollingFast||this.polling.set(this.cm.options.pollInterval,function(){e.poll(),e.cm.state.focused&&e.slowPoll()})},Qs.prototype.fastPoll=function(){function e(){r.poll()||t?(r.pollingFast=!1,r.slowPoll()):(t=!0,r.polling.set(60,e))}var t=!1,r=this;r.pollingFast=!0,r.polling.set(20,e)},Qs.prototype.poll=function(){var e=this,t=this.cm,r=this.textarea,n=this.prevInput;if(this.contextMenuPending||!t.state.focused||ts(r)&&!n&&!this.composing||t.isReadOnly()||t.options.disableInput||t.state.keySeq)return!1;var i=r.value;if(i==n&&!t.somethingSelected())return!1;if(gl&&vl>=9&&this.hasSelection===i||Ml&&/[\uf700-\uf7ff]/.test(i))return t.display.input.reset(),!1;if(t.doc.sel==t.display.selForContextMenu){var o=i.charCodeAt(0);if(8203!=o||n||(n="​"),8666==o)return this.reset(),this.cm.execCommand("undo")}for(var l=0,s=Math.min(n.length,i.length);l<s&&n.charCodeAt(l)==i.charCodeAt(l);)++l;return hn(t,function(){$o(t,i.slice(l),n.length-l,null,e.composing?"*compose":null),i.length>1e3||i.indexOf("\n")>-1?r.value=e.prevInput="":e.prevInput=i,e.composing&&(e.composing.range.clear(),e.composing.range=t.markText(e.composing.start,t.getCursor("to"),{className:"CodeMirror-composing"}))}),!0},Qs.prototype.ensurePolled=function(){this.pollingFast&&this.poll()&&(this.pollingFast=!1)},Qs.prototype.onKeyPress=function(){gl&&vl>=9&&(this.hasSelection=null),this.fastPoll()},Qs.prototype.onContextMenu=function(e){function t(){if(null!=l.selectionStart){var e=i.somethingSelected(),t="​"+(e?l.value:"");l.value="⇚",l.value=t,n.prevInput=e?"":"​",l.selectionStart=1,l.selectionEnd=t.length,o.selForContextMenu=i.doc.sel}}function r(){if(n.contextMenuPending=!1,n.wrapper.style.cssText=c,l.style.cssText=u,gl&&vl<9&&o.scrollbars.setScrollTop(o.scroller.scrollTop=a),null!=l.selectionStart){(!gl||gl&&vl<9)&&t();var e=0,r=function(){o.selForContextMenu==i.doc.sel&&0==l.selectionStart&&l.selectionEnd>0&&"​"==n.prevInput?dn(i,Mi)(i):e++<10?o.detectingSelectAll=setTimeout(r,500):(o.selForContextMenu=null,o.input.reset())};o.detectingSelectAll=setTimeout(r,200)}}var n=this,i=n.cm,o=i.display,l=n.textarea,s=Sr(i,e),a=o.scroller.scrollTop;if(s&&!wl){i.options.resetSelectionOnContextMenu&&-1==i.doc.sel.contains(s)&&dn(i,bi)(i.doc,Rn(s),Gl);var u=l.style.cssText,c=n.wrapper.style.cssText;n.wrapper.style.cssText="position: absolute";var f=n.wrapper.getBoundingClientRect();l.style.cssText="position: absolute; width: 30px; height: 30px;\n      top: "+(e.clientY-f.top-5)+"px; left: "+(e.clientX-f.left-5)+"px;\n      z-index: 1000; background: "+(gl?"rgba(255, 255, 255, .05)":"transparent")+";\n      outline: none; border-width: 0; outline: none; overflow: hidden; opacity: .05; filter: alpha(opacity=5);";var h;if(ml&&(h=window.scrollY),o.input.focus(),ml&&window.scrollTo(null,h),o.input.reset(),i.somethingSelected()||(l.value=n.prevInput=" "),n.contextMenuPending=!0,o.selForContextMenu=i.doc.sel,clearTimeout(o.detectingSelectAll),gl&&vl>=9&&t(),Hl){Fe(e);var d=function(){ke(window,"mouseup",d),setTimeout(r,20)};Ql(window,"mouseup",d)}else setTimeout(r,50)}},Qs.prototype.readOnlyChanged=function(e){e||this.reset(),this.textarea.disabled="nocursor"==e},Qs.prototype.setUneditable=function(){},Qs.prototype.needsContentAttribute=!1,function(e){function t(t,n,i,o){e.defaults[t]=n,i&&(r[t]=o?function(e,t,r){r!=Xs&&i(e,t,r)}:i)}var r=e.optionHandlers;e.defineOption=t,e.Init=Xs,t("value","",function(e,t){return e.setValue(t)},!0),t("mode",null,function(e,t){e.doc.modeOption=t,jn(e)},!0),t("indentUnit",2,jn,!0),t("indentWithTabs",!1),t("smartIndent",!0),t("tabSize",4,function(e){Xn(e),er(e),vn(e)},!0),t("lineSeparator",null,function(e,t){if(e.doc.lineSep=t,t){var r=[],n=e.doc.first;e.doc.iter(function(e){for(var i=0;;){var o=e.text.indexOf(t,i);if(-1==o)break;i=o+t.length,r.push(E(n,o))}n++});for(var i=r.length-1;i>=0;i--)Ei(e.doc,t,r[i],E(r[i].line,r[i].ch+t.length))}}),t("specialChars",/[\u0000-\u001f\u007f-\u009f\u00ad\u061c\u200b-\u200f\u2028\u2029\ufeff]/g,function(e,t,r){e.state.specialChars=new RegExp(t.source+(t.test("\t")?"":"|\t"),"g"),r!=Xs&&e.refresh()}),t("specialCharPlaceholder",at,function(e){return e.refresh()},!0),t("electricChars",!0),t("inputStyle",Tl?"contenteditable":"textarea",function(){throw new Error("inputStyle can not (yet) be changed in a running editor")},!0),t("spellcheck",!1,function(e,t){return e.getInputField().spellcheck=t},!0),t("rtlMoveVisually",!Ol),t("wholeLineUpdateBefore",!0),t("theme","default",function(e){Go(e),Uo(e)},!0),t("keyMap","default",function(e,t,r){var n=uo(t),i=r!=Xs&&uo(r);i&&i.detach&&i.detach(e,n),n.attach&&n.attach(e,i||null)}),t("extraKeys",null),t("configureMouse",null),t("lineWrapping",!1,Ko,!0),t("gutters",[],function(e){Fn(e.options),Uo(e)},!0),t("fixedGutter",!0,function(e,t){e.display.gutters.style.left=t?wr(e.display)+"px":"0",e.refresh()},!0),t("coverGutterNextToScrollbar",!1,function(e){return en(e)},!0),t("scrollbarStyle","native",function(e){rn(e),en(e),e.display.scrollbars.setScrollTop(e.doc.scrollTop),e.display.scrollbars.setScrollLeft(e.doc.scrollLeft)},!0),t("lineNumbers",!1,function(e){Fn(e.options),Uo(e)},!0),t("firstLineNumber",1,Uo,!0),t("lineNumberFormatter",function(e){return e},Uo,!0),t("showCursorWhenSelecting",!1,kr,!0),t("resetSelectionOnContextMenu",!0),t("lineWiseCopyCut",!0),t("pasteLinesPerSelection",!0),t("readOnly",!1,function(e,t){"nocursor"==t&&(Fr(e),e.display.input.blur()),e.display.input.readOnlyChanged(t)}),t("disableInput",!1,function(e,t){t||e.display.input.reset()},!0),t("dragDrop",!0,Vo),t("allowDropFileTypes",null),t("cursorBlinkRate",530),t("cursorScrollMargin",0),t("cursorHeight",1,kr,!0),t("singleCursorHeightPerLine",!0,kr,!0),t("workTime",100),t("workDelay",100),t("flattenSpans",!0,Xn,!0),t("addModeClass",!1,Xn,!0),t("pollInterval",100),t("undoDepth",200,function(e,t){return e.doc.history.undoDepth=t}),t("historyEventDelay",1250),t("viewportMargin",10,function(e){return e.refresh()},!0),t("maxHighlightLength",1e4,Xn,!0),t("moveInputWithCursor",!0,function(e,t){t||e.display.input.resetPosition()}),t("tabindex",null,function(e,t){return e.display.input.getField().tabIndex=t||""}),t("autofocus",null),t("direction","ltr",function(e,t){return e.doc.setDirection(t)},!0)}(jo),function(e){var t=e.optionHandlers,r=e.helpers={};e.prototype={constructor:e,focus:function(){window.focus(),this.display.input.focus()},setOption:function(e,r){var n=this.options,i=n[e];n[e]==r&&"mode"!=e||(n[e]=r,t.hasOwnProperty(e)&&dn(this,t[e])(this,r,i),Te(this,"optionChange",this,e))},getOption:function(e){return this.options[e]},getDoc:function(){return this.doc},addKeyMap:function(e,t){this.state.keyMaps[t?"push":"unshift"](uo(e))},removeKeyMap:function(e){for(var t=this.state.keyMaps,r=0;r<t.length;++r)if(t[r]==e||t[r].name==e)return t.splice(r,1),!0},addOverlay:pn(function(t,r){var n=t.token?t:e.getMode(this.options,t);if(n.startState)throw new Error("Overlays may not be stateful.");m(this.state.overlays,{mode:n,modeSpec:t,opaque:r&&r.opaque,priority:r&&r.priority||0},function(e){return e.priority}),this.state.modeGen++,vn(this)}),removeOverlay:pn(function(e){for(var t=this,r=this.state.overlays,n=0;n<r.length;++n){var i=r[n].modeSpec;if(i==e||"string"==typeof e&&i.name==e)return r.splice(n,1),t.state.modeGen++,void vn(t)}}),indentLine:pn(function(e,t,r){"string"!=typeof t&&"number"!=typeof t&&(t=null==t?this.options.smartIndent?"smart":"prev":t?"add":"subtract"),H(this.doc,e)&&Yo(this,e,t,r)}),indentSelection:pn(function(e){for(var t=this,r=this.doc.sel.ranges,n=-1,i=0;i<r.length;i++){var o=r[i];if(o.empty())o.head.line>n&&(Yo(t,o.head.line,e,!0),n=o.head.line,i==t.doc.sel.primIndex&&jr(t));else{var l=o.from(),s=o.to(),a=Math.max(n,l.line);n=Math.min(t.lastLine(),s.line-(s.ch?0:1))+1;for(var u=a;u<n;++u)Yo(t,u,e);var c=t.doc.sel.ranges;0==l.ch&&r.length==c.length&&c[i].from().ch>0&&gi(t.doc,i,new Ts(l,c[i].to()),Gl)}}}),getTokenAt:function(e,t){return Je(this,e,t)},getLineTokens:function(e,t){return Je(this,E(e),t,!0)},getTokenTypeAt:function(e){e=U(this.doc,e);var t,r=_e(this,M(this.doc,e.line)),n=0,i=(r.length-1)/2,o=e.ch;if(0==o)t=r[2];else for(;;){var l=n+i>>1;if((l?r[2*l-1]:0)>=o)i=l;else{if(!(r[2*l+1]<o)){t=r[2*l+2];break}n=l+1}}var s=t?t.indexOf("overlay "):-1;return s<0?t:0==s?null:t.slice(0,s-1)},getModeAt:function(t){var r=this.doc.mode;return r.innerMode?e.innerMode(r,this.getTokenAt(t).state).mode:r},getHelper:function(e,t){return this.getHelpers(e,t)[0]},getHelpers:function(e,t){var n=this,i=[];if(!r.hasOwnProperty(t))return i;var o=r[t],l=this.getModeAt(e);if("string"==typeof l[t])o[l[t]]&&i.push(o[l[t]]);else if(l[t])for(var s=0;s<l[t].length;s++){var a=o[l[t][s]];a&&i.push(a)}else l.helperType&&o[l.helperType]?i.push(o[l.helperType]):o[l.name]&&i.push(o[l.name]);for(var u=0;u<o._global.length;u++){var c=o._global[u];c.pred(l,n)&&-1==h(i,c.val)&&i.push(c.val)}return i},getStateAfter:function(e,t){var r=this.doc;return e=G(r,null==e?r.first+r.size-1:e),$e(this,e+1,t).state},cursorCoords:function(e,t){var r,n=this.doc.sel.primary();return r=null==e?n.head:"object"==typeof e?U(this.doc,e):e?n.from():n.to(),sr(this,r,t||"page")},charCoords:function(e,t){return lr(this,U(this.doc,e),t||"page")},coordsChar:function(e,t){return e=or(this,e,t||"page"),cr(this,e.left,e.top)},lineAtHeight:function(e,t){return e=or(this,{top:e,left:0},t||"page").top,D(this.doc,e+this.display.viewOffset)},heightAtLine:function(e,t,r){var n,i=!1;if("number"==typeof e){var o=this.doc.first+this.doc.size-1;e<this.doc.first?e=this.doc.first:e>o&&(e=o,i=!0),n=M(this.doc,e)}else n=e;return ir(this,n,{top:0,left:0},t||"page",r||i).top+(i?this.doc.height-ye(n):0)},defaultTextHeight:function(){return mr(this.display)},defaultCharWidth:function(){return yr(this.display)},getViewport:function(){return{from:this.display.viewFrom,to:this.display.viewTo}},addWidget:function(e,t,r,n,i){var o=this.display,l=(e=sr(this,U(this.doc,e))).bottom,s=e.left;if(t.style.position="absolute",t.setAttribute("cm-ignore-events","true"),this.display.input.setUneditable(t),o.sizer.appendChild(t),"over"==n)l=e.top;else if("above"==n||"near"==n){var a=Math.max(o.wrapper.clientHeight,this.doc.height),u=Math.max(o.sizer.clientWidth,o.lineSpace.clientWidth);("above"==n||e.bottom+t.offsetHeight>a)&&e.top>t.offsetHeight?l=e.top-t.offsetHeight:e.bottom+t.offsetHeight<=a&&(l=e.bottom),s+t.offsetWidth>u&&(s=u-t.offsetWidth)}t.style.top=l+"px",t.style.left=t.style.right="","right"==i?(s=o.sizer.clientWidth-t.offsetWidth,t.style.right="0px"):("left"==i?s=0:"middle"==i&&(s=(o.sizer.clientWidth-t.offsetWidth)/2),t.style.left=s+"px"),r&&Ur(this,{left:s,top:l,right:s+t.offsetWidth,bottom:l+t.offsetHeight})},triggerOnKeyDown:pn(Lo),triggerOnKeyPress:pn(Mo),triggerOnKeyUp:To,triggerOnMouseDown:pn(Oo),execCommand:function(e){if(Bs.hasOwnProperty(e))return Bs[e].call(null,this)},triggerElectric:pn(function(e){Zo(this,e)}),findPosH:function(e,t,r,n){var i=this,o=1;t<0&&(o=-1,t=-t);for(var l=U(this.doc,e),s=0;s<t&&!(l=tl(i.doc,l,o,r,n)).hitSide;++s);return l},moveH:pn(function(e,t){var r=this;this.extendSelectionsBy(function(n){return r.display.shift||r.doc.extend||n.empty()?tl(r.doc,n.head,e,t,r.options.rtlMoveVisually):e<0?n.from():n.to()},Vl)}),deleteH:pn(function(e,t){var r=this.doc.sel,n=this.doc;r.somethingSelected()?n.replaceSelection("",null,"+delete"):co(this,function(r){var i=tl(n,r.head,e,t,!1);return e<0?{from:i,to:r.head}:{from:r.head,to:i}})}),findPosV:function(e,t,r,n){var i=this,o=1,l=n;t<0&&(o=-1,t=-t);for(var s=U(this.doc,e),a=0;a<t;++a){var u=sr(i,s,"div");if(null==l?l=u.left:u.left=l,(s=rl(i,u,o,r)).hitSide)break}return s},moveV:pn(function(e,t){var r=this,n=this.doc,i=[],o=!this.display.shift&&!n.extend&&n.sel.somethingSelected();if(n.extendSelectionsBy(function(l){if(o)return e<0?l.from():l.to();var s=sr(r,l.head,"div");null!=l.goalColumn&&(s.left=l.goalColumn),i.push(s.left);var a=rl(r,s,e,t);return"page"==t&&l==n.sel.primary()&&Kr(r,lr(r,a,"div").top-s.top),a},Vl),i.length)for(var l=0;l<n.sel.ranges.length;l++)n.sel.ranges[l].goalColumn=i[l]}),findWordAt:function(e){var t=M(this.doc,e.line).text,r=e.ch,n=e.ch;if(t){var i=this.getHelper(e,"wordChars");"before"!=e.sticky&&n!=t.length||!r?++n:--r;for(var o=t.charAt(r),l=x(o,i)?function(e){return x(e,i)}:/\s/.test(o)?function(e){return/\s/.test(e)}:function(e){return!/\s/.test(e)&&!x(e)};r>0&&l(t.charAt(r-1));)--r;for(;n<t.length&&l(t.charAt(n));)++n}return new Ts(E(e.line,r),E(e.line,n))},toggleOverwrite:function(e){null!=e&&e==this.state.overwrite||((this.state.overwrite=!this.state.overwrite)?s(this.display.cursorDiv,"CodeMirror-overwrite"):Fl(this.display.cursorDiv,"CodeMirror-overwrite"),Te(this,"overwriteToggle",this,this.state.overwrite))},hasFocus:function(){return this.display.input.getField()==l()},isReadOnly:function(){return!(!this.options.readOnly&&!this.doc.cantEdit)},scrollTo:pn(function(e,t){Xr(this,e,t)}),getScrollInfo:function(){var e=this.display.scroller;return{left:e.scrollLeft,top:e.scrollTop,height:e.scrollHeight-zt(this)-this.display.barHeight,width:e.scrollWidth-zt(this)-this.display.barWidth,clientHeight:Bt(this),clientWidth:Rt(this)}},scrollIntoView:pn(function(e,t){null==e?(e={from:this.doc.sel.primary().head,to:null},null==t&&(t=this.options.cursorScrollMargin)):"number"==typeof e?e={from:E(e,0),to:null}:null==e.from&&(e={from:e,to:null}),e.to||(e.to=e.from),e.margin=t||0,null!=e.from.line?Yr(this,e):$r(this,e.from,e.to,e.margin)}),setSize:pn(function(e,t){var r=this,n=function(e){return"number"==typeof e||/^\d+$/.test(String(e))?e+"px":e};null!=e&&(this.display.wrapper.style.width=n(e)),null!=t&&(this.display.wrapper.style.height=n(t)),this.options.lineWrapping&&Jt(this);var i=this.display.viewFrom;this.doc.iter(i,this.display.viewTo,function(e){if(e.widgets)for(var t=0;t<e.widgets.length;t++)if(e.widgets[t].noHScroll){mn(r,i,"widget");break}++i}),this.curOp.forceUpdate=!0,Te(this,"refresh",this)}),operation:function(e){return hn(this,e)},startOperation:function(){return nn(this)},endOperation:function(){return on(this)},refresh:pn(function(){var e=this.display.cachedTextHeight;vn(this),this.curOp.forceUpdate=!0,er(this),Xr(this,this.doc.scrollLeft,this.doc.scrollTop),Wn(this),(null==e||Math.abs(e-mr(this.display))>.5)&&Cr(this),Te(this,"refresh",this)}),swapDoc:pn(function(e){var t=this.doc;return t.cm=null,qn(this,e),er(this),this.display.input.reset(),Xr(this,e.scrollLeft,e.scrollTop),this.curOp.forceScroll=!0,bt(this,"swapDoc",this,t),t}),getInputField:function(){return this.display.input.getField()},getWrapperElement:function(){return this.display.wrapper},getScrollerElement:function(){return this.display.scroller},getGutterElement:function(){return this.display.gutters}},Ae(e),e.registerHelper=function(t,n,i){r.hasOwnProperty(t)||(r[t]=e[t]={_global:[]}),r[t][n]=i},e.registerGlobalHelper=function(t,n,i,o){e.registerHelper(t,n,o),r[t]._global.push({pred:i,val:o})}}(jo);var Js="iter insert remove copy getEditor constructor".split(" ");for(var ea in Ds.prototype)Ds.prototype.hasOwnProperty(ea)&&h(Js,ea)<0&&(jo.prototype[ea]=function(e){return function(){return e.apply(this.doc,arguments)}}(Ds.prototype[ea]));return Ae(Ds),jo.inputStyles={textarea:Qs,contenteditable:Zs},jo.defineMode=function(e){jo.defaults.mode||"null"==e||(jo.defaults.mode=e),Be.apply(this,arguments)},jo.defineMIME=function(e,t){os[e]=t},jo.defineMode("null",function(){return{token:function(e){return e.skipToEnd()}}}),jo.defineMIME("text/plain","null"),jo.defineExtension=function(e,t){jo.prototype[e]=t},jo.defineDocExtension=function(e,t){Ds.prototype[e]=t},jo.fromTextArea=function(e,t){function r(){e.value=a.getValue()}if(t=t?c(t):{},t.value=e.value,!t.tabindex&&e.tabIndex&&(t.tabindex=e.tabIndex),!t.placeholder&&e.placeholder&&(t.placeholder=e.placeholder),null==t.autofocus){var n=l();t.autofocus=n==e||null!=e.getAttribute("autofocus")&&n==document.body}var i;if(e.form&&(Ql(e.form,"submit",r),!t.leaveSubmitMethodAlone)){var o=e.form;i=o.submit;try{var s=o.submit=function(){r(),o.submit=i,o.submit(),o.submit=s}}catch(e){}}t.finishInit=function(t){t.save=r,t.getTextArea=function(){return e},t.toTextArea=function(){t.toTextArea=isNaN,r(),e.parentNode.removeChild(t.getWrapperElement()),e.style.display="",e.form&&(ke(e.form,"submit",r),"function"==typeof e.form.submit&&(e.form.submit=i))}},e.style.display="none";var a=jo(function(t){return e.parentNode.insertBefore(t,e.nextSibling)},t);return a},function(e){e.off=ke,e.on=Ql,e.wheelEventPixels=Pn,e.Doc=Ds,e.splitLines=es,e.countColumn=f,e.findColumn=d,e.isWordChar=w,e.Pass=Bl,e.signal=Te,e.Line=fs,e.changeEnd=Bn,e.scrollbarModel=ws,e.Pos=E,e.cmpPos=P,e.modes=is,e.mimeModes=os,e.resolveMode=Ge,e.getMode=Ue,e.modeExtensions=ls,e.extendMode=Ve,e.copyState=Ke,e.startState=Xe,e.innerMode=je,e.commands=Bs,e.keyMap=Rs,e.keyName=ao,e.isModifierKey=lo,e.lookupKey=oo,e.normalizeKeyMap=io,e.StringStream=ss,e.SharedTextMarker=As,e.TextMarker=Os,e.LineWidget=Ms,e.e_preventDefault=We,e.e_stopPropagation=De,e.e_stop=Fe,e.addClass=s,e.contains=o,e.rmClass=Fl,e.keyNames=Es}(jo),jo.version="5.30.0",jo});
      !function(e){"object"==typeof exports&&"object"==typeof module?e(require("../../lib/codemirror")):"function"==typeof define&&define.amd?define(["../../lib/codemirror"],e):e(CodeMirror)}(function(e){"use strict";function t(e,t,n,r,o,a){this.indented=e,this.column=t,this.type=n,this.info=r,this.align=o,this.prev=a}function n(e,n,r,o){var a=e.indented;return e.context&&"statement"==e.context.type&&"statement"!=r&&(a=e.context.indented),e.context=new t(a,n,r,o,null,e.context)}function r(e){var t=e.context.type;return")"!=t&&"]"!=t&&"}"!=t||(e.indented=e.context.indented),e.context=e.context.prev}function o(e,t,n){return"variable"==t.prevToken||"type"==t.prevToken||(!!/\S(?:[^- ]>|[*\]])\s*$|\*$/.test(e.string.slice(0,n))||(!(!t.typeAtEndOfLine||e.column()!=e.indentation())||void 0))}function a(e){for(;;){if(!e||"top"==e.type)return!0;if("}"==e.type&&"namespace"!=e.prev.info)return!1;e=e.prev}}function i(e){for(var t={},n=e.split(" "),r=0;r<n.length;++r)t[n[r]]=!0;return t}function l(e,t){return"function"==typeof e?e(t):e.propertyIsEnumerable(t)}function s(e,t){if(!t.startOfLine)return!1;for(var n,r=null;n=e.peek();){if("\\"==n&&e.match(/^.$/)){r=s;break}if("/"==n&&e.match(/^\/[\/\*]/,!1))break;e.next()}return t.tokenize=r,"meta"}function c(e,t){return"type"==t.prevToken&&"type"}function u(e){return e.eatWhile(/[\w\.']/),"number"}function d(e,t){if(e.backUp(1),e.match(/(R|u8R|uR|UR|LR)/)){var n=e.match(/"([^\s\\()]{0,16})\(/);return!!n&&(t.cpp11RawStringDelim=n[1],t.tokenize=m,m(e,t))}return e.match(/(u8|u|U|L)/)?!!e.match(/["']/,!1)&&"string":(e.next(),!1)}function f(e){var t=/(\w+)::~?(\w+)$/.exec(e);return t&&t[1]==t[2]}function p(e,t){for(var n;null!=(n=e.next());)if('"'==n&&!e.eat('"')){t.tokenize=null;break}return"string"}function m(e,t){var n=t.cpp11RawStringDelim.replace(/[^\w\s]/g,"\\$&");return e.match(new RegExp(".*?\\)"+n+'"'))?t.tokenize=null:e.skipToEnd(),"string"}function h(t,n){function r(e){if(e)for(var t in e)e.hasOwnProperty(t)&&o.push(t)}"string"==typeof t&&(t=[t]);var o=[];r(n.keywords),r(n.types),r(n.builtin),r(n.atoms),o.length&&(n.helperType=t[0],e.registerHelper("hintWords",t[0],o));for(var a=0;a<t.length;++a)e.defineMIME(t[a],n)}function g(e,t){for(var n=!1;!e.eol();){if(!n&&e.match('"""')){t.tokenize=null;break}n="\\"==e.next()&&!n}return"string"}function y(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!e&&!o&&t.match('"')){a=!0;break}if(e&&t.match('"""')){a=!0;break}r=t.next(),!o&&"$"==r&&t.match("{")&&t.skipTo("}"),o=!o&&"\\"==r&&!e}return!a&&e||(n.tokenize=null),"string"}}function x(e){return function(t,n){for(var r,o=!1,a=!1;!t.eol();){if(!o&&t.match('"')&&("single"==e||t.match('""'))){a=!0;break}if(!o&&t.match("``")){w=x(e),a=!0;break}r=t.next(),o="single"==e&&!o&&"\\"==r}return a&&(n.tokenize=null),"string"}}e.defineMode("clike",function(i,s){function c(e,t){var n=e.next();if(S[n]){var r=S[n](e,t);if(!1!==r)return r}if('"'==n||"'"==n)return t.tokenize=u(n),t.tokenize(e,t);if(D.test(n))return p=n,null;if(L.test(n)){if(e.backUp(1),e.match(I))return"number";e.next()}if("/"==n){if(e.eat("*"))return t.tokenize=d,d(e,t);if(e.eat("/"))return e.skipToEnd(),"comment"}if(F.test(n)){for(;!e.match(/^\/[\/*]/,!1)&&e.eat(F););return"operator"}if(e.eatWhile(z),P)for(;e.match(P);)e.eatWhile(z);var o=e.current();return l(x,o)?(l(w,o)&&(p="newstatement"),l(v,o)&&(m=!0),"keyword"):l(b,o)?"type":l(k,o)?(l(w,o)&&(p="newstatement"),"builtin"):l(_,o)?"atom":"variable"}function u(e){return function(t,n){for(var r,o=!1,a=!1;null!=(r=t.next());){if(r==e&&!o){a=!0;break}o=!o&&"\\"==r}return(a||!o&&!C)&&(n.tokenize=null),"string"}}function d(e,t){for(var n,r=!1;n=e.next();){if("/"==n&&r){t.tokenize=null;break}r="*"==n}return"comment"}function f(e,t){s.typeFirstDefinitions&&e.eol()&&a(t.context)&&(t.typeAtEndOfLine=o(e,t,e.pos))}var p,m,h=i.indentUnit,g=s.statementIndentUnit||h,y=s.dontAlignCalls,x=s.keywords||{},b=s.types||{},k=s.builtin||{},w=s.blockKeywords||{},v=s.defKeywords||{},_=s.atoms||{},S=s.hooks||{},C=s.multiLineStrings,T=!1!==s.indentStatements,M=!1!==s.indentSwitch,P=s.namespaceSeparator,D=s.isPunctuationChar||/[\[\]{}\(\),;\:\.]/,L=s.numberStart||/[\d\.]/,I=s.number||/^(?:0x[a-f\d]+|0b[01]+|(?:\d+\.?\d*|\.\d+)(?:e[-+]?\d+)?)(u|ll?|l|f)?/i,F=s.isOperatorChar||/[+\-*&%=<>!?|\/]/,z=s.isIdentifierChar||/[\w\$_\xa1-\uffff]/;return{startState:function(e){return{tokenize:null,context:new t((e||0)-h,0,"top",null,!1),indented:0,startOfLine:!0,prevToken:null}},token:function(e,t){var i=t.context;if(e.sol()&&(null==i.align&&(i.align=!1),t.indented=e.indentation(),t.startOfLine=!0),e.eatSpace())return f(e,t),null;p=m=null;var l=(t.tokenize||c)(e,t);if("comment"==l||"meta"==l)return l;if(null==i.align&&(i.align=!0),";"==p||":"==p||","==p&&e.match(/^\s*(?:\/\/.*)?$/,!1))for(;"statement"==t.context.type;)r(t);else if("{"==p)n(t,e.column(),"}");else if("["==p)n(t,e.column(),"]");else if("("==p)n(t,e.column(),")");else if("}"==p){for(;"statement"==i.type;)i=r(t);for("}"==i.type&&(i=r(t));"statement"==i.type;)i=r(t)}else p==i.type?r(t):T&&(("}"==i.type||"top"==i.type)&&";"!=p||"statement"==i.type&&"newstatement"==p)&&n(t,e.column(),"statement",e.current());if("variable"==l&&("def"==t.prevToken||s.typeFirstDefinitions&&o(e,t,e.start)&&a(t.context)&&e.match(/^\s*\(/,!1))&&(l="def"),S.token){var u=S.token(e,t,l);void 0!==u&&(l=u)}return"def"==l&&!1===s.styleDefs&&(l="variable"),t.startOfLine=!1,t.prevToken=m?"def":l||p,f(e,t),l},indent:function(t,n){if(t.tokenize!=c&&null!=t.tokenize||t.typeAtEndOfLine)return e.Pass;var r=t.context,o=n&&n.charAt(0);if("statement"==r.type&&"}"==o&&(r=r.prev),s.dontIndentStatements)for(;"statement"==r.type&&s.dontIndentStatements.test(r.info);)r=r.prev;if(S.indent){var a=S.indent(t,r,n);if("number"==typeof a)return a}var i=o==r.type,l=r.prev&&"switch"==r.prev.info;if(s.allmanIndentation&&/[{(]/.test(o)){for(;"top"!=r.type&&"}"!=r.type;)r=r.prev;return r.indented}return"statement"==r.type?r.indented+("{"==o?0:g):!r.align||y&&")"==r.type?")"!=r.type||i?r.indented+(i?0:h)+(i||!l||/^(?:case|default)\b/.test(n)?0:h):r.indented+g:r.column+(i?0:1)},electricInput:M?/^\s*(?:case .*?:|default:|\{\}?|\})$/:/^\s*[{}]$/,blockCommentStart:"/*",blockCommentEnd:"*/",lineComment:"//",fold:"brace"}});var b="auto if break case register continue return default do sizeof static else struct switch extern typedef union for goto while enum const volatile",k="int long char short double float unsigned signed void size_t ptrdiff_t";h(["text/x-csrc","text/x-c","text/x-chdr"],{name:"clike",keywords:i(b),types:i(k+" bool _Complex _Bool float_t double_t intptr_t intmax_t int8_t int16_t int32_t int64_t uintptr_t uintmax_t uint8_t uint16_t uint32_t uint64_t"),blockKeywords:i("case do else for if switch while struct"),defKeywords:i("struct"),typeFirstDefinitions:!0,atoms:i("null true false"),hooks:{"#":s,"*":c},modeProps:{fold:["brace","include"]}}),h(["text/x-c++src","text/x-c++hdr"],{name:"clike",keywords:i(b+" asm dynamic_cast namespace reinterpret_cast try explicit new static_cast typeid catch operator template typename class friend private this using const_cast inline public throw virtual delete mutable protected alignas alignof constexpr decltype nullptr noexcept thread_local final static_assert override"),types:i(k+" bool wchar_t"),blockKeywords:i("catch class do else finally for if struct switch try while"),defKeywords:i("class namespace struct enum union"),typeFirstDefinitions:!0,atoms:i("true false null"),dontIndentStatements:/^template$/,isIdentifierChar:/[\w\$_~\xa1-\uffff]/,hooks:{"#":s,"*":c,u:d,U:d,L:d,R:d,0:u,1:u,2:u,3:u,4:u,5:u,6:u,7:u,8:u,9:u,token:function(e,t,n){if("variable"==n&&"("==e.peek()&&(";"==t.prevToken||null==t.prevToken||"}"==t.prevToken)&&f(e.current()))return"def"}},namespaceSeparator:"::",modeProps:{fold:["brace","include"]}}),h("text/x-java",{name:"clike",keywords:i("abstract assert break case catch class const continue default do else enum extends final finally float for goto if implements import instanceof interface native new package private protected public return static strictfp super switch synchronized this throw throws transient try volatile while @interface"),types:i("byte short int long float double boolean char void Boolean Byte Character Double Float Integer Long Number Object Short String StringBuffer StringBuilder Void"),blockKeywords:i("catch class do else finally for if switch try while"),defKeywords:i("class interface package enum @interface"),typeFirstDefinitions:!0,atoms:i("true false null"),number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,hooks:{"@":function(e){return!e.match("interface",!1)&&(e.eatWhile(/[\w\$_]/),"meta")}},modeProps:{fold:["brace","import"]}}),h("text/x-csharp",{name:"clike",keywords:i("abstract as async await base break case catch checked class const continue default delegate do else enum event explicit extern finally fixed for foreach goto if implicit in interface internal is lock namespace new operator out override params private protected public readonly ref return sealed sizeof stackalloc static struct switch this throw try typeof unchecked unsafe using virtual void volatile while add alias ascending descending dynamic from get global group into join let orderby partial remove select set value var yield"),types:i("Action Boolean Byte Char DateTime DateTimeOffset Decimal Double Func Guid Int16 Int32 Int64 Object SByte Single String Task TimeSpan UInt16 UInt32 UInt64 bool byte char decimal double short int long object sbyte float string ushort uint ulong"),blockKeywords:i("catch class do else finally for foreach if struct switch try while"),defKeywords:i("class interface namespace struct var"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"@":function(e,t){return e.eat('"')?(t.tokenize=p,p(e,t)):(e.eatWhile(/[\w\$_]/),"meta")}}}),h("text/x-scala",{name:"clike",keywords:i("abstract case catch class def do else extends final finally for forSome if implicit import lazy match new null object override package private protected return sealed super this throw trait try type val var while with yield _ assert assume require print println printf readLine readBoolean readByte readShort readChar readInt readLong readFloat readDouble"),types:i("AnyVal App Application Array BufferedIterator BigDecimal BigInt Char Console Either Enumeration Equiv Error Exception Fractional Function IndexedSeq Int Integral Iterable Iterator List Map Numeric Nil NotNull Option Ordered Ordering PartialFunction PartialOrdering Product Proxy Range Responder Seq Serializable Set Specializable Stream StringBuilder StringContext Symbol Throwable Traversable TraversableOnce Tuple Unit Vector Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),multiLineStrings:!0,blockKeywords:i("catch class enum do else finally for forSome if match switch try while"),defKeywords:i("class enum def object package trait type val var"),atoms:i("true false null"),indentStatements:!1,indentSwitch:!1,isOperatorChar:/[+\-*&%=<>!?|\/#:@]/,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return!!e.match('""')&&(t.tokenize=g,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},"=":function(e,n){var r=n.context;return!("}"!=r.type||!r.align||!e.eat(">"))&&(n.context=new t(r.indented,r.column,r.type,r.info,null,r.prev),"operator")}},modeProps:{closeBrackets:{triples:'"'}}}),h("text/x-kotlin",{name:"clike",keywords:i("package as typealias class interface this super val var fun for is in This throw return break continue object if else while do try when !in !is as? file import where by get set abstract enum open inner override private public internal protected catch finally out final vararg reified dynamic companion constructor init sealed field property receiver param sparam lateinit data inline noinline tailrec external annotation crossinline const operator infix suspend"),types:i("Boolean Byte Character CharSequence Class ClassLoader Cloneable Comparable Compiler Double Exception Float Integer Long Math Number Object Package Pair Process Runtime Runnable SecurityManager Short StackTraceElement StrictMath String StringBuffer System Thread ThreadGroup ThreadLocal Throwable Triple Void"),intendSwitch:!1,indentStatements:!1,multiLineStrings:!0,number:/^(?:0x[a-f\d_]+|0b[01_]+|(?:[\d_]+\.?\d*|\.\d+)(?:e[-+]?[\d_]+)?)(u|ll?|l|f)?/i,blockKeywords:i("catch class do else finally for if where try while enum"),defKeywords:i("class val var object package interface fun"),atoms:i("true false null this"),hooks:{'"':function(e,t){return t.tokenize=y(e.match('""')),t.tokenize(e,t)}},modeProps:{closeBrackets:{triples:'"'}}}),h(["x-shader/x-vertex","x-shader/x-fragment"],{name:"clike",keywords:i("sampler1D sampler2D sampler3D samplerCube sampler1DShadow sampler2DShadow const attribute uniform varying break continue discard return for while do if else struct in out inout"),types:i("float int bool void vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 mat2 mat3 mat4"),blockKeywords:i("for while do if else struct"),builtin:i("radians degrees sin cos tan asin acos atan pow exp log exp2 sqrt inversesqrt abs sign floor ceil fract mod min max clamp mix step smoothstep length distance dot cross normalize ftransform faceforward reflect refract matrixCompMult lessThan lessThanEqual greaterThan greaterThanEqual equal notEqual any all not texture1D texture1DProj texture1DLod texture1DProjLod texture2D texture2DProj texture2DLod texture2DProjLod texture3D texture3DProj texture3DLod texture3DProjLod textureCube textureCubeLod shadow1D shadow2D shadow1DProj shadow2DProj shadow1DLod shadow2DLod shadow1DProjLod shadow2DProjLod dFdx dFdy fwidth noise1 noise2 noise3 noise4"),atoms:i("true false gl_FragColor gl_SecondaryColor gl_Normal gl_Vertex gl_MultiTexCoord0 gl_MultiTexCoord1 gl_MultiTexCoord2 gl_MultiTexCoord3 gl_MultiTexCoord4 gl_MultiTexCoord5 gl_MultiTexCoord6 gl_MultiTexCoord7 gl_FogCoord gl_PointCoord gl_Position gl_PointSize gl_ClipVertex gl_FrontColor gl_BackColor gl_FrontSecondaryColor gl_BackSecondaryColor gl_TexCoord gl_FogFragCoord gl_FragCoord gl_FrontFacing gl_FragData gl_FragDepth gl_ModelViewMatrix gl_ProjectionMatrix gl_ModelViewProjectionMatrix gl_TextureMatrix gl_NormalMatrix gl_ModelViewMatrixInverse gl_ProjectionMatrixInverse gl_ModelViewProjectionMatrixInverse gl_TexureMatrixTranspose gl_ModelViewMatrixInverseTranspose gl_ProjectionMatrixInverseTranspose gl_ModelViewProjectionMatrixInverseTranspose gl_TextureMatrixInverseTranspose gl_NormalScale gl_DepthRange gl_ClipPlane gl_Point gl_FrontMaterial gl_BackMaterial gl_LightSource gl_LightModel gl_FrontLightModelProduct gl_BackLightModelProduct gl_TextureColor gl_EyePlaneS gl_EyePlaneT gl_EyePlaneR gl_EyePlaneQ gl_FogParameters gl_MaxLights gl_MaxClipPlanes gl_MaxTextureUnits gl_MaxTextureCoords gl_MaxVertexAttribs gl_MaxVertexUniformComponents gl_MaxVaryingFloats gl_MaxVertexTextureImageUnits gl_MaxTextureImageUnits gl_MaxFragmentUniformComponents gl_MaxCombineTextureImageUnits gl_MaxDrawBuffers"),indentSwitch:!1,hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-nesc",{name:"clike",keywords:i(b+"as atomic async call command component components configuration event generic implementation includes interface module new norace nx_struct nx_union post provides signal task uses abstract extends"),types:i(k),blockKeywords:i("case do else for if switch while struct"),atoms:i("null true false"),hooks:{"#":s},modeProps:{fold:["brace","include"]}}),h("text/x-objectivec",{name:"clike",keywords:i(b+"inline restrict _Bool _Complex _Imaginary BOOL Class bycopy byref id IMP in inout nil oneway out Protocol SEL self super atomic nonatomic retain copy readwrite readonly"),types:i(k),atoms:i("YES NO NULL NILL ON OFF true false"),hooks:{"@":function(e){return e.eatWhile(/[\w\$]/),"keyword"},"#":s,indent:function(e,t,n){if("statement"==t.type&&/^@\w/.test(n))return t.indented}},modeProps:{fold:"brace"}}),h("text/x-squirrel",{name:"clike",keywords:i("base break clone continue const default delete enum extends function in class foreach local resume return this throw typeof yield constructor instanceof static"),types:i(k),blockKeywords:i("case catch class else for foreach if switch try while"),defKeywords:i("function local class"),typeFirstDefinitions:!0,atoms:i("true false null"),hooks:{"#":s},modeProps:{fold:["brace","include"]}});var w=null;h("text/x-ceylon",{name:"clike",keywords:i("abstracts alias assembly assert assign break case catch class continue dynamic else exists extends finally for function given if import in interface is let module new nonempty object of out outer package return satisfies super switch then this throw try value void while"),types:function(e){var t=e.charAt(0);return t===t.toUpperCase()&&t!==t.toLowerCase()},blockKeywords:i("case catch class dynamic else finally for function if interface module new object switch try while"),defKeywords:i("class dynamic function interface module object package value"),builtin:i("abstract actual aliased annotation by default deprecated doc final formal late license native optional sealed see serializable shared suppressWarnings tagged throws variable"),isPunctuationChar:/[\[\]{}\(\),;\:\.`]/,isOperatorChar:/[+\-*&%=<>!?|^~:\/]/,numberStart:/[\d#$]/,number:/^(?:#[\da-fA-F_]+|\$[01_]+|[\d_]+[kMGTPmunpf]?|[\d_]+\.[\d_]+(?:[eE][-+]?\d+|[kMGTPmunpf]|)|)/i,multiLineStrings:!0,typeFirstDefinitions:!0,atoms:i("true false null larger smaller equal empty finished"),indentSwitch:!1,styleDefs:!1,hooks:{"@":function(e){return e.eatWhile(/[\w\$_]/),"meta"},'"':function(e,t){return t.tokenize=x(e.match('""')?"triple":"single"),t.tokenize(e,t)},"`":function(e,t){return!(!w||!e.match("`"))&&(t.tokenize=w,w=null,t.tokenize(e,t))},"'":function(e){return e.eatWhile(/[\w\$_\xa1-\uffff]/),"atom"},token:function(e,t,n){if(("variable"==n||"type"==n)&&"."==t.prevToken)return"variable-2"}},modeProps:{fold:["brace","import"],closeBrackets:{triples:'"'}}})});
      // -------------------------------------------------------------------------
//  Part of the CodeChecker project, under the Apache License v2.0 with
//  LLVM Exceptions. See LICENSE for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
// -------------------------------------------------------------------------

var BugViewer = {
  _files : [],
  _reports : [],
  _lineWidgets : [],
  _navigationMenuItems : [],
  _sourceFileData : null,
  _currentReport : null,
  _lastBugEvent  : null,

  init : function (files, reports) {
    this._files = files;
    this._reports = reports;

    this.initEscapeChars();
  },

  initEscapeChars : function () {
    this.escapeChars = {
      ' ' : 'nbsp',
      '<' : 'lt',
      '>' : 'gt',
      '"' : 'quot',
      '&' : 'amp'
    };

    var regexString = '[';
    for (var key in this.escapeChars) {
      regexString += key;
    }
    regexString += ']';

    this.escapeRegExp = new RegExp( regexString, 'g');
  },

  escapeHTML : function (str) {
    var that = this;

    return str.replace(this.escapeRegExp, function (m) {
      return '&' + that.escapeChars[m] + ';';
    });
  },

  initByUrl : function () {
    if (!this._reports) return;

    var state = {};
    window.location.hash.substr(1).split('&').forEach(function (s) {
      var parts = s.split('=');
      state[parts[0]] = parts[1];
    });

    for (var key in this._reports) {
      var report = this._reports[key];
      if (report.reportHash === state['reportHash']) {
        this.navigate(report);
        return;
      }
    }

    this.navigate(this._reports[0]);
  },

  create : function () {
    this._content = document.getElementById('editor-wrapper');
    this._filepath = document.getElementById('file-path');
    this._checkerName = document.getElementById('checker-name');
    this._reviewStatusWrapper =
      document.getElementById('review-status-wrapper');
    this._reviewStatus = document.getElementById('review-status');
    this._editor = document.getElementById('editor');

    this._codeMirror = CodeMirror(this._editor, {
      mode: 'text/x-c++src',
      matchBrackets : true,
      lineNumbers : true,
      readOnly : true,
      foldGutter : true,
      extraKeys : {},
      viewportMargin : 100
    });

    this._createNavigationMenu();
  },

  navigate : function (report, item) {
    if (!item) {
      var items = this._navigationMenuItems.filter(function (navItem) {
        return navItem.report.reportHash === report.reportHash;
      });

      if (!items.length) return;

      item = items[0].widget;
    }

    this._selectedReport.classList.remove('active');
    this._selectedReport = item;
    this._selectedReport.classList.add('active');
    this.setReport(report);
  },

  _createNavigationMenu : function () {
    var that = this;

    var nav = document.getElementById('report-nav');
    var list = document.createElement('ul');
    this._reports.forEach(function (report) {
      var events = report['events'];
      var lastBugEvent = events[events.length - 1];
      var item = document.createElement('li');

      var severity = document.createElement('i');
      severity.className = 'severity-' + report.severity.toLowerCase();

      item.appendChild(severity);
      item.appendChild(document.createTextNode(lastBugEvent.message));

      item.addEventListener('click', function () {
        that.navigate(report, item);
      })
      list.appendChild(item);
      that._navigationMenuItems.push({ report : report, widget : item });
    });

    if (!this._selectedReport && list.childNodes.length) {
      this._selectedReport = list.childNodes[0];
      this._selectedReport.classList.add('active');
    }

    nav.appendChild(list);
  },

  setReport : function (report) {
    this._currentReport = report;
    var events = report['events'];
    var lastBugEvent = events[events.length - 1];
    this.setCurrentBugEvent(lastBugEvent, events.length - 1);
    this.setCheckerName(report.checkerName);
    this.setReviewStatus(report.reviewStatus);

    window.location.hash = '#reportHash=' + report.reportHash;
  },

  setCurrentBugEvent : function (event, idx) {
    this._currentBugEvent = event;
    this.setSourceFileData(this._files[event.location.file]);
    this.drawBugPath();

    this.jumpTo(event.location.line, 0);
    this.highlightBugEvent(event, idx);
  },

  highlightBugEvent : function (event, idx) {
    this._lineWidgets.forEach(function (widget) {
      var lineIdx = widget.node.getAttribute('idx');
      if (parseInt(lineIdx) === idx) {
        widget.node.classList.add('current');
      }
    });
  },

  setCheckerName : function (checkerName) {
    this._checkerName.innerHTML = checkerName;
  },

  setReviewStatus : function (status) {
    if (status) {
      var className =
        'review-status-' + status.toLowerCase().split(' ').join('-');
      this._reviewStatus.className = "review-status " + className;

      this._reviewStatus.innerHTML = status;
      this._reviewStatusWrapper.style.display = 'block';
    } else {
      this._reviewStatusWrapper.style.display = 'none';
    }
  },

  setSourceFileData : function (file) {
    if (this._sourceFileData && file.id === this._sourceFileData.id) {
      return;
    }

    this._sourceFileData = file;
    this._filepath.innerHTML = file.path;
    this._codeMirror.doc.setValue(file.content);
    this._refresh();
  },

  _refresh : function () {
    var that = this;
    setTimeout(function () {
      var fullHeight = parseInt(that._content.clientHeight);
      var headerHeight = that._filepath.clientHeight;

      that._codeMirror.setSize('auto', fullHeight - headerHeight);
      that._codeMirror.refresh();
    }, 200);
  },

  clearBubbles : function () {
    this._lineWidgets.forEach(function (widget) { widget.clear(); });
    this._lineWidgets = [];
  },

  getMessage : function (event, kind) {
    if (kind === 'macro') {
      var name = 'macro expansion' + (event.name ? ': ' + event.name : '');

      return '<span class="tag macro">' + name + '</span>'
        + this.escapeHTML(event.expansion).replace(/(?:\r\n|\r|\n)/g, '<br>');
    } else if (kind === 'note') {
      return '<span class="tag note">note</span>'
        +  this.escapeHTML(event.message).replace(/(?:\r\n|\r|\n)/g, '<br>');
    }
  },

  addExtraPathEvents : function (events, kind) {
    var that = this;

    if (!events) {
      return;
    }

    events.forEach(function (event) {
      if (event.location.file !== that._currentBugEvent.location.file) {
        return;
      }

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + kind);

      var msg = document.createElement('span');
      msg.innerHTML = that.getMessage(event, kind);
      element.appendChild(msg);

      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  drawBugPath : function () {
    var that = this;

    this.clearBubbles();

    this.addExtraPathEvents(this._currentReport.macros, 'macro');
    this.addExtraPathEvents(this._currentReport.notes, 'note');

    // Processing bug path events.
    var currentEvents = this._currentReport.events;
    currentEvents.forEach(function (event, step) {
      if (event.location.file !== that._currentBugEvent.location.file)
        return;

      var left =
        that._codeMirror.defaultCharWidth() * event.location.col + 'px';
      var type = step === currentEvents.length - 1 ? 'error' : 'info';

      var element = document.createElement('div');
      element.setAttribute('style', 'margin-left: ' + left);
      element.setAttribute('class', 'check-msg ' + type);
      element.setAttribute('idx', step);

      var enumeration = document.createElement('span');
      enumeration.setAttribute('class', 'checker-enum ' + type);
      enumeration.innerHTML = step + 1;

      if (currentEvents.length > 1)
        element.appendChild(enumeration);

      var prevBugEvent = step - 1;
      if (step > 0) {
        var prevBug = document.createElement('span');
        prevBug.setAttribute('class', 'arrow left-arrow');
        prevBug.addEventListener('click', function () {
          var event = currentEvents[prevBugEvent];
          that.setCurrentBugEvent(event, prevBugEvent);
        });
        element.appendChild(prevBug);
      }

      var msg = document.createElement('span');
      msg.innerHTML = that.escapeHTML(event.message)
        .replace(/(?:\r\n|\r|\n)/g, '<br>');

      element.appendChild(msg);

      var nextBugEvent = step + 1;
      if (nextBugEvent < currentEvents.length) {
        var nextBug = document.createElement('span');
        nextBug.setAttribute('class', 'arrow right-arrow');
        nextBug.addEventListener('click', function () {
          var event = currentEvents[nextBugEvent];
          that.setCurrentBugEvent(event, nextBugEvent);
        });
        element.appendChild(nextBug);
      }


      that._lineWidgets.push(that._codeMirror.addLineWidget(
        event.location.line - 1, element));
    });
  },

  jumpTo : function (line, column) {
    var that = this;

    setTimeout(function () {
      var selPosPixel
        = that._codeMirror.charCoords({ line : line, ch : column }, 'local');
      var editorSize = {
        width  : that._editor.clientWidth,
        height : that._editor.clientHeight
      };

      that._codeMirror.scrollIntoView({
        top    : selPosPixel.top - 100,
        bottom : selPosPixel.top + editorSize.height - 150,
        left   : selPosPixel.left < editorSize.width - 100
               ? 0
               : selPosPixel.left - 50,
        right  : selPosPixel.left < editorSize.width - 100
               ? 10
               : selPosPixel.left + editorSize.width - 100
      });
    }, 0);
  }
}


      var data = {"files": {"0": {"id": 0, "path": "/src/kernel/workqueue.c", "content": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * kernel/workqueue.c - generic async execution with shared worker pool\n *\n * Copyright (C) 2002\t\tIngo Molnar\n *\n *   Derived from the taskqueue/keventd code by:\n *     David Woodhouse <dwmw2@infradead.org>\n *     Andrew Morton\n *     Kai Petzke <wpp@marie.physik.tu-berlin.de>\n *     Theodore Ts'o <tytso@mit.edu>\n *\n * Made to use alloc_percpu by Christoph Lameter.\n *\n * Copyright (C) 2010\t\tSUSE Linux Products GmbH\n * Copyright (C) 2010\t\tTejun Heo <tj@kernel.org>\n *\n * This is the generic async execution mechanism.  Work items as are\n * executed in process context.  The worker pool is shared and\n * automatically managed.  There are two worker pools for each CPU (one for\n * normal work items and the other for high priority ones) and some extra\n * pools for workqueues which are not bound to any specific CPU - the\n * number of these backing pools is dynamic.\n *\n * Please read Documentation/core-api/workqueue.rst for details.\n */\n\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/completion.h>\n#include <linux/workqueue.h>\n#include <linux/slab.h>\n#include <linux/cpu.h>\n#include <linux/notifier.h>\n#include <linux/kthread.h>\n#include <linux/hardirq.h>\n#include <linux/mempolicy.h>\n#include <linux/freezer.h>\n#include <linux/debug_locks.h>\n#include <linux/lockdep.h>\n#include <linux/idr.h>\n#include <linux/jhash.h>\n#include <linux/hashtable.h>\n#include <linux/rculist.h>\n#include <linux/nodemask.h>\n#include <linux/moduleparam.h>\n#include <linux/uaccess.h>\n#include <linux/sched/isolation.h>\n#include <linux/nmi.h>\n\n#include \"workqueue_internal.h\"\n\nenum {\n\t/*\n\t * worker_pool flags\n\t *\n\t * A bound pool is either associated or disassociated with its CPU.\n\t * While associated (!DISASSOCIATED), all workers are bound to the\n\t * CPU and none has %WORKER_UNBOUND set and concurrency management\n\t * is in effect.\n\t *\n\t * While DISASSOCIATED, the cpu may be offline and all workers have\n\t * %WORKER_UNBOUND set and concurrency management disabled, and may\n\t * be executing on any CPU.  The pool behaves as an unbound one.\n\t *\n\t * Note that DISASSOCIATED should be flipped only while holding\n\t * wq_pool_attach_mutex to avoid changing binding state while\n\t * worker_attach_to_pool() is in progress.\n\t */\n\tPOOL_MANAGER_ACTIVE\t= 1 << 0,\t/* being managed */\n\tPOOL_DISASSOCIATED\t= 1 << 2,\t/* cpu can't serve workers */\n\n\t/* worker flags */\n\tWORKER_DIE\t\t= 1 << 1,\t/* die die die */\n\tWORKER_IDLE\t\t= 1 << 2,\t/* is idle */\n\tWORKER_PREP\t\t= 1 << 3,\t/* preparing to run works */\n\tWORKER_CPU_INTENSIVE\t= 1 << 6,\t/* cpu intensive */\n\tWORKER_UNBOUND\t\t= 1 << 7,\t/* worker is unbound */\n\tWORKER_REBOUND\t\t= 1 << 8,\t/* worker was rebound */\n\n\tWORKER_NOT_RUNNING\t= WORKER_PREP | WORKER_CPU_INTENSIVE |\n\t\t\t\t  WORKER_UNBOUND | WORKER_REBOUND,\n\n\tNR_STD_WORKER_POOLS\t= 2,\t\t/* # standard pools per cpu */\n\n\tUNBOUND_POOL_HASH_ORDER\t= 6,\t\t/* hashed by pool->attrs */\n\tBUSY_WORKER_HASH_ORDER\t= 6,\t\t/* 64 pointers */\n\n\tMAX_IDLE_WORKERS_RATIO\t= 4,\t\t/* 1/4 of busy can be idle */\n\tIDLE_WORKER_TIMEOUT\t= 300 * HZ,\t/* keep idle ones for 5 mins */\n\n\tMAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,\n\t\t\t\t\t\t/* call for help after 10ms\n\t\t\t\t\t\t   (min two ticks) */\n\tMAYDAY_INTERVAL\t\t= HZ / 10,\t/* and then every 100ms */\n\tCREATE_COOLDOWN\t\t= HZ,\t\t/* time to breath after fail */\n\n\t/*\n\t * Rescue workers are used only on emergencies and shared by\n\t * all cpus.  Give MIN_NICE.\n\t */\n\tRESCUER_NICE_LEVEL\t= MIN_NICE,\n\tHIGHPRI_NICE_LEVEL\t= MIN_NICE,\n\n\tWQ_NAME_LEN\t\t= 24,\n};\n\n/*\n * Structure fields follow one of the following exclusion rules.\n *\n * I: Modifiable by initialization/destruction paths and read-only for\n *    everyone else.\n *\n * P: Preemption protected.  Disabling preemption is enough and should\n *    only be modified and accessed from the local cpu.\n *\n * L: pool->lock protected.  Access with pool->lock held.\n *\n * X: During normal operation, modification requires pool->lock and should\n *    be done only from local cpu.  Either disabling preemption on local\n *    cpu or grabbing pool->lock is enough for read access.  If\n *    POOL_DISASSOCIATED is set, it's identical to L.\n *\n * A: wq_pool_attach_mutex protected.\n *\n * PL: wq_pool_mutex protected.\n *\n * PR: wq_pool_mutex protected for writes.  RCU protected for reads.\n *\n * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.\n *\n * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or\n *      RCU for reads.\n *\n * WQ: wq->mutex protected.\n *\n * WR: wq->mutex protected for writes.  RCU protected for reads.\n *\n * MD: wq_mayday_lock protected.\n */\n\n/* struct worker is defined in workqueue_internal.h */\n\nstruct worker_pool {\n\traw_spinlock_t\t\tlock;\t\t/* the pool lock */\n\tint\t\t\tcpu;\t\t/* I: the associated cpu */\n\tint\t\t\tnode;\t\t/* I: the associated node ID */\n\tint\t\t\tid;\t\t/* I: pool ID */\n\tunsigned int\t\tflags;\t\t/* X: flags */\n\n\tunsigned long\t\twatchdog_ts;\t/* L: watchdog timestamp */\n\n\tstruct list_head\tworklist;\t/* L: list of pending works */\n\n\tint\t\t\tnr_workers;\t/* L: total number of workers */\n\tint\t\t\tnr_idle;\t/* L: currently idle workers */\n\n\tstruct list_head\tidle_list;\t/* X: list of idle workers */\n\tstruct timer_list\tidle_timer;\t/* L: worker idle timeout */\n\tstruct timer_list\tmayday_timer;\t/* L: SOS timer for workers */\n\n\t/* a workers is either on busy_hash or idle_list, or the manager */\n\tDECLARE_HASHTABLE(busy_hash, BUSY_WORKER_HASH_ORDER);\n\t\t\t\t\t\t/* L: hash of busy workers */\n\n\tstruct worker\t\t*manager;\t/* L: purely informational */\n\tstruct list_head\tworkers;\t/* A: attached workers */\n\tstruct completion\t*detach_completion; /* all workers detached */\n\n\tstruct ida\t\tworker_ida;\t/* worker IDs for task name */\n\n\tstruct workqueue_attrs\t*attrs;\t\t/* I: worker attributes */\n\tstruct hlist_node\thash_node;\t/* PL: unbound_pool_hash node */\n\tint\t\t\trefcnt;\t\t/* PL: refcnt for unbound pools */\n\n\t/*\n\t * The current concurrency level.  As it's likely to be accessed\n\t * from other CPUs during try_to_wake_up(), put it in a separate\n\t * cacheline.\n\t */\n\tatomic_t\t\tnr_running ____cacheline_aligned_in_smp;\n\n\t/*\n\t * Destruction of pool is RCU protected to allow dereferences\n\t * from get_work_pool().\n\t */\n\tstruct rcu_head\t\trcu;\n} ____cacheline_aligned_in_smp;\n\n/*\n * The per-pool workqueue.  While queued, the lower WORK_STRUCT_FLAG_BITS\n * of work_struct->data are used for flags and the remaining high bits\n * point to the pwq; thus, pwqs need to be aligned at two's power of the\n * number of flag bits.\n */\nstruct pool_workqueue {\n\tstruct worker_pool\t*pool;\t\t/* I: the associated pool */\n\tstruct workqueue_struct *wq;\t\t/* I: the owning workqueue */\n\tint\t\t\twork_color;\t/* L: current color */\n\tint\t\t\tflush_color;\t/* L: flushing color */\n\tint\t\t\trefcnt;\t\t/* L: reference count */\n\tint\t\t\tnr_in_flight[WORK_NR_COLORS];\n\t\t\t\t\t\t/* L: nr of in_flight works */\n\tint\t\t\tnr_active;\t/* L: nr of active works */\n\tint\t\t\tmax_active;\t/* L: max active works */\n\tstruct list_head\tdelayed_works;\t/* L: delayed works */\n\tstruct list_head\tpwqs_node;\t/* WR: node on wq->pwqs */\n\tstruct list_head\tmayday_node;\t/* MD: node on wq->maydays */\n\n\t/*\n\t * Release of unbound pwq is punted to system_wq.  See put_pwq()\n\t * and pwq_unbound_release_workfn() for details.  pool_workqueue\n\t * itself is also RCU protected so that the first pwq can be\n\t * determined without grabbing wq->mutex.\n\t */\n\tstruct work_struct\tunbound_release_work;\n\tstruct rcu_head\t\trcu;\n} __aligned(1 << WORK_STRUCT_FLAG_BITS);\n\n/*\n * Structure used to wait for workqueue flush.\n */\nstruct wq_flusher {\n\tstruct list_head\tlist;\t\t/* WQ: list of flushers */\n\tint\t\t\tflush_color;\t/* WQ: flush color waiting for */\n\tstruct completion\tdone;\t\t/* flush completion */\n};\n\nstruct wq_device;\n\n/*\n * The externally visible workqueue.  It relays the issued work items to\n * the appropriate worker_pool through its pool_workqueues.\n */\nstruct workqueue_struct {\n\tstruct list_head\tpwqs;\t\t/* WR: all pwqs of this wq */\n\tstruct list_head\tlist;\t\t/* PR: list of all workqueues */\n\n\tstruct mutex\t\tmutex;\t\t/* protects this wq */\n\tint\t\t\twork_color;\t/* WQ: current work color */\n\tint\t\t\tflush_color;\t/* WQ: current flush color */\n\tatomic_t\t\tnr_pwqs_to_flush; /* flush in progress */\n\tstruct wq_flusher\t*first_flusher;\t/* WQ: first flusher */\n\tstruct list_head\tflusher_queue;\t/* WQ: flush waiters */\n\tstruct list_head\tflusher_overflow; /* WQ: flush overflow list */\n\n\tstruct list_head\tmaydays;\t/* MD: pwqs requesting rescue */\n\tstruct worker\t\t*rescuer;\t/* MD: rescue worker */\n\n\tint\t\t\tnr_drainers;\t/* WQ: drain in progress */\n\tint\t\t\tsaved_max_active; /* WQ: saved pwq max_active */\n\n\tstruct workqueue_attrs\t*unbound_attrs;\t/* PW: only for unbound wqs */\n\tstruct pool_workqueue\t*dfl_pwq;\t/* PW: only for unbound wqs */\n\n#ifdef CONFIG_SYSFS\n\tstruct wq_device\t*wq_dev;\t/* I: for sysfs interface */\n#endif\n#ifdef CONFIG_LOCKDEP\n\tchar\t\t\t*lock_name;\n\tstruct lock_class_key\tkey;\n\tstruct lockdep_map\tlockdep_map;\n#endif\n\tchar\t\t\tname[WQ_NAME_LEN]; /* I: workqueue name */\n\n\t/*\n\t * Destruction of workqueue_struct is RCU protected to allow walking\n\t * the workqueues list without grabbing wq_pool_mutex.\n\t * This is used to dump all workqueues from sysrq.\n\t */\n\tstruct rcu_head\t\trcu;\n\n\t/* hot fields used during command issue, aligned to cacheline */\n\tunsigned int\t\tflags ____cacheline_aligned; /* WQ: WQ_* flags */\n\tstruct pool_workqueue __percpu *cpu_pwqs; /* I: per-cpu pwqs */\n\tstruct pool_workqueue __rcu *numa_pwq_tbl[]; /* PWR: unbound pwqs indexed by node */\n};\n\nstatic struct kmem_cache *pwq_cache;\n\nstatic cpumask_var_t *wq_numa_possible_cpumask;\n\t\t\t\t\t/* possible CPUs of each node */\n\nstatic bool wq_disable_numa;\nmodule_param_named(disable_numa, wq_disable_numa, bool, 0444);\n\n/* see the comment above the definition of WQ_POWER_EFFICIENT */\nstatic bool wq_power_efficient = IS_ENABLED(CONFIG_WQ_POWER_EFFICIENT_DEFAULT);\nmodule_param_named(power_efficient, wq_power_efficient, bool, 0444);\n\nstatic bool wq_online;\t\t\t/* can kworkers be created yet? */\n\nstatic bool wq_numa_enabled;\t\t/* unbound NUMA affinity enabled */\n\n/* buf for wq_update_unbound_numa_attrs(), protected by CPU hotplug exclusion */\nstatic struct workqueue_attrs *wq_update_unbound_numa_attrs_buf;\n\nstatic DEFINE_MUTEX(wq_pool_mutex);\t/* protects pools and workqueues list */\nstatic DEFINE_MUTEX(wq_pool_attach_mutex); /* protects worker attach/detach */\nstatic DEFINE_RAW_SPINLOCK(wq_mayday_lock);\t/* protects wq->maydays list */\n/* wait for manager to go away */\nstatic struct rcuwait manager_wait = __RCUWAIT_INITIALIZER(manager_wait);\n\nstatic LIST_HEAD(workqueues);\t\t/* PR: list of all workqueues */\nstatic bool workqueue_freezing;\t\t/* PL: have wqs started freezing? */\n\n/* PL: allowable cpus for unbound wqs and work items */\nstatic cpumask_var_t wq_unbound_cpumask;\n\n/* CPU where unbound work was last round robin scheduled from this CPU */\nstatic DEFINE_PER_CPU(int, wq_rr_cpu_last);\n\n/*\n * Local execution of unbound work items is no longer guaranteed.  The\n * following always forces round-robin CPU selection on unbound work items\n * to uncover usages which depend on it.\n */\n#ifdef CONFIG_DEBUG_WQ_FORCE_RR_CPU\nstatic bool wq_debug_force_rr_cpu = true;\n#else\nstatic bool wq_debug_force_rr_cpu = false;\n#endif\nmodule_param_named(debug_force_rr_cpu, wq_debug_force_rr_cpu, bool, 0644);\n\n/* the per-cpu worker pools */\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools);\n\nstatic DEFINE_IDR(worker_pool_idr);\t/* PR: idr of all pools */\n\n/* PL: hash of all unbound pools keyed by pool->attrs */\nstatic DEFINE_HASHTABLE(unbound_pool_hash, UNBOUND_POOL_HASH_ORDER);\n\n/* I: attributes used when instantiating standard unbound pools on demand */\nstatic struct workqueue_attrs *unbound_std_wq_attrs[NR_STD_WORKER_POOLS];\n\n/* I: attributes used when instantiating ordered pools on demand */\nstatic struct workqueue_attrs *ordered_wq_attrs[NR_STD_WORKER_POOLS];\n\nstruct workqueue_struct *system_wq __read_mostly;\nEXPORT_SYMBOL(system_wq);\nstruct workqueue_struct *system_highpri_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_highpri_wq);\nstruct workqueue_struct *system_long_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_long_wq);\nstruct workqueue_struct *system_unbound_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_unbound_wq);\nstruct workqueue_struct *system_freezable_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_freezable_wq);\nstruct workqueue_struct *system_power_efficient_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_power_efficient_wq);\nstruct workqueue_struct *system_freezable_power_efficient_wq __read_mostly;\nEXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);\n\nstatic int worker_thread(void *__worker);\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq);\nstatic void show_pwq(struct pool_workqueue *pwq);\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/workqueue.h>\n\n#define assert_rcu_or_pool_mutex()\t\t\t\t\t\\\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\\n\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\\n\t\t\t \"RCU or wq_pool_mutex should be held\")\n\n#define assert_rcu_or_wq_mutex_or_pool_mutex(wq)\t\t\t\\\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held() &&\t\t\t\\\n\t\t\t !lockdep_is_held(&wq->mutex) &&\t\t\\\n\t\t\t !lockdep_is_held(&wq_pool_mutex),\t\t\\\n\t\t\t \"RCU, wq->mutex or wq_pool_mutex should be held\")\n\n#define for_each_cpu_worker_pool(pool, cpu)\t\t\t\t\\\n\tfor ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];\t\t\\\n\t     (pool) < &per_cpu(cpu_worker_pools, cpu)[NR_STD_WORKER_POOLS]; \\\n\t     (pool)++)\n\n/**\n * for_each_pool - iterate through all worker_pools in the system\n * @pool: iteration cursor\n * @pi: integer used for iteration\n *\n * This must be called either with wq_pool_mutex held or RCU read\n * locked.  If the pool needs to be used beyond the locking in effect, the\n * caller is responsible for guaranteeing that the pool stays online.\n *\n * The if/else clause exists only for the lockdep assertion and can be\n * ignored.\n */\n#define for_each_pool(pool, pi)\t\t\t\t\t\t\\\n\tidr_for_each_entry(&worker_pool_idr, pool, pi)\t\t\t\\\n\t\tif (({ assert_rcu_or_pool_mutex(); false; })) { }\t\\\n\t\telse\n\n/**\n * for_each_pool_worker - iterate through all workers of a worker_pool\n * @worker: iteration cursor\n * @pool: worker_pool to iterate workers of\n *\n * This must be called with wq_pool_attach_mutex.\n *\n * The if/else clause exists only for the lockdep assertion and can be\n * ignored.\n */\n#define for_each_pool_worker(worker, pool)\t\t\t\t\\\n\tlist_for_each_entry((worker), &(pool)->workers, node)\t\t\\\n\t\tif (({ lockdep_assert_held(&wq_pool_attach_mutex); false; })) { } \\\n\t\telse\n\n/**\n * for_each_pwq - iterate through all pool_workqueues of the specified workqueue\n * @pwq: iteration cursor\n * @wq: the target workqueue\n *\n * This must be called either with wq->mutex held or RCU read locked.\n * If the pwq needs to be used beyond the locking in effect, the caller is\n * responsible for guaranteeing that the pwq stays online.\n *\n * The if/else clause exists only for the lockdep assertion and can be\n * ignored.\n */\n#define for_each_pwq(pwq, wq)\t\t\t\t\t\t\\\n\tlist_for_each_entry_rcu((pwq), &(wq)->pwqs, pwqs_node,\t\t\\\n\t\t\t\t lockdep_is_held(&(wq->mutex)))\n\n#ifdef CONFIG_DEBUG_OBJECTS_WORK\n\nstatic const struct debug_obj_descr work_debug_descr;\n\nstatic void *work_debug_hint(void *addr)\n{\n\treturn ((struct work_struct *) addr)->func;\n}\n\nstatic bool work_is_static_object(void *addr)\n{\n\tstruct work_struct *work = addr;\n\n\treturn test_bit(WORK_STRUCT_STATIC_BIT, work_data_bits(work));\n}\n\n/*\n * fixup_init is called when:\n * - an active object is initialized\n */\nstatic bool work_fixup_init(void *addr, enum debug_obj_state state)\n{\n\tstruct work_struct *work = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tcancel_work_sync(work);\n\t\tdebug_object_init(work, &work_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/*\n * fixup_free is called when:\n * - an active object is freed\n */\nstatic bool work_fixup_free(void *addr, enum debug_obj_state state)\n{\n\tstruct work_struct *work = addr;\n\n\tswitch (state) {\n\tcase ODEBUG_STATE_ACTIVE:\n\t\tcancel_work_sync(work);\n\t\tdebug_object_free(work, &work_debug_descr);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic const struct debug_obj_descr work_debug_descr = {\n\t.name\t\t= \"work_struct\",\n\t.debug_hint\t= work_debug_hint,\n\t.is_static_object = work_is_static_object,\n\t.fixup_init\t= work_fixup_init,\n\t.fixup_free\t= work_fixup_free,\n};\n\nstatic inline void debug_work_activate(struct work_struct *work)\n{\n\tdebug_object_activate(work, &work_debug_descr);\n}\n\nstatic inline void debug_work_deactivate(struct work_struct *work)\n{\n\tdebug_object_deactivate(work, &work_debug_descr);\n}\n\nvoid __init_work(struct work_struct *work, int onstack)\n{\n\tif (onstack)\n\t\tdebug_object_init_on_stack(work, &work_debug_descr);\n\telse\n\t\tdebug_object_init(work, &work_debug_descr);\n}\nEXPORT_SYMBOL_GPL(__init_work);\n\nvoid destroy_work_on_stack(struct work_struct *work)\n{\n\tdebug_object_free(work, &work_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_work_on_stack);\n\nvoid destroy_delayed_work_on_stack(struct delayed_work *work)\n{\n\tdestroy_timer_on_stack(&work->timer);\n\tdebug_object_free(&work->work, &work_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_delayed_work_on_stack);\n\n#else\nstatic inline void debug_work_activate(struct work_struct *work) { }\nstatic inline void debug_work_deactivate(struct work_struct *work) { }\n#endif\n\n/**\n * worker_pool_assign_id - allocate ID and assing it to @pool\n * @pool: the pool pointer of interest\n *\n * Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned\n * successfully, -errno on failure.\n */\nstatic int worker_pool_assign_id(struct worker_pool *pool)\n{\n\tint ret;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tret = idr_alloc(&worker_pool_idr, pool, 0, WORK_OFFQ_POOL_NONE,\n\t\t\tGFP_KERNEL);\n\tif (ret >= 0) {\n\t\tpool->id = ret;\n\t\treturn 0;\n\t}\n\treturn ret;\n}\n\n/**\n * unbound_pwq_by_node - return the unbound pool_workqueue for the given node\n * @wq: the target workqueue\n * @node: the node ID\n *\n * This must be called with any of wq_pool_mutex, wq->mutex or RCU\n * read locked.\n * If the pwq needs to be used beyond the locking in effect, the caller is\n * responsible for guaranteeing that the pwq stays online.\n *\n * Return: The unbound pool_workqueue for @node.\n */\nstatic struct pool_workqueue *unbound_pwq_by_node(struct workqueue_struct *wq,\n\t\t\t\t\t\t  int node)\n{\n\tassert_rcu_or_wq_mutex_or_pool_mutex(wq);\n\n\t/*\n\t * XXX: @node can be NUMA_NO_NODE if CPU goes offline while a\n\t * delayed item is pending.  The plan is to keep CPU -> NODE\n\t * mapping valid and stable across CPU on/offlines.  Once that\n\t * happens, this workaround can be removed.\n\t */\n\tif (unlikely(node == NUMA_NO_NODE))\n\t\treturn wq->dfl_pwq;\n\n\treturn rcu_dereference_raw(wq->numa_pwq_tbl[node]);\n}\n\nstatic unsigned int work_color_to_flags(int color)\n{\n\treturn color << WORK_STRUCT_COLOR_SHIFT;\n}\n\nstatic int get_work_color(struct work_struct *work)\n{\n\treturn (*work_data_bits(work) >> WORK_STRUCT_COLOR_SHIFT) &\n\t\t((1 << WORK_STRUCT_COLOR_BITS) - 1);\n}\n\nstatic int work_next_color(int color)\n{\n\treturn (color + 1) % WORK_NR_COLORS;\n}\n\n/*\n * While queued, %WORK_STRUCT_PWQ is set and non flag bits of a work's data\n * contain the pointer to the queued pwq.  Once execution starts, the flag\n * is cleared and the high bits contain OFFQ flags and pool ID.\n *\n * set_work_pwq(), set_work_pool_and_clear_pending(), mark_work_canceling()\n * and clear_work_data() can be used to set the pwq, pool or clear\n * work->data.  These functions should only be called while the work is\n * owned - ie. while the PENDING bit is set.\n *\n * get_work_pool() and get_work_pwq() can be used to obtain the pool or pwq\n * corresponding to a work.  Pool is available once the work has been\n * queued anywhere after initialization until it is sync canceled.  pwq is\n * available only while the work item is queued.\n *\n * %WORK_OFFQ_CANCELING is used to mark a work item which is being\n * canceled.  While being canceled, a work item may have its PENDING set\n * but stay off timer and worklist for arbitrarily long and nobody should\n * try to steal the PENDING bit.\n */\nstatic inline void set_work_data(struct work_struct *work, unsigned long data,\n\t\t\t\t unsigned long flags)\n{\n\tWARN_ON_ONCE(!work_pending(work));\n\tatomic_long_set(&work->data, data | flags | work_static(work));\n}\n\nstatic void set_work_pwq(struct work_struct *work, struct pool_workqueue *pwq,\n\t\t\t unsigned long extra_flags)\n{\n\tset_work_data(work, (unsigned long)pwq,\n\t\t      WORK_STRUCT_PENDING | WORK_STRUCT_PWQ | extra_flags);\n}\n\nstatic void set_work_pool_and_keep_pending(struct work_struct *work,\n\t\t\t\t\t   int pool_id)\n{\n\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT,\n\t\t      WORK_STRUCT_PENDING);\n}\n\nstatic void set_work_pool_and_clear_pending(struct work_struct *work,\n\t\t\t\t\t    int pool_id)\n{\n\t/*\n\t * The following wmb is paired with the implied mb in\n\t * test_and_set_bit(PENDING) and ensures all updates to @work made\n\t * here are visible to and precede any updates by the next PENDING\n\t * owner.\n\t */\n\tsmp_wmb();\n\tset_work_data(work, (unsigned long)pool_id << WORK_OFFQ_POOL_SHIFT, 0);\n\t/*\n\t * The following mb guarantees that previous clear of a PENDING bit\n\t * will not be reordered with any speculative LOADS or STORES from\n\t * work->current_func, which is executed afterwards.  This possible\n\t * reordering can lead to a missed execution on attempt to queue\n\t * the same @work.  E.g. consider this case:\n\t *\n\t *   CPU#0                         CPU#1\n\t *   ----------------------------  --------------------------------\n\t *\n\t * 1  STORE event_indicated\n\t * 2  queue_work_on() {\n\t * 3    test_and_set_bit(PENDING)\n\t * 4 }                             set_..._and_clear_pending() {\n\t * 5                                 set_work_data() # clear bit\n\t * 6                                 smp_mb()\n\t * 7                               work->current_func() {\n\t * 8\t\t\t\t      LOAD event_indicated\n\t *\t\t\t\t   }\n\t *\n\t * Without an explicit full barrier speculative LOAD on line 8 can\n\t * be executed before CPU#0 does STORE on line 1.  If that happens,\n\t * CPU#0 observes the PENDING bit is still set and new execution of\n\t * a @work is not queued in a hope, that CPU#1 will eventually\n\t * finish the queued @work.  Meanwhile CPU#1 does not see\n\t * event_indicated is set, because speculative LOAD was executed\n\t * before actual STORE.\n\t */\n\tsmp_mb();\n}\n\nstatic void clear_work_data(struct work_struct *work)\n{\n\tsmp_wmb();\t/* see set_work_pool_and_clear_pending() */\n\tset_work_data(work, WORK_STRUCT_NO_POOL, 0);\n}\n\nstatic struct pool_workqueue *get_work_pwq(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\n\tif (data & WORK_STRUCT_PWQ)\n\t\treturn (void *)(data & WORK_STRUCT_WQ_DATA_MASK);\n\telse\n\t\treturn NULL;\n}\n\n/**\n * get_work_pool - return the worker_pool a given work was associated with\n * @work: the work item of interest\n *\n * Pools are created and destroyed under wq_pool_mutex, and allows read\n * access under RCU read lock.  As such, this function should be\n * called under wq_pool_mutex or inside of a rcu_read_lock() region.\n *\n * All fields of the returned pool are accessible as long as the above\n * mentioned locking is in effect.  If the returned pool needs to be used\n * beyond the critical section, the caller is responsible for ensuring the\n * returned pool is and stays online.\n *\n * Return: The worker_pool @work was last associated with.  %NULL if none.\n */\nstatic struct worker_pool *get_work_pool(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\tint pool_id;\n\n\tassert_rcu_or_pool_mutex();\n\n\tif (data & WORK_STRUCT_PWQ)\n\t\treturn ((struct pool_workqueue *)\n\t\t\t(data & WORK_STRUCT_WQ_DATA_MASK))->pool;\n\n\tpool_id = data >> WORK_OFFQ_POOL_SHIFT;\n\tif (pool_id == WORK_OFFQ_POOL_NONE)\n\t\treturn NULL;\n\n\treturn idr_find(&worker_pool_idr, pool_id);\n}\n\n/**\n * get_work_pool_id - return the worker pool ID a given work is associated with\n * @work: the work item of interest\n *\n * Return: The worker_pool ID @work was last associated with.\n * %WORK_OFFQ_POOL_NONE if none.\n */\nstatic int get_work_pool_id(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\n\tif (data & WORK_STRUCT_PWQ)\n\t\treturn ((struct pool_workqueue *)\n\t\t\t(data & WORK_STRUCT_WQ_DATA_MASK))->pool->id;\n\n\treturn data >> WORK_OFFQ_POOL_SHIFT;\n}\n\nstatic void mark_work_canceling(struct work_struct *work)\n{\n\tunsigned long pool_id = get_work_pool_id(work);\n\n\tpool_id <<= WORK_OFFQ_POOL_SHIFT;\n\tset_work_data(work, pool_id | WORK_OFFQ_CANCELING, WORK_STRUCT_PENDING);\n}\n\nstatic bool work_is_canceling(struct work_struct *work)\n{\n\tunsigned long data = atomic_long_read(&work->data);\n\n\treturn !(data & WORK_STRUCT_PWQ) && (data & WORK_OFFQ_CANCELING);\n}\n\n/*\n * Policy functions.  These define the policies on how the global worker\n * pools are managed.  Unless noted otherwise, these functions assume that\n * they're being called with pool->lock held.\n */\n\nstatic bool __need_more_worker(struct worker_pool *pool)\n{\n\treturn !atomic_read(&pool->nr_running);\n}\n\n/*\n * Need to wake up a worker?  Called from anything but currently\n * running workers.\n *\n * Note that, because unbound workers never contribute to nr_running, this\n * function will always return %true for unbound pools as long as the\n * worklist isn't empty.\n */\nstatic bool need_more_worker(struct worker_pool *pool)\n{\n\treturn !list_empty(&pool->worklist) && __need_more_worker(pool);\n}\n\n/* Can I start working?  Called from busy but !running workers. */\nstatic bool may_start_working(struct worker_pool *pool)\n{\n\treturn pool->nr_idle;\n}\n\n/* Do I need to keep working?  Called from currently running workers. */\nstatic bool keep_working(struct worker_pool *pool)\n{\n\treturn !list_empty(&pool->worklist) &&\n\t\tatomic_read(&pool->nr_running) <= 1;\n}\n\n/* Do we need a new worker?  Called from manager. */\nstatic bool need_to_create_worker(struct worker_pool *pool)\n{\n\treturn need_more_worker(pool) && !may_start_working(pool);\n}\n\n/* Do we have too many workers and should some go away? */\nstatic bool too_many_workers(struct worker_pool *pool)\n{\n\tbool managing = pool->flags & POOL_MANAGER_ACTIVE;\n\tint nr_idle = pool->nr_idle + managing; /* manager is considered idle */\n\tint nr_busy = pool->nr_workers - nr_idle;\n\n\treturn nr_idle > 2 && (nr_idle - 2) * MAX_IDLE_WORKERS_RATIO >= nr_busy;\n}\n\n/*\n * Wake up functions.\n */\n\n/* Return the first idle worker.  Safe with preemption disabled */\nstatic struct worker *first_idle_worker(struct worker_pool *pool)\n{\n\tif (unlikely(list_empty(&pool->idle_list)))\n\t\treturn NULL;\n\n\treturn list_first_entry(&pool->idle_list, struct worker, entry);\n}\n\n/**\n * wake_up_worker - wake up an idle worker\n * @pool: worker pool to wake worker from\n *\n * Wake up the first idle worker of @pool.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void wake_up_worker(struct worker_pool *pool)\n{\n\tstruct worker *worker = first_idle_worker(pool);\n\n\tif (likely(worker))\n\t\twake_up_process(worker->task);\n}\n\n/**\n * wq_worker_running - a worker is running again\n * @task: task waking up\n *\n * This function is called when a worker returns from schedule()\n */\nvoid wq_worker_running(struct task_struct *task)\n{\n\tstruct worker *worker = kthread_data(task);\n\n\tif (!worker->sleeping)\n\t\treturn;\n\tif (!(worker->flags & WORKER_NOT_RUNNING))\n\t\tatomic_inc(&worker->pool->nr_running);\n\tworker->sleeping = 0;\n}\n\n/**\n * wq_worker_sleeping - a worker is going to sleep\n * @task: task going to sleep\n *\n * This function is called from schedule() when a busy worker is\n * going to sleep. Preemption needs to be disabled to protect ->sleeping\n * assignment.\n */\nvoid wq_worker_sleeping(struct task_struct *task)\n{\n\tstruct worker *next, *worker = kthread_data(task);\n\tstruct worker_pool *pool;\n\n\t/*\n\t * Rescuers, which may not have all the fields set up like normal\n\t * workers, also reach here, let's not access anything before\n\t * checking NOT_RUNNING.\n\t */\n\tif (worker->flags & WORKER_NOT_RUNNING)\n\t\treturn;\n\n\tpool = worker->pool;\n\n\t/* Return if preempted before wq_worker_running() was reached */\n\tif (worker->sleeping)\n\t\treturn;\n\n\tworker->sleeping = 1;\n\traw_spin_lock_irq(&pool->lock);\n\n\t/*\n\t * The counterpart of the following dec_and_test, implied mb,\n\t * worklist not empty test sequence is in insert_work().\n\t * Please read comment there.\n\t *\n\t * NOT_RUNNING is clear.  This means that we're bound to and\n\t * running on the local cpu w/ rq lock held and preemption\n\t * disabled, which in turn means that none else could be\n\t * manipulating idle_list, so dereferencing idle_list without pool\n\t * lock is safe.\n\t */\n\tif (atomic_dec_and_test(&pool->nr_running) &&\n\t    !list_empty(&pool->worklist)) {\n\t\tnext = first_idle_worker(pool);\n\t\tif (next)\n\t\t\twake_up_process(next->task);\n\t}\n\traw_spin_unlock_irq(&pool->lock);\n}\n\n/**\n * wq_worker_last_func - retrieve worker's last work function\n * @task: Task to retrieve last work function of.\n *\n * Determine the last function a worker executed. This is called from\n * the scheduler to get a worker's last known identity.\n *\n * CONTEXT:\n * raw_spin_lock_irq(rq->lock)\n *\n * This function is called during schedule() when a kworker is going\n * to sleep. It's used by psi to identify aggregation workers during\n * dequeuing, to allow periodic aggregation to shut-off when that\n * worker is the last task in the system or cgroup to go to sleep.\n *\n * As this function doesn't involve any workqueue-related locking, it\n * only returns stable values when called from inside the scheduler's\n * queuing and dequeuing paths, when @task, which must be a kworker,\n * is guaranteed to not be processing any works.\n *\n * Return:\n * The last work function %current executed as a worker, NULL if it\n * hasn't executed any work yet.\n */\nwork_func_t wq_worker_last_func(struct task_struct *task)\n{\n\tstruct worker *worker = kthread_data(task);\n\n\treturn worker->last_func;\n}\n\n/**\n * worker_set_flags - set worker flags and adjust nr_running accordingly\n * @worker: self\n * @flags: flags to set\n *\n * Set @flags in @worker->flags and adjust nr_running accordingly.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock)\n */\nstatic inline void worker_set_flags(struct worker *worker, unsigned int flags)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tWARN_ON_ONCE(worker->task != current);\n\n\t/* If transitioning into NOT_RUNNING, adjust nr_running. */\n\tif ((flags & WORKER_NOT_RUNNING) &&\n\t    !(worker->flags & WORKER_NOT_RUNNING)) {\n\t\tatomic_dec(&pool->nr_running);\n\t}\n\n\tworker->flags |= flags;\n}\n\n/**\n * worker_clr_flags - clear worker flags and adjust nr_running accordingly\n * @worker: self\n * @flags: flags to clear\n *\n * Clear @flags in @worker->flags and adjust nr_running accordingly.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock)\n */\nstatic inline void worker_clr_flags(struct worker *worker, unsigned int flags)\n{\n\tstruct worker_pool *pool = worker->pool;\n\tunsigned int oflags = worker->flags;\n\n\tWARN_ON_ONCE(worker->task != current);\n\n\tworker->flags &= ~flags;\n\n\t/*\n\t * If transitioning out of NOT_RUNNING, increment nr_running.  Note\n\t * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask\n\t * of multiple flags, not a single flag.\n\t */\n\tif ((flags & WORKER_NOT_RUNNING) && (oflags & WORKER_NOT_RUNNING))\n\t\tif (!(worker->flags & WORKER_NOT_RUNNING))\n\t\t\tatomic_inc(&pool->nr_running);\n}\n\n/**\n * find_worker_executing_work - find worker which is executing a work\n * @pool: pool of interest\n * @work: work to find worker for\n *\n * Find a worker which is executing @work on @pool by searching\n * @pool->busy_hash which is keyed by the address of @work.  For a worker\n * to match, its current execution should match the address of @work and\n * its work function.  This is to avoid unwanted dependency between\n * unrelated work executions through a work item being recycled while still\n * being executed.\n *\n * This is a bit tricky.  A work item may be freed once its execution\n * starts and nothing prevents the freed area from being recycled for\n * another work item.  If the same work item address ends up being reused\n * before the original execution finishes, workqueue will identify the\n * recycled work item as currently executing and make it wait until the\n * current execution finishes, introducing an unwanted dependency.\n *\n * This function checks the work item address and work function to avoid\n * false positives.  Note that this isn't complete as one may construct a\n * work function which can introduce dependency onto itself through a\n * recycled work item.  Well, if somebody wants to shoot oneself in the\n * foot that badly, there's only so much we can do, and if such deadlock\n * actually occurs, it should be easy to locate the culprit work function.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n *\n * Return:\n * Pointer to worker which is executing @work if found, %NULL\n * otherwise.\n */\nstatic struct worker *find_worker_executing_work(struct worker_pool *pool,\n\t\t\t\t\t\t struct work_struct *work)\n{\n\tstruct worker *worker;\n\n\thash_for_each_possible(pool->busy_hash, worker, hentry,\n\t\t\t       (unsigned long)work)\n\t\tif (worker->current_work == work &&\n\t\t    worker->current_func == work->func)\n\t\t\treturn worker;\n\n\treturn NULL;\n}\n\n/**\n * move_linked_works - move linked works to a list\n * @work: start of series of works to be scheduled\n * @head: target list to append @work to\n * @nextp: out parameter for nested worklist walking\n *\n * Schedule linked works starting from @work to @head.  Work series to\n * be scheduled starts at @work and includes any consecutive work with\n * WORK_STRUCT_LINKED set in its predecessor.\n *\n * If @nextp is not NULL, it's updated to point to the next work of\n * the last scheduled work.  This allows move_linked_works() to be\n * nested inside outer list_for_each_entry_safe().\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void move_linked_works(struct work_struct *work, struct list_head *head,\n\t\t\t      struct work_struct **nextp)\n{\n\tstruct work_struct *n;\n\n\t/*\n\t * Linked worklist will always end before the end of the list,\n\t * use NULL for list head.\n\t */\n\tlist_for_each_entry_safe_from(work, n, NULL, entry) {\n\t\tlist_move_tail(&work->entry, head);\n\t\tif (!(*work_data_bits(work) & WORK_STRUCT_LINKED))\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * If we're already inside safe list traversal and have moved\n\t * multiple works to the scheduled queue, the next position\n\t * needs to be updated.\n\t */\n\tif (nextp)\n\t\t*nextp = n;\n}\n\n/**\n * get_pwq - get an extra reference on the specified pool_workqueue\n * @pwq: pool_workqueue to get\n *\n * Obtain an extra reference on @pwq.  The caller should guarantee that\n * @pwq has positive refcnt and be holding the matching pool->lock.\n */\nstatic void get_pwq(struct pool_workqueue *pwq)\n{\n\tlockdep_assert_held(&pwq->pool->lock);\n\tWARN_ON_ONCE(pwq->refcnt <= 0);\n\tpwq->refcnt++;\n}\n\n/**\n * put_pwq - put a pool_workqueue reference\n * @pwq: pool_workqueue to put\n *\n * Drop a reference of @pwq.  If its refcnt reaches zero, schedule its\n * destruction.  The caller should be holding the matching pool->lock.\n */\nstatic void put_pwq(struct pool_workqueue *pwq)\n{\n\tlockdep_assert_held(&pwq->pool->lock);\n\tif (likely(--pwq->refcnt))\n\t\treturn;\n\tif (WARN_ON_ONCE(!(pwq->wq->flags & WQ_UNBOUND)))\n\t\treturn;\n\t/*\n\t * @pwq can't be released under pool->lock, bounce to\n\t * pwq_unbound_release_workfn().  This never recurses on the same\n\t * pool->lock as this path is taken only for unbound workqueues and\n\t * the release work item is scheduled on a per-cpu workqueue.  To\n\t * avoid lockdep warning, unbound pool->locks are given lockdep\n\t * subclass of 1 in get_unbound_pool().\n\t */\n\tschedule_work(&pwq->unbound_release_work);\n}\n\n/**\n * put_pwq_unlocked - put_pwq() with surrounding pool lock/unlock\n * @pwq: pool_workqueue to put (can be %NULL)\n *\n * put_pwq() with locking.  This function also allows %NULL @pwq.\n */\nstatic void put_pwq_unlocked(struct pool_workqueue *pwq)\n{\n\tif (pwq) {\n\t\t/*\n\t\t * As both pwqs and pools are RCU protected, the\n\t\t * following lock operations are safe.\n\t\t */\n\t\traw_spin_lock_irq(&pwq->pool->lock);\n\t\tput_pwq(pwq);\n\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\t}\n}\n\nstatic void pwq_activate_delayed_work(struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq = get_work_pwq(work);\n\n\ttrace_workqueue_activate_work(work);\n\tif (list_empty(&pwq->pool->worklist))\n\t\tpwq->pool->watchdog_ts = jiffies;\n\tmove_linked_works(work, &pwq->pool->worklist, NULL);\n\t__clear_bit(WORK_STRUCT_DELAYED_BIT, work_data_bits(work));\n\tpwq->nr_active++;\n}\n\nstatic void pwq_activate_first_delayed(struct pool_workqueue *pwq)\n{\n\tstruct work_struct *work = list_first_entry(&pwq->delayed_works,\n\t\t\t\t\t\t    struct work_struct, entry);\n\n\tpwq_activate_delayed_work(work);\n}\n\n/**\n * pwq_dec_nr_in_flight - decrement pwq's nr_in_flight\n * @pwq: pwq of interest\n * @color: color of work which left the queue\n *\n * A work either has completed or is removed from pending queue,\n * decrement nr_in_flight of its pwq and handle workqueue flushing.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void pwq_dec_nr_in_flight(struct pool_workqueue *pwq, int color)\n{\n\t/* uncolored work items don't participate in flushing or nr_active */\n\tif (color == WORK_NO_COLOR)\n\t\tgoto out_put;\n\n\tpwq->nr_in_flight[color]--;\n\n\tpwq->nr_active--;\n\tif (!list_empty(&pwq->delayed_works)) {\n\t\t/* one down, submit a delayed one */\n\t\tif (pwq->nr_active < pwq->max_active)\n\t\t\tpwq_activate_first_delayed(pwq);\n\t}\n\n\t/* is flush in progress and are we at the flushing tip? */\n\tif (likely(pwq->flush_color != color))\n\t\tgoto out_put;\n\n\t/* are there still in-flight works? */\n\tif (pwq->nr_in_flight[color])\n\t\tgoto out_put;\n\n\t/* this pwq is done, clear flush_color */\n\tpwq->flush_color = -1;\n\n\t/*\n\t * If this was the last pwq, wake up the first flusher.  It\n\t * will handle the rest.\n\t */\n\tif (atomic_dec_and_test(&pwq->wq->nr_pwqs_to_flush))\n\t\tcomplete(&pwq->wq->first_flusher->done);\nout_put:\n\tput_pwq(pwq);\n}\n\n/**\n * try_to_grab_pending - steal work item from worklist and disable irq\n * @work: work item to steal\n * @is_dwork: @work is a delayed_work\n * @flags: place to store irq state\n *\n * Try to grab PENDING bit of @work.  This function can handle @work in any\n * stable state - idle, on timer or on worklist.\n *\n * Return:\n *\n *  ========\t================================================================\n *  1\t\tif @work was pending and we successfully stole PENDING\n *  0\t\tif @work was idle and we claimed PENDING\n *  -EAGAIN\tif PENDING couldn't be grabbed at the moment, safe to busy-retry\n *  -ENOENT\tif someone else is canceling @work, this state may persist\n *\t\tfor arbitrarily long\n *  ========\t================================================================\n *\n * Note:\n * On >= 0 return, the caller owns @work's PENDING bit.  To avoid getting\n * interrupted while holding PENDING and @work off queue, irq must be\n * disabled on entry.  This, combined with delayed_work->timer being\n * irqsafe, ensures that we return -EAGAIN for finite short period of time.\n *\n * On successful return, >= 0, irq is disabled and the caller is\n * responsible for releasing it using local_irq_restore(*@flags).\n *\n * This function is safe to call from any context including IRQ handler.\n */\nstatic int try_to_grab_pending(struct work_struct *work, bool is_dwork,\n\t\t\t       unsigned long *flags)\n{\n\tstruct worker_pool *pool;\n\tstruct pool_workqueue *pwq;\n\n\tlocal_irq_save(*flags);\n\n\t/* try to steal the timer if it exists */\n\tif (is_dwork) {\n\t\tstruct delayed_work *dwork = to_delayed_work(work);\n\n\t\t/*\n\t\t * dwork->timer is irqsafe.  If del_timer() fails, it's\n\t\t * guaranteed that the timer is not queued anywhere and not\n\t\t * running on the local CPU.\n\t\t */\n\t\tif (likely(del_timer(&dwork->timer)))\n\t\t\treturn 1;\n\t}\n\n\t/* try to claim PENDING the normal way */\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))\n\t\treturn 0;\n\n\trcu_read_lock();\n\t/*\n\t * The queueing is in progress, or it is already queued. Try to\n\t * steal it from ->worklist without clearing WORK_STRUCT_PENDING.\n\t */\n\tpool = get_work_pool(work);\n\tif (!pool)\n\t\tgoto fail;\n\n\traw_spin_lock(&pool->lock);\n\t/*\n\t * work->data is guaranteed to point to pwq only while the work\n\t * item is queued on pwq->wq, and both updating work->data to point\n\t * to pwq on queueing and to pool on dequeueing are done under\n\t * pwq->pool->lock.  This in turn guarantees that, if work->data\n\t * points to pwq which is associated with a locked pool, the work\n\t * item is currently queued on that pool.\n\t */\n\tpwq = get_work_pwq(work);\n\tif (pwq && pwq->pool == pool) {\n\t\tdebug_work_deactivate(work);\n\n\t\t/*\n\t\t * A delayed work item cannot be grabbed directly because\n\t\t * it might have linked NO_COLOR work items which, if left\n\t\t * on the delayed_list, will confuse pwq->nr_active\n\t\t * management later on and cause stall.  Make sure the work\n\t\t * item is activated before grabbing.\n\t\t */\n\t\tif (*work_data_bits(work) & WORK_STRUCT_DELAYED)\n\t\t\tpwq_activate_delayed_work(work);\n\n\t\tlist_del_init(&work->entry);\n\t\tpwq_dec_nr_in_flight(pwq, get_work_color(work));\n\n\t\t/* work->data points to pwq iff queued, point to pool */\n\t\tset_work_pool_and_keep_pending(work, pool->id);\n\n\t\traw_spin_unlock(&pool->lock);\n\t\trcu_read_unlock();\n\t\treturn 1;\n\t}\n\traw_spin_unlock(&pool->lock);\nfail:\n\trcu_read_unlock();\n\tlocal_irq_restore(*flags);\n\tif (work_is_canceling(work))\n\t\treturn -ENOENT;\n\tcpu_relax();\n\treturn -EAGAIN;\n}\n\n/**\n * insert_work - insert a work into a pool\n * @pwq: pwq @work belongs to\n * @work: work to insert\n * @head: insertion point\n * @extra_flags: extra WORK_STRUCT_* flags to set\n *\n * Insert @work which belongs to @pwq after @head.  @extra_flags is or'd to\n * work_struct flags.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void insert_work(struct pool_workqueue *pwq, struct work_struct *work,\n\t\t\tstruct list_head *head, unsigned int extra_flags)\n{\n\tstruct worker_pool *pool = pwq->pool;\n\n\t/* record the work call stack in order to print it in KASAN reports */\n\tkasan_record_aux_stack(work);\n\n\t/* we own @work, set data and link */\n\tset_work_pwq(work, pwq, extra_flags);\n\tlist_add_tail(&work->entry, head);\n\tget_pwq(pwq);\n\n\t/*\n\t * Ensure either wq_worker_sleeping() sees the above\n\t * list_add_tail() or we see zero nr_running to avoid workers lying\n\t * around lazily while there are works to be processed.\n\t */\n\tsmp_mb();\n\n\tif (__need_more_worker(pool))\n\t\twake_up_worker(pool);\n}\n\n/*\n * Test whether @work is being queued from another work executing on the\n * same workqueue.\n */\nstatic bool is_chained_work(struct workqueue_struct *wq)\n{\n\tstruct worker *worker;\n\n\tworker = current_wq_worker();\n\t/*\n\t * Return %true iff I'm a worker executing a work item on @wq.  If\n\t * I'm @worker, it's safe to dereference it without locking.\n\t */\n\treturn worker && worker->current_pwq->wq == wq;\n}\n\n/*\n * When queueing an unbound work item to a wq, prefer local CPU if allowed\n * by wq_unbound_cpumask.  Otherwise, round robin among the allowed ones to\n * avoid perturbing sensitive tasks.\n */\nstatic int wq_select_unbound_cpu(int cpu)\n{\n\tstatic bool printed_dbg_warning;\n\tint new_cpu;\n\n\tif (likely(!wq_debug_force_rr_cpu)) {\n\t\tif (cpumask_test_cpu(cpu, wq_unbound_cpumask))\n\t\t\treturn cpu;\n\t} else if (!printed_dbg_warning) {\n\t\tpr_warn(\"workqueue: round-robin CPU selection forced, expect performance impact\\n\");\n\t\tprinted_dbg_warning = true;\n\t}\n\n\tif (cpumask_empty(wq_unbound_cpumask))\n\t\treturn cpu;\n\n\tnew_cpu = __this_cpu_read(wq_rr_cpu_last);\n\tnew_cpu = cpumask_next_and(new_cpu, wq_unbound_cpumask, cpu_online_mask);\n\tif (unlikely(new_cpu >= nr_cpu_ids)) {\n\t\tnew_cpu = cpumask_first_and(wq_unbound_cpumask, cpu_online_mask);\n\t\tif (unlikely(new_cpu >= nr_cpu_ids))\n\t\t\treturn cpu;\n\t}\n\t__this_cpu_write(wq_rr_cpu_last, new_cpu);\n\n\treturn new_cpu;\n}\n\nstatic void __queue_work(int cpu, struct workqueue_struct *wq,\n\t\t\t struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq;\n\tstruct worker_pool *last_pool;\n\tstruct list_head *worklist;\n\tunsigned int work_flags;\n\tunsigned int req_cpu = cpu;\n\n\t/*\n\t * While a work item is PENDING && off queue, a task trying to\n\t * steal the PENDING will busy-loop waiting for it to either get\n\t * queued or lose PENDING.  Grabbing PENDING and queueing should\n\t * happen with IRQ disabled.\n\t */\n\tlockdep_assert_irqs_disabled();\n\n\tdebug_work_activate(work);\n\n\t/* if draining, only works from the same workqueue are allowed */\n\tif (unlikely(wq->flags & __WQ_DRAINING) &&\n\t    WARN_ON_ONCE(!is_chained_work(wq)))\n\t\treturn;\n\trcu_read_lock();\nretry:\n\t/* pwq which will be used unless @work is executing elsewhere */\n\tif (wq->flags & WQ_UNBOUND) {\n\t\tif (req_cpu == WORK_CPU_UNBOUND)\n\t\t\tcpu = wq_select_unbound_cpu(raw_smp_processor_id());\n\t\tpwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));\n\t} else {\n\t\tif (req_cpu == WORK_CPU_UNBOUND)\n\t\t\tcpu = raw_smp_processor_id();\n\t\tpwq = per_cpu_ptr(wq->cpu_pwqs, cpu);\n\t}\n\n\t/*\n\t * If @work was previously on a different pool, it might still be\n\t * running there, in which case the work needs to be queued on that\n\t * pool to guarantee non-reentrancy.\n\t */\n\tlast_pool = get_work_pool(work);\n\tif (last_pool && last_pool != pwq->pool) {\n\t\tstruct worker *worker;\n\n\t\traw_spin_lock(&last_pool->lock);\n\n\t\tworker = find_worker_executing_work(last_pool, work);\n\n\t\tif (worker && worker->current_pwq->wq == wq) {\n\t\t\tpwq = worker->current_pwq;\n\t\t} else {\n\t\t\t/* meh... not running there, queue here */\n\t\t\traw_spin_unlock(&last_pool->lock);\n\t\t\traw_spin_lock(&pwq->pool->lock);\n\t\t}\n\t} else {\n\t\traw_spin_lock(&pwq->pool->lock);\n\t}\n\n\t/*\n\t * pwq is determined and locked.  For unbound pools, we could have\n\t * raced with pwq release and it could already be dead.  If its\n\t * refcnt is zero, repeat pwq selection.  Note that pwqs never die\n\t * without another pwq replacing it in the numa_pwq_tbl or while\n\t * work items are executing on it, so the retrying is guaranteed to\n\t * make forward-progress.\n\t */\n\tif (unlikely(!pwq->refcnt)) {\n\t\tif (wq->flags & WQ_UNBOUND) {\n\t\t\traw_spin_unlock(&pwq->pool->lock);\n\t\t\tcpu_relax();\n\t\t\tgoto retry;\n\t\t}\n\t\t/* oops */\n\t\tWARN_ONCE(true, \"workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt\",\n\t\t\t  wq->name, cpu);\n\t}\n\n\t/* pwq determined, queue */\n\ttrace_workqueue_queue_work(req_cpu, pwq, work);\n\n\tif (WARN_ON(!list_empty(&work->entry)))\n\t\tgoto out;\n\n\tpwq->nr_in_flight[pwq->work_color]++;\n\twork_flags = work_color_to_flags(pwq->work_color);\n\n\tif (likely(pwq->nr_active < pwq->max_active)) {\n\t\ttrace_workqueue_activate_work(work);\n\t\tpwq->nr_active++;\n\t\tworklist = &pwq->pool->worklist;\n\t\tif (list_empty(worklist))\n\t\t\tpwq->pool->watchdog_ts = jiffies;\n\t} else {\n\t\twork_flags |= WORK_STRUCT_DELAYED;\n\t\tworklist = &pwq->delayed_works;\n\t}\n\n\tinsert_work(pwq, work, worklist, work_flags);\n\nout:\n\traw_spin_unlock(&pwq->pool->lock);\n\trcu_read_unlock();\n}\n\n/**\n * queue_work_on - queue work on specific cpu\n * @cpu: CPU number to execute work on\n * @wq: workqueue to use\n * @work: work to queue\n *\n * We queue the work to a specific CPU, the caller must ensure it\n * can't go away.\n *\n * Return: %false if @work was already on a queue, %true otherwise.\n */\nbool queue_work_on(int cpu, struct workqueue_struct *wq,\n\t\t   struct work_struct *work)\n{\n\tbool ret = false;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\t__queue_work(cpu, wq, work);\n\t\tret = true;\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(queue_work_on);\n\n/**\n * workqueue_select_cpu_near - Select a CPU based on NUMA node\n * @node: NUMA node ID that we want to select a CPU from\n *\n * This function will attempt to find a \"random\" cpu available on a given\n * node. If there are no CPUs available on the given node it will return\n * WORK_CPU_UNBOUND indicating that we should just schedule to any\n * available CPU if we need to schedule this work.\n */\nstatic int workqueue_select_cpu_near(int node)\n{\n\tint cpu;\n\n\t/* No point in doing this if NUMA isn't enabled for workqueues */\n\tif (!wq_numa_enabled)\n\t\treturn WORK_CPU_UNBOUND;\n\n\t/* Delay binding to CPU if node is not valid or online */\n\tif (node < 0 || node >= MAX_NUMNODES || !node_online(node))\n\t\treturn WORK_CPU_UNBOUND;\n\n\t/* Use local node/cpu if we are already there */\n\tcpu = raw_smp_processor_id();\n\tif (node == cpu_to_node(cpu))\n\t\treturn cpu;\n\n\t/* Use \"random\" otherwise know as \"first\" online CPU of node */\n\tcpu = cpumask_any_and(cpumask_of_node(node), cpu_online_mask);\n\n\t/* If CPU is valid return that, otherwise just defer */\n\treturn cpu < nr_cpu_ids ? cpu : WORK_CPU_UNBOUND;\n}\n\n/**\n * queue_work_node - queue work on a \"random\" cpu for a given NUMA node\n * @node: NUMA node that we are targeting the work for\n * @wq: workqueue to use\n * @work: work to queue\n *\n * We queue the work to a \"random\" CPU within a given NUMA node. The basic\n * idea here is to provide a way to somehow associate work with a given\n * NUMA node.\n *\n * This function will only make a best effort attempt at getting this onto\n * the right NUMA node. If no node is requested or the requested node is\n * offline then we just fall back to standard queue_work behavior.\n *\n * Currently the \"random\" CPU ends up being the first available CPU in the\n * intersection of cpu_online_mask and the cpumask of the node, unless we\n * are running on the node. In that case we just use the current CPU.\n *\n * Return: %false if @work was already on a queue, %true otherwise.\n */\nbool queue_work_node(int node, struct workqueue_struct *wq,\n\t\t     struct work_struct *work)\n{\n\tunsigned long flags;\n\tbool ret = false;\n\n\t/*\n\t * This current implementation is specific to unbound workqueues.\n\t * Specifically we only return the first available CPU for a given\n\t * node instead of cycling through individual CPUs within the node.\n\t *\n\t * If this is used with a per-cpu workqueue then the logic in\n\t * workqueue_select_cpu_near would need to be updated to allow for\n\t * some round robin type logic.\n\t */\n\tWARN_ON_ONCE(!(wq->flags & WQ_UNBOUND));\n\n\tlocal_irq_save(flags);\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\tint cpu = workqueue_select_cpu_near(node);\n\n\t\t__queue_work(cpu, wq, work);\n\t\tret = true;\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(queue_work_node);\n\nvoid delayed_work_timer_fn(struct timer_list *t)\n{\n\tstruct delayed_work *dwork = from_timer(dwork, t, timer);\n\n\t/* should have been called from irqsafe timer with irq already off */\n\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);\n}\nEXPORT_SYMBOL(delayed_work_timer_fn);\n\nstatic void __queue_delayed_work(int cpu, struct workqueue_struct *wq,\n\t\t\t\tstruct delayed_work *dwork, unsigned long delay)\n{\n\tstruct timer_list *timer = &dwork->timer;\n\tstruct work_struct *work = &dwork->work;\n\n\tWARN_ON_ONCE(!wq);\n\tWARN_ON_ONCE(timer->function != delayed_work_timer_fn);\n\tWARN_ON_ONCE(timer_pending(timer));\n\tWARN_ON_ONCE(!list_empty(&work->entry));\n\n\t/*\n\t * If @delay is 0, queue @dwork->work immediately.  This is for\n\t * both optimization and correctness.  The earliest @timer can\n\t * expire is on the closest next tick and delayed_work users depend\n\t * on that there's no such delay when @delay is 0.\n\t */\n\tif (!delay) {\n\t\t__queue_work(cpu, wq, &dwork->work);\n\t\treturn;\n\t}\n\n\tdwork->wq = wq;\n\tdwork->cpu = cpu;\n\ttimer->expires = jiffies + delay;\n\n\tif (unlikely(cpu != WORK_CPU_UNBOUND))\n\t\tadd_timer_on(timer, cpu);\n\telse\n\t\tadd_timer(timer);\n}\n\n/**\n * queue_delayed_work_on - queue work on specific CPU after delay\n * @cpu: CPU number to execute work on\n * @wq: workqueue to use\n * @dwork: work to queue\n * @delay: number of jiffies to wait before queueing\n *\n * Return: %false if @work was already on a queue, %true otherwise.  If\n * @delay is zero and @dwork is idle, it will be scheduled for immediate\n * execution.\n */\nbool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\t   struct delayed_work *dwork, unsigned long delay)\n{\n\tstruct work_struct *work = &dwork->work;\n\tbool ret = false;\n\tunsigned long flags;\n\n\t/* read the comment in __queue_work() */\n\tlocal_irq_save(flags);\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\t__queue_delayed_work(cpu, wq, dwork, delay);\n\t\tret = true;\n\t}\n\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\nEXPORT_SYMBOL(queue_delayed_work_on);\n\n/**\n * mod_delayed_work_on - modify delay of or queue a delayed work on specific CPU\n * @cpu: CPU number to execute work on\n * @wq: workqueue to use\n * @dwork: work to queue\n * @delay: number of jiffies to wait before queueing\n *\n * If @dwork is idle, equivalent to queue_delayed_work_on(); otherwise,\n * modify @dwork's timer so that it expires after @delay.  If @delay is\n * zero, @work is guaranteed to be scheduled immediately regardless of its\n * current state.\n *\n * Return: %false if @dwork was idle and queued, %true if @dwork was\n * pending and its timer was modified.\n *\n * This function is safe to call from any context including IRQ handler.\n * See try_to_grab_pending() for details.\n */\nbool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\t struct delayed_work *dwork, unsigned long delay)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tret = try_to_grab_pending(&dwork->work, true, &flags);\n\t} while (unlikely(ret == -EAGAIN));\n\n\tif (likely(ret >= 0)) {\n\t\t__queue_delayed_work(cpu, wq, dwork, delay);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\t/* -ENOENT from try_to_grab_pending() becomes %true */\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mod_delayed_work_on);\n\nstatic void rcu_work_rcufn(struct rcu_head *rcu)\n{\n\tstruct rcu_work *rwork = container_of(rcu, struct rcu_work, rcu);\n\n\t/* read the comment in __queue_work() */\n\tlocal_irq_disable();\n\t__queue_work(WORK_CPU_UNBOUND, rwork->wq, &rwork->work);\n\tlocal_irq_enable();\n}\n\n/**\n * queue_rcu_work - queue work after a RCU grace period\n * @wq: workqueue to use\n * @rwork: work to queue\n *\n * Return: %false if @rwork was already pending, %true otherwise.  Note\n * that a full RCU grace period is guaranteed only after a %true return.\n * While @rwork is guaranteed to be executed after a %false return, the\n * execution may happen before a full RCU grace period has passed.\n */\nbool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork)\n{\n\tstruct work_struct *work = &rwork->work;\n\n\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {\n\t\trwork->wq = wq;\n\t\tcall_rcu(&rwork->rcu, rcu_work_rcufn);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL(queue_rcu_work);\n\n/**\n * worker_enter_idle - enter idle state\n * @worker: worker which is entering idle state\n *\n * @worker is entering idle state.  Update stats and idle timer if\n * necessary.\n *\n * LOCKING:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void worker_enter_idle(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||\n\t    WARN_ON_ONCE(!list_empty(&worker->entry) &&\n\t\t\t (worker->hentry.next || worker->hentry.pprev)))\n\t\treturn;\n\n\t/* can't use worker_set_flags(), also called from create_worker() */\n\tworker->flags |= WORKER_IDLE;\n\tpool->nr_idle++;\n\tworker->last_active = jiffies;\n\n\t/* idle_list is LIFO */\n\tlist_add(&worker->entry, &pool->idle_list);\n\n\tif (too_many_workers(pool) && !timer_pending(&pool->idle_timer))\n\t\tmod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);\n\n\t/*\n\t * Sanity check nr_running.  Because unbind_workers() releases\n\t * pool->lock between setting %WORKER_UNBOUND and zapping\n\t * nr_running, the warning may trigger spuriously.  Check iff\n\t * unbind is not in progress.\n\t */\n\tWARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&\n\t\t     pool->nr_workers == pool->nr_idle &&\n\t\t     atomic_read(&pool->nr_running));\n}\n\n/**\n * worker_leave_idle - leave idle state\n * @worker: worker which is leaving idle state\n *\n * @worker is leaving idle state.  Update stats.\n *\n * LOCKING:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void worker_leave_idle(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (WARN_ON_ONCE(!(worker->flags & WORKER_IDLE)))\n\t\treturn;\n\tworker_clr_flags(worker, WORKER_IDLE);\n\tpool->nr_idle--;\n\tlist_del_init(&worker->entry);\n}\n\nstatic struct worker *alloc_worker(int node)\n{\n\tstruct worker *worker;\n\n\tworker = kzalloc_node(sizeof(*worker), GFP_KERNEL, node);\n\tif (worker) {\n\t\tINIT_LIST_HEAD(&worker->entry);\n\t\tINIT_LIST_HEAD(&worker->scheduled);\n\t\tINIT_LIST_HEAD(&worker->node);\n\t\t/* on creation a worker is in !idle && prep state */\n\t\tworker->flags = WORKER_PREP;\n\t}\n\treturn worker;\n}\n\n/**\n * worker_attach_to_pool() - attach a worker to a pool\n * @worker: worker to be attached\n * @pool: the target pool\n *\n * Attach @worker to @pool.  Once attached, the %WORKER_UNBOUND flag and\n * cpu-binding of @worker are kept coordinated with the pool across\n * cpu-[un]hotplugs.\n */\nstatic void worker_attach_to_pool(struct worker *worker,\n\t\t\t\t   struct worker_pool *pool)\n{\n\tmutex_lock(&wq_pool_attach_mutex);\n\n\t/*\n\t * set_cpus_allowed_ptr() will fail if the cpumask doesn't have any\n\t * online CPUs.  It'll be re-applied when any of the CPUs come up.\n\t */\n\tset_cpus_allowed_ptr(worker->task, pool->attrs->cpumask);\n\n\t/*\n\t * The wq_pool_attach_mutex ensures %POOL_DISASSOCIATED remains\n\t * stable across this function.  See the comments above the flag\n\t * definition for details.\n\t */\n\tif (pool->flags & POOL_DISASSOCIATED)\n\t\tworker->flags |= WORKER_UNBOUND;\n\n\tlist_add_tail(&worker->node, &pool->workers);\n\tworker->pool = pool;\n\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\n/**\n * worker_detach_from_pool() - detach a worker from its pool\n * @worker: worker which is attached to its pool\n *\n * Undo the attaching which had been done in worker_attach_to_pool().  The\n * caller worker shouldn't access to the pool after detached except it has\n * other reference to the pool.\n */\nstatic void worker_detach_from_pool(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\tstruct completion *detach_completion = NULL;\n\n\tmutex_lock(&wq_pool_attach_mutex);\n\n\tlist_del(&worker->node);\n\tworker->pool = NULL;\n\n\tif (list_empty(&pool->workers))\n\t\tdetach_completion = pool->detach_completion;\n\tmutex_unlock(&wq_pool_attach_mutex);\n\n\t/* clear leftover flags without pool->lock after it is detached */\n\tworker->flags &= ~(WORKER_UNBOUND | WORKER_REBOUND);\n\n\tif (detach_completion)\n\t\tcomplete(detach_completion);\n}\n\n/**\n * create_worker - create a new workqueue worker\n * @pool: pool the new worker will belong to\n *\n * Create and start a new worker which is attached to @pool.\n *\n * CONTEXT:\n * Might sleep.  Does GFP_KERNEL allocations.\n *\n * Return:\n * Pointer to the newly created worker.\n */\nstatic struct worker *create_worker(struct worker_pool *pool)\n{\n\tstruct worker *worker = NULL;\n\tint id = -1;\n\tchar id_buf[16];\n\n\t/* ID is needed to determine kthread name */\n\tid = ida_simple_get(&pool->worker_ida, 0, 0, GFP_KERNEL);\n\tif (id < 0)\n\t\tgoto fail;\n\n\tworker = alloc_worker(pool->node);\n\tif (!worker)\n\t\tgoto fail;\n\n\tworker->id = id;\n\n\tif (pool->cpu >= 0)\n\t\tsnprintf(id_buf, sizeof(id_buf), \"%d:%d%s\", pool->cpu, id,\n\t\t\t pool->attrs->nice < 0  ? \"H\" : \"\");\n\telse\n\t\tsnprintf(id_buf, sizeof(id_buf), \"u%d:%d\", pool->id, id);\n\n\tworker->task = kthread_create_on_node(worker_thread, worker, pool->node,\n\t\t\t\t\t      \"kworker/%s\", id_buf);\n\tif (IS_ERR(worker->task))\n\t\tgoto fail;\n\n\tset_user_nice(worker->task, pool->attrs->nice);\n\tkthread_bind_mask(worker->task, pool->attrs->cpumask);\n\n\t/* successful, attach the worker to the pool */\n\tworker_attach_to_pool(worker, pool);\n\n\t/* start the newly created worker */\n\traw_spin_lock_irq(&pool->lock);\n\tworker->pool->nr_workers++;\n\tworker_enter_idle(worker);\n\twake_up_process(worker->task);\n\traw_spin_unlock_irq(&pool->lock);\n\n\treturn worker;\n\nfail:\n\tif (id >= 0)\n\t\tida_simple_remove(&pool->worker_ida, id);\n\tkfree(worker);\n\treturn NULL;\n}\n\n/**\n * destroy_worker - destroy a workqueue worker\n * @worker: worker to be destroyed\n *\n * Destroy @worker and adjust @pool stats accordingly.  The worker should\n * be idle.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void destroy_worker(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tlockdep_assert_held(&pool->lock);\n\n\t/* sanity check frenzy */\n\tif (WARN_ON(worker->current_work) ||\n\t    WARN_ON(!list_empty(&worker->scheduled)) ||\n\t    WARN_ON(!(worker->flags & WORKER_IDLE)))\n\t\treturn;\n\n\tpool->nr_workers--;\n\tpool->nr_idle--;\n\n\tlist_del_init(&worker->entry);\n\tworker->flags |= WORKER_DIE;\n\twake_up_process(worker->task);\n}\n\nstatic void idle_worker_timeout(struct timer_list *t)\n{\n\tstruct worker_pool *pool = from_timer(pool, t, idle_timer);\n\n\traw_spin_lock_irq(&pool->lock);\n\n\twhile (too_many_workers(pool)) {\n\t\tstruct worker *worker;\n\t\tunsigned long expires;\n\n\t\t/* idle_list is kept in LIFO order, check the last one */\n\t\tworker = list_entry(pool->idle_list.prev, struct worker, entry);\n\t\texpires = worker->last_active + IDLE_WORKER_TIMEOUT;\n\n\t\tif (time_before(jiffies, expires)) {\n\t\t\tmod_timer(&pool->idle_timer, expires);\n\t\t\tbreak;\n\t\t}\n\n\t\tdestroy_worker(worker);\n\t}\n\n\traw_spin_unlock_irq(&pool->lock);\n}\n\nstatic void send_mayday(struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq = get_work_pwq(work);\n\tstruct workqueue_struct *wq = pwq->wq;\n\n\tlockdep_assert_held(&wq_mayday_lock);\n\n\tif (!wq->rescuer)\n\t\treturn;\n\n\t/* mayday mayday mayday */\n\tif (list_empty(&pwq->mayday_node)) {\n\t\t/*\n\t\t * If @pwq is for an unbound wq, its base ref may be put at\n\t\t * any time due to an attribute change.  Pin @pwq until the\n\t\t * rescuer is done with it.\n\t\t */\n\t\tget_pwq(pwq);\n\t\tlist_add_tail(&pwq->mayday_node, &wq->maydays);\n\t\twake_up_process(wq->rescuer->task);\n\t}\n}\n\nstatic void pool_mayday_timeout(struct timer_list *t)\n{\n\tstruct worker_pool *pool = from_timer(pool, t, mayday_timer);\n\tstruct work_struct *work;\n\n\traw_spin_lock_irq(&pool->lock);\n\traw_spin_lock(&wq_mayday_lock);\t\t/* for wq->maydays */\n\n\tif (need_to_create_worker(pool)) {\n\t\t/*\n\t\t * We've been trying to create a new worker but\n\t\t * haven't been successful.  We might be hitting an\n\t\t * allocation deadlock.  Send distress signals to\n\t\t * rescuers.\n\t\t */\n\t\tlist_for_each_entry(work, &pool->worklist, entry)\n\t\t\tsend_mayday(work);\n\t}\n\n\traw_spin_unlock(&wq_mayday_lock);\n\traw_spin_unlock_irq(&pool->lock);\n\n\tmod_timer(&pool->mayday_timer, jiffies + MAYDAY_INTERVAL);\n}\n\n/**\n * maybe_create_worker - create a new worker if necessary\n * @pool: pool to create a new worker for\n *\n * Create a new worker for @pool if necessary.  @pool is guaranteed to\n * have at least one idle worker on return from this function.  If\n * creating a new worker takes longer than MAYDAY_INTERVAL, mayday is\n * sent to all rescuers with works scheduled on @pool to resolve\n * possible allocation deadlock.\n *\n * On return, need_to_create_worker() is guaranteed to be %false and\n * may_start_working() %true.\n *\n * LOCKING:\n * raw_spin_lock_irq(pool->lock) which may be released and regrabbed\n * multiple times.  Does GFP_KERNEL allocations.  Called only from\n * manager.\n */\nstatic void maybe_create_worker(struct worker_pool *pool)\n__releases(&pool->lock)\n__acquires(&pool->lock)\n{\nrestart:\n\traw_spin_unlock_irq(&pool->lock);\n\n\t/* if we don't make progress in MAYDAY_INITIAL_TIMEOUT, call for help */\n\tmod_timer(&pool->mayday_timer, jiffies + MAYDAY_INITIAL_TIMEOUT);\n\n\twhile (true) {\n\t\tif (create_worker(pool) || !need_to_create_worker(pool))\n\t\t\tbreak;\n\n\t\tschedule_timeout_interruptible(CREATE_COOLDOWN);\n\n\t\tif (!need_to_create_worker(pool))\n\t\t\tbreak;\n\t}\n\n\tdel_timer_sync(&pool->mayday_timer);\n\traw_spin_lock_irq(&pool->lock);\n\t/*\n\t * This is necessary even after a new worker was just successfully\n\t * created as @pool->lock was dropped and the new worker might have\n\t * already become busy.\n\t */\n\tif (need_to_create_worker(pool))\n\t\tgoto restart;\n}\n\n/**\n * manage_workers - manage worker pool\n * @worker: self\n *\n * Assume the manager role and manage the worker pool @worker belongs\n * to.  At any given time, there can be only zero or one manager per\n * pool.  The exclusion is handled automatically by this function.\n *\n * The caller can safely start processing works on false return.  On\n * true return, it's guaranteed that need_to_create_worker() is false\n * and may_start_working() is true.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock) which may be released and regrabbed\n * multiple times.  Does GFP_KERNEL allocations.\n *\n * Return:\n * %false if the pool doesn't need management and the caller can safely\n * start processing works, %true if management function was performed and\n * the conditions that the caller verified before calling the function may\n * no longer be true.\n */\nstatic bool manage_workers(struct worker *worker)\n{\n\tstruct worker_pool *pool = worker->pool;\n\n\tif (pool->flags & POOL_MANAGER_ACTIVE)\n\t\treturn false;\n\n\tpool->flags |= POOL_MANAGER_ACTIVE;\n\tpool->manager = worker;\n\n\tmaybe_create_worker(pool);\n\n\tpool->manager = NULL;\n\tpool->flags &= ~POOL_MANAGER_ACTIVE;\n\trcuwait_wake_up(&manager_wait);\n\treturn true;\n}\n\n/**\n * process_one_work - process single work\n * @worker: self\n * @work: work to process\n *\n * Process @work.  This function contains all the logics necessary to\n * process a single work including synchronization against and\n * interaction with other workers on the same cpu, queueing and\n * flushing.  As long as context requirement is met, any worker can\n * call this function to process a work.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock) which is released and regrabbed.\n */\nstatic void process_one_work(struct worker *worker, struct work_struct *work)\n__releases(&pool->lock)\n__acquires(&pool->lock)\n{\n\tstruct pool_workqueue *pwq = get_work_pwq(work);\n\tstruct worker_pool *pool = worker->pool;\n\tbool cpu_intensive = pwq->wq->flags & WQ_CPU_INTENSIVE;\n\tint work_color;\n\tstruct worker *collision;\n#ifdef CONFIG_LOCKDEP\n\t/*\n\t * It is permissible to free the struct work_struct from\n\t * inside the function that is called from it, this we need to\n\t * take into account for lockdep too.  To avoid bogus \"held\n\t * lock freed\" warnings as well as problems when looking into\n\t * work->lockdep_map, make a copy and use that here.\n\t */\n\tstruct lockdep_map lockdep_map;\n\n\tlockdep_copy_map(&lockdep_map, &work->lockdep_map);\n#endif\n\t/* ensure we're on the correct CPU */\n\tWARN_ON_ONCE(!(pool->flags & POOL_DISASSOCIATED) &&\n\t\t     raw_smp_processor_id() != pool->cpu);\n\n\t/*\n\t * A single work shouldn't be executed concurrently by\n\t * multiple workers on a single cpu.  Check whether anyone is\n\t * already processing the work.  If so, defer the work to the\n\t * currently executing one.\n\t */\n\tcollision = find_worker_executing_work(pool, work);\n\tif (unlikely(collision)) {\n\t\tmove_linked_works(work, &collision->scheduled, NULL);\n\t\treturn;\n\t}\n\n\t/* claim and dequeue */\n\tdebug_work_deactivate(work);\n\thash_add(pool->busy_hash, &worker->hentry, (unsigned long)work);\n\tworker->current_work = work;\n\tworker->current_func = work->func;\n\tworker->current_pwq = pwq;\n\twork_color = get_work_color(work);\n\n\t/*\n\t * Record wq name for cmdline and debug reporting, may get\n\t * overridden through set_worker_desc().\n\t */\n\tstrscpy(worker->desc, pwq->wq->name, WORKER_DESC_LEN);\n\n\tlist_del_init(&work->entry);\n\n\t/*\n\t * CPU intensive works don't participate in concurrency management.\n\t * They're the scheduler's responsibility.  This takes @worker out\n\t * of concurrency management and the next code block will chain\n\t * execution of the pending work items.\n\t */\n\tif (unlikely(cpu_intensive))\n\t\tworker_set_flags(worker, WORKER_CPU_INTENSIVE);\n\n\t/*\n\t * Wake up another worker if necessary.  The condition is always\n\t * false for normal per-cpu workers since nr_running would always\n\t * be >= 1 at this point.  This is used to chain execution of the\n\t * pending work items for WORKER_NOT_RUNNING workers such as the\n\t * UNBOUND and CPU_INTENSIVE ones.\n\t */\n\tif (need_more_worker(pool))\n\t\twake_up_worker(pool);\n\n\t/*\n\t * Record the last pool and clear PENDING which should be the last\n\t * update to @work.  Also, do this inside @pool->lock so that\n\t * PENDING and queued state changes happen together while IRQ is\n\t * disabled.\n\t */\n\tset_work_pool_and_clear_pending(work, pool->id);\n\n\traw_spin_unlock_irq(&pool->lock);\n\n\tlock_map_acquire(&pwq->wq->lockdep_map);\n\tlock_map_acquire(&lockdep_map);\n\t/*\n\t * Strictly speaking we should mark the invariant state without holding\n\t * any locks, that is, before these two lock_map_acquire()'s.\n\t *\n\t * However, that would result in:\n\t *\n\t *   A(W1)\n\t *   WFC(C)\n\t *\t\tA(W1)\n\t *\t\tC(C)\n\t *\n\t * Which would create W1->C->W1 dependencies, even though there is no\n\t * actual deadlock possible. There are two solutions, using a\n\t * read-recursive acquire on the work(queue) 'locks', but this will then\n\t * hit the lockdep limitation on recursive locks, or simply discard\n\t * these locks.\n\t *\n\t * AFAICT there is no possible deadlock scenario between the\n\t * flush_work() and complete() primitives (except for single-threaded\n\t * workqueues), so hiding them isn't a problem.\n\t */\n\tlockdep_invariant_state(true);\n\ttrace_workqueue_execute_start(work);\n\tworker->current_func(work);\n\t/*\n\t * While we must be careful to not use \"work\" after this, the trace\n\t * point will only record its address.\n\t */\n\ttrace_workqueue_execute_end(work, worker->current_func);\n\tlock_map_release(&lockdep_map);\n\tlock_map_release(&pwq->wq->lockdep_map);\n\n\tif (unlikely(in_atomic() || lockdep_depth(current) > 0)) {\n\t\tpr_err(\"BUG: workqueue leaked lock or atomic: %s/0x%08x/%d\\n\"\n\t\t       \"     last function: %ps\\n\",\n\t\t       current->comm, preempt_count(), task_pid_nr(current),\n\t\t       worker->current_func);\n\t\tdebug_show_held_locks(current);\n\t\tdump_stack();\n\t}\n\n\t/*\n\t * The following prevents a kworker from hogging CPU on !PREEMPTION\n\t * kernels, where a requeueing work item waiting for something to\n\t * happen could deadlock with stop_machine as such work item could\n\t * indefinitely requeue itself while all other CPUs are trapped in\n\t * stop_machine. At the same time, report a quiescent RCU state so\n\t * the same condition doesn't freeze RCU.\n\t */\n\tcond_resched();\n\n\traw_spin_lock_irq(&pool->lock);\n\n\t/* clear cpu intensive status */\n\tif (unlikely(cpu_intensive))\n\t\tworker_clr_flags(worker, WORKER_CPU_INTENSIVE);\n\n\t/* tag the worker for identification in schedule() */\n\tworker->last_func = worker->current_func;\n\n\t/* we're done with it, release */\n\thash_del(&worker->hentry);\n\tworker->current_work = NULL;\n\tworker->current_func = NULL;\n\tworker->current_pwq = NULL;\n\tpwq_dec_nr_in_flight(pwq, work_color);\n}\n\n/**\n * process_scheduled_works - process scheduled works\n * @worker: self\n *\n * Process all scheduled works.  Please note that the scheduled list\n * may change while processing a work, so this function repeatedly\n * fetches a work from the top and executes it.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock) which may be released and regrabbed\n * multiple times.\n */\nstatic void process_scheduled_works(struct worker *worker)\n{\n\twhile (!list_empty(&worker->scheduled)) {\n\t\tstruct work_struct *work = list_first_entry(&worker->scheduled,\n\t\t\t\t\t\tstruct work_struct, entry);\n\t\tprocess_one_work(worker, work);\n\t}\n}\n\nstatic void set_pf_worker(bool val)\n{\n\tmutex_lock(&wq_pool_attach_mutex);\n\tif (val)\n\t\tcurrent->flags |= PF_WQ_WORKER;\n\telse\n\t\tcurrent->flags &= ~PF_WQ_WORKER;\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\n/**\n * worker_thread - the worker thread function\n * @__worker: self\n *\n * The worker thread function.  All workers belong to a worker_pool -\n * either a per-cpu one or dynamic unbound one.  These workers process all\n * work items regardless of their specific target workqueue.  The only\n * exception is work items which belong to workqueues with a rescuer which\n * will be explained in rescuer_thread().\n *\n * Return: 0\n */\nstatic int worker_thread(void *__worker)\n{\n\tstruct worker *worker = __worker;\n\tstruct worker_pool *pool = worker->pool;\n\n\t/* tell the scheduler that this is a workqueue worker */\n\tset_pf_worker(true);\nwoke_up:\n\traw_spin_lock_irq(&pool->lock);\n\n\t/* am I supposed to die? */\n\tif (unlikely(worker->flags & WORKER_DIE)) {\n\t\traw_spin_unlock_irq(&pool->lock);\n\t\tWARN_ON_ONCE(!list_empty(&worker->entry));\n\t\tset_pf_worker(false);\n\n\t\tset_task_comm(worker->task, \"kworker/dying\");\n\t\tida_simple_remove(&pool->worker_ida, worker->id);\n\t\tworker_detach_from_pool(worker);\n\t\tkfree(worker);\n\t\treturn 0;\n\t}\n\n\tworker_leave_idle(worker);\nrecheck:\n\t/* no more worker necessary? */\n\tif (!need_more_worker(pool))\n\t\tgoto sleep;\n\n\t/* do we need to manage? */\n\tif (unlikely(!may_start_working(pool)) && manage_workers(worker))\n\t\tgoto recheck;\n\n\t/*\n\t * ->scheduled list can only be filled while a worker is\n\t * preparing to process a work or actually processing it.\n\t * Make sure nobody diddled with it while I was sleeping.\n\t */\n\tWARN_ON_ONCE(!list_empty(&worker->scheduled));\n\n\t/*\n\t * Finish PREP stage.  We're guaranteed to have at least one idle\n\t * worker or that someone else has already assumed the manager\n\t * role.  This is where @worker starts participating in concurrency\n\t * management if applicable and concurrency management is restored\n\t * after being rebound.  See rebind_workers() for details.\n\t */\n\tworker_clr_flags(worker, WORKER_PREP | WORKER_REBOUND);\n\n\tdo {\n\t\tstruct work_struct *work =\n\t\t\tlist_first_entry(&pool->worklist,\n\t\t\t\t\t struct work_struct, entry);\n\n\t\tpool->watchdog_ts = jiffies;\n\n\t\tif (likely(!(*work_data_bits(work) & WORK_STRUCT_LINKED))) {\n\t\t\t/* optimization path, not strictly necessary */\n\t\t\tprocess_one_work(worker, work);\n\t\t\tif (unlikely(!list_empty(&worker->scheduled)))\n\t\t\t\tprocess_scheduled_works(worker);\n\t\t} else {\n\t\t\tmove_linked_works(work, &worker->scheduled, NULL);\n\t\t\tprocess_scheduled_works(worker);\n\t\t}\n\t} while (keep_working(pool));\n\n\tworker_set_flags(worker, WORKER_PREP);\nsleep:\n\t/*\n\t * pool->lock is held and there's no work to process and no need to\n\t * manage, sleep.  Workers are woken up only while holding\n\t * pool->lock or from local cpu, so setting the current state\n\t * before releasing pool->lock is enough to prevent losing any\n\t * event.\n\t */\n\tworker_enter_idle(worker);\n\t__set_current_state(TASK_IDLE);\n\traw_spin_unlock_irq(&pool->lock);\n\tschedule();\n\tgoto woke_up;\n}\n\n/**\n * rescuer_thread - the rescuer thread function\n * @__rescuer: self\n *\n * Workqueue rescuer thread function.  There's one rescuer for each\n * workqueue which has WQ_MEM_RECLAIM set.\n *\n * Regular work processing on a pool may block trying to create a new\n * worker which uses GFP_KERNEL allocation which has slight chance of\n * developing into deadlock if some works currently on the same queue\n * need to be processed to satisfy the GFP_KERNEL allocation.  This is\n * the problem rescuer solves.\n *\n * When such condition is possible, the pool summons rescuers of all\n * workqueues which have works queued on the pool and let them process\n * those works so that forward progress can be guaranteed.\n *\n * This should happen rarely.\n *\n * Return: 0\n */\nstatic int rescuer_thread(void *__rescuer)\n{\n\tstruct worker *rescuer = __rescuer;\n\tstruct workqueue_struct *wq = rescuer->rescue_wq;\n\tstruct list_head *scheduled = &rescuer->scheduled;\n\tbool should_stop;\n\n\tset_user_nice(current, RESCUER_NICE_LEVEL);\n\n\t/*\n\t * Mark rescuer as worker too.  As WORKER_PREP is never cleared, it\n\t * doesn't participate in concurrency management.\n\t */\n\tset_pf_worker(true);\nrepeat:\n\tset_current_state(TASK_IDLE);\n\n\t/*\n\t * By the time the rescuer is requested to stop, the workqueue\n\t * shouldn't have any work pending, but @wq->maydays may still have\n\t * pwq(s) queued.  This can happen by non-rescuer workers consuming\n\t * all the work items before the rescuer got to them.  Go through\n\t * @wq->maydays processing before acting on should_stop so that the\n\t * list is always empty on exit.\n\t */\n\tshould_stop = kthread_should_stop();\n\n\t/* see whether any pwq is asking for help */\n\traw_spin_lock_irq(&wq_mayday_lock);\n\n\twhile (!list_empty(&wq->maydays)) {\n\t\tstruct pool_workqueue *pwq = list_first_entry(&wq->maydays,\n\t\t\t\t\tstruct pool_workqueue, mayday_node);\n\t\tstruct worker_pool *pool = pwq->pool;\n\t\tstruct work_struct *work, *n;\n\t\tbool first = true;\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tlist_del_init(&pwq->mayday_node);\n\n\t\traw_spin_unlock_irq(&wq_mayday_lock);\n\n\t\tworker_attach_to_pool(rescuer, pool);\n\n\t\traw_spin_lock_irq(&pool->lock);\n\n\t\t/*\n\t\t * Slurp in all works issued via this workqueue and\n\t\t * process'em.\n\t\t */\n\t\tWARN_ON_ONCE(!list_empty(scheduled));\n\t\tlist_for_each_entry_safe(work, n, &pool->worklist, entry) {\n\t\t\tif (get_work_pwq(work) == pwq) {\n\t\t\t\tif (first)\n\t\t\t\t\tpool->watchdog_ts = jiffies;\n\t\t\t\tmove_linked_works(work, scheduled, &n);\n\t\t\t}\n\t\t\tfirst = false;\n\t\t}\n\n\t\tif (!list_empty(scheduled)) {\n\t\t\tprocess_scheduled_works(rescuer);\n\n\t\t\t/*\n\t\t\t * The above execution of rescued work items could\n\t\t\t * have created more to rescue through\n\t\t\t * pwq_activate_first_delayed() or chained\n\t\t\t * queueing.  Let's put @pwq back on mayday list so\n\t\t\t * that such back-to-back work items, which may be\n\t\t\t * being used to relieve memory pressure, don't\n\t\t\t * incur MAYDAY_INTERVAL delay inbetween.\n\t\t\t */\n\t\t\tif (pwq->nr_active && need_to_create_worker(pool)) {\n\t\t\t\traw_spin_lock(&wq_mayday_lock);\n\t\t\t\t/*\n\t\t\t\t * Queue iff we aren't racing destruction\n\t\t\t\t * and somebody else hasn't queued it already.\n\t\t\t\t */\n\t\t\t\tif (wq->rescuer && list_empty(&pwq->mayday_node)) {\n\t\t\t\t\tget_pwq(pwq);\n\t\t\t\t\tlist_add_tail(&pwq->mayday_node, &wq->maydays);\n\t\t\t\t}\n\t\t\t\traw_spin_unlock(&wq_mayday_lock);\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Put the reference grabbed by send_mayday().  @pool won't\n\t\t * go away while we're still attached to it.\n\t\t */\n\t\tput_pwq(pwq);\n\n\t\t/*\n\t\t * Leave this pool.  If need_more_worker() is %true, notify a\n\t\t * regular worker; otherwise, we end up with 0 concurrency\n\t\t * and stalling the execution.\n\t\t */\n\t\tif (need_more_worker(pool))\n\t\t\twake_up_worker(pool);\n\n\t\traw_spin_unlock_irq(&pool->lock);\n\n\t\tworker_detach_from_pool(rescuer);\n\n\t\traw_spin_lock_irq(&wq_mayday_lock);\n\t}\n\n\traw_spin_unlock_irq(&wq_mayday_lock);\n\n\tif (should_stop) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tset_pf_worker(false);\n\t\treturn 0;\n\t}\n\n\t/* rescuers should never participate in concurrency management */\n\tWARN_ON_ONCE(!(rescuer->flags & WORKER_NOT_RUNNING));\n\tschedule();\n\tgoto repeat;\n}\n\n/**\n * check_flush_dependency - check for flush dependency sanity\n * @target_wq: workqueue being flushed\n * @target_work: work item being flushed (NULL for workqueue flushes)\n *\n * %current is trying to flush the whole @target_wq or @target_work on it.\n * If @target_wq doesn't have %WQ_MEM_RECLAIM, verify that %current is not\n * reclaiming memory or running on a workqueue which doesn't have\n * %WQ_MEM_RECLAIM as that can break forward-progress guarantee leading to\n * a deadlock.\n */\nstatic void check_flush_dependency(struct workqueue_struct *target_wq,\n\t\t\t\t   struct work_struct *target_work)\n{\n\twork_func_t target_func = target_work ? target_work->func : NULL;\n\tstruct worker *worker;\n\n\tif (target_wq->flags & WQ_MEM_RECLAIM)\n\t\treturn;\n\n\tworker = current_wq_worker();\n\n\tWARN_ONCE(current->flags & PF_MEMALLOC,\n\t\t  \"workqueue: PF_MEMALLOC task %d(%s) is flushing !WQ_MEM_RECLAIM %s:%ps\",\n\t\t  current->pid, current->comm, target_wq->name, target_func);\n\tWARN_ONCE(worker && ((worker->current_pwq->wq->flags &\n\t\t\t      (WQ_MEM_RECLAIM | __WQ_LEGACY)) == WQ_MEM_RECLAIM),\n\t\t  \"workqueue: WQ_MEM_RECLAIM %s:%ps is flushing !WQ_MEM_RECLAIM %s:%ps\",\n\t\t  worker->current_pwq->wq->name, worker->current_func,\n\t\t  target_wq->name, target_func);\n}\n\nstruct wq_barrier {\n\tstruct work_struct\twork;\n\tstruct completion\tdone;\n\tstruct task_struct\t*task;\t/* purely informational */\n};\n\nstatic void wq_barrier_func(struct work_struct *work)\n{\n\tstruct wq_barrier *barr = container_of(work, struct wq_barrier, work);\n\tcomplete(&barr->done);\n}\n\n/**\n * insert_wq_barrier - insert a barrier work\n * @pwq: pwq to insert barrier into\n * @barr: wq_barrier to insert\n * @target: target work to attach @barr to\n * @worker: worker currently executing @target, NULL if @target is not executing\n *\n * @barr is linked to @target such that @barr is completed only after\n * @target finishes execution.  Please note that the ordering\n * guarantee is observed only with respect to @target and on the local\n * cpu.\n *\n * Currently, a queued barrier can't be canceled.  This is because\n * try_to_grab_pending() can't determine whether the work to be\n * grabbed is at the head of the queue and thus can't clear LINKED\n * flag of the previous work while there must be a valid next work\n * after a work with LINKED flag set.\n *\n * Note that when @worker is non-NULL, @target may be modified\n * underneath us, so we can't reliably determine pwq from @target.\n *\n * CONTEXT:\n * raw_spin_lock_irq(pool->lock).\n */\nstatic void insert_wq_barrier(struct pool_workqueue *pwq,\n\t\t\t      struct wq_barrier *barr,\n\t\t\t      struct work_struct *target, struct worker *worker)\n{\n\tstruct list_head *head;\n\tunsigned int linked = 0;\n\n\t/*\n\t * debugobject calls are safe here even with pool->lock locked\n\t * as we know for sure that this will not trigger any of the\n\t * checks and call back into the fixup functions where we\n\t * might deadlock.\n\t */\n\tINIT_WORK_ONSTACK(&barr->work, wq_barrier_func);\n\t__set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&barr->work));\n\n\tinit_completion_map(&barr->done, &target->lockdep_map);\n\n\tbarr->task = current;\n\n\t/*\n\t * If @target is currently being executed, schedule the\n\t * barrier to the worker; otherwise, put it after @target.\n\t */\n\tif (worker)\n\t\thead = worker->scheduled.next;\n\telse {\n\t\tunsigned long *bits = work_data_bits(target);\n\n\t\thead = target->entry.next;\n\t\t/* there can already be other linked works, inherit and set */\n\t\tlinked = *bits & WORK_STRUCT_LINKED;\n\t\t__set_bit(WORK_STRUCT_LINKED_BIT, bits);\n\t}\n\n\tdebug_work_activate(&barr->work);\n\tinsert_work(pwq, &barr->work, head,\n\t\t    work_color_to_flags(WORK_NO_COLOR) | linked);\n}\n\n/**\n * flush_workqueue_prep_pwqs - prepare pwqs for workqueue flushing\n * @wq: workqueue being flushed\n * @flush_color: new flush color, < 0 for no-op\n * @work_color: new work color, < 0 for no-op\n *\n * Prepare pwqs for workqueue flushing.\n *\n * If @flush_color is non-negative, flush_color on all pwqs should be\n * -1.  If no pwq has in-flight commands at the specified color, all\n * pwq->flush_color's stay at -1 and %false is returned.  If any pwq\n * has in flight commands, its pwq->flush_color is set to\n * @flush_color, @wq->nr_pwqs_to_flush is updated accordingly, pwq\n * wakeup logic is armed and %true is returned.\n *\n * The caller should have initialized @wq->first_flusher prior to\n * calling this function with non-negative @flush_color.  If\n * @flush_color is negative, no flush color update is done and %false\n * is returned.\n *\n * If @work_color is non-negative, all pwqs should have the same\n * work_color which is previous to @work_color and all will be\n * advanced to @work_color.\n *\n * CONTEXT:\n * mutex_lock(wq->mutex).\n *\n * Return:\n * %true if @flush_color >= 0 and there's something to flush.  %false\n * otherwise.\n */\nstatic bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,\n\t\t\t\t      int flush_color, int work_color)\n{\n\tbool wait = false;\n\tstruct pool_workqueue *pwq;\n\n\tif (flush_color >= 0) {\n\t\tWARN_ON_ONCE(atomic_read(&wq->nr_pwqs_to_flush));\n\t\tatomic_set(&wq->nr_pwqs_to_flush, 1);\n\t}\n\n\tfor_each_pwq(pwq, wq) {\n\t\tstruct worker_pool *pool = pwq->pool;\n\n\t\traw_spin_lock_irq(&pool->lock);\n\n\t\tif (flush_color >= 0) {\n\t\t\tWARN_ON_ONCE(pwq->flush_color != -1);\n\n\t\t\tif (pwq->nr_in_flight[flush_color]) {\n\t\t\t\tpwq->flush_color = flush_color;\n\t\t\t\tatomic_inc(&wq->nr_pwqs_to_flush);\n\t\t\t\twait = true;\n\t\t\t}\n\t\t}\n\n\t\tif (work_color >= 0) {\n\t\t\tWARN_ON_ONCE(work_color != work_next_color(pwq->work_color));\n\t\t\tpwq->work_color = work_color;\n\t\t}\n\n\t\traw_spin_unlock_irq(&pool->lock);\n\t}\n\n\tif (flush_color >= 0 && atomic_dec_and_test(&wq->nr_pwqs_to_flush))\n\t\tcomplete(&wq->first_flusher->done);\n\n\treturn wait;\n}\n\n/**\n * flush_workqueue - ensure that any scheduled work has run to completion.\n * @wq: workqueue to flush\n *\n * This function sleeps until all work items which were queued on entry\n * have finished execution, but it is not livelocked by new incoming ones.\n */\nvoid flush_workqueue(struct workqueue_struct *wq)\n{\n\tstruct wq_flusher this_flusher = {\n\t\t.list = LIST_HEAD_INIT(this_flusher.list),\n\t\t.flush_color = -1,\n\t\t.done = COMPLETION_INITIALIZER_ONSTACK_MAP(this_flusher.done, wq->lockdep_map),\n\t};\n\tint next_color;\n\n\tif (WARN_ON(!wq_online))\n\t\treturn;\n\n\tlock_map_acquire(&wq->lockdep_map);\n\tlock_map_release(&wq->lockdep_map);\n\n\tmutex_lock(&wq->mutex);\n\n\t/*\n\t * Start-to-wait phase\n\t */\n\tnext_color = work_next_color(wq->work_color);\n\n\tif (next_color != wq->flush_color) {\n\t\t/*\n\t\t * Color space is not full.  The current work_color\n\t\t * becomes our flush_color and work_color is advanced\n\t\t * by one.\n\t\t */\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow));\n\t\tthis_flusher.flush_color = wq->work_color;\n\t\twq->work_color = next_color;\n\n\t\tif (!wq->first_flusher) {\n\t\t\t/* no flush in progress, become the first flusher */\n\t\t\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\t\t\twq->first_flusher = &this_flusher;\n\n\t\t\tif (!flush_workqueue_prep_pwqs(wq, wq->flush_color,\n\t\t\t\t\t\t       wq->work_color)) {\n\t\t\t\t/* nothing to flush, done */\n\t\t\t\twq->flush_color = next_color;\n\t\t\t\twq->first_flusher = NULL;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t} else {\n\t\t\t/* wait in queue */\n\t\t\tWARN_ON_ONCE(wq->flush_color == this_flusher.flush_color);\n\t\t\tlist_add_tail(&this_flusher.list, &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * Oops, color space is full, wait on overflow queue.\n\t\t * The next flush completion will assign us\n\t\t * flush_color and transfer to flusher_queue.\n\t\t */\n\t\tlist_add_tail(&this_flusher.list, &wq->flusher_overflow);\n\t}\n\n\tcheck_flush_dependency(wq, NULL);\n\n\tmutex_unlock(&wq->mutex);\n\n\twait_for_completion(&this_flusher.done);\n\n\t/*\n\t * Wake-up-and-cascade phase\n\t *\n\t * First flushers are responsible for cascading flushes and\n\t * handling overflow.  Non-first flushers can simply return.\n\t */\n\tif (READ_ONCE(wq->first_flusher) != &this_flusher)\n\t\treturn;\n\n\tmutex_lock(&wq->mutex);\n\n\t/* we might have raced, check again with mutex held */\n\tif (wq->first_flusher != &this_flusher)\n\t\tgoto out_unlock;\n\n\tWRITE_ONCE(wq->first_flusher, NULL);\n\n\tWARN_ON_ONCE(!list_empty(&this_flusher.list));\n\tWARN_ON_ONCE(wq->flush_color != this_flusher.flush_color);\n\n\twhile (true) {\n\t\tstruct wq_flusher *next, *tmp;\n\n\t\t/* complete all the flushers sharing the current flush color */\n\t\tlist_for_each_entry_safe(next, tmp, &wq->flusher_queue, list) {\n\t\t\tif (next->flush_color != wq->flush_color)\n\t\t\t\tbreak;\n\t\t\tlist_del_init(&next->list);\n\t\t\tcomplete(&next->done);\n\t\t}\n\n\t\tWARN_ON_ONCE(!list_empty(&wq->flusher_overflow) &&\n\t\t\t     wq->flush_color != work_next_color(wq->work_color));\n\n\t\t/* this flush_color is finished, advance by one */\n\t\twq->flush_color = work_next_color(wq->flush_color);\n\n\t\t/* one color has been freed, handle overflow queue */\n\t\tif (!list_empty(&wq->flusher_overflow)) {\n\t\t\t/*\n\t\t\t * Assign the same color to all overflowed\n\t\t\t * flushers, advance work_color and append to\n\t\t\t * flusher_queue.  This is the start-to-wait\n\t\t\t * phase for these overflowed flushers.\n\t\t\t */\n\t\t\tlist_for_each_entry(tmp, &wq->flusher_overflow, list)\n\t\t\t\ttmp->flush_color = wq->work_color;\n\n\t\t\twq->work_color = work_next_color(wq->work_color);\n\n\t\t\tlist_splice_tail_init(&wq->flusher_overflow,\n\t\t\t\t\t      &wq->flusher_queue);\n\t\t\tflush_workqueue_prep_pwqs(wq, -1, wq->work_color);\n\t\t}\n\n\t\tif (list_empty(&wq->flusher_queue)) {\n\t\t\tWARN_ON_ONCE(wq->flush_color != wq->work_color);\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Need to flush more colors.  Make the next flusher\n\t\t * the new first flusher and arm pwqs.\n\t\t */\n\t\tWARN_ON_ONCE(wq->flush_color == wq->work_color);\n\t\tWARN_ON_ONCE(wq->flush_color != next->flush_color);\n\n\t\tlist_del_init(&next->list);\n\t\twq->first_flusher = next;\n\n\t\tif (flush_workqueue_prep_pwqs(wq, wq->flush_color, -1))\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Meh... this color is already done, clear first\n\t\t * flusher and repeat cascading.\n\t\t */\n\t\twq->first_flusher = NULL;\n\t}\n\nout_unlock:\n\tmutex_unlock(&wq->mutex);\n}\nEXPORT_SYMBOL(flush_workqueue);\n\n/**\n * drain_workqueue - drain a workqueue\n * @wq: workqueue to drain\n *\n * Wait until the workqueue becomes empty.  While draining is in progress,\n * only chain queueing is allowed.  IOW, only currently pending or running\n * work items on @wq can queue further work items on it.  @wq is flushed\n * repeatedly until it becomes empty.  The number of flushing is determined\n * by the depth of chaining and should be relatively short.  Whine if it\n * takes too long.\n */\nvoid drain_workqueue(struct workqueue_struct *wq)\n{\n\tunsigned int flush_cnt = 0;\n\tstruct pool_workqueue *pwq;\n\n\t/*\n\t * __queue_work() needs to test whether there are drainers, is much\n\t * hotter than drain_workqueue() and already looks at @wq->flags.\n\t * Use __WQ_DRAINING so that queue doesn't have to check nr_drainers.\n\t */\n\tmutex_lock(&wq->mutex);\n\tif (!wq->nr_drainers++)\n\t\twq->flags |= __WQ_DRAINING;\n\tmutex_unlock(&wq->mutex);\nreflush:\n\tflush_workqueue(wq);\n\n\tmutex_lock(&wq->mutex);\n\n\tfor_each_pwq(pwq, wq) {\n\t\tbool drained;\n\n\t\traw_spin_lock_irq(&pwq->pool->lock);\n\t\tdrained = !pwq->nr_active && list_empty(&pwq->delayed_works);\n\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\n\t\tif (drained)\n\t\t\tcontinue;\n\n\t\tif (++flush_cnt == 10 ||\n\t\t    (flush_cnt % 100 == 0 && flush_cnt <= 1000))\n\t\t\tpr_warn(\"workqueue %s: drain_workqueue() isn't complete after %u tries\\n\",\n\t\t\t\twq->name, flush_cnt);\n\n\t\tmutex_unlock(&wq->mutex);\n\t\tgoto reflush;\n\t}\n\n\tif (!--wq->nr_drainers)\n\t\twq->flags &= ~__WQ_DRAINING;\n\tmutex_unlock(&wq->mutex);\n}\nEXPORT_SYMBOL_GPL(drain_workqueue);\n\nstatic bool start_flush_work(struct work_struct *work, struct wq_barrier *barr,\n\t\t\t     bool from_cancel)\n{\n\tstruct worker *worker = NULL;\n\tstruct worker_pool *pool;\n\tstruct pool_workqueue *pwq;\n\n\tmight_sleep();\n\n\trcu_read_lock();\n\tpool = get_work_pool(work);\n\tif (!pool) {\n\t\trcu_read_unlock();\n\t\treturn false;\n\t}\n\n\traw_spin_lock_irq(&pool->lock);\n\t/* see the comment in try_to_grab_pending() with the same code */\n\tpwq = get_work_pwq(work);\n\tif (pwq) {\n\t\tif (unlikely(pwq->pool != pool))\n\t\t\tgoto already_gone;\n\t} else {\n\t\tworker = find_worker_executing_work(pool, work);\n\t\tif (!worker)\n\t\t\tgoto already_gone;\n\t\tpwq = worker->current_pwq;\n\t}\n\n\tcheck_flush_dependency(pwq->wq, work);\n\n\tinsert_wq_barrier(pwq, barr, work, worker);\n\traw_spin_unlock_irq(&pool->lock);\n\n\t/*\n\t * Force a lock recursion deadlock when using flush_work() inside a\n\t * single-threaded or rescuer equipped workqueue.\n\t *\n\t * For single threaded workqueues the deadlock happens when the work\n\t * is after the work issuing the flush_work(). For rescuer equipped\n\t * workqueues the deadlock happens when the rescuer stalls, blocking\n\t * forward progress.\n\t */\n\tif (!from_cancel &&\n\t    (pwq->wq->saved_max_active == 1 || pwq->wq->rescuer)) {\n\t\tlock_map_acquire(&pwq->wq->lockdep_map);\n\t\tlock_map_release(&pwq->wq->lockdep_map);\n\t}\n\trcu_read_unlock();\n\treturn true;\nalready_gone:\n\traw_spin_unlock_irq(&pool->lock);\n\trcu_read_unlock();\n\treturn false;\n}\n\nstatic bool __flush_work(struct work_struct *work, bool from_cancel)\n{\n\tstruct wq_barrier barr;\n\n\tif (WARN_ON(!wq_online))\n\t\treturn false;\n\n\tif (WARN_ON(!work->func))\n\t\treturn false;\n\n\tif (!from_cancel) {\n\t\tlock_map_acquire(&work->lockdep_map);\n\t\tlock_map_release(&work->lockdep_map);\n\t}\n\n\tif (start_flush_work(work, &barr, from_cancel)) {\n\t\twait_for_completion(&barr.done);\n\t\tdestroy_work_on_stack(&barr.work);\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n\n/**\n * flush_work - wait for a work to finish executing the last queueing instance\n * @work: the work to flush\n *\n * Wait until @work has finished execution.  @work is guaranteed to be idle\n * on return if it hasn't been requeued since flush started.\n *\n * Return:\n * %true if flush_work() waited for the work to finish execution,\n * %false if it was already idle.\n */\nbool flush_work(struct work_struct *work)\n{\n\treturn __flush_work(work, false);\n}\nEXPORT_SYMBOL_GPL(flush_work);\n\nstruct cwt_wait {\n\twait_queue_entry_t\t\twait;\n\tstruct work_struct\t*work;\n};\n\nstatic int cwt_wakefn(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tstruct cwt_wait *cwait = container_of(wait, struct cwt_wait, wait);\n\n\tif (cwait->work != key)\n\t\treturn 0;\n\treturn autoremove_wake_function(wait, mode, sync, key);\n}\n\nstatic bool __cancel_work_timer(struct work_struct *work, bool is_dwork)\n{\n\tstatic DECLARE_WAIT_QUEUE_HEAD(cancel_waitq);\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tret = try_to_grab_pending(work, is_dwork, &flags);\n\t\t/*\n\t\t * If someone else is already canceling, wait for it to\n\t\t * finish.  flush_work() doesn't work for PREEMPT_NONE\n\t\t * because we may get scheduled between @work's completion\n\t\t * and the other canceling task resuming and clearing\n\t\t * CANCELING - flush_work() will return false immediately\n\t\t * as @work is no longer busy, try_to_grab_pending() will\n\t\t * return -ENOENT as @work is still being canceled and the\n\t\t * other canceling task won't be able to clear CANCELING as\n\t\t * we're hogging the CPU.\n\t\t *\n\t\t * Let's wait for completion using a waitqueue.  As this\n\t\t * may lead to the thundering herd problem, use a custom\n\t\t * wake function which matches @work along with exclusive\n\t\t * wait and wakeup.\n\t\t */\n\t\tif (unlikely(ret == -ENOENT)) {\n\t\t\tstruct cwt_wait cwait;\n\n\t\t\tinit_wait(&cwait.wait);\n\t\t\tcwait.wait.func = cwt_wakefn;\n\t\t\tcwait.work = work;\n\n\t\t\tprepare_to_wait_exclusive(&cancel_waitq, &cwait.wait,\n\t\t\t\t\t\t  TASK_UNINTERRUPTIBLE);\n\t\t\tif (work_is_canceling(work))\n\t\t\t\tschedule();\n\t\t\tfinish_wait(&cancel_waitq, &cwait.wait);\n\t\t}\n\t} while (unlikely(ret < 0));\n\n\t/* tell other tasks trying to grab @work to back off */\n\tmark_work_canceling(work);\n\tlocal_irq_restore(flags);\n\n\t/*\n\t * This allows canceling during early boot.  We know that @work\n\t * isn't executing.\n\t */\n\tif (wq_online)\n\t\t__flush_work(work, true);\n\n\tclear_work_data(work);\n\n\t/*\n\t * Paired with prepare_to_wait() above so that either\n\t * waitqueue_active() is visible here or !work_is_canceling() is\n\t * visible there.\n\t */\n\tsmp_mb();\n\tif (waitqueue_active(&cancel_waitq))\n\t\t__wake_up(&cancel_waitq, TASK_NORMAL, 1, work);\n\n\treturn ret;\n}\n\n/**\n * cancel_work_sync - cancel a work and wait for it to finish\n * @work: the work to cancel\n *\n * Cancel @work and wait for its execution to finish.  This function\n * can be used even if the work re-queues itself or migrates to\n * another workqueue.  On return from this function, @work is\n * guaranteed to be not pending or executing on any CPU.\n *\n * cancel_work_sync(&delayed_work->work) must not be used for\n * delayed_work's.  Use cancel_delayed_work_sync() instead.\n *\n * The caller must ensure that the workqueue on which @work was last\n * queued can't be destroyed before this function returns.\n *\n * Return:\n * %true if @work was pending, %false otherwise.\n */\nbool cancel_work_sync(struct work_struct *work)\n{\n\treturn __cancel_work_timer(work, false);\n}\nEXPORT_SYMBOL_GPL(cancel_work_sync);\n\n/**\n * flush_delayed_work - wait for a dwork to finish executing the last queueing\n * @dwork: the delayed work to flush\n *\n * Delayed timer is cancelled and the pending work is queued for\n * immediate execution.  Like flush_work(), this function only\n * considers the last queueing instance of @dwork.\n *\n * Return:\n * %true if flush_work() waited for the work to finish execution,\n * %false if it was already idle.\n */\nbool flush_delayed_work(struct delayed_work *dwork)\n{\n\tlocal_irq_disable();\n\tif (del_timer_sync(&dwork->timer))\n\t\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);\n\tlocal_irq_enable();\n\treturn flush_work(&dwork->work);\n}\nEXPORT_SYMBOL(flush_delayed_work);\n\n/**\n * flush_rcu_work - wait for a rwork to finish executing the last queueing\n * @rwork: the rcu work to flush\n *\n * Return:\n * %true if flush_rcu_work() waited for the work to finish execution,\n * %false if it was already idle.\n */\nbool flush_rcu_work(struct rcu_work *rwork)\n{\n\tif (test_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(&rwork->work))) {\n\t\trcu_barrier();\n\t\tflush_work(&rwork->work);\n\t\treturn true;\n\t} else {\n\t\treturn flush_work(&rwork->work);\n\t}\n}\nEXPORT_SYMBOL(flush_rcu_work);\n\nstatic bool __cancel_work(struct work_struct *work, bool is_dwork)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tdo {\n\t\tret = try_to_grab_pending(work, is_dwork, &flags);\n\t} while (unlikely(ret == -EAGAIN));\n\n\tif (unlikely(ret < 0))\n\t\treturn false;\n\n\tset_work_pool_and_clear_pending(work, get_work_pool_id(work));\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/**\n * cancel_delayed_work - cancel a delayed work\n * @dwork: delayed_work to cancel\n *\n * Kill off a pending delayed_work.\n *\n * Return: %true if @dwork was pending and canceled; %false if it wasn't\n * pending.\n *\n * Note:\n * The work callback function may still be running on return, unless\n * it returns %true and the work doesn't re-arm itself.  Explicitly flush or\n * use cancel_delayed_work_sync() to wait on it.\n *\n * This function is safe to call from any context including IRQ handler.\n */\nbool cancel_delayed_work(struct delayed_work *dwork)\n{\n\treturn __cancel_work(&dwork->work, true);\n}\nEXPORT_SYMBOL(cancel_delayed_work);\n\n/**\n * cancel_delayed_work_sync - cancel a delayed work and wait for it to finish\n * @dwork: the delayed work cancel\n *\n * This is cancel_work_sync() for delayed works.\n *\n * Return:\n * %true if @dwork was pending, %false otherwise.\n */\nbool cancel_delayed_work_sync(struct delayed_work *dwork)\n{\n\treturn __cancel_work_timer(&dwork->work, true);\n}\nEXPORT_SYMBOL(cancel_delayed_work_sync);\n\n/**\n * schedule_on_each_cpu - execute a function synchronously on each online CPU\n * @func: the function to call\n *\n * schedule_on_each_cpu() executes @func on each online CPU using the\n * system workqueue and blocks until all CPUs have completed.\n * schedule_on_each_cpu() is very slow.\n *\n * Return:\n * 0 on success, -errno on failure.\n */\nint schedule_on_each_cpu(work_func_t func)\n{\n\tint cpu;\n\tstruct work_struct __percpu *works;\n\n\tworks = alloc_percpu(struct work_struct);\n\tif (!works)\n\t\treturn -ENOMEM;\n\n\tget_online_cpus();\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct work_struct *work = per_cpu_ptr(works, cpu);\n\n\t\tINIT_WORK(work, func);\n\t\tschedule_work_on(cpu, work);\n\t}\n\n\tfor_each_online_cpu(cpu)\n\t\tflush_work(per_cpu_ptr(works, cpu));\n\n\tput_online_cpus();\n\tfree_percpu(works);\n\treturn 0;\n}\n\n/**\n * execute_in_process_context - reliably execute the routine with user context\n * @fn:\t\tthe function to execute\n * @ew:\t\tguaranteed storage for the execute work structure (must\n *\t\tbe available when the work executes)\n *\n * Executes the function immediately if process context is available,\n * otherwise schedules the function for delayed execution.\n *\n * Return:\t0 - function was executed\n *\t\t1 - function was scheduled for execution\n */\nint execute_in_process_context(work_func_t fn, struct execute_work *ew)\n{\n\tif (!in_interrupt()) {\n\t\tfn(&ew->work);\n\t\treturn 0;\n\t}\n\n\tINIT_WORK(&ew->work, fn);\n\tschedule_work(&ew->work);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(execute_in_process_context);\n\n/**\n * free_workqueue_attrs - free a workqueue_attrs\n * @attrs: workqueue_attrs to free\n *\n * Undo alloc_workqueue_attrs().\n */\nvoid free_workqueue_attrs(struct workqueue_attrs *attrs)\n{\n\tif (attrs) {\n\t\tfree_cpumask_var(attrs->cpumask);\n\t\tkfree(attrs);\n\t}\n}\n\n/**\n * alloc_workqueue_attrs - allocate a workqueue_attrs\n *\n * Allocate a new workqueue_attrs, initialize with default settings and\n * return it.\n *\n * Return: The allocated new workqueue_attr on success. %NULL on failure.\n */\nstruct workqueue_attrs *alloc_workqueue_attrs(void)\n{\n\tstruct workqueue_attrs *attrs;\n\n\tattrs = kzalloc(sizeof(*attrs), GFP_KERNEL);\n\tif (!attrs)\n\t\tgoto fail;\n\tif (!alloc_cpumask_var(&attrs->cpumask, GFP_KERNEL))\n\t\tgoto fail;\n\n\tcpumask_copy(attrs->cpumask, cpu_possible_mask);\n\treturn attrs;\nfail:\n\tfree_workqueue_attrs(attrs);\n\treturn NULL;\n}\n\nstatic void copy_workqueue_attrs(struct workqueue_attrs *to,\n\t\t\t\t const struct workqueue_attrs *from)\n{\n\tto->nice = from->nice;\n\tcpumask_copy(to->cpumask, from->cpumask);\n\t/*\n\t * Unlike hash and equality test, this function doesn't ignore\n\t * ->no_numa as it is used for both pool and wq attrs.  Instead,\n\t * get_unbound_pool() explicitly clears ->no_numa after copying.\n\t */\n\tto->no_numa = from->no_numa;\n}\n\n/* hash value of the content of @attr */\nstatic u32 wqattrs_hash(const struct workqueue_attrs *attrs)\n{\n\tu32 hash = 0;\n\n\thash = jhash_1word(attrs->nice, hash);\n\thash = jhash(cpumask_bits(attrs->cpumask),\n\t\t     BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long), hash);\n\treturn hash;\n}\n\n/* content equality test */\nstatic bool wqattrs_equal(const struct workqueue_attrs *a,\n\t\t\t  const struct workqueue_attrs *b)\n{\n\tif (a->nice != b->nice)\n\t\treturn false;\n\tif (!cpumask_equal(a->cpumask, b->cpumask))\n\t\treturn false;\n\treturn true;\n}\n\n/**\n * init_worker_pool - initialize a newly zalloc'd worker_pool\n * @pool: worker_pool to initialize\n *\n * Initialize a newly zalloc'd @pool.  It also allocates @pool->attrs.\n *\n * Return: 0 on success, -errno on failure.  Even on failure, all fields\n * inside @pool proper are initialized and put_unbound_pool() can be called\n * on @pool safely to release it.\n */\nstatic int init_worker_pool(struct worker_pool *pool)\n{\n\traw_spin_lock_init(&pool->lock);\n\tpool->id = -1;\n\tpool->cpu = -1;\n\tpool->node = NUMA_NO_NODE;\n\tpool->flags |= POOL_DISASSOCIATED;\n\tpool->watchdog_ts = jiffies;\n\tINIT_LIST_HEAD(&pool->worklist);\n\tINIT_LIST_HEAD(&pool->idle_list);\n\thash_init(pool->busy_hash);\n\n\ttimer_setup(&pool->idle_timer, idle_worker_timeout, TIMER_DEFERRABLE);\n\n\ttimer_setup(&pool->mayday_timer, pool_mayday_timeout, 0);\n\n\tINIT_LIST_HEAD(&pool->workers);\n\n\tida_init(&pool->worker_ida);\n\tINIT_HLIST_NODE(&pool->hash_node);\n\tpool->refcnt = 1;\n\n\t/* shouldn't fail above this point */\n\tpool->attrs = alloc_workqueue_attrs();\n\tif (!pool->attrs)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic void wq_init_lockdep(struct workqueue_struct *wq)\n{\n\tchar *lock_name;\n\n\tlockdep_register_key(&wq->key);\n\tlock_name = kasprintf(GFP_KERNEL, \"%s%s\", \"(wq_completion)\", wq->name);\n\tif (!lock_name)\n\t\tlock_name = wq->name;\n\n\twq->lock_name = lock_name;\n\tlockdep_init_map(&wq->lockdep_map, lock_name, &wq->key, 0);\n}\n\nstatic void wq_unregister_lockdep(struct workqueue_struct *wq)\n{\n\tlockdep_unregister_key(&wq->key);\n}\n\nstatic void wq_free_lockdep(struct workqueue_struct *wq)\n{\n\tif (wq->lock_name != wq->name)\n\t\tkfree(wq->lock_name);\n}\n#else\nstatic void wq_init_lockdep(struct workqueue_struct *wq)\n{\n}\n\nstatic void wq_unregister_lockdep(struct workqueue_struct *wq)\n{\n}\n\nstatic void wq_free_lockdep(struct workqueue_struct *wq)\n{\n}\n#endif\n\nstatic void rcu_free_wq(struct rcu_head *rcu)\n{\n\tstruct workqueue_struct *wq =\n\t\tcontainer_of(rcu, struct workqueue_struct, rcu);\n\n\twq_free_lockdep(wq);\n\n\tif (!(wq->flags & WQ_UNBOUND))\n\t\tfree_percpu(wq->cpu_pwqs);\n\telse\n\t\tfree_workqueue_attrs(wq->unbound_attrs);\n\n\tkfree(wq);\n}\n\nstatic void rcu_free_pool(struct rcu_head *rcu)\n{\n\tstruct worker_pool *pool = container_of(rcu, struct worker_pool, rcu);\n\n\tida_destroy(&pool->worker_ida);\n\tfree_workqueue_attrs(pool->attrs);\n\tkfree(pool);\n}\n\n/* This returns with the lock held on success (pool manager is inactive). */\nstatic bool wq_manager_inactive(struct worker_pool *pool)\n{\n\traw_spin_lock_irq(&pool->lock);\n\n\tif (pool->flags & POOL_MANAGER_ACTIVE) {\n\t\traw_spin_unlock_irq(&pool->lock);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n/**\n * put_unbound_pool - put a worker_pool\n * @pool: worker_pool to put\n *\n * Put @pool.  If its refcnt reaches zero, it gets destroyed in RCU\n * safe manner.  get_unbound_pool() calls this function on its failure path\n * and this function should be able to release pools which went through,\n * successfully or not, init_worker_pool().\n *\n * Should be called with wq_pool_mutex held.\n */\nstatic void put_unbound_pool(struct worker_pool *pool)\n{\n\tDECLARE_COMPLETION_ONSTACK(detach_completion);\n\tstruct worker *worker;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tif (--pool->refcnt)\n\t\treturn;\n\n\t/* sanity checks */\n\tif (WARN_ON(!(pool->cpu < 0)) ||\n\t    WARN_ON(!list_empty(&pool->worklist)))\n\t\treturn;\n\n\t/* release id and unhash */\n\tif (pool->id >= 0)\n\t\tidr_remove(&worker_pool_idr, pool->id);\n\thash_del(&pool->hash_node);\n\n\t/*\n\t * Become the manager and destroy all workers.  This prevents\n\t * @pool's workers from blocking on attach_mutex.  We're the last\n\t * manager and @pool gets freed with the flag set.\n\t * Because of how wq_manager_inactive() works, we will hold the\n\t * spinlock after a successful wait.\n\t */\n\trcuwait_wait_event(&manager_wait, wq_manager_inactive(pool),\n\t\t\t   TASK_UNINTERRUPTIBLE);\n\tpool->flags |= POOL_MANAGER_ACTIVE;\n\n\twhile ((worker = first_idle_worker(pool)))\n\t\tdestroy_worker(worker);\n\tWARN_ON(pool->nr_workers || pool->nr_idle);\n\traw_spin_unlock_irq(&pool->lock);\n\n\tmutex_lock(&wq_pool_attach_mutex);\n\tif (!list_empty(&pool->workers))\n\t\tpool->detach_completion = &detach_completion;\n\tmutex_unlock(&wq_pool_attach_mutex);\n\n\tif (pool->detach_completion)\n\t\twait_for_completion(pool->detach_completion);\n\n\t/* shut down the timers */\n\tdel_timer_sync(&pool->idle_timer);\n\tdel_timer_sync(&pool->mayday_timer);\n\n\t/* RCU protected to allow dereferences from get_work_pool() */\n\tcall_rcu(&pool->rcu, rcu_free_pool);\n}\n\n/**\n * get_unbound_pool - get a worker_pool with the specified attributes\n * @attrs: the attributes of the worker_pool to get\n *\n * Obtain a worker_pool which has the same attributes as @attrs, bump the\n * reference count and return it.  If there already is a matching\n * worker_pool, it will be used; otherwise, this function attempts to\n * create a new one.\n *\n * Should be called with wq_pool_mutex held.\n *\n * Return: On success, a worker_pool with the same attributes as @attrs.\n * On failure, %NULL.\n */\nstatic struct worker_pool *get_unbound_pool(const struct workqueue_attrs *attrs)\n{\n\tu32 hash = wqattrs_hash(attrs);\n\tstruct worker_pool *pool;\n\tint node;\n\tint target_node = NUMA_NO_NODE;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\t/* do we already have a matching pool? */\n\thash_for_each_possible(unbound_pool_hash, pool, hash_node, hash) {\n\t\tif (wqattrs_equal(pool->attrs, attrs)) {\n\t\t\tpool->refcnt++;\n\t\t\treturn pool;\n\t\t}\n\t}\n\n\t/* if cpumask is contained inside a NUMA node, we belong to that node */\n\tif (wq_numa_enabled) {\n\t\tfor_each_node(node) {\n\t\t\tif (cpumask_subset(attrs->cpumask,\n\t\t\t\t\t   wq_numa_possible_cpumask[node])) {\n\t\t\t\ttarget_node = node;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* nope, create a new one */\n\tpool = kzalloc_node(sizeof(*pool), GFP_KERNEL, target_node);\n\tif (!pool || init_worker_pool(pool) < 0)\n\t\tgoto fail;\n\n\tlockdep_set_subclass(&pool->lock, 1);\t/* see put_pwq() */\n\tcopy_workqueue_attrs(pool->attrs, attrs);\n\tpool->node = target_node;\n\n\t/*\n\t * no_numa isn't a worker_pool attribute, always clear it.  See\n\t * 'struct workqueue_attrs' comments for detail.\n\t */\n\tpool->attrs->no_numa = false;\n\n\tif (worker_pool_assign_id(pool) < 0)\n\t\tgoto fail;\n\n\t/* create and start the initial worker */\n\tif (wq_online && !create_worker(pool))\n\t\tgoto fail;\n\n\t/* install */\n\thash_add(unbound_pool_hash, &pool->hash_node, hash);\n\n\treturn pool;\nfail:\n\tif (pool)\n\t\tput_unbound_pool(pool);\n\treturn NULL;\n}\n\nstatic void rcu_free_pwq(struct rcu_head *rcu)\n{\n\tkmem_cache_free(pwq_cache,\n\t\t\tcontainer_of(rcu, struct pool_workqueue, rcu));\n}\n\n/*\n * Scheduled on system_wq by put_pwq() when an unbound pwq hits zero refcnt\n * and needs to be destroyed.\n */\nstatic void pwq_unbound_release_workfn(struct work_struct *work)\n{\n\tstruct pool_workqueue *pwq = container_of(work, struct pool_workqueue,\n\t\t\t\t\t\t  unbound_release_work);\n\tstruct workqueue_struct *wq = pwq->wq;\n\tstruct worker_pool *pool = pwq->pool;\n\tbool is_last;\n\n\tif (WARN_ON_ONCE(!(wq->flags & WQ_UNBOUND)))\n\t\treturn;\n\n\tmutex_lock(&wq->mutex);\n\tlist_del_rcu(&pwq->pwqs_node);\n\tis_last = list_empty(&wq->pwqs);\n\tmutex_unlock(&wq->mutex);\n\n\tmutex_lock(&wq_pool_mutex);\n\tput_unbound_pool(pool);\n\tmutex_unlock(&wq_pool_mutex);\n\n\tcall_rcu(&pwq->rcu, rcu_free_pwq);\n\n\t/*\n\t * If we're the last pwq going away, @wq is already dead and no one\n\t * is gonna access it anymore.  Schedule RCU free.\n\t */\n\tif (is_last) {\n\t\twq_unregister_lockdep(wq);\n\t\tcall_rcu(&wq->rcu, rcu_free_wq);\n\t}\n}\n\n/**\n * pwq_adjust_max_active - update a pwq's max_active to the current setting\n * @pwq: target pool_workqueue\n *\n * If @pwq isn't freezing, set @pwq->max_active to the associated\n * workqueue's saved_max_active and activate delayed work items\n * accordingly.  If @pwq is freezing, clear @pwq->max_active to zero.\n */\nstatic void pwq_adjust_max_active(struct pool_workqueue *pwq)\n{\n\tstruct workqueue_struct *wq = pwq->wq;\n\tbool freezable = wq->flags & WQ_FREEZABLE;\n\tunsigned long flags;\n\n\t/* for @wq->saved_max_active */\n\tlockdep_assert_held(&wq->mutex);\n\n\t/* fast exit for non-freezable wqs */\n\tif (!freezable && pwq->max_active == wq->saved_max_active)\n\t\treturn;\n\n\t/* this function can be called during early boot w/ irq disabled */\n\traw_spin_lock_irqsave(&pwq->pool->lock, flags);\n\n\t/*\n\t * During [un]freezing, the caller is responsible for ensuring that\n\t * this function is called at least once after @workqueue_freezing\n\t * is updated and visible.\n\t */\n\tif (!freezable || !workqueue_freezing) {\n\t\tbool kick = false;\n\n\t\tpwq->max_active = wq->saved_max_active;\n\n\t\twhile (!list_empty(&pwq->delayed_works) &&\n\t\t       pwq->nr_active < pwq->max_active) {\n\t\t\tpwq_activate_first_delayed(pwq);\n\t\t\tkick = true;\n\t\t}\n\n\t\t/*\n\t\t * Need to kick a worker after thawed or an unbound wq's\n\t\t * max_active is bumped. In realtime scenarios, always kicking a\n\t\t * worker will cause interference on the isolated cpu cores, so\n\t\t * let's kick iff work items were activated.\n\t\t */\n\t\tif (kick)\n\t\t\twake_up_worker(pwq->pool);\n\t} else {\n\t\tpwq->max_active = 0;\n\t}\n\n\traw_spin_unlock_irqrestore(&pwq->pool->lock, flags);\n}\n\n/* initialize newly alloced @pwq which is associated with @wq and @pool */\nstatic void init_pwq(struct pool_workqueue *pwq, struct workqueue_struct *wq,\n\t\t     struct worker_pool *pool)\n{\n\tBUG_ON((unsigned long)pwq & WORK_STRUCT_FLAG_MASK);\n\n\tmemset(pwq, 0, sizeof(*pwq));\n\n\tpwq->pool = pool;\n\tpwq->wq = wq;\n\tpwq->flush_color = -1;\n\tpwq->refcnt = 1;\n\tINIT_LIST_HEAD(&pwq->delayed_works);\n\tINIT_LIST_HEAD(&pwq->pwqs_node);\n\tINIT_LIST_HEAD(&pwq->mayday_node);\n\tINIT_WORK(&pwq->unbound_release_work, pwq_unbound_release_workfn);\n}\n\n/* sync @pwq with the current state of its associated wq and link it */\nstatic void link_pwq(struct pool_workqueue *pwq)\n{\n\tstruct workqueue_struct *wq = pwq->wq;\n\n\tlockdep_assert_held(&wq->mutex);\n\n\t/* may be called multiple times, ignore if already linked */\n\tif (!list_empty(&pwq->pwqs_node))\n\t\treturn;\n\n\t/* set the matching work_color */\n\tpwq->work_color = wq->work_color;\n\n\t/* sync max_active to the current setting */\n\tpwq_adjust_max_active(pwq);\n\n\t/* link in @pwq */\n\tlist_add_rcu(&pwq->pwqs_node, &wq->pwqs);\n}\n\n/* obtain a pool matching @attr and create a pwq associating the pool and @wq */\nstatic struct pool_workqueue *alloc_unbound_pwq(struct workqueue_struct *wq,\n\t\t\t\t\tconst struct workqueue_attrs *attrs)\n{\n\tstruct worker_pool *pool;\n\tstruct pool_workqueue *pwq;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tpool = get_unbound_pool(attrs);\n\tif (!pool)\n\t\treturn NULL;\n\n\tpwq = kmem_cache_alloc_node(pwq_cache, GFP_KERNEL, pool->node);\n\tif (!pwq) {\n\t\tput_unbound_pool(pool);\n\t\treturn NULL;\n\t}\n\n\tinit_pwq(pwq, wq, pool);\n\treturn pwq;\n}\n\n/**\n * wq_calc_node_cpumask - calculate a wq_attrs' cpumask for the specified node\n * @attrs: the wq_attrs of the default pwq of the target workqueue\n * @node: the target NUMA node\n * @cpu_going_down: if >= 0, the CPU to consider as offline\n * @cpumask: outarg, the resulting cpumask\n *\n * Calculate the cpumask a workqueue with @attrs should use on @node.  If\n * @cpu_going_down is >= 0, that cpu is considered offline during\n * calculation.  The result is stored in @cpumask.\n *\n * If NUMA affinity is not enabled, @attrs->cpumask is always used.  If\n * enabled and @node has online CPUs requested by @attrs, the returned\n * cpumask is the intersection of the possible CPUs of @node and\n * @attrs->cpumask.\n *\n * The caller is responsible for ensuring that the cpumask of @node stays\n * stable.\n *\n * Return: %true if the resulting @cpumask is different from @attrs->cpumask,\n * %false if equal.\n */\nstatic bool wq_calc_node_cpumask(const struct workqueue_attrs *attrs, int node,\n\t\t\t\t int cpu_going_down, cpumask_t *cpumask)\n{\n\tif (!wq_numa_enabled || attrs->no_numa)\n\t\tgoto use_dfl;\n\n\t/* does @node have any online CPUs @attrs wants? */\n\tcpumask_and(cpumask, cpumask_of_node(node), attrs->cpumask);\n\tif (cpu_going_down >= 0)\n\t\tcpumask_clear_cpu(cpu_going_down, cpumask);\n\n\tif (cpumask_empty(cpumask))\n\t\tgoto use_dfl;\n\n\t/* yeap, return possible CPUs in @node that @attrs wants */\n\tcpumask_and(cpumask, attrs->cpumask, wq_numa_possible_cpumask[node]);\n\n\tif (cpumask_empty(cpumask)) {\n\t\tpr_warn_once(\"WARNING: workqueue cpumask: online intersect > \"\n\t\t\t\t\"possible intersect\\n\");\n\t\treturn false;\n\t}\n\n\treturn !cpumask_equal(cpumask, attrs->cpumask);\n\nuse_dfl:\n\tcpumask_copy(cpumask, attrs->cpumask);\n\treturn false;\n}\n\n/* install @pwq into @wq's numa_pwq_tbl[] for @node and return the old pwq */\nstatic struct pool_workqueue *numa_pwq_tbl_install(struct workqueue_struct *wq,\n\t\t\t\t\t\t   int node,\n\t\t\t\t\t\t   struct pool_workqueue *pwq)\n{\n\tstruct pool_workqueue *old_pwq;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\tlockdep_assert_held(&wq->mutex);\n\n\t/* link_pwq() can handle duplicate calls */\n\tlink_pwq(pwq);\n\n\told_pwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);\n\trcu_assign_pointer(wq->numa_pwq_tbl[node], pwq);\n\treturn old_pwq;\n}\n\n/* context to store the prepared attrs & pwqs before applying */\nstruct apply_wqattrs_ctx {\n\tstruct workqueue_struct\t*wq;\t\t/* target workqueue */\n\tstruct workqueue_attrs\t*attrs;\t\t/* attrs to apply */\n\tstruct list_head\tlist;\t\t/* queued for batching commit */\n\tstruct pool_workqueue\t*dfl_pwq;\n\tstruct pool_workqueue\t*pwq_tbl[];\n};\n\n/* free the resources after success or abort */\nstatic void apply_wqattrs_cleanup(struct apply_wqattrs_ctx *ctx)\n{\n\tif (ctx) {\n\t\tint node;\n\n\t\tfor_each_node(node)\n\t\t\tput_pwq_unlocked(ctx->pwq_tbl[node]);\n\t\tput_pwq_unlocked(ctx->dfl_pwq);\n\n\t\tfree_workqueue_attrs(ctx->attrs);\n\n\t\tkfree(ctx);\n\t}\n}\n\n/* allocate the attrs and pwqs for later installation */\nstatic struct apply_wqattrs_ctx *\napply_wqattrs_prepare(struct workqueue_struct *wq,\n\t\t      const struct workqueue_attrs *attrs)\n{\n\tstruct apply_wqattrs_ctx *ctx;\n\tstruct workqueue_attrs *new_attrs, *tmp_attrs;\n\tint node;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tctx = kzalloc(struct_size(ctx, pwq_tbl, nr_node_ids), GFP_KERNEL);\n\n\tnew_attrs = alloc_workqueue_attrs();\n\ttmp_attrs = alloc_workqueue_attrs();\n\tif (!ctx || !new_attrs || !tmp_attrs)\n\t\tgoto out_free;\n\n\t/*\n\t * Calculate the attrs of the default pwq.\n\t * If the user configured cpumask doesn't overlap with the\n\t * wq_unbound_cpumask, we fallback to the wq_unbound_cpumask.\n\t */\n\tcopy_workqueue_attrs(new_attrs, attrs);\n\tcpumask_and(new_attrs->cpumask, new_attrs->cpumask, wq_unbound_cpumask);\n\tif (unlikely(cpumask_empty(new_attrs->cpumask)))\n\t\tcpumask_copy(new_attrs->cpumask, wq_unbound_cpumask);\n\n\t/*\n\t * We may create multiple pwqs with differing cpumasks.  Make a\n\t * copy of @new_attrs which will be modified and used to obtain\n\t * pools.\n\t */\n\tcopy_workqueue_attrs(tmp_attrs, new_attrs);\n\n\t/*\n\t * If something goes wrong during CPU up/down, we'll fall back to\n\t * the default pwq covering whole @attrs->cpumask.  Always create\n\t * it even if we don't use it immediately.\n\t */\n\tctx->dfl_pwq = alloc_unbound_pwq(wq, new_attrs);\n\tif (!ctx->dfl_pwq)\n\t\tgoto out_free;\n\n\tfor_each_node(node) {\n\t\tif (wq_calc_node_cpumask(new_attrs, node, -1, tmp_attrs->cpumask)) {\n\t\t\tctx->pwq_tbl[node] = alloc_unbound_pwq(wq, tmp_attrs);\n\t\t\tif (!ctx->pwq_tbl[node])\n\t\t\t\tgoto out_free;\n\t\t} else {\n\t\t\tctx->dfl_pwq->refcnt++;\n\t\t\tctx->pwq_tbl[node] = ctx->dfl_pwq;\n\t\t}\n\t}\n\n\t/* save the user configured attrs and sanitize it. */\n\tcopy_workqueue_attrs(new_attrs, attrs);\n\tcpumask_and(new_attrs->cpumask, new_attrs->cpumask, cpu_possible_mask);\n\tctx->attrs = new_attrs;\n\n\tctx->wq = wq;\n\tfree_workqueue_attrs(tmp_attrs);\n\treturn ctx;\n\nout_free:\n\tfree_workqueue_attrs(tmp_attrs);\n\tfree_workqueue_attrs(new_attrs);\n\tapply_wqattrs_cleanup(ctx);\n\treturn NULL;\n}\n\n/* set attrs and install prepared pwqs, @ctx points to old pwqs on return */\nstatic void apply_wqattrs_commit(struct apply_wqattrs_ctx *ctx)\n{\n\tint node;\n\n\t/* all pwqs have been created successfully, let's install'em */\n\tmutex_lock(&ctx->wq->mutex);\n\n\tcopy_workqueue_attrs(ctx->wq->unbound_attrs, ctx->attrs);\n\n\t/* save the previous pwq and install the new one */\n\tfor_each_node(node)\n\t\tctx->pwq_tbl[node] = numa_pwq_tbl_install(ctx->wq, node,\n\t\t\t\t\t\t\t  ctx->pwq_tbl[node]);\n\n\t/* @dfl_pwq might not have been used, ensure it's linked */\n\tlink_pwq(ctx->dfl_pwq);\n\tswap(ctx->wq->dfl_pwq, ctx->dfl_pwq);\n\n\tmutex_unlock(&ctx->wq->mutex);\n}\n\nstatic void apply_wqattrs_lock(void)\n{\n\t/* CPUs should stay stable across pwq creations and installations */\n\tget_online_cpus();\n\tmutex_lock(&wq_pool_mutex);\n}\n\nstatic void apply_wqattrs_unlock(void)\n{\n\tmutex_unlock(&wq_pool_mutex);\n\tput_online_cpus();\n}\n\nstatic int apply_workqueue_attrs_locked(struct workqueue_struct *wq,\n\t\t\t\t\tconst struct workqueue_attrs *attrs)\n{\n\tstruct apply_wqattrs_ctx *ctx;\n\n\t/* only unbound workqueues can change attributes */\n\tif (WARN_ON(!(wq->flags & WQ_UNBOUND)))\n\t\treturn -EINVAL;\n\n\t/* creating multiple pwqs breaks ordering guarantee */\n\tif (!list_empty(&wq->pwqs)) {\n\t\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))\n\t\t\treturn -EINVAL;\n\n\t\twq->flags &= ~__WQ_ORDERED;\n\t}\n\n\tctx = apply_wqattrs_prepare(wq, attrs);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\t/* the ctx has been prepared successfully, let's commit it */\n\tapply_wqattrs_commit(ctx);\n\tapply_wqattrs_cleanup(ctx);\n\n\treturn 0;\n}\n\n/**\n * apply_workqueue_attrs - apply new workqueue_attrs to an unbound workqueue\n * @wq: the target workqueue\n * @attrs: the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()\n *\n * Apply @attrs to an unbound workqueue @wq.  Unless disabled, on NUMA\n * machines, this function maps a separate pwq to each NUMA node with\n * possibles CPUs in @attrs->cpumask so that work items are affine to the\n * NUMA node it was issued on.  Older pwqs are released as in-flight work\n * items finish.  Note that a work item which repeatedly requeues itself\n * back-to-back will stay on its current pwq.\n *\n * Performs GFP_KERNEL allocations.\n *\n * Assumes caller has CPU hotplug read exclusion, i.e. get_online_cpus().\n *\n * Return: 0 on success and -errno on failure.\n */\nint apply_workqueue_attrs(struct workqueue_struct *wq,\n\t\t\t  const struct workqueue_attrs *attrs)\n{\n\tint ret;\n\n\tlockdep_assert_cpus_held();\n\n\tmutex_lock(&wq_pool_mutex);\n\tret = apply_workqueue_attrs_locked(wq, attrs);\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn ret;\n}\n\n/**\n * wq_update_unbound_numa - update NUMA affinity of a wq for CPU hot[un]plug\n * @wq: the target workqueue\n * @cpu: the CPU coming up or going down\n * @online: whether @cpu is coming up or going down\n *\n * This function is to be called from %CPU_DOWN_PREPARE, %CPU_ONLINE and\n * %CPU_DOWN_FAILED.  @cpu is being hot[un]plugged, update NUMA affinity of\n * @wq accordingly.\n *\n * If NUMA affinity can't be adjusted due to memory allocation failure, it\n * falls back to @wq->dfl_pwq which may not be optimal but is always\n * correct.\n *\n * Note that when the last allowed CPU of a NUMA node goes offline for a\n * workqueue with a cpumask spanning multiple nodes, the workers which were\n * already executing the work items for the workqueue will lose their CPU\n * affinity and may execute on any CPU.  This is similar to how per-cpu\n * workqueues behave on CPU_DOWN.  If a workqueue user wants strict\n * affinity, it's the user's responsibility to flush the work item from\n * CPU_DOWN_PREPARE.\n */\nstatic void wq_update_unbound_numa(struct workqueue_struct *wq, int cpu,\n\t\t\t\t   bool online)\n{\n\tint node = cpu_to_node(cpu);\n\tint cpu_off = online ? -1 : cpu;\n\tstruct pool_workqueue *old_pwq = NULL, *pwq;\n\tstruct workqueue_attrs *target_attrs;\n\tcpumask_t *cpumask;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tif (!wq_numa_enabled || !(wq->flags & WQ_UNBOUND) ||\n\t    wq->unbound_attrs->no_numa)\n\t\treturn;\n\n\t/*\n\t * We don't wanna alloc/free wq_attrs for each wq for each CPU.\n\t * Let's use a preallocated one.  The following buf is protected by\n\t * CPU hotplug exclusion.\n\t */\n\ttarget_attrs = wq_update_unbound_numa_attrs_buf;\n\tcpumask = target_attrs->cpumask;\n\n\tcopy_workqueue_attrs(target_attrs, wq->unbound_attrs);\n\tpwq = unbound_pwq_by_node(wq, node);\n\n\t/*\n\t * Let's determine what needs to be done.  If the target cpumask is\n\t * different from the default pwq's, we need to compare it to @pwq's\n\t * and create a new one if they don't match.  If the target cpumask\n\t * equals the default pwq's, the default pwq should be used.\n\t */\n\tif (wq_calc_node_cpumask(wq->dfl_pwq->pool->attrs, node, cpu_off, cpumask)) {\n\t\tif (cpumask_equal(cpumask, pwq->pool->attrs->cpumask))\n\t\t\treturn;\n\t} else {\n\t\tgoto use_dfl_pwq;\n\t}\n\n\t/* create a new pwq */\n\tpwq = alloc_unbound_pwq(wq, target_attrs);\n\tif (!pwq) {\n\t\tpr_warn(\"workqueue: allocation failed while updating NUMA affinity of \\\"%s\\\"\\n\",\n\t\t\twq->name);\n\t\tgoto use_dfl_pwq;\n\t}\n\n\t/* Install the new pwq. */\n\tmutex_lock(&wq->mutex);\n\told_pwq = numa_pwq_tbl_install(wq, node, pwq);\n\tgoto out_unlock;\n\nuse_dfl_pwq:\n\tmutex_lock(&wq->mutex);\n\traw_spin_lock_irq(&wq->dfl_pwq->pool->lock);\n\tget_pwq(wq->dfl_pwq);\n\traw_spin_unlock_irq(&wq->dfl_pwq->pool->lock);\n\told_pwq = numa_pwq_tbl_install(wq, node, wq->dfl_pwq);\nout_unlock:\n\tmutex_unlock(&wq->mutex);\n\tput_pwq_unlocked(old_pwq);\n}\n\nstatic int alloc_and_link_pwqs(struct workqueue_struct *wq)\n{\n\tbool highpri = wq->flags & WQ_HIGHPRI;\n\tint cpu, ret;\n\n\tif (!(wq->flags & WQ_UNBOUND)) {\n\t\twq->cpu_pwqs = alloc_percpu(struct pool_workqueue);\n\t\tif (!wq->cpu_pwqs)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct pool_workqueue *pwq =\n\t\t\t\tper_cpu_ptr(wq->cpu_pwqs, cpu);\n\t\t\tstruct worker_pool *cpu_pools =\n\t\t\t\tper_cpu(cpu_worker_pools, cpu);\n\n\t\t\tinit_pwq(pwq, wq, &cpu_pools[highpri]);\n\n\t\t\tmutex_lock(&wq->mutex);\n\t\t\tlink_pwq(pwq);\n\t\t\tmutex_unlock(&wq->mutex);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tget_online_cpus();\n\tif (wq->flags & __WQ_ORDERED) {\n\t\tret = apply_workqueue_attrs(wq, ordered_wq_attrs[highpri]);\n\t\t/* there should only be single pwq for ordering guarantee */\n\t\tWARN(!ret && (wq->pwqs.next != &wq->dfl_pwq->pwqs_node ||\n\t\t\t      wq->pwqs.prev != &wq->dfl_pwq->pwqs_node),\n\t\t     \"ordering guarantee broken for workqueue %s\\n\", wq->name);\n\t} else {\n\t\tret = apply_workqueue_attrs(wq, unbound_std_wq_attrs[highpri]);\n\t}\n\tput_online_cpus();\n\n\treturn ret;\n}\n\nstatic int wq_clamp_max_active(int max_active, unsigned int flags,\n\t\t\t       const char *name)\n{\n\tint lim = flags & WQ_UNBOUND ? WQ_UNBOUND_MAX_ACTIVE : WQ_MAX_ACTIVE;\n\n\tif (max_active < 1 || max_active > lim)\n\t\tpr_warn(\"workqueue: max_active %d requested for %s is out of range, clamping between %d and %d\\n\",\n\t\t\tmax_active, name, 1, lim);\n\n\treturn clamp_val(max_active, 1, lim);\n}\n\n/*\n * Workqueues which may be used during memory reclaim should have a rescuer\n * to guarantee forward progress.\n */\nstatic int init_rescuer(struct workqueue_struct *wq)\n{\n\tstruct worker *rescuer;\n\tint ret;\n\n\tif (!(wq->flags & WQ_MEM_RECLAIM))\n\t\treturn 0;\n\n\trescuer = alloc_worker(NUMA_NO_NODE);\n\tif (!rescuer)\n\t\treturn -ENOMEM;\n\n\trescuer->rescue_wq = wq;\n\trescuer->task = kthread_create(rescuer_thread, rescuer, \"%s\", wq->name);\n\tif (IS_ERR(rescuer->task)) {\n\t\tret = PTR_ERR(rescuer->task);\n\t\tkfree(rescuer);\n\t\treturn ret;\n\t}\n\n\twq->rescuer = rescuer;\n\tkthread_bind_mask(rescuer->task, cpu_possible_mask);\n\twake_up_process(rescuer->task);\n\n\treturn 0;\n}\n\n__printf(1, 4)\nstruct workqueue_struct *alloc_workqueue(const char *fmt,\n\t\t\t\t\t unsigned int flags,\n\t\t\t\t\t int max_active, ...)\n{\n\tsize_t tbl_size = 0;\n\tva_list args;\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\t/*\n\t * Unbound && max_active == 1 used to imply ordered, which is no\n\t * longer the case on NUMA machines due to per-node pools.  While\n\t * alloc_ordered_workqueue() is the right way to create an ordered\n\t * workqueue, keep the previous behavior to avoid subtle breakages\n\t * on NUMA.\n\t */\n\tif ((flags & WQ_UNBOUND) && max_active == 1)\n\t\tflags |= __WQ_ORDERED;\n\n\t/* see the comment above the definition of WQ_POWER_EFFICIENT */\n\tif ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)\n\t\tflags |= WQ_UNBOUND;\n\n\t/* allocate wq and format name */\n\tif (flags & WQ_UNBOUND)\n\t\ttbl_size = nr_node_ids * sizeof(wq->numa_pwq_tbl[0]);\n\n\twq = kzalloc(sizeof(*wq) + tbl_size, GFP_KERNEL);\n\tif (!wq)\n\t\treturn NULL;\n\n\tif (flags & WQ_UNBOUND) {\n\t\twq->unbound_attrs = alloc_workqueue_attrs();\n\t\tif (!wq->unbound_attrs)\n\t\t\tgoto err_free_wq;\n\t}\n\n\tva_start(args, max_active);\n\tvsnprintf(wq->name, sizeof(wq->name), fmt, args);\n\tva_end(args);\n\n\tmax_active = max_active ?: WQ_DFL_ACTIVE;\n\tmax_active = wq_clamp_max_active(max_active, flags, wq->name);\n\n\t/* init wq */\n\twq->flags = flags;\n\twq->saved_max_active = max_active;\n\tmutex_init(&wq->mutex);\n\tatomic_set(&wq->nr_pwqs_to_flush, 0);\n\tINIT_LIST_HEAD(&wq->pwqs);\n\tINIT_LIST_HEAD(&wq->flusher_queue);\n\tINIT_LIST_HEAD(&wq->flusher_overflow);\n\tINIT_LIST_HEAD(&wq->maydays);\n\n\twq_init_lockdep(wq);\n\tINIT_LIST_HEAD(&wq->list);\n\n\tif (alloc_and_link_pwqs(wq) < 0)\n\t\tgoto err_unreg_lockdep;\n\n\tif (wq_online && init_rescuer(wq) < 0)\n\t\tgoto err_destroy;\n\n\tif ((wq->flags & WQ_SYSFS) && workqueue_sysfs_register(wq))\n\t\tgoto err_destroy;\n\n\t/*\n\t * wq_pool_mutex protects global freeze state and workqueues list.\n\t * Grab it, adjust max_active and add the new @wq to workqueues\n\t * list.\n\t */\n\tmutex_lock(&wq_pool_mutex);\n\n\tmutex_lock(&wq->mutex);\n\tfor_each_pwq(pwq, wq)\n\t\tpwq_adjust_max_active(pwq);\n\tmutex_unlock(&wq->mutex);\n\n\tlist_add_tail_rcu(&wq->list, &workqueues);\n\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn wq;\n\nerr_unreg_lockdep:\n\twq_unregister_lockdep(wq);\n\twq_free_lockdep(wq);\nerr_free_wq:\n\tfree_workqueue_attrs(wq->unbound_attrs);\n\tkfree(wq);\n\treturn NULL;\nerr_destroy:\n\tdestroy_workqueue(wq);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(alloc_workqueue);\n\nstatic bool pwq_busy(struct pool_workqueue *pwq)\n{\n\tint i;\n\n\tfor (i = 0; i < WORK_NR_COLORS; i++)\n\t\tif (pwq->nr_in_flight[i])\n\t\t\treturn true;\n\n\tif ((pwq != pwq->wq->dfl_pwq) && (pwq->refcnt > 1))\n\t\treturn true;\n\tif (pwq->nr_active || !list_empty(&pwq->delayed_works))\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * destroy_workqueue - safely terminate a workqueue\n * @wq: target workqueue\n *\n * Safely destroy a workqueue. All work currently pending will be done first.\n */\nvoid destroy_workqueue(struct workqueue_struct *wq)\n{\n\tstruct pool_workqueue *pwq;\n\tint node;\n\n\t/*\n\t * Remove it from sysfs first so that sanity check failure doesn't\n\t * lead to sysfs name conflicts.\n\t */\n\tworkqueue_sysfs_unregister(wq);\n\n\t/* drain it before proceeding with destruction */\n\tdrain_workqueue(wq);\n\n\t/* kill rescuer, if sanity checks fail, leave it w/o rescuer */\n\tif (wq->rescuer) {\n\t\tstruct worker *rescuer = wq->rescuer;\n\n\t\t/* this prevents new queueing */\n\t\traw_spin_lock_irq(&wq_mayday_lock);\n\t\twq->rescuer = NULL;\n\t\traw_spin_unlock_irq(&wq_mayday_lock);\n\n\t\t/* rescuer will empty maydays list before exiting */\n\t\tkthread_stop(rescuer->task);\n\t\tkfree(rescuer);\n\t}\n\n\t/*\n\t * Sanity checks - grab all the locks so that we wait for all\n\t * in-flight operations which may do put_pwq().\n\t */\n\tmutex_lock(&wq_pool_mutex);\n\tmutex_lock(&wq->mutex);\n\tfor_each_pwq(pwq, wq) {\n\t\traw_spin_lock_irq(&pwq->pool->lock);\n\t\tif (WARN_ON(pwq_busy(pwq))) {\n\t\t\tpr_warn(\"%s: %s has the following busy pwq\\n\",\n\t\t\t\t__func__, wq->name);\n\t\t\tshow_pwq(pwq);\n\t\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\t\t\tmutex_unlock(&wq->mutex);\n\t\t\tmutex_unlock(&wq_pool_mutex);\n\t\t\tshow_workqueue_state();\n\t\t\treturn;\n\t\t}\n\t\traw_spin_unlock_irq(&pwq->pool->lock);\n\t}\n\tmutex_unlock(&wq->mutex);\n\n\t/*\n\t * wq list is used to freeze wq, remove from list after\n\t * flushing is complete in case freeze races us.\n\t */\n\tlist_del_rcu(&wq->list);\n\tmutex_unlock(&wq_pool_mutex);\n\n\tif (!(wq->flags & WQ_UNBOUND)) {\n\t\twq_unregister_lockdep(wq);\n\t\t/*\n\t\t * The base ref is never dropped on per-cpu pwqs.  Directly\n\t\t * schedule RCU free.\n\t\t */\n\t\tcall_rcu(&wq->rcu, rcu_free_wq);\n\t} else {\n\t\t/*\n\t\t * We're the sole accessor of @wq at this point.  Directly\n\t\t * access numa_pwq_tbl[] and dfl_pwq to put the base refs.\n\t\t * @wq will be freed when the last pwq is released.\n\t\t */\n\t\tfor_each_node(node) {\n\t\t\tpwq = rcu_access_pointer(wq->numa_pwq_tbl[node]);\n\t\t\tRCU_INIT_POINTER(wq->numa_pwq_tbl[node], NULL);\n\t\t\tput_pwq_unlocked(pwq);\n\t\t}\n\n\t\t/*\n\t\t * Put dfl_pwq.  @wq may be freed any time after dfl_pwq is\n\t\t * put.  Don't access it afterwards.\n\t\t */\n\t\tpwq = wq->dfl_pwq;\n\t\twq->dfl_pwq = NULL;\n\t\tput_pwq_unlocked(pwq);\n\t}\n}\nEXPORT_SYMBOL_GPL(destroy_workqueue);\n\n/**\n * workqueue_set_max_active - adjust max_active of a workqueue\n * @wq: target workqueue\n * @max_active: new max_active value.\n *\n * Set max_active of @wq to @max_active.\n *\n * CONTEXT:\n * Don't call from IRQ context.\n */\nvoid workqueue_set_max_active(struct workqueue_struct *wq, int max_active)\n{\n\tstruct pool_workqueue *pwq;\n\n\t/* disallow meddling with max_active for ordered workqueues */\n\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))\n\t\treturn;\n\n\tmax_active = wq_clamp_max_active(max_active, wq->flags, wq->name);\n\n\tmutex_lock(&wq->mutex);\n\n\twq->flags &= ~__WQ_ORDERED;\n\twq->saved_max_active = max_active;\n\n\tfor_each_pwq(pwq, wq)\n\t\tpwq_adjust_max_active(pwq);\n\n\tmutex_unlock(&wq->mutex);\n}\nEXPORT_SYMBOL_GPL(workqueue_set_max_active);\n\n/**\n * current_work - retrieve %current task's work struct\n *\n * Determine if %current task is a workqueue worker and what it's working on.\n * Useful to find out the context that the %current task is running in.\n *\n * Return: work struct if %current task is a workqueue worker, %NULL otherwise.\n */\nstruct work_struct *current_work(void)\n{\n\tstruct worker *worker = current_wq_worker();\n\n\treturn worker ? worker->current_work : NULL;\n}\nEXPORT_SYMBOL(current_work);\n\n/**\n * current_is_workqueue_rescuer - is %current workqueue rescuer?\n *\n * Determine whether %current is a workqueue rescuer.  Can be used from\n * work functions to determine whether it's being run off the rescuer task.\n *\n * Return: %true if %current is a workqueue rescuer. %false otherwise.\n */\nbool current_is_workqueue_rescuer(void)\n{\n\tstruct worker *worker = current_wq_worker();\n\n\treturn worker && worker->rescue_wq;\n}\n\n/**\n * workqueue_congested - test whether a workqueue is congested\n * @cpu: CPU in question\n * @wq: target workqueue\n *\n * Test whether @wq's cpu workqueue for @cpu is congested.  There is\n * no synchronization around this function and the test result is\n * unreliable and only useful as advisory hints or for debugging.\n *\n * If @cpu is WORK_CPU_UNBOUND, the test is performed on the local CPU.\n * Note that both per-cpu and unbound workqueues may be associated with\n * multiple pool_workqueues which have separate congested states.  A\n * workqueue being congested on one CPU doesn't mean the workqueue is also\n * contested on other CPUs / NUMA nodes.\n *\n * Return:\n * %true if congested, %false otherwise.\n */\nbool workqueue_congested(int cpu, struct workqueue_struct *wq)\n{\n\tstruct pool_workqueue *pwq;\n\tbool ret;\n\n\trcu_read_lock();\n\tpreempt_disable();\n\n\tif (cpu == WORK_CPU_UNBOUND)\n\t\tcpu = smp_processor_id();\n\n\tif (!(wq->flags & WQ_UNBOUND))\n\t\tpwq = per_cpu_ptr(wq->cpu_pwqs, cpu);\n\telse\n\t\tpwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));\n\n\tret = !list_empty(&pwq->delayed_works);\n\tpreempt_enable();\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(workqueue_congested);\n\n/**\n * work_busy - test whether a work is currently pending or running\n * @work: the work to be tested\n *\n * Test whether @work is currently pending or running.  There is no\n * synchronization around this function and the test result is\n * unreliable and only useful as advisory hints or for debugging.\n *\n * Return:\n * OR'd bitmask of WORK_BUSY_* bits.\n */\nunsigned int work_busy(struct work_struct *work)\n{\n\tstruct worker_pool *pool;\n\tunsigned long flags;\n\tunsigned int ret = 0;\n\n\tif (work_pending(work))\n\t\tret |= WORK_BUSY_PENDING;\n\n\trcu_read_lock();\n\tpool = get_work_pool(work);\n\tif (pool) {\n\t\traw_spin_lock_irqsave(&pool->lock, flags);\n\t\tif (find_worker_executing_work(pool, work))\n\t\t\tret |= WORK_BUSY_RUNNING;\n\t\traw_spin_unlock_irqrestore(&pool->lock, flags);\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(work_busy);\n\n/**\n * set_worker_desc - set description for the current work item\n * @fmt: printf-style format string\n * @...: arguments for the format string\n *\n * This function can be called by a running work function to describe what\n * the work item is about.  If the worker task gets dumped, this\n * information will be printed out together to help debugging.  The\n * description can be at most WORKER_DESC_LEN including the trailing '\\0'.\n */\nvoid set_worker_desc(const char *fmt, ...)\n{\n\tstruct worker *worker = current_wq_worker();\n\tva_list args;\n\n\tif (worker) {\n\t\tva_start(args, fmt);\n\t\tvsnprintf(worker->desc, sizeof(worker->desc), fmt, args);\n\t\tva_end(args);\n\t}\n}\nEXPORT_SYMBOL_GPL(set_worker_desc);\n\n/**\n * print_worker_info - print out worker information and description\n * @log_lvl: the log level to use when printing\n * @task: target task\n *\n * If @task is a worker and currently executing a work item, print out the\n * name of the workqueue being serviced and worker description set with\n * set_worker_desc() by the currently executing work item.\n *\n * This function can be safely called on any task as long as the\n * task_struct itself is accessible.  While safe, this function isn't\n * synchronized and may print out mixups or garbages of limited length.\n */\nvoid print_worker_info(const char *log_lvl, struct task_struct *task)\n{\n\twork_func_t *fn = NULL;\n\tchar name[WQ_NAME_LEN] = { };\n\tchar desc[WORKER_DESC_LEN] = { };\n\tstruct pool_workqueue *pwq = NULL;\n\tstruct workqueue_struct *wq = NULL;\n\tstruct worker *worker;\n\n\tif (!(task->flags & PF_WQ_WORKER))\n\t\treturn;\n\n\t/*\n\t * This function is called without any synchronization and @task\n\t * could be in any state.  Be careful with dereferences.\n\t */\n\tworker = kthread_probe_data(task);\n\n\t/*\n\t * Carefully copy the associated workqueue's workfn, name and desc.\n\t * Keep the original last '\\0' in case the original is garbage.\n\t */\n\tcopy_from_kernel_nofault(&fn, &worker->current_func, sizeof(fn));\n\tcopy_from_kernel_nofault(&pwq, &worker->current_pwq, sizeof(pwq));\n\tcopy_from_kernel_nofault(&wq, &pwq->wq, sizeof(wq));\n\tcopy_from_kernel_nofault(name, wq->name, sizeof(name) - 1);\n\tcopy_from_kernel_nofault(desc, worker->desc, sizeof(desc) - 1);\n\n\tif (fn || name[0] || desc[0]) {\n\t\tprintk(\"%sWorkqueue: %s %ps\", log_lvl, name, fn);\n\t\tif (strcmp(name, desc))\n\t\t\tpr_cont(\" (%s)\", desc);\n\t\tpr_cont(\"\\n\");\n\t}\n}\n\nstatic void pr_cont_pool_info(struct worker_pool *pool)\n{\n\tpr_cont(\" cpus=%*pbl\", nr_cpumask_bits, pool->attrs->cpumask);\n\tif (pool->node != NUMA_NO_NODE)\n\t\tpr_cont(\" node=%d\", pool->node);\n\tpr_cont(\" flags=0x%x nice=%d\", pool->flags, pool->attrs->nice);\n}\n\nstatic void pr_cont_work(bool comma, struct work_struct *work)\n{\n\tif (work->func == wq_barrier_func) {\n\t\tstruct wq_barrier *barr;\n\n\t\tbarr = container_of(work, struct wq_barrier, work);\n\n\t\tpr_cont(\"%s BAR(%d)\", comma ? \",\" : \"\",\n\t\t\ttask_pid_nr(barr->task));\n\t} else {\n\t\tpr_cont(\"%s %ps\", comma ? \",\" : \"\", work->func);\n\t}\n}\n\nstatic void show_pwq(struct pool_workqueue *pwq)\n{\n\tstruct worker_pool *pool = pwq->pool;\n\tstruct work_struct *work;\n\tstruct worker *worker;\n\tbool has_in_flight = false, has_pending = false;\n\tint bkt;\n\n\tpr_info(\"  pwq %d:\", pool->id);\n\tpr_cont_pool_info(pool);\n\n\tpr_cont(\" active=%d/%d refcnt=%d%s\\n\",\n\t\tpwq->nr_active, pwq->max_active, pwq->refcnt,\n\t\t!list_empty(&pwq->mayday_node) ? \" MAYDAY\" : \"\");\n\n\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\n\t\tif (worker->current_pwq == pwq) {\n\t\t\thas_in_flight = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (has_in_flight) {\n\t\tbool comma = false;\n\n\t\tpr_info(\"    in-flight:\");\n\t\thash_for_each(pool->busy_hash, bkt, worker, hentry) {\n\t\t\tif (worker->current_pwq != pwq)\n\t\t\t\tcontinue;\n\n\t\t\tpr_cont(\"%s %d%s:%ps\", comma ? \",\" : \"\",\n\t\t\t\ttask_pid_nr(worker->task),\n\t\t\t\tworker->rescue_wq ? \"(RESCUER)\" : \"\",\n\t\t\t\tworker->current_func);\n\t\t\tlist_for_each_entry(work, &worker->scheduled, entry)\n\t\t\t\tpr_cont_work(false, work);\n\t\t\tcomma = true;\n\t\t}\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tlist_for_each_entry(work, &pool->worklist, entry) {\n\t\tif (get_work_pwq(work) == pwq) {\n\t\t\thas_pending = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (has_pending) {\n\t\tbool comma = false;\n\n\t\tpr_info(\"    pending:\");\n\t\tlist_for_each_entry(work, &pool->worklist, entry) {\n\t\t\tif (get_work_pwq(work) != pwq)\n\t\t\t\tcontinue;\n\n\t\t\tpr_cont_work(comma, work);\n\t\t\tcomma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);\n\t\t}\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tif (!list_empty(&pwq->delayed_works)) {\n\t\tbool comma = false;\n\n\t\tpr_info(\"    delayed:\");\n\t\tlist_for_each_entry(work, &pwq->delayed_works, entry) {\n\t\t\tpr_cont_work(comma, work);\n\t\t\tcomma = !(*work_data_bits(work) & WORK_STRUCT_LINKED);\n\t\t}\n\t\tpr_cont(\"\\n\");\n\t}\n}\n\n/**\n * show_workqueue_state - dump workqueue state\n *\n * Called from a sysrq handler or try_to_freeze_tasks() and prints out\n * all busy workqueues and pools.\n */\nvoid show_workqueue_state(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct worker_pool *pool;\n\tunsigned long flags;\n\tint pi;\n\n\trcu_read_lock();\n\n\tpr_info(\"Showing busy workqueues and worker pools:\\n\");\n\n\tlist_for_each_entry_rcu(wq, &workqueues, list) {\n\t\tstruct pool_workqueue *pwq;\n\t\tbool idle = true;\n\n\t\tfor_each_pwq(pwq, wq) {\n\t\t\tif (pwq->nr_active || !list_empty(&pwq->delayed_works)) {\n\t\t\t\tidle = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (idle)\n\t\t\tcontinue;\n\n\t\tpr_info(\"workqueue %s: flags=0x%x\\n\", wq->name, wq->flags);\n\n\t\tfor_each_pwq(pwq, wq) {\n\t\t\traw_spin_lock_irqsave(&pwq->pool->lock, flags);\n\t\t\tif (pwq->nr_active || !list_empty(&pwq->delayed_works))\n\t\t\t\tshow_pwq(pwq);\n\t\t\traw_spin_unlock_irqrestore(&pwq->pool->lock, flags);\n\t\t\t/*\n\t\t\t * We could be printing a lot from atomic context, e.g.\n\t\t\t * sysrq-t -> show_workqueue_state(). Avoid triggering\n\t\t\t * hard lockup.\n\t\t\t */\n\t\t\ttouch_nmi_watchdog();\n\t\t}\n\t}\n\n\tfor_each_pool(pool, pi) {\n\t\tstruct worker *worker;\n\t\tbool first = true;\n\n\t\traw_spin_lock_irqsave(&pool->lock, flags);\n\t\tif (pool->nr_workers == pool->nr_idle)\n\t\t\tgoto next_pool;\n\n\t\tpr_info(\"pool %d:\", pool->id);\n\t\tpr_cont_pool_info(pool);\n\t\tpr_cont(\" hung=%us workers=%d\",\n\t\t\tjiffies_to_msecs(jiffies - pool->watchdog_ts) / 1000,\n\t\t\tpool->nr_workers);\n\t\tif (pool->manager)\n\t\t\tpr_cont(\" manager: %d\",\n\t\t\t\ttask_pid_nr(pool->manager->task));\n\t\tlist_for_each_entry(worker, &pool->idle_list, entry) {\n\t\t\tpr_cont(\" %s%d\", first ? \"idle: \" : \"\",\n\t\t\t\ttask_pid_nr(worker->task));\n\t\t\tfirst = false;\n\t\t}\n\t\tpr_cont(\"\\n\");\n\tnext_pool:\n\t\traw_spin_unlock_irqrestore(&pool->lock, flags);\n\t\t/*\n\t\t * We could be printing a lot from atomic context, e.g.\n\t\t * sysrq-t -> show_workqueue_state(). Avoid triggering\n\t\t * hard lockup.\n\t\t */\n\t\ttouch_nmi_watchdog();\n\t}\n\n\trcu_read_unlock();\n}\n\n/* used to show worker information through /proc/PID/{comm,stat,status} */\nvoid wq_worker_comm(char *buf, size_t size, struct task_struct *task)\n{\n\tint off;\n\n\t/* always show the actual comm */\n\toff = strscpy(buf, task->comm, size);\n\tif (off < 0)\n\t\treturn;\n\n\t/* stabilize PF_WQ_WORKER and worker pool association */\n\tmutex_lock(&wq_pool_attach_mutex);\n\n\tif (task->flags & PF_WQ_WORKER) {\n\t\tstruct worker *worker = kthread_data(task);\n\t\tstruct worker_pool *pool = worker->pool;\n\n\t\tif (pool) {\n\t\t\traw_spin_lock_irq(&pool->lock);\n\t\t\t/*\n\t\t\t * ->desc tracks information (wq name or\n\t\t\t * set_worker_desc()) for the latest execution.  If\n\t\t\t * current, prepend '+', otherwise '-'.\n\t\t\t */\n\t\t\tif (worker->desc[0] != '\\0') {\n\t\t\t\tif (worker->current_work)\n\t\t\t\t\tscnprintf(buf + off, size - off, \"+%s\",\n\t\t\t\t\t\t  worker->desc);\n\t\t\t\telse\n\t\t\t\t\tscnprintf(buf + off, size - off, \"-%s\",\n\t\t\t\t\t\t  worker->desc);\n\t\t\t}\n\t\t\traw_spin_unlock_irq(&pool->lock);\n\t\t}\n\t}\n\n\tmutex_unlock(&wq_pool_attach_mutex);\n}\n\n#ifdef CONFIG_SMP\n\n/*\n * CPU hotplug.\n *\n * There are two challenges in supporting CPU hotplug.  Firstly, there\n * are a lot of assumptions on strong associations among work, pwq and\n * pool which make migrating pending and scheduled works very\n * difficult to implement without impacting hot paths.  Secondly,\n * worker pools serve mix of short, long and very long running works making\n * blocked draining impractical.\n *\n * This is solved by allowing the pools to be disassociated from the CPU\n * running as an unbound one and allowing it to be reattached later if the\n * cpu comes back online.\n */\n\nstatic void unbind_workers(int cpu)\n{\n\tstruct worker_pool *pool;\n\tstruct worker *worker;\n\n\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\tmutex_lock(&wq_pool_attach_mutex);\n\t\traw_spin_lock_irq(&pool->lock);\n\n\t\t/*\n\t\t * We've blocked all attach/detach operations. Make all workers\n\t\t * unbound and set DISASSOCIATED.  Before this, all workers\n\t\t * except for the ones which are still executing works from\n\t\t * before the last CPU down must be on the cpu.  After\n\t\t * this, they may become diasporas.\n\t\t */\n\t\tfor_each_pool_worker(worker, pool)\n\t\t\tworker->flags |= WORKER_UNBOUND;\n\n\t\tpool->flags |= POOL_DISASSOCIATED;\n\n\t\traw_spin_unlock_irq(&pool->lock);\n\n\t\tfor_each_pool_worker(worker, pool)\n\t\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, cpu_active_mask) < 0);\n\n\t\tmutex_unlock(&wq_pool_attach_mutex);\n\n\t\t/*\n\t\t * Call schedule() so that we cross rq->lock and thus can\n\t\t * guarantee sched callbacks see the %WORKER_UNBOUND flag.\n\t\t * This is necessary as scheduler callbacks may be invoked\n\t\t * from other cpus.\n\t\t */\n\t\tschedule();\n\n\t\t/*\n\t\t * Sched callbacks are disabled now.  Zap nr_running.\n\t\t * After this, nr_running stays zero and need_more_worker()\n\t\t * and keep_working() are always true as long as the\n\t\t * worklist is not empty.  This pool now behaves as an\n\t\t * unbound (in terms of concurrency management) pool which\n\t\t * are served by workers tied to the pool.\n\t\t */\n\t\tatomic_set(&pool->nr_running, 0);\n\n\t\t/*\n\t\t * With concurrency management just turned off, a busy\n\t\t * worker blocking could lead to lengthy stalls.  Kick off\n\t\t * unbound chain execution of currently pending work items.\n\t\t */\n\t\traw_spin_lock_irq(&pool->lock);\n\t\twake_up_worker(pool);\n\t\traw_spin_unlock_irq(&pool->lock);\n\t}\n}\n\n/**\n * rebind_workers - rebind all workers of a pool to the associated CPU\n * @pool: pool of interest\n *\n * @pool->cpu is coming online.  Rebind all workers to the CPU.\n */\nstatic void rebind_workers(struct worker_pool *pool)\n{\n\tstruct worker *worker;\n\n\tlockdep_assert_held(&wq_pool_attach_mutex);\n\n\t/*\n\t * Restore CPU affinity of all workers.  As all idle workers should\n\t * be on the run-queue of the associated CPU before any local\n\t * wake-ups for concurrency management happen, restore CPU affinity\n\t * of all workers first and then clear UNBOUND.  As we're called\n\t * from CPU_ONLINE, the following shouldn't fail.\n\t */\n\tfor_each_pool_worker(worker, pool)\n\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task,\n\t\t\t\t\t\t  pool->attrs->cpumask) < 0);\n\n\traw_spin_lock_irq(&pool->lock);\n\n\tpool->flags &= ~POOL_DISASSOCIATED;\n\n\tfor_each_pool_worker(worker, pool) {\n\t\tunsigned int worker_flags = worker->flags;\n\n\t\t/*\n\t\t * A bound idle worker should actually be on the runqueue\n\t\t * of the associated CPU for local wake-ups targeting it to\n\t\t * work.  Kick all idle workers so that they migrate to the\n\t\t * associated CPU.  Doing this in the same loop as\n\t\t * replacing UNBOUND with REBOUND is safe as no worker will\n\t\t * be bound before @pool->lock is released.\n\t\t */\n\t\tif (worker_flags & WORKER_IDLE)\n\t\t\twake_up_process(worker->task);\n\n\t\t/*\n\t\t * We want to clear UNBOUND but can't directly call\n\t\t * worker_clr_flags() or adjust nr_running.  Atomically\n\t\t * replace UNBOUND with another NOT_RUNNING flag REBOUND.\n\t\t * @worker will clear REBOUND using worker_clr_flags() when\n\t\t * it initiates the next execution cycle thus restoring\n\t\t * concurrency management.  Note that when or whether\n\t\t * @worker clears REBOUND doesn't affect correctness.\n\t\t *\n\t\t * WRITE_ONCE() is necessary because @worker->flags may be\n\t\t * tested without holding any lock in\n\t\t * wq_worker_running().  Without it, NOT_RUNNING test may\n\t\t * fail incorrectly leading to premature concurrency\n\t\t * management operations.\n\t\t */\n\t\tWARN_ON_ONCE(!(worker_flags & WORKER_UNBOUND));\n\t\tworker_flags |= WORKER_REBOUND;\n\t\tworker_flags &= ~WORKER_UNBOUND;\n\t\tWRITE_ONCE(worker->flags, worker_flags);\n\t}\n\n\traw_spin_unlock_irq(&pool->lock);\n}\n\n/**\n * restore_unbound_workers_cpumask - restore cpumask of unbound workers\n * @pool: unbound pool of interest\n * @cpu: the CPU which is coming up\n *\n * An unbound pool may end up with a cpumask which doesn't have any online\n * CPUs.  When a worker of such pool get scheduled, the scheduler resets\n * its cpus_allowed.  If @cpu is in @pool's cpumask which didn't have any\n * online CPU before, cpus_allowed of all its workers should be restored.\n */\nstatic void restore_unbound_workers_cpumask(struct worker_pool *pool, int cpu)\n{\n\tstatic cpumask_t cpumask;\n\tstruct worker *worker;\n\n\tlockdep_assert_held(&wq_pool_attach_mutex);\n\n\t/* is @cpu allowed for @pool? */\n\tif (!cpumask_test_cpu(cpu, pool->attrs->cpumask))\n\t\treturn;\n\n\tcpumask_and(&cpumask, pool->attrs->cpumask, cpu_online_mask);\n\n\t/* as we're called from CPU_ONLINE, the following shouldn't fail */\n\tfor_each_pool_worker(worker, pool)\n\t\tWARN_ON_ONCE(set_cpus_allowed_ptr(worker->task, &cpumask) < 0);\n}\n\nint workqueue_prepare_cpu(unsigned int cpu)\n{\n\tstruct worker_pool *pool;\n\n\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\tif (pool->nr_workers)\n\t\t\tcontinue;\n\t\tif (!create_worker(pool))\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nint workqueue_online_cpu(unsigned int cpu)\n{\n\tstruct worker_pool *pool;\n\tstruct workqueue_struct *wq;\n\tint pi;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tfor_each_pool(pool, pi) {\n\t\tmutex_lock(&wq_pool_attach_mutex);\n\n\t\tif (pool->cpu == cpu)\n\t\t\trebind_workers(pool);\n\t\telse if (pool->cpu < 0)\n\t\t\trestore_unbound_workers_cpumask(pool, cpu);\n\n\t\tmutex_unlock(&wq_pool_attach_mutex);\n\t}\n\n\t/* update NUMA affinity of unbound workqueues */\n\tlist_for_each_entry(wq, &workqueues, list)\n\t\twq_update_unbound_numa(wq, cpu, true);\n\n\tmutex_unlock(&wq_pool_mutex);\n\treturn 0;\n}\n\nint workqueue_offline_cpu(unsigned int cpu)\n{\n\tstruct workqueue_struct *wq;\n\n\t/* unbinding per-cpu workers should happen on the local CPU */\n\tif (WARN_ON(cpu != smp_processor_id()))\n\t\treturn -1;\n\n\tunbind_workers(cpu);\n\n\t/* update NUMA affinity of unbound workqueues */\n\tmutex_lock(&wq_pool_mutex);\n\tlist_for_each_entry(wq, &workqueues, list)\n\t\twq_update_unbound_numa(wq, cpu, false);\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn 0;\n}\n\nstruct work_for_cpu {\n\tstruct work_struct work;\n\tlong (*fn)(void *);\n\tvoid *arg;\n\tlong ret;\n};\n\nstatic void work_for_cpu_fn(struct work_struct *work)\n{\n\tstruct work_for_cpu *wfc = container_of(work, struct work_for_cpu, work);\n\n\twfc->ret = wfc->fn(wfc->arg);\n}\n\n/**\n * work_on_cpu - run a function in thread context on a particular cpu\n * @cpu: the cpu to run on\n * @fn: the function to run\n * @arg: the function arg\n *\n * It is up to the caller to ensure that the cpu doesn't go offline.\n * The caller must not hold any locks which would prevent @fn from completing.\n *\n * Return: The value @fn returns.\n */\nlong work_on_cpu(int cpu, long (*fn)(void *), void *arg)\n{\n\tstruct work_for_cpu wfc = { .fn = fn, .arg = arg };\n\n\tINIT_WORK_ONSTACK(&wfc.work, work_for_cpu_fn);\n\tschedule_work_on(cpu, &wfc.work);\n\tflush_work(&wfc.work);\n\tdestroy_work_on_stack(&wfc.work);\n\treturn wfc.ret;\n}\nEXPORT_SYMBOL_GPL(work_on_cpu);\n\n/**\n * work_on_cpu_safe - run a function in thread context on a particular cpu\n * @cpu: the cpu to run on\n * @fn:  the function to run\n * @arg: the function argument\n *\n * Disables CPU hotplug and calls work_on_cpu(). The caller must not hold\n * any locks which would prevent @fn from completing.\n *\n * Return: The value @fn returns.\n */\nlong work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg)\n{\n\tlong ret = -ENODEV;\n\n\tget_online_cpus();\n\tif (cpu_online(cpu))\n\t\tret = work_on_cpu(cpu, fn, arg);\n\tput_online_cpus();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(work_on_cpu_safe);\n#endif /* CONFIG_SMP */\n\n#ifdef CONFIG_FREEZER\n\n/**\n * freeze_workqueues_begin - begin freezing workqueues\n *\n * Start freezing workqueues.  After this function returns, all freezable\n * workqueues will queue new works to their delayed_works list instead of\n * pool->worklist.\n *\n * CONTEXT:\n * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's.\n */\nvoid freeze_workqueues_begin(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tWARN_ON_ONCE(workqueue_freezing);\n\tworkqueue_freezing = true;\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tmutex_lock(&wq->mutex);\n\t\tfor_each_pwq(pwq, wq)\n\t\t\tpwq_adjust_max_active(pwq);\n\t\tmutex_unlock(&wq->mutex);\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n}\n\n/**\n * freeze_workqueues_busy - are freezable workqueues still busy?\n *\n * Check whether freezing is complete.  This function must be called\n * between freeze_workqueues_begin() and thaw_workqueues().\n *\n * CONTEXT:\n * Grabs and releases wq_pool_mutex.\n *\n * Return:\n * %true if some freezable workqueues are still busy.  %false if freezing\n * is complete.\n */\nbool freeze_workqueues_busy(void)\n{\n\tbool busy = false;\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tWARN_ON_ONCE(!workqueue_freezing);\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tif (!(wq->flags & WQ_FREEZABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * nr_active is monotonically decreasing.  It's safe\n\t\t * to peek without lock.\n\t\t */\n\t\trcu_read_lock();\n\t\tfor_each_pwq(pwq, wq) {\n\t\t\tWARN_ON_ONCE(pwq->nr_active < 0);\n\t\t\tif (pwq->nr_active) {\n\t\t\t\tbusy = true;\n\t\t\t\trcu_read_unlock();\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\nout_unlock:\n\tmutex_unlock(&wq_pool_mutex);\n\treturn busy;\n}\n\n/**\n * thaw_workqueues - thaw workqueues\n *\n * Thaw workqueues.  Normal queueing is restored and all collected\n * frozen works are transferred to their respective pool worklists.\n *\n * CONTEXT:\n * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's.\n */\nvoid thaw_workqueues(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct pool_workqueue *pwq;\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tif (!workqueue_freezing)\n\t\tgoto out_unlock;\n\n\tworkqueue_freezing = false;\n\n\t/* restore max_active and repopulate worklist */\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tmutex_lock(&wq->mutex);\n\t\tfor_each_pwq(pwq, wq)\n\t\t\tpwq_adjust_max_active(pwq);\n\t\tmutex_unlock(&wq->mutex);\n\t}\n\nout_unlock:\n\tmutex_unlock(&wq_pool_mutex);\n}\n#endif /* CONFIG_FREEZER */\n\nstatic int workqueue_apply_unbound_cpumask(void)\n{\n\tLIST_HEAD(ctxs);\n\tint ret = 0;\n\tstruct workqueue_struct *wq;\n\tstruct apply_wqattrs_ctx *ctx, *n;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\tif (!(wq->flags & WQ_UNBOUND))\n\t\t\tcontinue;\n\t\t/* creating multiple pwqs breaks ordering guarantee */\n\t\tif (wq->flags & __WQ_ORDERED)\n\t\t\tcontinue;\n\n\t\tctx = apply_wqattrs_prepare(wq, wq->unbound_attrs);\n\t\tif (!ctx) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add_tail(&ctx->list, &ctxs);\n\t}\n\n\tlist_for_each_entry_safe(ctx, n, &ctxs, list) {\n\t\tif (!ret)\n\t\t\tapply_wqattrs_commit(ctx);\n\t\tapply_wqattrs_cleanup(ctx);\n\t}\n\n\treturn ret;\n}\n\n/**\n *  workqueue_set_unbound_cpumask - Set the low-level unbound cpumask\n *  @cpumask: the cpumask to set\n *\n *  The low-level workqueues cpumask is a global cpumask that limits\n *  the affinity of all unbound workqueues.  This function check the @cpumask\n *  and apply it to all unbound workqueues and updates all pwqs of them.\n *\n *  Retun:\t0\t- Success\n *  \t\t-EINVAL\t- Invalid @cpumask\n *  \t\t-ENOMEM\t- Failed to allocate memory for attrs or pwqs.\n */\nint workqueue_set_unbound_cpumask(cpumask_var_t cpumask)\n{\n\tint ret = -EINVAL;\n\tcpumask_var_t saved_cpumask;\n\n\tif (!zalloc_cpumask_var(&saved_cpumask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Not excluding isolated cpus on purpose.\n\t * If the user wishes to include them, we allow that.\n\t */\n\tcpumask_and(cpumask, cpumask, cpu_possible_mask);\n\tif (!cpumask_empty(cpumask)) {\n\t\tapply_wqattrs_lock();\n\n\t\t/* save the old wq_unbound_cpumask. */\n\t\tcpumask_copy(saved_cpumask, wq_unbound_cpumask);\n\n\t\t/* update wq_unbound_cpumask at first and apply it to wqs. */\n\t\tcpumask_copy(wq_unbound_cpumask, cpumask);\n\t\tret = workqueue_apply_unbound_cpumask();\n\n\t\t/* restore the wq_unbound_cpumask when failed. */\n\t\tif (ret < 0)\n\t\t\tcpumask_copy(wq_unbound_cpumask, saved_cpumask);\n\n\t\tapply_wqattrs_unlock();\n\t}\n\n\tfree_cpumask_var(saved_cpumask);\n\treturn ret;\n}\n\n#ifdef CONFIG_SYSFS\n/*\n * Workqueues with WQ_SYSFS flag set is visible to userland via\n * /sys/bus/workqueue/devices/WQ_NAME.  All visible workqueues have the\n * following attributes.\n *\n *  per_cpu\tRO bool\t: whether the workqueue is per-cpu or unbound\n *  max_active\tRW int\t: maximum number of in-flight work items\n *\n * Unbound workqueues have the following extra attributes.\n *\n *  pool_ids\tRO int\t: the associated pool IDs for each node\n *  nice\tRW int\t: nice value of the workers\n *  cpumask\tRW mask\t: bitmask of allowed CPUs for the workers\n *  numa\tRW bool\t: whether enable NUMA affinity\n */\nstruct wq_device {\n\tstruct workqueue_struct\t\t*wq;\n\tstruct device\t\t\tdev;\n};\n\nstatic struct workqueue_struct *dev_to_wq(struct device *dev)\n{\n\tstruct wq_device *wq_dev = container_of(dev, struct wq_device, dev);\n\n\treturn wq_dev->wq;\n}\n\nstatic ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", (bool)!(wq->flags & WQ_UNBOUND));\n}\nstatic DEVICE_ATTR_RO(per_cpu);\n\nstatic ssize_t max_active_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\n\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->saved_max_active);\n}\n\nstatic ssize_t max_active_store(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, const char *buf,\n\t\t\t\tsize_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint val;\n\n\tif (sscanf(buf, \"%d\", &val) != 1 || val <= 0)\n\t\treturn -EINVAL;\n\n\tworkqueue_set_max_active(wq, val);\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(max_active);\n\nstatic struct attribute *wq_sysfs_attrs[] = {\n\t&dev_attr_per_cpu.attr,\n\t&dev_attr_max_active.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(wq_sysfs);\n\nstatic ssize_t wq_pool_ids_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tconst char *delim = \"\";\n\tint node, written = 0;\n\n\tget_online_cpus();\n\trcu_read_lock();\n\tfor_each_node(node) {\n\t\twritten += scnprintf(buf + written, PAGE_SIZE - written,\n\t\t\t\t     \"%s%d:%d\", delim, node,\n\t\t\t\t     unbound_pwq_by_node(wq, node)->pool->id);\n\t\tdelim = \" \";\n\t}\n\twritten += scnprintf(buf + written, PAGE_SIZE - written, \"\\n\");\n\trcu_read_unlock();\n\tput_online_cpus();\n\n\treturn written;\n}\n\nstatic ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint written;\n\n\tmutex_lock(&wq->mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->unbound_attrs->nice);\n\tmutex_unlock(&wq->mutex);\n\n\treturn written;\n}\n\n/* prepare workqueue_attrs for sysfs store operations */\nstatic struct workqueue_attrs *wq_sysfs_prep_attrs(struct workqueue_struct *wq)\n{\n\tstruct workqueue_attrs *attrs;\n\n\tlockdep_assert_held(&wq_pool_mutex);\n\n\tattrs = alloc_workqueue_attrs();\n\tif (!attrs)\n\t\treturn NULL;\n\n\tcopy_workqueue_attrs(attrs, wq->unbound_attrs);\n\treturn attrs;\n}\n\nstatic ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint ret = -ENOMEM;\n\n\tapply_wqattrs_lock();\n\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (!attrs)\n\t\tgoto out_unlock;\n\n\tif (sscanf(buf, \"%d\", &attrs->nice) == 1 &&\n\t    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\telse\n\t\tret = -EINVAL;\n\nout_unlock:\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic ssize_t wq_cpumask_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint written;\n\n\tmutex_lock(&wq->mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\",\n\t\t\t    cpumask_pr_args(wq->unbound_attrs->cpumask));\n\tmutex_unlock(&wq->mutex);\n\treturn written;\n}\n\nstatic ssize_t wq_cpumask_store(struct device *dev,\n\t\t\t\tstruct device_attribute *attr,\n\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint ret = -ENOMEM;\n\n\tapply_wqattrs_lock();\n\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (!attrs)\n\t\tgoto out_unlock;\n\n\tret = cpumask_parse(buf, attrs->cpumask);\n\tif (!ret)\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\nout_unlock:\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic ssize_t wq_numa_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tint written;\n\n\tmutex_lock(&wq->mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t\t    !wq->unbound_attrs->no_numa);\n\tmutex_unlock(&wq->mutex);\n\n\treturn written;\n}\n\nstatic ssize_t wq_numa_store(struct device *dev, struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tstruct workqueue_struct *wq = dev_to_wq(dev);\n\tstruct workqueue_attrs *attrs;\n\tint v, ret = -ENOMEM;\n\n\tapply_wqattrs_lock();\n\n\tattrs = wq_sysfs_prep_attrs(wq);\n\tif (!attrs)\n\t\tgoto out_unlock;\n\n\tret = -EINVAL;\n\tif (sscanf(buf, \"%d\", &v) == 1) {\n\t\tattrs->no_numa = !v;\n\t\tret = apply_workqueue_attrs_locked(wq, attrs);\n\t}\n\nout_unlock:\n\tapply_wqattrs_unlock();\n\tfree_workqueue_attrs(attrs);\n\treturn ret ?: count;\n}\n\nstatic struct device_attribute wq_sysfs_unbound_attrs[] = {\n\t__ATTR(pool_ids, 0444, wq_pool_ids_show, NULL),\n\t__ATTR(nice, 0644, wq_nice_show, wq_nice_store),\n\t__ATTR(cpumask, 0644, wq_cpumask_show, wq_cpumask_store),\n\t__ATTR(numa, 0644, wq_numa_show, wq_numa_store),\n\t__ATTR_NULL,\n};\n\nstatic struct bus_type wq_subsys = {\n\t.name\t\t\t\t= \"workqueue\",\n\t.dev_groups\t\t\t= wq_sysfs_groups,\n};\n\nstatic ssize_t wq_unbound_cpumask_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tint written;\n\n\tmutex_lock(&wq_pool_mutex);\n\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\",\n\t\t\t    cpumask_pr_args(wq_unbound_cpumask));\n\tmutex_unlock(&wq_pool_mutex);\n\n\treturn written;\n}\n\nstatic ssize_t wq_unbound_cpumask_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tcpumask_var_t cpumask;\n\tint ret;\n\n\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = cpumask_parse(buf, cpumask);\n\tif (!ret)\n\t\tret = workqueue_set_unbound_cpumask(cpumask);\n\n\tfree_cpumask_var(cpumask);\n\treturn ret ? ret : count;\n}\n\nstatic struct device_attribute wq_sysfs_cpumask_attr =\n\t__ATTR(cpumask, 0644, wq_unbound_cpumask_show,\n\t       wq_unbound_cpumask_store);\n\nstatic int __init wq_sysfs_init(void)\n{\n\tint err;\n\n\terr = subsys_virtual_register(&wq_subsys, NULL);\n\tif (err)\n\t\treturn err;\n\n\treturn device_create_file(wq_subsys.dev_root, &wq_sysfs_cpumask_attr);\n}\ncore_initcall(wq_sysfs_init);\n\nstatic void wq_device_release(struct device *dev)\n{\n\tstruct wq_device *wq_dev = container_of(dev, struct wq_device, dev);\n\n\tkfree(wq_dev);\n}\n\n/**\n * workqueue_sysfs_register - make a workqueue visible in sysfs\n * @wq: the workqueue to register\n *\n * Expose @wq in sysfs under /sys/bus/workqueue/devices.\n * alloc_workqueue*() automatically calls this function if WQ_SYSFS is set\n * which is the preferred method.\n *\n * Workqueue user should use this function directly iff it wants to apply\n * workqueue_attrs before making the workqueue visible in sysfs; otherwise,\n * apply_workqueue_attrs() may race against userland updating the\n * attributes.\n *\n * Return: 0 on success, -errno on failure.\n */\nint workqueue_sysfs_register(struct workqueue_struct *wq)\n{\n\tstruct wq_device *wq_dev;\n\tint ret;\n\n\t/*\n\t * Adjusting max_active or creating new pwqs by applying\n\t * attributes breaks ordering guarantee.  Disallow exposing ordered\n\t * workqueues.\n\t */\n\tif (WARN_ON(wq->flags & __WQ_ORDERED_EXPLICIT))\n\t\treturn -EINVAL;\n\n\twq->wq_dev = wq_dev = kzalloc(sizeof(*wq_dev), GFP_KERNEL);\n\tif (!wq_dev)\n\t\treturn -ENOMEM;\n\n\twq_dev->wq = wq;\n\twq_dev->dev.bus = &wq_subsys;\n\twq_dev->dev.release = wq_device_release;\n\tdev_set_name(&wq_dev->dev, \"%s\", wq->name);\n\n\t/*\n\t * unbound_attrs are created separately.  Suppress uevent until\n\t * everything is ready.\n\t */\n\tdev_set_uevent_suppress(&wq_dev->dev, true);\n\n\tret = device_register(&wq_dev->dev);\n\tif (ret) {\n\t\tput_device(&wq_dev->dev);\n\t\twq->wq_dev = NULL;\n\t\treturn ret;\n\t}\n\n\tif (wq->flags & WQ_UNBOUND) {\n\t\tstruct device_attribute *attr;\n\n\t\tfor (attr = wq_sysfs_unbound_attrs; attr->attr.name; attr++) {\n\t\t\tret = device_create_file(&wq_dev->dev, attr);\n\t\t\tif (ret) {\n\t\t\t\tdevice_unregister(&wq_dev->dev);\n\t\t\t\twq->wq_dev = NULL;\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_set_uevent_suppress(&wq_dev->dev, false);\n\tkobject_uevent(&wq_dev->dev.kobj, KOBJ_ADD);\n\treturn 0;\n}\n\n/**\n * workqueue_sysfs_unregister - undo workqueue_sysfs_register()\n * @wq: the workqueue to unregister\n *\n * If @wq is registered to sysfs by workqueue_sysfs_register(), unregister.\n */\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq)\n{\n\tstruct wq_device *wq_dev = wq->wq_dev;\n\n\tif (!wq->wq_dev)\n\t\treturn;\n\n\twq->wq_dev = NULL;\n\tdevice_unregister(&wq_dev->dev);\n}\n#else\t/* CONFIG_SYSFS */\nstatic void workqueue_sysfs_unregister(struct workqueue_struct *wq)\t{ }\n#endif\t/* CONFIG_SYSFS */\n\n/*\n * Workqueue watchdog.\n *\n * Stall may be caused by various bugs - missing WQ_MEM_RECLAIM, illegal\n * flush dependency, a concurrency managed work item which stays RUNNING\n * indefinitely.  Workqueue stalls can be very difficult to debug as the\n * usual warning mechanisms don't trigger and internal workqueue state is\n * largely opaque.\n *\n * Workqueue watchdog monitors all worker pools periodically and dumps\n * state if some pools failed to make forward progress for a while where\n * forward progress is defined as the first item on ->worklist changing.\n *\n * This mechanism is controlled through the kernel parameter\n * \"workqueue.watchdog_thresh\" which can be updated at runtime through the\n * corresponding sysfs parameter file.\n */\n#ifdef CONFIG_WQ_WATCHDOG\n\nstatic unsigned long wq_watchdog_thresh = 30;\nstatic struct timer_list wq_watchdog_timer;\n\nstatic unsigned long wq_watchdog_touched = INITIAL_JIFFIES;\nstatic DEFINE_PER_CPU(unsigned long, wq_watchdog_touched_cpu) = INITIAL_JIFFIES;\n\nstatic void wq_watchdog_reset_touched(void)\n{\n\tint cpu;\n\n\twq_watchdog_touched = jiffies;\n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;\n}\n\nstatic void wq_watchdog_timer_fn(struct timer_list *unused)\n{\n\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;\n\tbool lockup_detected = false;\n\tstruct worker_pool *pool;\n\tint pi;\n\n\tif (!thresh)\n\t\treturn;\n\n\trcu_read_lock();\n\n\tfor_each_pool(pool, pi) {\n\t\tunsigned long pool_ts, touched, ts;\n\n\t\tif (list_empty(&pool->worklist))\n\t\t\tcontinue;\n\n\t\t/* get the latest of pool and touched timestamps */\n\t\tpool_ts = READ_ONCE(pool->watchdog_ts);\n\t\ttouched = READ_ONCE(wq_watchdog_touched);\n\n\t\tif (time_after(pool_ts, touched))\n\t\t\tts = pool_ts;\n\t\telse\n\t\t\tts = touched;\n\n\t\tif (pool->cpu >= 0) {\n\t\t\tunsigned long cpu_touched =\n\t\t\t\tREAD_ONCE(per_cpu(wq_watchdog_touched_cpu,\n\t\t\t\t\t\t  pool->cpu));\n\t\t\tif (time_after(cpu_touched, ts))\n\t\t\t\tts = cpu_touched;\n\t\t}\n\n\t\t/* did we stall? */\n\t\tif (time_after(jiffies, ts + thresh)) {\n\t\t\tlockup_detected = true;\n\t\t\tpr_emerg(\"BUG: workqueue lockup - pool\");\n\t\t\tpr_cont_pool_info(pool);\n\t\t\tpr_cont(\" stuck for %us!\\n\",\n\t\t\t\tjiffies_to_msecs(jiffies - pool_ts) / 1000);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\tif (lockup_detected)\n\t\tshow_workqueue_state();\n\n\twq_watchdog_reset_touched();\n\tmod_timer(&wq_watchdog_timer, jiffies + thresh);\n}\n\nnotrace void wq_watchdog_touch(int cpu)\n{\n\tif (cpu >= 0)\n\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;\n\telse\n\t\twq_watchdog_touched = jiffies;\n}\n\nstatic void wq_watchdog_set_thresh(unsigned long thresh)\n{\n\twq_watchdog_thresh = 0;\n\tdel_timer_sync(&wq_watchdog_timer);\n\n\tif (thresh) {\n\t\twq_watchdog_thresh = thresh;\n\t\twq_watchdog_reset_touched();\n\t\tmod_timer(&wq_watchdog_timer, jiffies + thresh * HZ);\n\t}\n}\n\nstatic int wq_watchdog_param_set_thresh(const char *val,\n\t\t\t\t\tconst struct kernel_param *kp)\n{\n\tunsigned long thresh;\n\tint ret;\n\n\tret = kstrtoul(val, 0, &thresh);\n\tif (ret)\n\t\treturn ret;\n\n\tif (system_wq)\n\t\twq_watchdog_set_thresh(thresh);\n\telse\n\t\twq_watchdog_thresh = thresh;\n\n\treturn 0;\n}\n\nstatic const struct kernel_param_ops wq_watchdog_thresh_ops = {\n\t.set\t= wq_watchdog_param_set_thresh,\n\t.get\t= param_get_ulong,\n};\n\nmodule_param_cb(watchdog_thresh, &wq_watchdog_thresh_ops, &wq_watchdog_thresh,\n\t\t0644);\n\nstatic void wq_watchdog_init(void)\n{\n\ttimer_setup(&wq_watchdog_timer, wq_watchdog_timer_fn, TIMER_DEFERRABLE);\n\twq_watchdog_set_thresh(wq_watchdog_thresh);\n}\n\n#else\t/* CONFIG_WQ_WATCHDOG */\n\nstatic inline void wq_watchdog_init(void) { }\n\n#endif\t/* CONFIG_WQ_WATCHDOG */\n\nstatic void __init wq_numa_init(void)\n{\n\tcpumask_var_t *tbl;\n\tint node, cpu;\n\n\tif (num_possible_nodes() <= 1)\n\t\treturn;\n\n\tif (wq_disable_numa) {\n\t\tpr_info(\"workqueue: NUMA affinity support disabled\\n\");\n\t\treturn;\n\t}\n\n\twq_update_unbound_numa_attrs_buf = alloc_workqueue_attrs();\n\tBUG_ON(!wq_update_unbound_numa_attrs_buf);\n\n\t/*\n\t * We want masks of possible CPUs of each node which isn't readily\n\t * available.  Build one from cpu_to_node() which should have been\n\t * fully initialized by now.\n\t */\n\ttbl = kcalloc(nr_node_ids, sizeof(tbl[0]), GFP_KERNEL);\n\tBUG_ON(!tbl);\n\n\tfor_each_node(node)\n\t\tBUG_ON(!zalloc_cpumask_var_node(&tbl[node], GFP_KERNEL,\n\t\t\t\tnode_online(node) ? node : NUMA_NO_NODE));\n\n\tfor_each_possible_cpu(cpu) {\n\t\tnode = cpu_to_node(cpu);\n\t\tif (WARN_ON(node == NUMA_NO_NODE)) {\n\t\t\tpr_warn(\"workqueue: NUMA node mapping not available for cpu%d, disabling NUMA support\\n\", cpu);\n\t\t\t/* happens iff arch is bonkers, let's just proceed */\n\t\t\treturn;\n\t\t}\n\t\tcpumask_set_cpu(cpu, tbl[node]);\n\t}\n\n\twq_numa_possible_cpumask = tbl;\n\twq_numa_enabled = true;\n}\n\n/**\n * workqueue_init_early - early init for workqueue subsystem\n *\n * This is the first half of two-staged workqueue subsystem initialization\n * and invoked as soon as the bare basics - memory allocation, cpumasks and\n * idr are up.  It sets up all the data structures and system workqueues\n * and allows early boot code to create workqueues and queue/cancel work\n * items.  Actual work item execution starts only after kthreads can be\n * created and scheduled right before early initcalls.\n */\nvoid __init workqueue_init_early(void)\n{\n\tint std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };\n\tint hk_flags = HK_FLAG_DOMAIN | HK_FLAG_WQ;\n\tint i, cpu;\n\n\tBUILD_BUG_ON(__alignof__(struct pool_workqueue) < __alignof__(long long));\n\n\tBUG_ON(!alloc_cpumask_var(&wq_unbound_cpumask, GFP_KERNEL));\n\tcpumask_copy(wq_unbound_cpumask, housekeeping_cpumask(hk_flags));\n\n\tpwq_cache = KMEM_CACHE(pool_workqueue, SLAB_PANIC);\n\n\t/* initialize CPU pools */\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct worker_pool *pool;\n\n\t\ti = 0;\n\t\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\t\tBUG_ON(init_worker_pool(pool));\n\t\t\tpool->cpu = cpu;\n\t\t\tcpumask_copy(pool->attrs->cpumask, cpumask_of(cpu));\n\t\t\tpool->attrs->nice = std_nice[i++];\n\t\t\tpool->node = cpu_to_node(cpu);\n\n\t\t\t/* alloc pool ID */\n\t\t\tmutex_lock(&wq_pool_mutex);\n\t\t\tBUG_ON(worker_pool_assign_id(pool));\n\t\t\tmutex_unlock(&wq_pool_mutex);\n\t\t}\n\t}\n\n\t/* create default unbound and ordered wq attrs */\n\tfor (i = 0; i < NR_STD_WORKER_POOLS; i++) {\n\t\tstruct workqueue_attrs *attrs;\n\n\t\tBUG_ON(!(attrs = alloc_workqueue_attrs()));\n\t\tattrs->nice = std_nice[i];\n\t\tunbound_std_wq_attrs[i] = attrs;\n\n\t\t/*\n\t\t * An ordered wq should have only one pwq as ordering is\n\t\t * guaranteed by max_active which is enforced by pwqs.\n\t\t * Turn off NUMA so that dfl_pwq is used for all nodes.\n\t\t */\n\t\tBUG_ON(!(attrs = alloc_workqueue_attrs()));\n\t\tattrs->nice = std_nice[i];\n\t\tattrs->no_numa = true;\n\t\tordered_wq_attrs[i] = attrs;\n\t}\n\n\tsystem_wq = alloc_workqueue(\"events\", 0, 0);\n\tsystem_highpri_wq = alloc_workqueue(\"events_highpri\", WQ_HIGHPRI, 0);\n\tsystem_long_wq = alloc_workqueue(\"events_long\", 0, 0);\n\tsystem_unbound_wq = alloc_workqueue(\"events_unbound\", WQ_UNBOUND,\n\t\t\t\t\t    WQ_UNBOUND_MAX_ACTIVE);\n\tsystem_freezable_wq = alloc_workqueue(\"events_freezable\",\n\t\t\t\t\t      WQ_FREEZABLE, 0);\n\tsystem_power_efficient_wq = alloc_workqueue(\"events_power_efficient\",\n\t\t\t\t\t      WQ_POWER_EFFICIENT, 0);\n\tsystem_freezable_power_efficient_wq = alloc_workqueue(\"events_freezable_power_efficient\",\n\t\t\t\t\t      WQ_FREEZABLE | WQ_POWER_EFFICIENT,\n\t\t\t\t\t      0);\n\tBUG_ON(!system_wq || !system_highpri_wq || !system_long_wq ||\n\t       !system_unbound_wq || !system_freezable_wq ||\n\t       !system_power_efficient_wq ||\n\t       !system_freezable_power_efficient_wq);\n}\n\n/**\n * workqueue_init - bring workqueue subsystem fully online\n *\n * This is the latter half of two-staged workqueue subsystem initialization\n * and invoked as soon as kthreads can be created and scheduled.\n * Workqueues have been created and work items queued on them, but there\n * are no kworkers executing the work items yet.  Populate the worker pools\n * with the initial workers and enable future kworker creations.\n */\nvoid __init workqueue_init(void)\n{\n\tstruct workqueue_struct *wq;\n\tstruct worker_pool *pool;\n\tint cpu, bkt;\n\n\t/*\n\t * It'd be simpler to initialize NUMA in workqueue_init_early() but\n\t * CPU to node mapping may not be available that early on some\n\t * archs such as power and arm64.  As per-cpu pools created\n\t * previously could be missing node hint and unbound pools NUMA\n\t * affinity, fix them up.\n\t *\n\t * Also, while iterating workqueues, create rescuers if requested.\n\t */\n\twq_numa_init();\n\n\tmutex_lock(&wq_pool_mutex);\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\t\tpool->node = cpu_to_node(cpu);\n\t\t}\n\t}\n\n\tlist_for_each_entry(wq, &workqueues, list) {\n\t\twq_update_unbound_numa(wq, smp_processor_id(), true);\n\t\tWARN(init_rescuer(wq),\n\t\t     \"workqueue: failed to create early rescuer for %s\",\n\t\t     wq->name);\n\t}\n\n\tmutex_unlock(&wq_pool_mutex);\n\n\t/* create the initial workers */\n\tfor_each_online_cpu(cpu) {\n\t\tfor_each_cpu_worker_pool(pool, cpu) {\n\t\t\tpool->flags &= ~POOL_DISASSOCIATED;\n\t\t\tBUG_ON(!create_worker(pool));\n\t\t}\n\t}\n\n\thash_for_each(unbound_pool_hash, bkt, pool, hash_node)\n\t\tBUG_ON(!create_worker(pool));\n\n\twq_online = true;\n\twq_watchdog_init();\n}\n"}, "1": {"id": 1, "path": "/src/include/linux/build_bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_BUILD_BUG_H\n#define _LINUX_BUILD_BUG_H\n\n#include <linux/compiler.h>\n\n#ifdef __CHECKER__\n#define BUILD_BUG_ON_ZERO(e) (0)\n#else /* __CHECKER__ */\n/*\n * Force a compilation error if condition is true, but also produce a\n * result (of value 0 and type int), so the expression can be used\n * e.g. in a structure initializer (or where-ever else comma expressions\n * aren't permitted).\n */\n#define BUILD_BUG_ON_ZERO(e) ((int)(sizeof(struct { int:(-!!(e)); })))\n#endif /* __CHECKER__ */\n\n/* Force a compilation error if a constant expression is not a power of 2 */\n#define __BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\\\n\tBUILD_BUG_ON(((n) & ((n) - 1)) != 0)\n#define BUILD_BUG_ON_NOT_POWER_OF_2(n)\t\t\t\\\n\tBUILD_BUG_ON((n) == 0 || (((n) & ((n) - 1)) != 0))\n\n/*\n * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the\n * expression but avoids the generation of any code, even if that expression\n * has side-effects.\n */\n#define BUILD_BUG_ON_INVALID(e) ((void)(sizeof((__force long)(e))))\n\n/**\n * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied\n *\t\t      error message.\n * @condition: the condition which the compiler should know is false.\n *\n * See BUILD_BUG_ON for description.\n */\n#define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)\n\n/**\n * BUILD_BUG_ON - break compile if a condition is true.\n * @condition: the condition which the compiler should know is false.\n *\n * If you have some code which relies on certain constants being equal, or\n * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to\n * detect if someone changes it.\n */\n#define BUILD_BUG_ON(condition) \\\n\tBUILD_BUG_ON_MSG(condition, \"BUILD_BUG_ON failed: \" #condition)\n\n/**\n * BUILD_BUG - break compile if used.\n *\n * If you have some code that you expect the compiler to eliminate at\n * build time, you should use BUILD_BUG to detect if it is\n * unexpectedly used.\n */\n#define BUILD_BUG() BUILD_BUG_ON_MSG(1, \"BUILD_BUG failed\")\n\n/**\n * static_assert - check integer constant expression at build time\n *\n * static_assert() is a wrapper for the C11 _Static_assert, with a\n * little macro magic to make the message optional (defaulting to the\n * stringification of the tested expression).\n *\n * Contrary to BUILD_BUG_ON(), static_assert() can be used at global\n * scope, but requires the expression to be an integer constant\n * expression (i.e., it is not enough that __builtin_constant_p() is\n * true for expr).\n *\n * Also note that BUILD_BUG_ON() fails the build if the condition is\n * true, while static_assert() fails the build if the expression is\n * false.\n */\n#define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr)\n#define __static_assert(expr, msg, ...) _Static_assert(expr, msg)\n\n#endif\t/* _LINUX_BUILD_BUG_H */\n"}, "2": {"id": 2, "path": "/src/include/linux/compiler_types.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_COMPILER_TYPES_H\n#define __LINUX_COMPILER_TYPES_H\n\n#ifndef __ASSEMBLY__\n\n#ifdef __CHECKER__\n/* address spaces */\n# define __kernel\t__attribute__((address_space(0)))\n# define __user\t\t__attribute__((noderef, address_space(__user)))\n# define __iomem\t__attribute__((noderef, address_space(__iomem)))\n# define __percpu\t__attribute__((noderef, address_space(__percpu)))\n# define __rcu\t\t__attribute__((noderef, address_space(__rcu)))\nstatic inline void __chk_user_ptr(const volatile void __user *ptr) { }\nstatic inline void __chk_io_ptr(const volatile void __iomem *ptr) { }\n/* context/locking */\n# define __must_hold(x)\t__attribute__((context(x,1,1)))\n# define __acquires(x)\t__attribute__((context(x,0,1)))\n# define __releases(x)\t__attribute__((context(x,1,0)))\n# define __acquire(x)\t__context__(x,1)\n# define __release(x)\t__context__(x,-1)\n# define __cond_lock(x,c)\t((c) ? ({ __acquire(x); 1; }) : 0)\n/* other */\n# define __force\t__attribute__((force))\n# define __nocast\t__attribute__((nocast))\n# define __safe\t\t__attribute__((safe))\n# define __private\t__attribute__((noderef))\n# define ACCESS_PRIVATE(p, member) (*((typeof((p)->member) __force *) &(p)->member))\n#else /* __CHECKER__ */\n/* address spaces */\n# define __kernel\n# ifdef STRUCTLEAK_PLUGIN\n#  define __user\t__attribute__((user))\n# else\n#  define __user\n# endif\n# define __iomem\n# define __percpu\n# define __rcu\n# define __chk_user_ptr(x)\t(void)0\n# define __chk_io_ptr(x)\t(void)0\n/* context/locking */\n# define __must_hold(x)\n# define __acquires(x)\n# define __releases(x)\n# define __acquire(x)\t(void)0\n# define __release(x)\t(void)0\n# define __cond_lock(x,c) (c)\n/* other */\n# define __force\n# define __nocast\n# define __safe\n# define __private\n# define ACCESS_PRIVATE(p, member) ((p)->member)\n# define __builtin_warning(x, y...) (1)\n#endif /* __CHECKER__ */\n\n/* Indirect macros required for expanded argument pasting, eg. __LINE__. */\n#define ___PASTE(a,b) a##b\n#define __PASTE(a,b) ___PASTE(a,b)\n\n#ifdef __KERNEL__\n\n/* Attributes */\n#include <linux/compiler_attributes.h>\n\n/* Builtins */\n\n/*\n * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.\n * In the meantime, to support gcc < 10, we implement __has_builtin\n * by hand.\n */\n#ifndef __has_builtin\n#define __has_builtin(x) (0)\n#endif\n\n/* Compiler specific macros. */\n#ifdef __clang__\n#include <linux/compiler-clang.h>\n#elif defined(__INTEL_COMPILER)\n#include <linux/compiler-intel.h>\n#elif defined(__GNUC__)\n/* The above compilers also define __GNUC__, so order is important here. */\n#include <linux/compiler-gcc.h>\n#else\n#error \"Unknown compiler\"\n#endif\n\n/*\n * Some architectures need to provide custom definitions of macros provided\n * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that\n * conditionally rather than using an asm-generic wrapper in order to avoid\n * build failures if any C compilation, which will include this file via an\n * -include argument in c_flags, occurs prior to the asm-generic wrappers being\n * generated.\n */\n#ifdef CONFIG_HAVE_ARCH_COMPILER_H\n#include <asm/compiler.h>\n#endif\n\nstruct ftrace_branch_data {\n\tconst char *func;\n\tconst char *file;\n\tunsigned line;\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long correct;\n\t\t\tunsigned long incorrect;\n\t\t};\n\t\tstruct {\n\t\t\tunsigned long miss;\n\t\t\tunsigned long hit;\n\t\t};\n\t\tunsigned long miss_hit[2];\n\t};\n};\n\nstruct ftrace_likely_data {\n\tstruct ftrace_branch_data\tdata;\n\tunsigned long\t\t\tconstant;\n};\n\n#if defined(CC_USING_HOTPATCH)\n#define notrace\t\t\t__attribute__((hotpatch(0, 0)))\n#elif defined(CC_USING_PATCHABLE_FUNCTION_ENTRY)\n#define notrace\t\t\t__attribute__((patchable_function_entry(0, 0)))\n#else\n#define notrace\t\t\t__attribute__((__no_instrument_function__))\n#endif\n\n/*\n * it doesn't make sense on ARM (currently the only user of __naked)\n * to trace naked functions because then mcount is called without\n * stack and frame pointer being set up and there is no chance to\n * restore the lr register to the value before mcount was called.\n */\n#define __naked\t\t\t__attribute__((__naked__)) notrace\n\n#define __compiler_offsetof(a, b)\t__builtin_offsetof(a, b)\n\n/*\n * Prefer gnu_inline, so that extern inline functions do not emit an\n * externally visible function. This makes extern inline behave as per gnu89\n * semantics rather than c99. This prevents multiple symbol definition errors\n * of extern inline functions at link time.\n * A lot of inline functions can cause havoc with function tracing.\n */\n#define inline inline __gnu_inline __inline_maybe_unused notrace\n\n/*\n * gcc provides both __inline__ and __inline as alternate spellings of\n * the inline keyword, though the latter is undocumented. New kernel\n * code should only use the inline spelling, but some existing code\n * uses __inline__. Since we #define inline above, to ensure\n * __inline__ has the same semantics, we need this #define.\n *\n * However, the spelling __inline is strictly reserved for referring\n * to the bare keyword.\n */\n#define __inline__ inline\n\n/*\n * GCC does not warn about unused static inline functions for -Wunused-function.\n * Suppress the warning in clang as well by using __maybe_unused, but enable it\n * for W=1 build. This will allow clang to find unused functions. Remove the\n * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.\n */\n#ifdef KBUILD_EXTRA_WARN1\n#define __inline_maybe_unused\n#else\n#define __inline_maybe_unused __maybe_unused\n#endif\n\n/*\n * Rather then using noinline to prevent stack consumption, use\n * noinline_for_stack instead.  For documentation reasons.\n */\n#define noinline_for_stack noinline\n\n/*\n * Sanitizer helper attributes: Because using __always_inline and\n * __no_sanitize_* conflict, provide helper attributes that will either expand\n * to __no_sanitize_* in compilation units where instrumentation is enabled\n * (__SANITIZE_*__), or __always_inline in compilation units without\n * instrumentation (__SANITIZE_*__ undefined).\n */\n#ifdef __SANITIZE_ADDRESS__\n/*\n * We can't declare function 'inline' because __no_sanitize_address conflicts\n * with inlining. Attempt to inline it may cause a build failure.\n *     https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368\n * '__maybe_unused' allows us to avoid defined-but-not-used warnings.\n */\n# define __no_kasan_or_inline __no_sanitize_address notrace __maybe_unused\n# define __no_sanitize_or_inline __no_kasan_or_inline\n#else\n# define __no_kasan_or_inline __always_inline\n#endif\n\n#define __no_kcsan __no_sanitize_thread\n#ifdef __SANITIZE_THREAD__\n# define __no_sanitize_or_inline __no_kcsan notrace __maybe_unused\n#endif\n\n#ifndef __no_sanitize_or_inline\n#define __no_sanitize_or_inline __always_inline\n#endif\n\n/* Section for code which can't be instrumented at all */\n#define noinstr\t\t\t\t\t\t\t\t\\\n\tnoinline notrace __attribute((__section__(\".noinstr.text\")))\t\\\n\t__no_kcsan __no_sanitize_address\n\n#endif /* __KERNEL__ */\n\n#endif /* __ASSEMBLY__ */\n\n/*\n * The below symbols may be defined for one or more, but not ALL, of the above\n * compilers. We don't consider that to be an error, so set them to nothing.\n * For example, some of them are for compiler specific plugins.\n */\n#ifndef __latent_entropy\n# define __latent_entropy\n#endif\n\n#ifndef __randomize_layout\n# define __randomize_layout __designated_init\n#endif\n\n#ifndef __no_randomize_layout\n# define __no_randomize_layout\n#endif\n\n#ifndef randomized_struct_fields_start\n# define randomized_struct_fields_start\n# define randomized_struct_fields_end\n#endif\n\n#ifndef __noscs\n# define __noscs\n#endif\n\n#ifndef asm_volatile_goto\n#define asm_volatile_goto(x...) asm goto(x)\n#endif\n\n#ifdef CONFIG_CC_HAS_ASM_INLINE\n#define asm_inline asm __inline\n#else\n#define asm_inline asm\n#endif\n\n/* Are two types/vars the same type (ignoring qualifiers)? */\n#define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))\n\n/*\n * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving\n *\t\t\t       non-scalar types unchanged.\n */\n/*\n * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'\n * is not type-compatible with 'signed char', and we define a separate case.\n */\n#define __scalar_type_to_expr_cases(type)\t\t\t\t\\\n\t\tunsigned type:\t(unsigned type)0,\t\t\t\\\n\t\tsigned type:\t(signed type)0\n\n#define __unqual_scalar_typeof(x) typeof(\t\t\t\t\\\n\t\t_Generic((x),\t\t\t\t\t\t\\\n\t\t\t char:\t(char)0,\t\t\t\t\\\n\t\t\t __scalar_type_to_expr_cases(char),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(short),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(int),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long),\t\t\\\n\t\t\t __scalar_type_to_expr_cases(long long),\t\\\n\t\t\t default: (x)))\n\n/* Is this type a native word size -- useful for atomic operations */\n#define __native_word(t) \\\n\t(sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || \\\n\t sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))\n\n/* Compile time object size, -1 for unknown */\n#ifndef __compiletime_object_size\n# define __compiletime_object_size(obj) -1\n#endif\n#ifndef __compiletime_warning\n# define __compiletime_warning(message)\n#endif\n#ifndef __compiletime_error\n# define __compiletime_error(message)\n#endif\n\n#ifdef __OPTIMIZE__\n# define __compiletime_assert(condition, msg, prefix, suffix)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\textern void prefix ## suffix(void) __compiletime_error(msg); \\\n\t\tif (!(condition))\t\t\t\t\t\\\n\t\t\tprefix ## suffix();\t\t\t\t\\\n\t} while (0)\n#else\n# define __compiletime_assert(condition, msg, prefix, suffix) do { } while (0)\n#endif\n\n#define _compiletime_assert(condition, msg, prefix, suffix) \\\n\t__compiletime_assert(condition, msg, prefix, suffix)\n\n/**\n * compiletime_assert - break build and emit msg if condition is false\n * @condition: a compile-time constant condition to check\n * @msg:       a message to emit if condition is false\n *\n * In tradition of POSIX assert, this macro will break the build if the\n * supplied condition is *false*, emitting the supplied error message if the\n * compiler has support to do so.\n */\n#define compiletime_assert(condition, msg) \\\n\t_compiletime_assert(condition, msg, __compiletime_assert_, __COUNTER__)\n\n#define compiletime_assert_atomic_type(t)\t\t\t\t\\\n\tcompiletime_assert(__native_word(t),\t\t\t\t\\\n\t\t\"Need native word sized stores/loads for atomicity.\")\n\n/* Helpers for emitting diagnostics in pragmas. */\n#ifndef __diag\n#define __diag(string)\n#endif\n\n#ifndef __diag_GCC\n#define __diag_GCC(version, severity, string)\n#endif\n\n#define __diag_push()\t__diag(push)\n#define __diag_pop()\t__diag(pop)\n\n#define __diag_ignore(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, ignore, option)\n#define __diag_warn(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, warn, option)\n#define __diag_error(compiler, version, option, comment) \\\n\t__diag_ ## compiler(version, error, option)\n\n#endif /* __LINUX_COMPILER_TYPES_H */\n"}, "3": {"id": 3, "path": "/src/include/asm-generic/bug.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_GENERIC_BUG_H\n#define _ASM_GENERIC_BUG_H\n\n#include <linux/compiler.h>\n#include <linux/instrumentation.h>\n\n#define CUT_HERE\t\t\"------------[ cut here ]------------\\n\"\n\n#ifdef CONFIG_GENERIC_BUG\n#define BUGFLAG_WARNING\t\t(1 << 0)\n#define BUGFLAG_ONCE\t\t(1 << 1)\n#define BUGFLAG_DONE\t\t(1 << 2)\n#define BUGFLAG_NO_CUT_HERE\t(1 << 3)\t/* CUT_HERE already sent */\n#define BUGFLAG_TAINT(taint)\t((taint) << 8)\n#define BUG_GET_TAINT(bug)\t((bug)->flags >> 8)\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/kernel.h>\n\n#ifdef CONFIG_BUG\n\n#ifdef CONFIG_GENERIC_BUG\nstruct bug_entry {\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tunsigned long\tbug_addr;\n#else\n\tsigned int\tbug_addr_disp;\n#endif\n#ifdef CONFIG_DEBUG_BUGVERBOSE\n#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS\n\tconst char\t*file;\n#else\n\tsigned int\tfile_disp;\n#endif\n\tunsigned short\tline;\n#endif\n\tunsigned short\tflags;\n};\n#endif\t/* CONFIG_GENERIC_BUG */\n\n/*\n * Don't use BUG() or BUG_ON() unless there's really no way out; one\n * example might be detecting data structure corruption in the middle\n * of an operation that can't be backed out of.  If the (sub)system\n * can somehow continue operating, perhaps with reduced functionality,\n * it's probably not BUG-worthy.\n *\n * If you're tempted to BUG(), think again:  is completely giving up\n * really the *only* solution?  There are usually better options, where\n * users don't need to reboot ASAP and can mostly shut down cleanly.\n */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do { \\\n\tprintk(\"BUG: failure at %s:%d/%s()!\\n\", __FILE__, __LINE__, __func__); \\\n\tbarrier_before_unreachable(); \\\n\tpanic(\"BUG!\"); \\\n} while (0)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n/*\n * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report\n * significant kernel issues that need prompt attention if they should ever\n * appear at runtime.\n *\n * Do not use these macros when checking for invalid external inputs\n * (e.g. invalid system call arguments, or invalid data coming from\n * network/devices), and on transient conditions like ENOMEM or EAGAIN.\n * These macros should be used for recoverable kernel issues only.\n * For invalid external inputs, transient conditions, etc use\n * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.\n * Do not include \"BUG\"/\"WARNING\" in format strings manually to make these\n * conditions distinguishable from kernel issues.\n *\n * Use the versions with printk format strings to provide better diagnostics.\n */\n#ifndef __WARN_FLAGS\nextern __printf(4, 5)\nvoid warn_slowpath_fmt(const char *file, const int line, unsigned taint,\n\t\t       const char *fmt, ...);\n#define __WARN()\t\t__WARN_printf(TAINT_WARN, NULL)\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\twarn_slowpath_fmt(__FILE__, __LINE__, taint, arg);\t\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#else\nextern __printf(1, 2) void __warn_printk(const char *fmt, ...);\n#define __WARN()\t\t__WARN_FLAGS(BUGFLAG_TAINT(TAINT_WARN))\n#define __WARN_printf(taint, arg...) do {\t\t\t\t\\\n\t\tinstrumentation_begin();\t\t\t\t\\\n\t\t__warn_printk(arg);\t\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_NO_CUT_HERE | BUGFLAG_TAINT(taint));\\\n\t\tinstrumentation_end();\t\t\t\t\t\\\n\t} while (0)\n#define WARN_ON_ONCE(condition) ({\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\\\n\t\t__WARN_FLAGS(BUGFLAG_ONCE |\t\t\t\\\n\t\t\t     BUGFLAG_TAINT(TAINT_WARN));\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\\\n})\n#endif\n\n/* used internally by panic.c */\nstruct warn_args;\nstruct pt_regs;\n\nvoid __warn(const char *file, int line, void *caller, unsigned taint,\n\t    struct pt_regs *regs, struct warn_args *args);\n\n#ifndef WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN();\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(TAINT_WARN, format);\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_TAINT(condition, taint, format...) ({\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tif (unlikely(__ret_warn_on))\t\t\t\t\t\\\n\t\t__WARN_printf(taint, format);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n\n#ifndef WARN_ON_ONCE\n#define WARN_ON_ONCE(condition)\t({\t\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_ON(1);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n#endif\n\n#define WARN_ONCE(condition, format...)\t({\t\t\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN(1, format);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#define WARN_TAINT_ONCE(condition, taint, format...)\t({\t\\\n\tstatic bool __section(\".data.once\") __warned;\t\t\\\n\tint __ret_warn_once = !!(condition);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (unlikely(__ret_warn_once && !__warned)) {\t\t\\\n\t\t__warned = true;\t\t\t\t\\\n\t\tWARN_TAINT(1, taint, format);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_once);\t\t\t\t\\\n})\n\n#else /* !CONFIG_BUG */\n#ifndef HAVE_ARCH_BUG\n#define BUG() do {} while (1)\n#endif\n\n#ifndef HAVE_ARCH_BUG_ON\n#define BUG_ON(condition) do { if (unlikely(condition)) BUG(); } while (0)\n#endif\n\n#ifndef HAVE_ARCH_WARN_ON\n#define WARN_ON(condition) ({\t\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#ifndef WARN\n#define WARN(condition, format...) ({\t\t\t\t\t\\\n\tint __ret_warn_on = !!(condition);\t\t\t\t\\\n\tno_printk(format);\t\t\t\t\t\t\\\n\tunlikely(__ret_warn_on);\t\t\t\t\t\\\n})\n#endif\n\n#define WARN_ON_ONCE(condition) WARN_ON(condition)\n#define WARN_ONCE(condition, format...) WARN(condition, format)\n#define WARN_TAINT(condition, taint, format...) WARN(condition, format)\n#define WARN_TAINT_ONCE(condition, taint, format...) WARN(condition, format)\n\n#endif\n\n/*\n * WARN_ON_SMP() is for cases that the warning is either\n * meaningless for !SMP or may even cause failures.\n * It can also be used with values that are only defined\n * on SMP:\n *\n * struct foo {\n *  [...]\n * #ifdef CONFIG_SMP\n *\tint bar;\n * #endif\n * };\n *\n * void func(struct foo *zoot)\n * {\n *\tWARN_ON_SMP(!zoot->bar);\n *\n * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),\n * and should be a nop and return false for uniprocessor.\n *\n * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set\n * and x is true.\n */\n#ifdef CONFIG_SMP\n# define WARN_ON_SMP(x)\t\t\tWARN_ON(x)\n#else\n/*\n * Use of ({0;}) because WARN_ON_SMP(x) may be used either as\n * a stand alone line statement or as a condition in an if ()\n * statement.\n * A simple \"0\" would cause gcc to give a \"statement has no effect\"\n * warning.\n */\n# define WARN_ON_SMP(x)\t\t\t({0;})\n#endif\n\n#endif /* __ASSEMBLY__ */\n\n#endif\n"}, "4": {"id": 4, "path": "/src/include/linux/cpumask.h", "content": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_CPUMASK_H\n#define __LINUX_CPUMASK_H\n\n/*\n * Cpumasks provide a bitmap suitable for representing the\n * set of CPU's in a system, one bit position per CPU number.  In general,\n * only nr_cpu_ids (<= NR_CPUS) bits are valid.\n */\n#include <linux/kernel.h>\n#include <linux/threads.h>\n#include <linux/bitmap.h>\n#include <linux/atomic.h>\n#include <linux/bug.h>\n\n/* Don't assign or return these: may not be this big! */\ntypedef struct cpumask { DECLARE_BITMAP(bits, NR_CPUS); } cpumask_t;\n\n/**\n * cpumask_bits - get the bits in a cpumask\n * @maskp: the struct cpumask *\n *\n * You should only assume nr_cpu_ids bits of this mask are valid.  This is\n * a macro so it's const-correct.\n */\n#define cpumask_bits(maskp) ((maskp)->bits)\n\n/**\n * cpumask_pr_args - printf args to output a cpumask\n * @maskp: cpumask to be printed\n *\n * Can be used to provide arguments for '%*pb[l]' when printing a cpumask.\n */\n#define cpumask_pr_args(maskp)\t\tnr_cpu_ids, cpumask_bits(maskp)\n\n#if NR_CPUS == 1\n#define nr_cpu_ids\t\t1U\n#else\nextern unsigned int nr_cpu_ids;\n#endif\n\n#ifdef CONFIG_CPUMASK_OFFSTACK\n/* Assuming NR_CPUS is huge, a runtime limit is more efficient.  Also,\n * not all bits may be allocated. */\n#define nr_cpumask_bits\tnr_cpu_ids\n#else\n#define nr_cpumask_bits\t((unsigned int)NR_CPUS)\n#endif\n\n/*\n * The following particular system cpumasks and operations manage\n * possible, present, active and online cpus.\n *\n *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable\n *     cpu_present_mask - has bit 'cpu' set iff cpu is populated\n *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler\n *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration\n *\n *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.\n *\n *  The cpu_possible_mask is fixed at boot time, as the set of CPU id's\n *  that it is possible might ever be plugged in at anytime during the\n *  life of that system boot.  The cpu_present_mask is dynamic(*),\n *  representing which CPUs are currently plugged in.  And\n *  cpu_online_mask is the dynamic subset of cpu_present_mask,\n *  indicating those CPUs available for scheduling.\n *\n *  If HOTPLUG is enabled, then cpu_possible_mask is forced to have\n *  all NR_CPUS bits set, otherwise it is just the set of CPUs that\n *  ACPI reports present at boot.\n *\n *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,\n *  depending on what ACPI reports as currently plugged in, otherwise\n *  cpu_present_mask is just a copy of cpu_possible_mask.\n *\n *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not\n *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.\n *\n * Subtleties:\n * 1) UP arch's (NR_CPUS == 1, CONFIG_SMP not defined) hardcode\n *    assumption that their single CPU is online.  The UP\n *    cpu_{online,possible,present}_masks are placebos.  Changing them\n *    will have no useful affect on the following num_*_cpus()\n *    and cpu_*() macros in the UP case.  This ugliness is a UP\n *    optimization - don't waste any instructions or memory references\n *    asking if you're online or how many CPUs there are if there is\n *    only one CPU.\n */\n\nextern struct cpumask __cpu_possible_mask;\nextern struct cpumask __cpu_online_mask;\nextern struct cpumask __cpu_present_mask;\nextern struct cpumask __cpu_active_mask;\n#define cpu_possible_mask ((const struct cpumask *)&__cpu_possible_mask)\n#define cpu_online_mask   ((const struct cpumask *)&__cpu_online_mask)\n#define cpu_present_mask  ((const struct cpumask *)&__cpu_present_mask)\n#define cpu_active_mask   ((const struct cpumask *)&__cpu_active_mask)\n\nextern atomic_t __num_online_cpus;\n\n#if NR_CPUS > 1\n/**\n * num_online_cpus() - Read the number of online CPUs\n *\n * Despite the fact that __num_online_cpus is of type atomic_t, this\n * interface gives only a momentary snapshot and is not protected against\n * concurrent CPU hotplug operations unless invoked from a cpuhp_lock held\n * region.\n */\nstatic inline unsigned int num_online_cpus(void)\n{\n\treturn atomic_read(&__num_online_cpus);\n}\n#define num_possible_cpus()\tcpumask_weight(cpu_possible_mask)\n#define num_present_cpus()\tcpumask_weight(cpu_present_mask)\n#define num_active_cpus()\tcpumask_weight(cpu_active_mask)\n#define cpu_online(cpu)\t\tcpumask_test_cpu((cpu), cpu_online_mask)\n#define cpu_possible(cpu)\tcpumask_test_cpu((cpu), cpu_possible_mask)\n#define cpu_present(cpu)\tcpumask_test_cpu((cpu), cpu_present_mask)\n#define cpu_active(cpu)\t\tcpumask_test_cpu((cpu), cpu_active_mask)\n#else\n#define num_online_cpus()\t1U\n#define num_possible_cpus()\t1U\n#define num_present_cpus()\t1U\n#define num_active_cpus()\t1U\n#define cpu_online(cpu)\t\t((cpu) == 0)\n#define cpu_possible(cpu)\t((cpu) == 0)\n#define cpu_present(cpu)\t((cpu) == 0)\n#define cpu_active(cpu)\t\t((cpu) == 0)\n#endif\n\nextern cpumask_t cpus_booted_once_mask;\n\nstatic inline void cpu_max_bits_warn(unsigned int cpu, unsigned int bits)\n{\n#ifdef CONFIG_DEBUG_PER_CPU_MAPS\n\tWARN_ON_ONCE(cpu >= bits);\n#endif /* CONFIG_DEBUG_PER_CPU_MAPS */\n}\n\n/* verify cpu argument to cpumask_* operators */\nstatic inline unsigned int cpumask_check(unsigned int cpu)\n{\n\tcpu_max_bits_warn(cpu, nr_cpumask_bits);\n\treturn cpu;\n}\n\n#if NR_CPUS == 1\n/* Uniprocessor.  Assume all masks are \"1\". */\nstatic inline unsigned int cpumask_first(const struct cpumask *srcp)\n{\n\treturn 0;\n}\n\nstatic inline unsigned int cpumask_last(const struct cpumask *srcp)\n{\n\treturn 0;\n}\n\n/* Valid inputs for n are -1 and 0. */\nstatic inline unsigned int cpumask_next(int n, const struct cpumask *srcp)\n{\n\treturn n+1;\n}\n\nstatic inline unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)\n{\n\treturn n+1;\n}\n\nstatic inline unsigned int cpumask_next_and(int n,\n\t\t\t\t\t    const struct cpumask *srcp,\n\t\t\t\t\t    const struct cpumask *andp)\n{\n\treturn n+1;\n}\n\nstatic inline unsigned int cpumask_next_wrap(int n, const struct cpumask *mask,\n\t\t\t\t\t     int start, bool wrap)\n{\n\t/* cpu0 unless stop condition, wrap and at cpu0, then nr_cpumask_bits */\n\treturn (wrap && n == 0);\n}\n\n/* cpu must be a valid cpu, ie 0, so there's no other choice. */\nstatic inline unsigned int cpumask_any_but(const struct cpumask *mask,\n\t\t\t\t\t   unsigned int cpu)\n{\n\treturn 1;\n}\n\nstatic inline unsigned int cpumask_local_spread(unsigned int i, int node)\n{\n\treturn 0;\n}\n\nstatic inline int cpumask_any_and_distribute(const struct cpumask *src1p,\n\t\t\t\t\t     const struct cpumask *src2p) {\n\treturn cpumask_next_and(-1, src1p, src2p);\n}\n\nstatic inline int cpumask_any_distribute(const struct cpumask *srcp)\n{\n\treturn cpumask_first(srcp);\n}\n\n#define for_each_cpu(cpu, mask)\t\t\t\\\n\tfor ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask)\n#define for_each_cpu_not(cpu, mask)\t\t\\\n\tfor ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask)\n#define for_each_cpu_wrap(cpu, mask, start)\t\\\n\tfor ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask, (void)(start))\n#define for_each_cpu_and(cpu, mask1, mask2)\t\\\n\tfor ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask1, (void)mask2)\n#else\n/**\n * cpumask_first - get the first cpu in a cpumask\n * @srcp: the cpumask pointer\n *\n * Returns >= nr_cpu_ids if no cpus set.\n */\nstatic inline unsigned int cpumask_first(const struct cpumask *srcp)\n{\n\treturn find_first_bit(cpumask_bits(srcp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_last - get the last CPU in a cpumask\n * @srcp:\t- the cpumask pointer\n *\n * Returns\t>= nr_cpumask_bits if no CPUs set.\n */\nstatic inline unsigned int cpumask_last(const struct cpumask *srcp)\n{\n\treturn find_last_bit(cpumask_bits(srcp), nr_cpumask_bits);\n}\n\nunsigned int cpumask_next(int n, const struct cpumask *srcp);\n\n/**\n * cpumask_next_zero - get the next unset cpu in a cpumask\n * @n: the cpu prior to the place to search (ie. return will be > @n)\n * @srcp: the cpumask pointer\n *\n * Returns >= nr_cpu_ids if no further cpus unset.\n */\nstatic inline unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)\n{\n\t/* -1 is a legal arg here. */\n\tif (n != -1)\n\t\tcpumask_check(n);\n\treturn find_next_zero_bit(cpumask_bits(srcp), nr_cpumask_bits, n+1);\n}\n\nint cpumask_next_and(int n, const struct cpumask *, const struct cpumask *);\nint cpumask_any_but(const struct cpumask *mask, unsigned int cpu);\nunsigned int cpumask_local_spread(unsigned int i, int node);\nint cpumask_any_and_distribute(const struct cpumask *src1p,\n\t\t\t       const struct cpumask *src2p);\nint cpumask_any_distribute(const struct cpumask *srcp);\n\n/**\n * for_each_cpu - iterate over every cpu in a mask\n * @cpu: the (optionally unsigned) integer iterator\n * @mask: the cpumask pointer\n *\n * After the loop, cpu is >= nr_cpu_ids.\n */\n#define for_each_cpu(cpu, mask)\t\t\t\t\\\n\tfor ((cpu) = -1;\t\t\t\t\\\n\t\t(cpu) = cpumask_next((cpu), (mask)),\t\\\n\t\t(cpu) < nr_cpu_ids;)\n\n/**\n * for_each_cpu_not - iterate over every cpu in a complemented mask\n * @cpu: the (optionally unsigned) integer iterator\n * @mask: the cpumask pointer\n *\n * After the loop, cpu is >= nr_cpu_ids.\n */\n#define for_each_cpu_not(cpu, mask)\t\t\t\t\\\n\tfor ((cpu) = -1;\t\t\t\t\t\\\n\t\t(cpu) = cpumask_next_zero((cpu), (mask)),\t\\\n\t\t(cpu) < nr_cpu_ids;)\n\nextern int cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);\n\n/**\n * for_each_cpu_wrap - iterate over every cpu in a mask, starting at a specified location\n * @cpu: the (optionally unsigned) integer iterator\n * @mask: the cpumask poiter\n * @start: the start location\n *\n * The implementation does not assume any bit in @mask is set (including @start).\n *\n * After the loop, cpu is >= nr_cpu_ids.\n */\n#define for_each_cpu_wrap(cpu, mask, start)\t\t\t\t\t\\\n\tfor ((cpu) = cpumask_next_wrap((start)-1, (mask), (start), false);\t\\\n\t     (cpu) < nr_cpumask_bits;\t\t\t\t\t\t\\\n\t     (cpu) = cpumask_next_wrap((cpu), (mask), (start), true))\n\n/**\n * for_each_cpu_and - iterate over every cpu in both masks\n * @cpu: the (optionally unsigned) integer iterator\n * @mask1: the first cpumask pointer\n * @mask2: the second cpumask pointer\n *\n * This saves a temporary CPU mask in many places.  It is equivalent to:\n *\tstruct cpumask tmp;\n *\tcpumask_and(&tmp, &mask1, &mask2);\n *\tfor_each_cpu(cpu, &tmp)\n *\t\t...\n *\n * After the loop, cpu is >= nr_cpu_ids.\n */\n#define for_each_cpu_and(cpu, mask1, mask2)\t\t\t\t\\\n\tfor ((cpu) = -1;\t\t\t\t\t\t\\\n\t\t(cpu) = cpumask_next_and((cpu), (mask1), (mask2)),\t\\\n\t\t(cpu) < nr_cpu_ids;)\n#endif /* SMP */\n\n#define CPU_BITS_NONE\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t[0 ... BITS_TO_LONGS(NR_CPUS)-1] = 0UL\t\t\t\\\n}\n\n#define CPU_BITS_CPU0\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t[0] =  1UL\t\t\t\t\t\t\\\n}\n\n/**\n * cpumask_set_cpu - set a cpu in a cpumask\n * @cpu: cpu number (< nr_cpu_ids)\n * @dstp: the cpumask pointer\n */\nstatic inline void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)\n{\n\tset_bit(cpumask_check(cpu), cpumask_bits(dstp));\n}\n\nstatic inline void __cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)\n{\n\t__set_bit(cpumask_check(cpu), cpumask_bits(dstp));\n}\n\n\n/**\n * cpumask_clear_cpu - clear a cpu in a cpumask\n * @cpu: cpu number (< nr_cpu_ids)\n * @dstp: the cpumask pointer\n */\nstatic inline void cpumask_clear_cpu(int cpu, struct cpumask *dstp)\n{\n\tclear_bit(cpumask_check(cpu), cpumask_bits(dstp));\n}\n\nstatic inline void __cpumask_clear_cpu(int cpu, struct cpumask *dstp)\n{\n\t__clear_bit(cpumask_check(cpu), cpumask_bits(dstp));\n}\n\n/**\n * cpumask_test_cpu - test for a cpu in a cpumask\n * @cpu: cpu number (< nr_cpu_ids)\n * @cpumask: the cpumask pointer\n *\n * Returns 1 if @cpu is set in @cpumask, else returns 0\n */\nstatic inline int cpumask_test_cpu(int cpu, const struct cpumask *cpumask)\n{\n\treturn test_bit(cpumask_check(cpu), cpumask_bits((cpumask)));\n}\n\n/**\n * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask\n * @cpu: cpu number (< nr_cpu_ids)\n * @cpumask: the cpumask pointer\n *\n * Returns 1 if @cpu is set in old bitmap of @cpumask, else returns 0\n *\n * test_and_set_bit wrapper for cpumasks.\n */\nstatic inline int cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)\n{\n\treturn test_and_set_bit(cpumask_check(cpu), cpumask_bits(cpumask));\n}\n\n/**\n * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask\n * @cpu: cpu number (< nr_cpu_ids)\n * @cpumask: the cpumask pointer\n *\n * Returns 1 if @cpu is set in old bitmap of @cpumask, else returns 0\n *\n * test_and_clear_bit wrapper for cpumasks.\n */\nstatic inline int cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)\n{\n\treturn test_and_clear_bit(cpumask_check(cpu), cpumask_bits(cpumask));\n}\n\n/**\n * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask\n * @dstp: the cpumask pointer\n */\nstatic inline void cpumask_setall(struct cpumask *dstp)\n{\n\tbitmap_fill(cpumask_bits(dstp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask\n * @dstp: the cpumask pointer\n */\nstatic inline void cpumask_clear(struct cpumask *dstp)\n{\n\tbitmap_zero(cpumask_bits(dstp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_and - *dstp = *src1p & *src2p\n * @dstp: the cpumask result\n * @src1p: the first input\n * @src2p: the second input\n *\n * If *@dstp is empty, returns 0, else returns 1\n */\nstatic inline int cpumask_and(struct cpumask *dstp,\n\t\t\t       const struct cpumask *src1p,\n\t\t\t       const struct cpumask *src2p)\n{\n\treturn bitmap_and(cpumask_bits(dstp), cpumask_bits(src1p),\n\t\t\t\t       cpumask_bits(src2p), nr_cpumask_bits);\n}\n\n/**\n * cpumask_or - *dstp = *src1p | *src2p\n * @dstp: the cpumask result\n * @src1p: the first input\n * @src2p: the second input\n */\nstatic inline void cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,\n\t\t\t      const struct cpumask *src2p)\n{\n\tbitmap_or(cpumask_bits(dstp), cpumask_bits(src1p),\n\t\t\t\t      cpumask_bits(src2p), nr_cpumask_bits);\n}\n\n/**\n * cpumask_xor - *dstp = *src1p ^ *src2p\n * @dstp: the cpumask result\n * @src1p: the first input\n * @src2p: the second input\n */\nstatic inline void cpumask_xor(struct cpumask *dstp,\n\t\t\t       const struct cpumask *src1p,\n\t\t\t       const struct cpumask *src2p)\n{\n\tbitmap_xor(cpumask_bits(dstp), cpumask_bits(src1p),\n\t\t\t\t       cpumask_bits(src2p), nr_cpumask_bits);\n}\n\n/**\n * cpumask_andnot - *dstp = *src1p & ~*src2p\n * @dstp: the cpumask result\n * @src1p: the first input\n * @src2p: the second input\n *\n * If *@dstp is empty, returns 0, else returns 1\n */\nstatic inline int cpumask_andnot(struct cpumask *dstp,\n\t\t\t\t  const struct cpumask *src1p,\n\t\t\t\t  const struct cpumask *src2p)\n{\n\treturn bitmap_andnot(cpumask_bits(dstp), cpumask_bits(src1p),\n\t\t\t\t\t  cpumask_bits(src2p), nr_cpumask_bits);\n}\n\n/**\n * cpumask_complement - *dstp = ~*srcp\n * @dstp: the cpumask result\n * @srcp: the input to invert\n */\nstatic inline void cpumask_complement(struct cpumask *dstp,\n\t\t\t\t      const struct cpumask *srcp)\n{\n\tbitmap_complement(cpumask_bits(dstp), cpumask_bits(srcp),\n\t\t\t\t\t      nr_cpumask_bits);\n}\n\n/**\n * cpumask_equal - *src1p == *src2p\n * @src1p: the first input\n * @src2p: the second input\n */\nstatic inline bool cpumask_equal(const struct cpumask *src1p,\n\t\t\t\tconst struct cpumask *src2p)\n{\n\treturn bitmap_equal(cpumask_bits(src1p), cpumask_bits(src2p),\n\t\t\t\t\t\t nr_cpumask_bits);\n}\n\n/**\n * cpumask_or_equal - *src1p | *src2p == *src3p\n * @src1p: the first input\n * @src2p: the second input\n * @src3p: the third input\n */\nstatic inline bool cpumask_or_equal(const struct cpumask *src1p,\n\t\t\t\t    const struct cpumask *src2p,\n\t\t\t\t    const struct cpumask *src3p)\n{\n\treturn bitmap_or_equal(cpumask_bits(src1p), cpumask_bits(src2p),\n\t\t\t       cpumask_bits(src3p), nr_cpumask_bits);\n}\n\n/**\n * cpumask_intersects - (*src1p & *src2p) != 0\n * @src1p: the first input\n * @src2p: the second input\n */\nstatic inline bool cpumask_intersects(const struct cpumask *src1p,\n\t\t\t\t     const struct cpumask *src2p)\n{\n\treturn bitmap_intersects(cpumask_bits(src1p), cpumask_bits(src2p),\n\t\t\t\t\t\t      nr_cpumask_bits);\n}\n\n/**\n * cpumask_subset - (*src1p & ~*src2p) == 0\n * @src1p: the first input\n * @src2p: the second input\n *\n * Returns 1 if *@src1p is a subset of *@src2p, else returns 0\n */\nstatic inline int cpumask_subset(const struct cpumask *src1p,\n\t\t\t\t const struct cpumask *src2p)\n{\n\treturn bitmap_subset(cpumask_bits(src1p), cpumask_bits(src2p),\n\t\t\t\t\t\t  nr_cpumask_bits);\n}\n\n/**\n * cpumask_empty - *srcp == 0\n * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.\n */\nstatic inline bool cpumask_empty(const struct cpumask *srcp)\n{\n\treturn bitmap_empty(cpumask_bits(srcp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_full - *srcp == 0xFFFFFFFF...\n * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.\n */\nstatic inline bool cpumask_full(const struct cpumask *srcp)\n{\n\treturn bitmap_full(cpumask_bits(srcp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_weight - Count of bits in *srcp\n * @srcp: the cpumask to count bits (< nr_cpu_ids) in.\n */\nstatic inline unsigned int cpumask_weight(const struct cpumask *srcp)\n{\n\treturn bitmap_weight(cpumask_bits(srcp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_shift_right - *dstp = *srcp >> n\n * @dstp: the cpumask result\n * @srcp: the input to shift\n * @n: the number of bits to shift by\n */\nstatic inline void cpumask_shift_right(struct cpumask *dstp,\n\t\t\t\t       const struct cpumask *srcp, int n)\n{\n\tbitmap_shift_right(cpumask_bits(dstp), cpumask_bits(srcp), n,\n\t\t\t\t\t       nr_cpumask_bits);\n}\n\n/**\n * cpumask_shift_left - *dstp = *srcp << n\n * @dstp: the cpumask result\n * @srcp: the input to shift\n * @n: the number of bits to shift by\n */\nstatic inline void cpumask_shift_left(struct cpumask *dstp,\n\t\t\t\t      const struct cpumask *srcp, int n)\n{\n\tbitmap_shift_left(cpumask_bits(dstp), cpumask_bits(srcp), n,\n\t\t\t\t\t      nr_cpumask_bits);\n}\n\n/**\n * cpumask_copy - *dstp = *srcp\n * @dstp: the result\n * @srcp: the input cpumask\n */\nstatic inline void cpumask_copy(struct cpumask *dstp,\n\t\t\t\tconst struct cpumask *srcp)\n{\n\tbitmap_copy(cpumask_bits(dstp), cpumask_bits(srcp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_any - pick a \"random\" cpu from *srcp\n * @srcp: the input cpumask\n *\n * Returns >= nr_cpu_ids if no cpus set.\n */\n#define cpumask_any(srcp) cpumask_first(srcp)\n\n/**\n * cpumask_first_and - return the first cpu from *srcp1 & *srcp2\n * @src1p: the first input\n * @src2p: the second input\n *\n * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().\n */\n#define cpumask_first_and(src1p, src2p) cpumask_next_and(-1, (src1p), (src2p))\n\n/**\n * cpumask_any_and - pick a \"random\" cpu from *mask1 & *mask2\n * @mask1: the first input cpumask\n * @mask2: the second input cpumask\n *\n * Returns >= nr_cpu_ids if no cpus set.\n */\n#define cpumask_any_and(mask1, mask2) cpumask_first_and((mask1), (mask2))\n\n/**\n * cpumask_of - the cpumask containing just a given cpu\n * @cpu: the cpu (<= nr_cpu_ids)\n */\n#define cpumask_of(cpu) (get_cpu_mask(cpu))\n\n/**\n * cpumask_parse_user - extract a cpumask from a user string\n * @buf: the buffer to extract from\n * @len: the length of the buffer\n * @dstp: the cpumask to set.\n *\n * Returns -errno, or 0 for success.\n */\nstatic inline int cpumask_parse_user(const char __user *buf, int len,\n\t\t\t\t     struct cpumask *dstp)\n{\n\treturn bitmap_parse_user(buf, len, cpumask_bits(dstp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_parselist_user - extract a cpumask from a user string\n * @buf: the buffer to extract from\n * @len: the length of the buffer\n * @dstp: the cpumask to set.\n *\n * Returns -errno, or 0 for success.\n */\nstatic inline int cpumask_parselist_user(const char __user *buf, int len,\n\t\t\t\t     struct cpumask *dstp)\n{\n\treturn bitmap_parselist_user(buf, len, cpumask_bits(dstp),\n\t\t\t\t     nr_cpumask_bits);\n}\n\n/**\n * cpumask_parse - extract a cpumask from a string\n * @buf: the buffer to extract from\n * @dstp: the cpumask to set.\n *\n * Returns -errno, or 0 for success.\n */\nstatic inline int cpumask_parse(const char *buf, struct cpumask *dstp)\n{\n\treturn bitmap_parse(buf, UINT_MAX, cpumask_bits(dstp), nr_cpumask_bits);\n}\n\n/**\n * cpulist_parse - extract a cpumask from a user string of ranges\n * @buf: the buffer to extract from\n * @dstp: the cpumask to set.\n *\n * Returns -errno, or 0 for success.\n */\nstatic inline int cpulist_parse(const char *buf, struct cpumask *dstp)\n{\n\treturn bitmap_parselist(buf, cpumask_bits(dstp), nr_cpumask_bits);\n}\n\n/**\n * cpumask_size - size to allocate for a 'struct cpumask' in bytes\n */\nstatic inline unsigned int cpumask_size(void)\n{\n\treturn BITS_TO_LONGS(nr_cpumask_bits) * sizeof(long);\n}\n\n/*\n * cpumask_var_t: struct cpumask for stack usage.\n *\n * Oh, the wicked games we play!  In order to make kernel coding a\n * little more difficult, we typedef cpumask_var_t to an array or a\n * pointer: doing &mask on an array is a noop, so it still works.\n *\n * ie.\n *\tcpumask_var_t tmpmask;\n *\tif (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))\n *\t\treturn -ENOMEM;\n *\n *\t  ... use 'tmpmask' like a normal struct cpumask * ...\n *\n *\tfree_cpumask_var(tmpmask);\n *\n *\n * However, one notable exception is there. alloc_cpumask_var() allocates\n * only nr_cpumask_bits bits (in the other hand, real cpumask_t always has\n * NR_CPUS bits). Therefore you don't have to dereference cpumask_var_t.\n *\n *\tcpumask_var_t tmpmask;\n *\tif (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))\n *\t\treturn -ENOMEM;\n *\n *\tvar = *tmpmask;\n *\n * This code makes NR_CPUS length memcopy and brings to a memory corruption.\n * cpumask_copy() provide safe copy functionality.\n *\n * Note that there is another evil here: If you define a cpumask_var_t\n * as a percpu variable then the way to obtain the address of the cpumask\n * structure differently influences what this_cpu_* operation needs to be\n * used. Please use this_cpu_cpumask_var_t in those cases. The direct use\n * of this_cpu_ptr() or this_cpu_read() will lead to failures when the\n * other type of cpumask_var_t implementation is configured.\n *\n * Please also note that __cpumask_var_read_mostly can be used to declare\n * a cpumask_var_t variable itself (not its content) as read mostly.\n */\n#ifdef CONFIG_CPUMASK_OFFSTACK\ntypedef struct cpumask *cpumask_var_t;\n\n#define this_cpu_cpumask_var_ptr(x)\tthis_cpu_read(x)\n#define __cpumask_var_read_mostly\t__read_mostly\n\nbool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);\nbool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags);\nbool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);\nbool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags);\nvoid alloc_bootmem_cpumask_var(cpumask_var_t *mask);\nvoid free_cpumask_var(cpumask_var_t mask);\nvoid free_bootmem_cpumask_var(cpumask_var_t mask);\n\nstatic inline bool cpumask_available(cpumask_var_t mask)\n{\n\treturn mask != NULL;\n}\n\n#else\ntypedef struct cpumask cpumask_var_t[1];\n\n#define this_cpu_cpumask_var_ptr(x) this_cpu_ptr(x)\n#define __cpumask_var_read_mostly\n\nstatic inline bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)\n{\n\treturn true;\n}\n\nstatic inline bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,\n\t\t\t\t\t  int node)\n{\n\treturn true;\n}\n\nstatic inline bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)\n{\n\tcpumask_clear(*mask);\n\treturn true;\n}\n\nstatic inline bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,\n\t\t\t\t\t  int node)\n{\n\tcpumask_clear(*mask);\n\treturn true;\n}\n\nstatic inline void alloc_bootmem_cpumask_var(cpumask_var_t *mask)\n{\n}\n\nstatic inline void free_cpumask_var(cpumask_var_t mask)\n{\n}\n\nstatic inline void free_bootmem_cpumask_var(cpumask_var_t mask)\n{\n}\n\nstatic inline bool cpumask_available(cpumask_var_t mask)\n{\n\treturn true;\n}\n#endif /* CONFIG_CPUMASK_OFFSTACK */\n\n/* It's common to want to use cpu_all_mask in struct member initializers,\n * so it has to refer to an address rather than a pointer. */\nextern const DECLARE_BITMAP(cpu_all_bits, NR_CPUS);\n#define cpu_all_mask to_cpumask(cpu_all_bits)\n\n/* First bits of cpu_bit_bitmap are in fact unset. */\n#define cpu_none_mask to_cpumask(cpu_bit_bitmap[0])\n\n#define for_each_possible_cpu(cpu) for_each_cpu((cpu), cpu_possible_mask)\n#define for_each_online_cpu(cpu)   for_each_cpu((cpu), cpu_online_mask)\n#define for_each_present_cpu(cpu)  for_each_cpu((cpu), cpu_present_mask)\n\n/* Wrappers for arch boot code to manipulate normally-constant masks */\nvoid init_cpu_present(const struct cpumask *src);\nvoid init_cpu_possible(const struct cpumask *src);\nvoid init_cpu_online(const struct cpumask *src);\n\nstatic inline void reset_cpu_possible_mask(void)\n{\n\tbitmap_zero(cpumask_bits(&__cpu_possible_mask), NR_CPUS);\n}\n\nstatic inline void\nset_cpu_possible(unsigned int cpu, bool possible)\n{\n\tif (possible)\n\t\tcpumask_set_cpu(cpu, &__cpu_possible_mask);\n\telse\n\t\tcpumask_clear_cpu(cpu, &__cpu_possible_mask);\n}\n\nstatic inline void\nset_cpu_present(unsigned int cpu, bool present)\n{\n\tif (present)\n\t\tcpumask_set_cpu(cpu, &__cpu_present_mask);\n\telse\n\t\tcpumask_clear_cpu(cpu, &__cpu_present_mask);\n}\n\nvoid set_cpu_online(unsigned int cpu, bool online);\n\nstatic inline void\nset_cpu_active(unsigned int cpu, bool active)\n{\n\tif (active)\n\t\tcpumask_set_cpu(cpu, &__cpu_active_mask);\n\telse\n\t\tcpumask_clear_cpu(cpu, &__cpu_active_mask);\n}\n\n\n/**\n * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *\n * @bitmap: the bitmap\n *\n * There are a few places where cpumask_var_t isn't appropriate and\n * static cpumasks must be used (eg. very early boot), yet we don't\n * expose the definition of 'struct cpumask'.\n *\n * This does the conversion, and can be used as a constant initializer.\n */\n#define to_cpumask(bitmap)\t\t\t\t\t\t\\\n\t((struct cpumask *)(1 ? (bitmap)\t\t\t\t\\\n\t\t\t    : (void *)sizeof(__check_is_bitmap(bitmap))))\n\nstatic inline int __check_is_bitmap(const unsigned long *bitmap)\n{\n\treturn 1;\n}\n\n/*\n * Special-case data structure for \"single bit set only\" constant CPU masks.\n *\n * We pre-generate all the 64 (or 32) possible bit positions, with enough\n * padding to the left and the right, and return the constant pointer\n * appropriately offset.\n */\nextern const unsigned long\n\tcpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)];\n\nstatic inline const struct cpumask *get_cpu_mask(unsigned int cpu)\n{\n\tconst unsigned long *p = cpu_bit_bitmap[1 + cpu % BITS_PER_LONG];\n\tp -= cpu / BITS_PER_LONG;\n\treturn to_cpumask(p);\n}\n\n#define cpu_is_offline(cpu)\tunlikely(!cpu_online(cpu))\n\n#if NR_CPUS <= BITS_PER_LONG\n#define CPU_BITS_ALL\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)\t\\\n}\n\n#else /* NR_CPUS > BITS_PER_LONG */\n\n#define CPU_BITS_ALL\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t[0 ... BITS_TO_LONGS(NR_CPUS)-2] = ~0UL,\t\t\\\n\t[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)\t\\\n}\n#endif /* NR_CPUS > BITS_PER_LONG */\n\n/**\n * cpumap_print_to_pagebuf  - copies the cpumask into the buffer either\n *\tas comma-separated list of cpus or hex values of cpumask\n * @list: indicates whether the cpumap must be list\n * @mask: the cpumask to copy\n * @buf: the buffer to copy into\n *\n * Returns the length of the (null-terminated) @buf string, zero if\n * nothing is copied.\n */\nstatic inline ssize_t\ncpumap_print_to_pagebuf(bool list, char *buf, const struct cpumask *mask)\n{\n\treturn bitmap_print_to_pagebuf(list, buf, cpumask_bits(mask),\n\t\t\t\t      nr_cpu_ids);\n}\n\n#if NR_CPUS <= BITS_PER_LONG\n#define CPU_MASK_ALL\t\t\t\t\t\t\t\\\n(cpumask_t) { {\t\t\t\t\t\t\t\t\\\n\t[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)\t\\\n} }\n#else\n#define CPU_MASK_ALL\t\t\t\t\t\t\t\\\n(cpumask_t) { {\t\t\t\t\t\t\t\t\\\n\t[0 ... BITS_TO_LONGS(NR_CPUS)-2] = ~0UL,\t\t\t\\\n\t[BITS_TO_LONGS(NR_CPUS)-1] = BITMAP_LAST_WORD_MASK(NR_CPUS)\t\\\n} }\n#endif /* NR_CPUS > BITS_PER_LONG */\n\n#define CPU_MASK_NONE\t\t\t\t\t\t\t\\\n(cpumask_t) { {\t\t\t\t\t\t\t\t\\\n\t[0 ... BITS_TO_LONGS(NR_CPUS)-1] =  0UL\t\t\t\t\\\n} }\n\n#define CPU_MASK_CPU0\t\t\t\t\t\t\t\\\n(cpumask_t) { {\t\t\t\t\t\t\t\t\\\n\t[0] =  1UL\t\t\t\t\t\t\t\\\n} }\n\n#endif /* __LINUX_CPUMASK_H */\n"}, "5": {"id": 5, "path": "/src/include/linux/percpu-defs.h", "content": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n * linux/percpu-defs.h - basic definitions for percpu areas\n *\n * DO NOT INCLUDE DIRECTLY OUTSIDE PERCPU IMPLEMENTATION PROPER.\n *\n * This file is separate from linux/percpu.h to avoid cyclic inclusion\n * dependency from arch header files.  Only to be included from\n * asm/percpu.h.\n *\n * This file includes macros necessary to declare percpu sections and\n * variables, and definitions of percpu accessors and operations.  It\n * should provide enough percpu features to arch header files even when\n * they can only include asm/percpu.h to avoid cyclic inclusion dependency.\n */\n\n#ifndef _LINUX_PERCPU_DEFS_H\n#define _LINUX_PERCPU_DEFS_H\n\n#ifdef CONFIG_SMP\n\n#ifdef MODULE\n#define PER_CPU_SHARED_ALIGNED_SECTION \"\"\n#define PER_CPU_ALIGNED_SECTION \"\"\n#else\n#define PER_CPU_SHARED_ALIGNED_SECTION \"..shared_aligned\"\n#define PER_CPU_ALIGNED_SECTION \"..shared_aligned\"\n#endif\n#define PER_CPU_FIRST_SECTION \"..first\"\n\n#else\n\n#define PER_CPU_SHARED_ALIGNED_SECTION \"\"\n#define PER_CPU_ALIGNED_SECTION \"..shared_aligned\"\n#define PER_CPU_FIRST_SECTION \"\"\n\n#endif\n\n/*\n * Base implementations of per-CPU variable declarations and definitions, where\n * the section in which the variable is to be placed is provided by the\n * 'sec' argument.  This may be used to affect the parameters governing the\n * variable's storage.\n *\n * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest\n * linkage errors occur due the compiler generating the wrong code to access\n * that section.\n */\n#define __PCPU_ATTRS(sec)\t\t\t\t\t\t\\\n\t__percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))\t\\\n\tPER_CPU_ATTRIBUTES\n\n#define __PCPU_DUMMY_ATTRS\t\t\t\t\t\t\\\n\t__section(\".discard\") __attribute__((unused))\n\n/*\n * s390 and alpha modules require percpu variables to be defined as\n * weak to force the compiler to generate GOT based external\n * references for them.  This is necessary because percpu sections\n * will be located outside of the usually addressable area.\n *\n * This definition puts the following two extra restrictions when\n * defining percpu variables.\n *\n * 1. The symbol must be globally unique, even the static ones.\n * 2. Static percpu variables cannot be defined inside a function.\n *\n * Archs which need weak percpu definitions should define\n * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.\n *\n * To ensure that the generic code observes the above two\n * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak\n * definition is used for all cases.\n */\n#if defined(ARCH_NEEDS_WEAK_PER_CPU) || defined(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)\n/*\n * __pcpu_scope_* dummy variable is used to enforce scope.  It\n * receives the static modifier when it's used in front of\n * DEFINE_PER_CPU() and will trigger build failure if\n * DECLARE_PER_CPU() is used for the same variable.\n *\n * __pcpu_unique_* dummy variable is used to enforce symbol uniqueness\n * such that hidden weak symbol collision, which will cause unrelated\n * variables to share the same address, can be detected during build.\n */\n#define DECLARE_PER_CPU_SECTION(type, name, sec)\t\t\t\\\n\textern __PCPU_DUMMY_ATTRS char __pcpu_scope_##name;\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name\n\n#define DEFINE_PER_CPU_SECTION(type, name, sec)\t\t\t\t\\\n\t__PCPU_DUMMY_ATTRS char __pcpu_scope_##name;\t\t\t\\\n\textern __PCPU_DUMMY_ATTRS char __pcpu_unique_##name;\t\t\\\n\t__PCPU_DUMMY_ATTRS char __pcpu_unique_##name;\t\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name;\t\t\t\\\n\t__PCPU_ATTRS(sec) __weak __typeof__(type) name\n#else\n/*\n * Normal declaration and definition macros.\n */\n#define DECLARE_PER_CPU_SECTION(type, name, sec)\t\t\t\\\n\textern __PCPU_ATTRS(sec) __typeof__(type) name\n\n#define DEFINE_PER_CPU_SECTION(type, name, sec)\t\t\t\t\\\n\t__PCPU_ATTRS(sec) __typeof__(type) name\n#endif\n\n/*\n * Variant on the per-CPU variable declaration/definition theme used for\n * ordinary per-CPU variables.\n */\n#define DECLARE_PER_CPU(type, name)\t\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"\")\n\n#define DEFINE_PER_CPU(type, name)\t\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"\")\n\n/*\n * Declaration/definition used for per-CPU variables that must come first in\n * the set of variables.\n */\n#define DECLARE_PER_CPU_FIRST(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_FIRST_SECTION)\n\n#define DEFINE_PER_CPU_FIRST(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_FIRST_SECTION)\n\n/*\n * Declaration/definition used for per-CPU variables that must be cacheline\n * aligned under SMP conditions so that, whilst a particular instance of the\n * data corresponds to a particular CPU, inefficiencies due to direct access by\n * other CPUs are reduced by preventing the data from unnecessarily spanning\n * cachelines.\n *\n * An example of this would be statistical data, where each CPU's set of data\n * is updated by that CPU alone, but the data from across all CPUs is collated\n * by a CPU processing a read from a proc file.\n */\n#define DECLARE_PER_CPU_SHARED_ALIGNED(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \\\n\t____cacheline_aligned_in_smp\n\n#define DEFINE_PER_CPU_SHARED_ALIGNED(type, name)\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \\\n\t____cacheline_aligned_in_smp\n\n#define DECLARE_PER_CPU_ALIGNED(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)\t\\\n\t____cacheline_aligned\n\n#define DEFINE_PER_CPU_ALIGNED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)\t\\\n\t____cacheline_aligned\n\n/*\n * Declaration/definition used for per-CPU variables that must be page aligned.\n */\n#define DECLARE_PER_CPU_PAGE_ALIGNED(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..page_aligned\")\t\t\\\n\t__aligned(PAGE_SIZE)\n\n#define DEFINE_PER_CPU_PAGE_ALIGNED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..page_aligned\")\t\t\\\n\t__aligned(PAGE_SIZE)\n\n/*\n * Declaration/definition used for per-CPU variables that must be read mostly.\n */\n#define DECLARE_PER_CPU_READ_MOSTLY(type, name)\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..read_mostly\")\n\n#define DEFINE_PER_CPU_READ_MOSTLY(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..read_mostly\")\n\n/*\n * Declaration/definition used for per-CPU variables that should be accessed\n * as decrypted when memory encryption is enabled in the guest.\n */\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n#define DECLARE_PER_CPU_DECRYPTED(type, name)\t\t\t\t\\\n\tDECLARE_PER_CPU_SECTION(type, name, \"..decrypted\")\n\n#define DEFINE_PER_CPU_DECRYPTED(type, name)\t\t\t\t\\\n\tDEFINE_PER_CPU_SECTION(type, name, \"..decrypted\")\n#else\n#define DEFINE_PER_CPU_DECRYPTED(type, name)\tDEFINE_PER_CPU(type, name)\n#endif\n\n/*\n * Intermodule exports for per-CPU variables.  sparse forgets about\n * address space across EXPORT_SYMBOL(), change EXPORT_SYMBOL() to\n * noop if __CHECKER__.\n */\n#ifndef __CHECKER__\n#define EXPORT_PER_CPU_SYMBOL(var) EXPORT_SYMBOL(var)\n#define EXPORT_PER_CPU_SYMBOL_GPL(var) EXPORT_SYMBOL_GPL(var)\n#else\n#define EXPORT_PER_CPU_SYMBOL(var)\n#define EXPORT_PER_CPU_SYMBOL_GPL(var)\n#endif\n\n/*\n * Accessors and operations.\n */\n#ifndef __ASSEMBLY__\n\n/*\n * __verify_pcpu_ptr() verifies @ptr is a percpu pointer without evaluating\n * @ptr and is invoked once before a percpu area is accessed by all\n * accessors and operations.  This is performed in the generic part of\n * percpu and arch overrides don't need to worry about it; however, if an\n * arch wants to implement an arch-specific percpu accessor or operation,\n * it may use __verify_pcpu_ptr() to verify the parameters.\n *\n * + 0 is required in order to convert the pointer type from a\n * potential array type to a pointer to a single item of the array.\n */\n#define __verify_pcpu_ptr(ptr)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tconst void __percpu *__vpp_verify = (typeof((ptr) + 0))NULL;\t\\\n\t(void)__vpp_verify;\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_SMP\n\n/*\n * Add an offset to a pointer but keep the pointer as-is.  Use RELOC_HIDE()\n * to prevent the compiler from making incorrect assumptions about the\n * pointer value.  The weird cast keeps both GCC and sparse happy.\n */\n#define SHIFT_PERCPU_PTR(__p, __offset)\t\t\t\t\t\\\n\tRELOC_HIDE((typeof(*(__p)) __kernel __force *)(__p), (__offset))\n\n#define per_cpu_ptr(ptr, cpu)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tSHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)));\t\t\t\\\n})\n\n#define raw_cpu_ptr(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tarch_raw_cpu_ptr(ptr);\t\t\t\t\t\t\\\n})\n\n#ifdef CONFIG_DEBUG_PREEMPT\n#define this_cpu_ptr(ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(ptr);\t\t\t\t\t\t\\\n\tSHIFT_PERCPU_PTR(ptr, my_cpu_offset);\t\t\t\t\\\n})\n#else\n#define this_cpu_ptr(ptr) raw_cpu_ptr(ptr)\n#endif\n\n#else\t/* CONFIG_SMP */\n\n#define VERIFY_PERCPU_PTR(__p)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(__p);\t\t\t\t\t\t\\\n\t(typeof(*(__p)) __kernel __force *)(__p);\t\t\t\\\n})\n\n#define per_cpu_ptr(ptr, cpu)\t({ (void)(cpu); VERIFY_PERCPU_PTR(ptr); })\n#define raw_cpu_ptr(ptr)\tper_cpu_ptr(ptr, 0)\n#define this_cpu_ptr(ptr)\traw_cpu_ptr(ptr)\n\n#endif\t/* CONFIG_SMP */\n\n#define per_cpu(var, cpu)\t(*per_cpu_ptr(&(var), cpu))\n\n/*\n * Must be an lvalue. Since @var must be a simple identifier,\n * we force a syntax error here if it isn't.\n */\n#define get_cpu_var(var)\t\t\t\t\t\t\\\n(*({\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tthis_cpu_ptr(&var);\t\t\t\t\t\t\\\n}))\n\n/*\n * The weird & is necessary because sparse considers (void)(var) to be\n * a direct dereference of percpu variable (var).\n */\n#define put_cpu_var(var)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t(void)&(var);\t\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\n#define get_cpu_ptr(var)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\tthis_cpu_ptr(var);\t\t\t\t\t\t\\\n})\n\n#define put_cpu_ptr(var)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t(void)(var);\t\t\t\t\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * Branching function to split up a function into a set of functions that\n * are called for different scalar sizes of the objects handled.\n */\n\nextern void __bad_size_call_parameter(void);\n\n#ifdef CONFIG_DEBUG_PREEMPT\nextern void __this_cpu_preempt_check(const char *op);\n#else\nstatic inline void __this_cpu_preempt_check(const char *op) { }\n#endif\n\n#define __pcpu_size_call_return(stem, variable)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(variable) pscr_ret__;\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\tcase 1: pscr_ret__ = stem##1(variable); break;\t\t\t\\\n\tcase 2: pscr_ret__ = stem##2(variable); break;\t\t\t\\\n\tcase 4: pscr_ret__ = stem##4(variable); break;\t\t\t\\\n\tcase 8: pscr_ret__ = stem##8(variable); break;\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpscr_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define __pcpu_size_call_return2(stem, variable, ...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(variable) pscr2_ret__;\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\tcase 1: pscr2_ret__ = stem##1(variable, __VA_ARGS__); break;\t\\\n\tcase 2: pscr2_ret__ = stem##2(variable, __VA_ARGS__); break;\t\\\n\tcase 4: pscr2_ret__ = stem##4(variable, __VA_ARGS__); break;\t\\\n\tcase 8: pscr2_ret__ = stem##8(variable, __VA_ARGS__); break;\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpscr2_ret__;\t\t\t\t\t\t\t\\\n})\n\n/*\n * Special handling for cmpxchg_double.  cmpxchg_double is passed two\n * percpu variables.  The first has to be aligned to a double word\n * boundary and the second has to follow directly thereafter.\n * We enforce this on all architectures even if they don't support\n * a double cmpxchg instruction, since it's a cheap requirement, and it\n * avoids breaking the requirement for architectures with the instruction.\n */\n#define __pcpu_double_call_return_bool(stem, pcp1, pcp2, ...)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tbool pdcrb_ret__;\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(pcp1));\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(pcp1) != sizeof(pcp2));\t\t\t\\\n\tVM_BUG_ON((unsigned long)(&(pcp1)) % (2 * sizeof(pcp1)));\t\\\n\tVM_BUG_ON((unsigned long)(&(pcp2)) !=\t\t\t\t\\\n\t\t  (unsigned long)(&(pcp1)) + sizeof(pcp1));\t\t\\\n\tswitch(sizeof(pcp1)) {\t\t\t\t\t\t\\\n\tcase 1: pdcrb_ret__ = stem##1(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 2: pdcrb_ret__ = stem##2(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 4: pdcrb_ret__ = stem##4(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tcase 8: pdcrb_ret__ = stem##8(pcp1, pcp2, __VA_ARGS__); break;\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__bad_size_call_parameter(); break;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpdcrb_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define __pcpu_size_call(stem, variable, ...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__verify_pcpu_ptr(&(variable));\t\t\t\t\t\\\n\tswitch(sizeof(variable)) {\t\t\t\t\t\\\n\t\tcase 1: stem##1(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 2: stem##2(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 4: stem##4(variable, __VA_ARGS__);break;\t\t\\\n\t\tcase 8: stem##8(variable, __VA_ARGS__);break;\t\t\\\n\t\tdefault: \t\t\t\t\t\t\\\n\t\t\t__bad_size_call_parameter();break;\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * this_cpu operations (C) 2008-2013 Christoph Lameter <cl@linux.com>\n *\n * Optimized manipulation for memory allocated through the per cpu\n * allocator or for addresses of per cpu variables.\n *\n * These operation guarantee exclusivity of access for other operations\n * on the *same* processor. The assumption is that per cpu data is only\n * accessed by a single processor instance (the current one).\n *\n * The arch code can provide optimized implementation by defining macros\n * for certain scalar sizes. F.e. provide this_cpu_add_2() to provide per\n * cpu atomic operations for 2 byte sized RMW actions. If arch code does\n * not provide operations for a scalar size then the fallback in the\n * generic code will be used.\n *\n * cmpxchg_double replaces two adjacent scalars at once.  The first two\n * parameters are per cpu variables which have to be of the same size.  A\n * truth value is returned to indicate success or failure (since a double\n * register result is difficult to handle).  There is very limited hardware\n * support for these operations, so only certain sizes may work.\n */\n\n/*\n * Operations for contexts where we do not want to do any checks for\n * preemptions.  Unless strictly necessary, always use [__]this_cpu_*()\n * instead.\n *\n * If there is no other protection through preempt disable and/or disabling\n * interupts then one of these RMW operations can show unexpected behavior\n * because the execution thread was rescheduled on another processor or an\n * interrupt occurred and the same percpu variable was modified from the\n * interrupt context.\n */\n#define raw_cpu_read(pcp)\t\t__pcpu_size_call_return(raw_cpu_read_, pcp)\n#define raw_cpu_write(pcp, val)\t\t__pcpu_size_call(raw_cpu_write_, pcp, val)\n#define raw_cpu_add(pcp, val)\t\t__pcpu_size_call(raw_cpu_add_, pcp, val)\n#define raw_cpu_and(pcp, val)\t\t__pcpu_size_call(raw_cpu_and_, pcp, val)\n#define raw_cpu_or(pcp, val)\t\t__pcpu_size_call(raw_cpu_or_, pcp, val)\n#define raw_cpu_add_return(pcp, val)\t__pcpu_size_call_return2(raw_cpu_add_return_, pcp, val)\n#define raw_cpu_xchg(pcp, nval)\t\t__pcpu_size_call_return2(raw_cpu_xchg_, pcp, nval)\n#define raw_cpu_cmpxchg(pcp, oval, nval) \\\n\t__pcpu_size_call_return2(raw_cpu_cmpxchg_, pcp, oval, nval)\n#define raw_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n\t__pcpu_double_call_return_bool(raw_cpu_cmpxchg_double_, pcp1, pcp2, oval1, oval2, nval1, nval2)\n\n#define raw_cpu_sub(pcp, val)\t\traw_cpu_add(pcp, -(val))\n#define raw_cpu_inc(pcp)\t\traw_cpu_add(pcp, 1)\n#define raw_cpu_dec(pcp)\t\traw_cpu_sub(pcp, 1)\n#define raw_cpu_sub_return(pcp, val)\traw_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define raw_cpu_inc_return(pcp)\t\traw_cpu_add_return(pcp, 1)\n#define raw_cpu_dec_return(pcp)\t\traw_cpu_add_return(pcp, -1)\n\n/*\n * Operations for contexts that are safe from preemption/interrupts.  These\n * operations verify that preemption is disabled.\n */\n#define __this_cpu_read(pcp)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"read\");\t\t\t\t\\\n\traw_cpu_read(pcp);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_write(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"write\");\t\t\t\t\\\n\traw_cpu_write(pcp, val);\t\t\t\t\t\\\n})\n\n#define __this_cpu_add(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"add\");\t\t\t\t\\\n\traw_cpu_add(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_and(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"and\");\t\t\t\t\\\n\traw_cpu_and(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_or(pcp, val)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"or\");\t\t\t\t\t\\\n\traw_cpu_or(pcp, val);\t\t\t\t\t\t\\\n})\n\n#define __this_cpu_add_return(pcp, val)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"add_return\");\t\t\t\t\\\n\traw_cpu_add_return(pcp, val);\t\t\t\t\t\\\n})\n\n#define __this_cpu_xchg(pcp, nval)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"xchg\");\t\t\t\t\\\n\traw_cpu_xchg(pcp, nval);\t\t\t\t\t\\\n})\n\n#define __this_cpu_cmpxchg(pcp, oval, nval)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__this_cpu_preempt_check(\"cmpxchg\");\t\t\t\t\\\n\traw_cpu_cmpxchg(pcp, oval, nval);\t\t\t\t\\\n})\n\n#define __this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n({\t__this_cpu_preempt_check(\"cmpxchg_double\");\t\t\t\\\n\traw_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2);\t\\\n})\n\n#define __this_cpu_sub(pcp, val)\t__this_cpu_add(pcp, -(typeof(pcp))(val))\n#define __this_cpu_inc(pcp)\t\t__this_cpu_add(pcp, 1)\n#define __this_cpu_dec(pcp)\t\t__this_cpu_sub(pcp, 1)\n#define __this_cpu_sub_return(pcp, val)\t__this_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define __this_cpu_inc_return(pcp)\t__this_cpu_add_return(pcp, 1)\n#define __this_cpu_dec_return(pcp)\t__this_cpu_add_return(pcp, -1)\n\n/*\n * Operations with implied preemption/interrupt protection.  These\n * operations can be used without worrying about preemption or interrupt.\n */\n#define this_cpu_read(pcp)\t\t__pcpu_size_call_return(this_cpu_read_, pcp)\n#define this_cpu_write(pcp, val)\t__pcpu_size_call(this_cpu_write_, pcp, val)\n#define this_cpu_add(pcp, val)\t\t__pcpu_size_call(this_cpu_add_, pcp, val)\n#define this_cpu_and(pcp, val)\t\t__pcpu_size_call(this_cpu_and_, pcp, val)\n#define this_cpu_or(pcp, val)\t\t__pcpu_size_call(this_cpu_or_, pcp, val)\n#define this_cpu_add_return(pcp, val)\t__pcpu_size_call_return2(this_cpu_add_return_, pcp, val)\n#define this_cpu_xchg(pcp, nval)\t__pcpu_size_call_return2(this_cpu_xchg_, pcp, nval)\n#define this_cpu_cmpxchg(pcp, oval, nval) \\\n\t__pcpu_size_call_return2(this_cpu_cmpxchg_, pcp, oval, nval)\n#define this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2) \\\n\t__pcpu_double_call_return_bool(this_cpu_cmpxchg_double_, pcp1, pcp2, oval1, oval2, nval1, nval2)\n\n#define this_cpu_sub(pcp, val)\t\tthis_cpu_add(pcp, -(typeof(pcp))(val))\n#define this_cpu_inc(pcp)\t\tthis_cpu_add(pcp, 1)\n#define this_cpu_dec(pcp)\t\tthis_cpu_sub(pcp, 1)\n#define this_cpu_sub_return(pcp, val)\tthis_cpu_add_return(pcp, -(typeof(pcp))(val))\n#define this_cpu_inc_return(pcp)\tthis_cpu_add_return(pcp, 1)\n#define this_cpu_dec_return(pcp)\tthis_cpu_add_return(pcp, -1)\n\n#endif /* __ASSEMBLY__ */\n#endif /* _LINUX_PERCPU_DEFS_H */\n"}}, "reports": [{"events": [{"location": {"col": 2, "file": 0, "line": 5937}, "message": "Taking false branch"}, {"location": {"col": 2, "file": 1, "line": 50}, "message": "expanded from macro 'BUILD_BUG_ON'"}, {"location": {"col": 37, "file": 1, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 2, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 2, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 3, "file": 2, "line": 300}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 5937}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 2, "file": 1, "line": 50}, "message": "expanded from macro 'BUILD_BUG_ON'"}, {"location": {"col": 37, "file": 1, "line": 39}, "message": "expanded from macro 'BUILD_BUG_ON_MSG'"}, {"location": {"col": 2, "file": 2, "line": 320}, "message": "expanded from macro 'compiletime_assert'"}, {"location": {"col": 2, "file": 2, "line": 308}, "message": "expanded from macro '_compiletime_assert'"}, {"location": {"col": 2, "file": 2, "line": 298}, "message": "expanded from macro '__compiletime_assert'"}, {"location": {"col": 2, "file": 0, "line": 5939}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 5939}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 2, "file": 0, "line": 5945}, "message": "Assuming 'cpu' is < 'nr_cpu_ids'"}, {"location": {"col": 36, "file": 4, "line": 817}, "message": "expanded from macro 'for_each_possible_cpu'"}, {"location": {"col": 3, "file": 4, "line": 272}, "message": "expanded from macro 'for_each_cpu'"}, {"location": {"col": 2, "file": 0, "line": 5945}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 36, "file": 4, "line": 817}, "message": "expanded from macro 'for_each_possible_cpu'"}, {"location": {"col": 2, "file": 4, "line": 270}, "message": "expanded from macro 'for_each_cpu'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 17, "file": 0, "line": 376}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 29, "file": 5, "line": 269}, "message": "expanded from macro 'per_cpu'"}, {"location": {"col": 2, "file": 5, "line": 235}, "message": "expanded from macro 'per_cpu_ptr'"}, {"location": {"col": 37, "file": 5, "line": 217}, "message": "expanded from macro '__verify_pcpu_ptr'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 17, "file": 0, "line": 377}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 29, "file": 5, "line": 269}, "message": "expanded from macro 'per_cpu'"}, {"location": {"col": 2, "file": 5, "line": 235}, "message": "expanded from macro 'per_cpu_ptr'"}, {"location": {"col": 37, "file": 5, "line": 217}, "message": "expanded from macro '__verify_pcpu_ptr'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 0, "line": 376}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 4, "file": 0, "line": 5950}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5950}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5958}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5958}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 17, "file": 0, "line": 377}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 29, "file": 5, "line": 269}, "message": "expanded from macro 'per_cpu'"}, {"location": {"col": 2, "file": 5, "line": 235}, "message": "expanded from macro 'per_cpu_ptr'"}, {"location": {"col": 37, "file": 5, "line": 217}, "message": "expanded from macro '__verify_pcpu_ptr'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 0, "line": 376}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 4, "file": 0, "line": 5950}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5950}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5958}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5958}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 17, "file": 0, "line": 377}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 29, "file": 5, "line": 269}, "message": "expanded from macro 'per_cpu'"}, {"location": {"col": 2, "file": 5, "line": 235}, "message": "expanded from macro 'per_cpu_ptr'"}, {"location": {"col": 37, "file": 5, "line": 217}, "message": "expanded from macro '__verify_pcpu_ptr'"}, {"location": {"col": 3, "file": 0, "line": 5949}, "message": "Loop condition is true.  Entering loop body"}, {"location": {"col": 2, "file": 0, "line": 376}, "message": "expanded from macro 'for_each_cpu_worker_pool'"}, {"location": {"col": 4, "file": 0, "line": 5950}, "message": "Taking false branch"}, {"location": {"col": 32, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 4, "file": 0, "line": 5950}, "message": "Loop condition is false.  Exiting loop"}, {"location": {"col": 27, "file": 3, "line": 63}, "message": "expanded from macro 'BUG_ON'"}, {"location": {"col": 22, "file": 0, "line": 5953}, "message": "Assigned value is garbage or undefined"}, {"location": {"col": 22, "file": 0, "line": 5953}, "message": "Assigned value is garbage or undefined"}], "macros": [], "notes": [], "path": "/src/kernel/workqueue.c", "reportHash": "b2457409aaf1c91d5ee18fe7548fd2c4", "checkerName": "clang-analyzer-core.uninitialized.Assign", "reviewStatus": null, "severity": "UNSPECIFIED"}]};
      window.onload = function() {
        if (!browserCompatible) {
          setNonCompatibleBrowserMessage();
        } else {
          BugViewer.init(data.files, data.reports);
          BugViewer.create();
          BugViewer.initByUrl();
        }
      };
    </script>
  </head>
  <body>
  <div class="container">
    <div id="content">
      <div id="side-bar">
        <div class="header">
          <a href="index.html" class="button">&#8249; Return to List</a>
        </div>
        <div id="report-nav">
          <div class="header">Reports</div>
        </div>
      </div>
      <div id="editor-wrapper">
        <div class="header">
          <div id="file">
            <span class="label">File:</span>
            <span id="file-path"></span>
          </div>
          <div id="checker">
            <span class="label">Checker name:</span>
            <span id="checker-name"></span>
          </div>
          <div id="review-status-wrapper">
            <span class="label">Review status:</span>
            <span id="review-status"></span>
          </div>
        </div>
        <div id="editor"></div>
      </div>
    </div>
  </div>
  </body>
</html>
